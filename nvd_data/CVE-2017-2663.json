{
  "cve_id": "CVE-2017-2663",
  "github_data": {
    "repository": "candlepin/subscription-manager",
    "fix_commit": "2aa48ef65",
    "related_commits": [
      "2aa48ef65",
      "2aa48ef65"
    ],
    "patch_url": "https://github.com/candlepin/subscription-manager/commit/2aa48ef65.patch",
    "fix_commit_details": {
      "sha": "2aa48ef65",
      "commit_date": "2017-01-04T17:56:15Z",
      "author": {
        "login": "awood",
        "type": "User",
        "stats": {
          "total_commits": 900,
          "average_weekly_commits": 1.1583011583011582,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 242
        }
      },
      "commit_message": {
        "title": "Provide DBus objects for configuration, facts, and registration.",
        "length": 658,
        "has_description": true,
        "references_issue": false
      },
      "stats": {
        "total": 6664,
        "additions": 5884,
        "deletions": 780
      },
      "files": [
        {
          "filename": "Makefile",
          "status": "modified",
          "additions": 55,
          "deletions": 16,
          "patch": "@@ -13,18 +13,20 @@\n #   just the fastest or easiest way.\n \n SHELL := /bin/bash\n-PREFIX ?=\n+PREFIX ?= /\n SYSCONF ?= etc\n INSTALL_DIR = usr/share\n \n-PYTHON_SITELIB ?= usr/lib/python2.7/site-packages\n-# Note the underscore used instead of a hyphen\n-PYTHON_INST_DIR = $(PREFIX)/$(PYTHON_SITELIB)/subscription_manager\n-\n OS = $(shell lsb_release -i | awk '{ print $$3 }' | awk -F. '{ print $$1}')\n OS_VERSION = $(shell lsb_release -r | awk '{ print $$2 }' | awk -F. '{ print $$1}')\n OS_DIST ?= $(shell rpm --eval='%dist')\n \n+PYTHON_VER ?= $(python -c 'import sys; print(\"python%s.%s\" % sys.version_info[:2])')\n+\n+PYTHON_SITELIB ?= usr/lib/$(PYTHON_VER)/site-packages\n+# Note the underscore used instead of a hyphen\n+PYTHON_INST_DIR = $(PREFIX)/$(PYTHON_SITELIB)/subscription_manager\n+\n # Where various bits of code live in the git repo\n SRC_DIR := src/subscription_manager\n RCT_SRC_DIR := src/rct\n@@ -41,6 +43,7 @@ RHSM_PLUGIN_DIR := $(PREFIX)/usr/share/rhsm-plugins/\n RHSM_PLUGIN_CONF_DIR := $(PREFIX)/etc/rhsm/pluginconf.d/\n ANACONDA_ADDON_INST_DIR := $(PREFIX)/usr/share/anaconda/addons\n INITIAL_SETUP_INST_DIR := $(ANACONDA_ADDON_INST_DIR)/$(ANACONDA_ADDON_NAME)\n+POLKIT_ACTIONS_INST_DIR := $(PREFIX)/$(INSTALL_DIR)/polkit-1/actions\n LIBEXEC_DIR ?= $(shell rpm --eval='%_libexecdir')\n \n # If we skip install ostree plugin, unset by default\n@@ -53,6 +56,7 @@ ifeq ($(OS_DIST),.el6)\n    FIRSTBOOT_MODULES_DIR?=$(PREFIX)/usr/share/rhn/up2date_client/firstboot\n    INSTALL_FIRSTBOOT?=true\n    INSTALL_INITIAL_SETUP?=false\n+   DBUS_SERVICE_FILE_TYPE?=dbus\n else ifeq ($(OS),SUSE)\n    GTK_VERSION?=2\n    FIRSTBOOT_MODULES_DIR?=$(PREFIX)/usr/share/rhn/up2date_client/firstboot\n@@ -63,6 +67,21 @@ else\n    FIRSTBOOT_MODULES_DIR?=$(PREFIX)/usr/share/firstboot/modules\n    INSTALL_FIRSTBOOT?=true\n    INSTALL_INITIAL_SETUP?=true\n+   DBUS_SERVICE_FILE_TYPE?=systemd\n+endif\n+\n+DBUS_SERVICES_CONF_INST_DIR := $(PREFIX)/usr/share/dbus-1/system-services\n+FACTS_INST_DBUS_SERVICE_FILE = $(DBUS_SERVICES_CONF_INST_DIR)/com.redhat.RHSM1.Facts.service\n+MAIN_INST_DBUS_SERVICE_FILE = $(DBUS_SERVICES_CONF_INST_DIR)/com.redhat.RHSM1.service\n+# TODO Ideally these service files would be installed by distutils, but the file we actually\n+# install depends on the distro we are using.  Add a --without-systemd or similar flag to the\n+# custom install_data class we have in setup.py\n+ifeq ($(DBUS_SERVICE_FILE_TYPE),dbus)\n+FACTS_SRC_DBUS_SERVICE_FILE = etc-conf/dbus/com.redhat.RHSM1.Facts.service-dbus\n+MAIN_SRC_DBUS_SERVICE_FILE = etc-conf/dbus/com.redhat.RHSM1.service-dbus\n+else\n+FACTS_SRC_DBUS_SERVICE_FILE = etc-conf/dbus/com.redhat.RHSM1.Facts.service\n+MAIN_SRC_DBUS_SERVICE_FILE = etc-conf/dbus/com.redhat.RHSM1.service\n endif\n \n # always true until fedora is just dnf\n@@ -119,17 +138,36 @@ rhsm-icon: $(RHSM_ICON_SRC_DIR)/rhsm_icon.c\n check-syntax:\n \t$(CC) -fsyntax-only $(CFLAGS) $(LDFLAGS) $(ICON_FLAGS) `find -name '*.c'`\n \n-.PHONY: dbus-service-install\n-dbus-service-install:\n+dbus-common-install:\n \tinstall -d $(PREFIX)/etc/dbus-1/system.d\n+\tif [ \"$(DBUS_SERVICE_FILE_TYPE)\" == \"systemd\" ]; then \\\n+\t\tinstall -d $(SYSTEMD_INST_DIR) ; \\\n+\tfi\n \tinstall -d $(PREFIX)/$(INSTALL_DIR)/dbus-1/system-services\n \tinstall -d $(PREFIX)/$(LIBEXEC_DIR)\n-\tinstall -m 644 etc-conf/com.redhat.SubscriptionManager.conf \\\n-\t\t$(PREFIX)/etc/dbus-1/system.d\n-\tinstall -m 644 etc-conf/com.redhat.SubscriptionManager.service \\\n-\t\t$(PREFIX)/$(INSTALL_DIR)/dbus-1/system-services\n-\tinstall -m 744 $(DAEMONS_SRC_DIR)/rhsm_d.py \\\n-\t\t$(PREFIX)/$(LIBEXEC_DIR)/rhsmd\n+\tinstall -d $(PREFIX)/etc/bash_completion.d\n+\n+dbus-rhsmd-service-install: dbus-common-install\n+\tinstall -m 644 etc-conf/dbus/com.redhat.SubscriptionManager.conf $(PREFIX)/etc/dbus-1/system.d\n+\tinstall -m 644 etc-conf/dbus/com.redhat.SubscriptionManager.service $(PREFIX)/$(INSTALL_DIR)/dbus-1/system-services\n+\tinstall -m 744 $(DAEMONS_SRC_DIR)/rhsm_d.py $(PREFIX)/$(LIBEXEC_DIR)/rhsmd\n+\n+dbus-facts-service-install: dbus-common-install\n+\tinstall -m 644 etc-conf/dbus/com.redhat.RHSM1.Facts.conf $(PREFIX)/etc/dbus-1/system.d\n+\tif [ \"$(DBUS_SERVICE_FILE_TYPE)\" == \"systemd\" ]; then \\\n+\t\tinstall -m 644 etc-conf/dbus/rhsm-facts.service $(SYSTEMD_INST_DIR) ; \\\n+\tfi\n+\tinstall -m 644 $(FACTS_SRC_DBUS_SERVICE_FILE) $(FACTS_INST_DBUS_SERVICE_FILE)\n+\n+dbus-main-service-install: dbus-common-install\n+\tinstall -m 644 etc-conf/dbus/com.redhat.RHSM1.conf $(PREFIX)/etc/dbus-1/system.d\n+\tif [ \"$(DBUS_SERVICE_FILE_TYPE)\" == \"systemd\" ]; then \\\n+\t\tinstall -m 644 etc-conf/dbus/rhsm.service $(SYSTEMD_INST_DIR) ; \\\n+\tfi\n+\tinstall -m 644 $(MAIN_SRC_DBUS_SERVICE_FILE) $(MAIN_INST_DBUS_SERVICE_FILE)\n+\n+.PHONY: dbus-install\n+dbus-install: dbus-facts-service-install dbus-rhsmd-service-install dbus-main-service-install\n \n .PHONY: install-conf\n install-conf:\n@@ -150,6 +188,9 @@ install-conf:\n \tinstall -m 644 etc-conf/rhsmcertd.completion.sh $(PREFIX)/etc/bash_completion.d/rhsmcertd\n \tinstall -d $(PREFIX)/usr/share/appdata\n \tinstall -m 644 etc-conf/subscription-manager-gui.appdata.xml $(PREFIX)/$(INSTALL_DIR)/appdata/subscription-manager-gui.appdata.xml\n+\tinstall -d $(POLKIT_ACTIONS_INST_DIR)\n+\tinstall -m 644 etc-conf/dbus/com.redhat.RHSM1.policy $(POLKIT_ACTIONS_INST_DIR)\n+\tinstall -m 644 etc-conf/dbus/com.redhat.RHSM1.Facts.policy $(POLKIT_ACTIONS_INST_DIR)\n \n .PHONY: install-plugins\n install-plugins:\n@@ -251,7 +292,7 @@ install-via-setup:\n install: install-via-setup install-files\n \n .PHONY: install-files\n-install-files: dbus-service-install install-conf install-plugins install-post-boot install-ga\n+install-files: dbus-install install-conf install-plugins install-post-boot install-ga\n \tinstall -d $(PREFIX)/var/log/rhsm\n \tinstall -d $(PREFIX)/var/spool/rhsm/debug\n \tinstall -d $(PREFIX)/var/run/rhsm\n@@ -260,7 +301,6 @@ install-files: dbus-service-install install-conf install-plugins install-post-bo\n \t# Set up rhsmcertd daemon. If installing on Fedora or RHEL 7+\n \t# we prefer systemd over sysv as this is the new trend.\n \tif [ $(OS) = Fedora ] ; then \\\n-\t\tinstall -d $(SYSTEMD_INST_DIR); \\\n \t\tinstall -d $(PREFIX)/usr/lib/tmpfiles.d; \\\n \t\tinstall etc-conf/rhsmcertd.service $(SYSTEMD_INST_DIR); \\\n \t\tinstall etc-conf/subscription-manager.conf.tmpfiles \\\n@@ -281,7 +321,6 @@ install-files: dbus-service-install install-conf install-plugins install-post-bo\n \t\t\tinstall etc-conf/rhsmcertd.init.d \\\n \t\t\t\t$(PREFIX)/etc/rc.d/init.d/rhsmcertd; \\\n \t\telse \\\n-\t\t\tinstall -d $(SYSTEMD_INST_DIR); \\\n \t\t\tinstall -d $(PREFIX)/usr/lib/tmpfiles.d; \\\n \t\t\tinstall etc-conf/rhsmcertd.service $(SYSTEMD_INST_DIR); \\\n \t\t\tinstall etc-conf/subscription-manager.conf.tmpfiles \\"
        },
        {
          "filename": "bin/rhsm-facts-service",
          "status": "added",
          "additions": 43,
          "deletions": 0,
          "patch": "@@ -0,0 +1,43 @@\n+#! /usr/bin/env python\n+#\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+\n+# Init logging very early so we can log any issues that occur at import time\n+logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)5s [%(name)s:%(lineno)s] %(message)s\")\n+log = logging.getLogger('')\n+log.setLevel(logging.INFO)\n+\n+import sys\n+from rhsmlib.dbus import service_wrapper\n+from rhsmlib.dbus.facts import base, constants\n+\n+if __name__ == \"__main__\":\n+    try:\n+        object_classes = [\n+            base.AllFacts,\n+        ]\n+        sys.exit(service_wrapper.main(\n+            sys.argv,\n+            object_classes=object_classes,\n+            default_bus_name=constants.FACTS_DBUS_NAME)\n+        )\n+    except Exception:\n+        log.exception(\"DBus service startup failed\")\n+else:\n+    # Importing this module would screw up the importer's logging configuration since\n+    # we're setting up logging very early in module scope to catch any log messages that\n+    # occur during the loading of the dependent modules.\n+    raise ImportError(\"This module is not meant to be imported\")"
        },
        {
          "filename": "bin/rhsm-service",
          "status": "added",
          "additions": 41,
          "deletions": 0,
          "patch": "@@ -0,0 +1,41 @@\n+#!/usr/bin/env python\n+#\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+\n+# Init logging very early so we can log any issues that occur at import time\n+logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)5s [%(name)s:%(lineno)s] %(message)s\")\n+log = logging.getLogger('')\n+log.setLevel(logging.INFO)\n+\n+import sys\n+from rhsmlib.dbus import service_wrapper\n+from rhsmlib.dbus import objects\n+\n+if __name__ == \"__main__\":\n+    try:\n+        object_classes = [\n+            objects.ConfigDBusObject,\n+            objects.RegisterDBusObject,\n+            objects.Main\n+        ]\n+        sys.exit(service_wrapper.main(sys.argv, object_classes=object_classes))\n+    except Exception:\n+        log.exception(\"DBus service startup failed\")\n+else:\n+    # Importing this module would screw up the importer's logging configuration since\n+    # we're setting up logging very early in module scope to catch any log messages that\n+    # occur during the loading of the dependent modules.\n+    raise ImportError(\"This module is not meant to be imported\")"
        },
        {
          "filename": "bin/subscription-manager",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -57,6 +57,10 @@ try:\n     from subscription_manager.injectioninit import init_dep_injection\n     init_dep_injection()\n \n+    import subscription_manager.injection as inj\n+    # Set up DBus mainloop via DBUS_IFACE\n+    inj.require(inj.DBUS_IFACE)\n+\n     from subscription_manager import managercli\n     from subscription_manager.managercli import handle_exception\n "
        },
        {
          "filename": "bin/subscription-manager-gui",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -133,6 +133,10 @@ try:\n     from subscription_manager.injectioninit import init_dep_injection\n     init_dep_injection()\n \n+    import subscription_manager.injection as inj\n+    # Set up DBus mainloop via DBUS_IFACE\n+    inj.require(inj.DBUS_IFACE)\n+\n     from subscription_manager.gui import managergui\n     from subscription_manager.i18n_optparse import OptionParser, \\\n         WrappedIndentedHelpFormatter, USAGE"
        },
        {
          "filename": "build_ext/lint.py",
          "status": "modified",
          "additions": 108,
          "deletions": 0,
          "patch": "@@ -12,6 +12,7 @@\n # in this software or its documentation.\n import ast\n import re\n+import tokenize\n \n from distutils.spawn import spawn\n from distutils.text_file import TextFile\n@@ -21,6 +22,9 @@\n # These dependencies aren't available in build environments.  We won't need any\n # linting functionality there though, so just create a dummy class so we can proceed.\n try:\n+    # These dependencies aren't available in build environments.  We won't need any\n+    # linting functionality there though, so just create a dummy class so we can proceed.\n+    import pep8\n     import pkg_resources\n except ImportError:\n     pass\n@@ -364,6 +368,110 @@ def err(self, node, msg=None):\n         return ret\n \n \n+def detect_overindent(logical_line, tokens, indent_level, hang_closing, indent_char, noqa, verbose):\n+    \"\"\"Flag lines that are overindented.  This includes lines that are indented solely to align\n+    vertically with an opening brace.  This rule allows continuation lines to be relatively\n+    indented up to 8 spaces and closes braces to be relatively indented up to 4 spaces.  Heavily\n+    adapted from pep8's continued_indentation method\n+\n+    Okay: foo = my_func('hello',\n+              'world'\n+              )\n+    Okay: foo = my_func('hello',\n+                  'world')\n+\n+    Okay: foo = my_func('hello',\n+              )\n+\n+    E198: foo = my_func('hello',\n+                       )\n+    E199: foo = my_func('hello',\n+                        'world')\n+    \"\"\"\n+    first_row = tokens[0][2][0]\n+    nrows = 1 + tokens[-1][2][0] - first_row\n+    if noqa or nrows == 1:\n+        return\n+\n+    row = depth = 0\n+\n+    # relative indents of physical lines\n+    rel_indent = [0] * nrows\n+    open_rows = [[0]]\n+    last_indent = tokens[0][2]\n+    indent = [last_indent[1]]\n+\n+    last_token_multiline = False\n+\n+    if verbose >= 3:\n+        print(\">>> \" + tokens[0][4].rstrip())\n+\n+    for token_type, text, start, end, line in tokens:\n+        newline = row < start[0] - first_row\n+        if newline:\n+            row = start[0] - first_row\n+            newline = not last_token_multiline and token_type not in pep8.NEWLINE\n+\n+        if newline:\n+            # this is the beginning of a continuation line.\n+            last_indent = start\n+            if verbose >= 3:\n+                print(\"... \" + line.rstrip())\n+\n+            # record the initial indent.\n+            rel_indent[row] = pep8.expand_indent(line) - indent_level\n+\n+            # identify closing bracket\n+            close_bracket = (token_type == tokenize.OP and text in ']})')\n+\n+            # is the indent relative to an opening bracket line?\n+            for open_row in reversed(open_rows[depth]):\n+                hang = rel_indent[row] - rel_indent[open_row]\n+\n+            if not close_bracket and hang > 8:\n+                yield start, \"E199 continuation line over-indented\"\n+\n+            if close_bracket and hang > 4:\n+                yield (start, \"E198 closing bracket over-indented\")\n+\n+        # Keep track of bracket depth to check for proper indentation in nested\n+        # brackets\n+        # E.g.\n+        # Okay: foo = [[\n+        #           '1'\n+        #       ]]\n+        #\n+        # but even though we are nested twice, we should only allow one level of indentation, so:\n+        #\n+        # E199: foo = [[\n+        #               '1'\n+        #       ]]\n+\n+        if token_type == tokenize.OP:\n+            if text in '([{':\n+                depth += 1\n+                indent.append(0)\n+                if len(open_rows) == depth:\n+                    open_rows.append([])\n+                open_rows[depth].append(row)\n+                if verbose >= 4:\n+                    print(\"bracket depth %s seen, col %s, visual min = %s\" %\n+                          (depth, start[1], indent[depth]))\n+            elif text in ')]}' and depth > 0:\n+                # parent indents should not be more than this one\n+                prev_indent = indent.pop() or last_indent[1]\n+                for d in range(depth):\n+                    if indent[d] > prev_indent:\n+                        indent[d] = 0\n+                del open_rows[depth + 1:]\n+                depth -= 1\n+            assert len(indent) == depth + 1\n+\n+        last_token_multiline = (start[0] != end[0])\n+        if last_token_multiline:\n+            rel_indent[end[0] - first_row] = rel_indent[row]\n+\n+\n class PluginLoadingFlake8(Flake8):\n     \"\"\"A Flake8 runner that will load our custom plugins.  It's important to note\n     that this has to be invoked via `./setup.py flake8`.  Just running `flake8` won't"
        },
        {
          "filename": "dev-requirements.txt",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -1,3 +1,3 @@\n+-r test-requirements.txt\n yanc\n-nose-randomly\n xtraceback"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.Facts.conf",
          "status": "added",
          "additions": 37,
          "deletions": 0,
          "patch": "@@ -0,0 +1,37 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- -*- XML -*- -->\n+\n+<!DOCTYPE busconfig PUBLIC\n+ \"-//freedesktop//DTD D-BUS Bus Configuration 1.0//EN\"\n+ \"http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd\">\n+<busconfig>\n+\n+    <policy user=\"root\">\n+        <allow own=\"com.redhat.RHSM1.Facts\"/>\n+\n+        <!-- Basic D-Bus API stuff -->\n+        <allow send_destination=\"com.redhat.RHSM1.Facts\"\n+            send_interface=\"org.freedesktop.DBus.Introspectable\"/>\n+        <allow send_destination=\"com.redhat.RHSM1.Facts\"\n+            send_interface=\"org.freedesktop.DBus.Properties\"/>\n+        <allow send_destination=\"com.redhat.RHSM1.Facts\"\n+            send_interface=\"org.freedesktop.DBus.ObjectManager\"/>\n+    </policy>\n+\n+\n+    <policy context=\"default\">\n+        <!-- TODO: make these read-only by default -->\n+\n+        <allow send_destination=\"com.redhat.RHSM1.Facts\"\n+          send_interface=\"com.redhat.RHSM1.Facts\"/>\n+\n+      <!-- Basic D-Bus API stuff -->\n+      <allow send_destination=\"com.redhat.RHSM1.Facts\"\n+          send_interface=\"org.freedesktop.DBus.Introspectable\"/>\n+      <allow send_destination=\"com.redhat.RHSM1.Facts\"\n+          send_interface=\"org.freedesktop.DBus.Properties\"/>\n+      <allow send_destination=\"com.redhat.RHSM1.Facts\"\n+          send_interface=\"org.freedesktop.DBus.ObjectManager\"/>\n+  </policy>\n+\n+</busconfig>\n+"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.Facts.policy",
          "status": "added",
          "additions": 29,
          "deletions": 0,
          "patch": "@@ -0,0 +1,29 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!DOCTYPE policyconfig PUBLIC\n+ \"-//freedesktop//DTD PolicyKit Policy Configuration 1.0//EN\"\n+ \"http://www.freedesktop.org/standards/PolicyKit/1/policyconfig.dtd\">\n+<policyconfig>\n+  <vendor>Red Hat Subscription Management Facts</vendor>\n+  <vendor_url>http://redhat.com</vendor_url>\n+\n+  <action id=\"com.redhat.RHSM1.Facts.default\">\n+    <description>RHSM default</description>\n+    <message>System policy prevents access to com.redhat.RHSM1.Facts</message>\n+    <defaults>\n+        <allow_any>yes</allow_any>\n+        <allow_inactive>yes</allow_inactive>\n+        <allow_active>yes</allow_active>\n+\n+    </defaults>\n+  </action>\n+\n+   <action id=\"com.redhat.RHSM1.Facts.collect\">\n+    <description>RHSM Facts collection</description>\n+    <message>System policy prevents collect action to admin Facts service</message>\n+    <defaults>\n+        <allow_any>yes</allow_any>\n+        <allow_inactive>yes</allow_inactive>\n+        <allow_active>yes</allow_active>\n+    </defaults>\n+  </action>\n+</policyconfig>"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.Facts.service",
          "status": "added",
          "additions": 5,
          "deletions": 0,
          "patch": "@@ -0,0 +1,5 @@\n+[D-BUS Service]\n+Name=com.redhat.RHSM1.Facts\n+Exec=/usr/libexec/rhsm-facts-service\n+User=root\n+SystemdService=rhsm-facts.service"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.Facts.service-dbus",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+[D-BUS Service]\n+Name=com.redhat.RHSM1.Facts\n+Exec=/usr/libexec/rhsm-facts-service\n+User=root"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.conf",
          "status": "added",
          "additions": 35,
          "deletions": 0,
          "patch": "@@ -0,0 +1,35 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- -*- XML -*- -->\n+\n+<!DOCTYPE busconfig PUBLIC\n+ \"-//freedesktop//DTD D-BUS Bus Configuration 1.0//EN\"\n+ \"http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd\">\n+<busconfig>\n+    <policy user=\"root\">\n+        <allow own=\"com.redhat.RHSM1\"/>\n+\n+        <!-- Basic D-Bus API stuff -->\n+        <allow send_destination=\"com.redhat.RHSM1\"\n+            send_interface=\"org.freedesktop.DBus.Introspectable\"/>\n+        <allow send_destination=\"com.redhat.RHSM1\"\n+            send_interface=\"org.freedesktop.DBus.Properties\"/>\n+        <allow send_destination=\"com.redhat.RHSM1\"\n+            send_interface=\"org.freedesktop.DBus.ObjectManager\"/>\n+    </policy>\n+\n+\n+    <policy context=\"default\">\n+        <!-- TODO: make these read-only by default -->\n+\n+        <allow send_destination=\"com.redhat.RHSM1\"\n+            send_interface=\"com.redhat.RHSM1\"/>\n+\n+        <!-- Basic D-Bus API stuff -->\n+        <allow send_destination=\"com.redhat.RHSM1\"\n+            send_interface=\"org.freedesktop.DBus.Introspectable\"/>\n+        <allow send_destination=\"com.redhat.RHSM1\"\n+            send_interface=\"org.freedesktop.DBus.Properties\"/>\n+        <allow send_destination=\"com.redhat.RHSM1\"\n+            send_interface=\"org.freedesktop.DBus.ObjectManager\"/>\n+    </policy>\n+</busconfig>\n+"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.policy",
          "status": "added",
          "additions": 18,
          "deletions": 0,
          "patch": "@@ -0,0 +1,18 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!DOCTYPE policyconfig PUBLIC\n+ \"-//freedesktop//DTD PolicyKit Policy Configuration 1.0//EN\"\n+ \"http://www.freedesktop.org/standards/PolicyKit/1/policyconfig.dtd\">\n+<policyconfig>\n+  <vendor>Red Hat Subscription Management</vendor>\n+  <vendor_url>http://redhat.com</vendor_url>\n+\n+  <action id=\"com.redhat.RHSM1.default\">\n+    <description>RHSM default</description>\n+    <message>System policy prevents access to com.redhat.RHSM1</message>\n+    <defaults>\n+        <allow_any>yes</allow_any>\n+        <allow_inactive>yes</allow_inactive>\n+        <allow_active>yes</allow_active>\n+    </defaults>\n+  </action>\n+</policyconfig>"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.service",
          "status": "added",
          "additions": 5,
          "deletions": 0,
          "patch": "@@ -0,0 +1,5 @@\n+[D-BUS Service]\n+Name=com.redhat.RHSM1\n+Exec=/usr/libexec/rhsm-service\n+User=root\n+SystemdService=rhsm.service"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.RHSM1.service-dbus",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+[D-BUS Service]\n+Name=com.redhat.RHSM1\n+Exec=/usr/libexec/rhsm-service\n+User=root"
        },
        {
          "filename": "etc-conf/dbus/com.redhat.SubscriptionManager.conf",
          "status": "renamed",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "etc-conf/dbus/com.redhat.SubscriptionManager.service",
          "status": "renamed",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "etc-conf/dbus/rhsm-facts.service",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": "@@ -0,0 +1,11 @@\n+[Unit]\n+Description=RHSM system Facts dbus service\n+After=syslog.target network.target\n+\n+[Service]\n+Type=dbus\n+BusName=com.redhat.Subscriptions1.Facts\n+ExecStart=/usr/libexec/rhsm-facts-service\n+\n+[Install]\n+WantedBy=basic.target"
        },
        {
          "filename": "etc-conf/dbus/rhsm.service",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": "@@ -0,0 +1,11 @@\n+[Unit]\n+Description=RHSM dbus service\n+After=syslog.target network.target\n+\n+[Service]\n+Type=dbus\n+BusName=com.redhat.RHSM1\n+ExecStart=/usr/libexec/rhsm-service\n+\n+[Install]\n+WantedBy=basic.target"
        },
        {
          "filename": "requirements.txt",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -1,3 +1,4 @@\n # note there are also rpm packaged deps that aren't\n # setup in pypi or with setup.py\n git+https://github.com/candlepin/python-rhsm.git\n+decorator"
        },
        {
          "filename": "scripts/dbus-show.sh",
          "status": "added",
          "additions": 61,
          "deletions": 0,
          "patch": "@@ -0,0 +1,61 @@\n+#!/bin/bash\n+\n+# For a given bus/service name, find all of it's object paths\n+# and show introspection data for each.\n+# Uses 'busctl' cli for the heavy work.\n+#\n+# If a service name isn't specified, default to all the\n+# well known names that have been acquired. This potentially\n+# doesn't include apps that only use a unique name, or services\n+# that need to be activated.\n+#\n+# Example usage:\n+# $ dbus-show org.storaged.Storaged\n+\n+BUS=${BUS:-\"system\"}\n+\n+\n+if [ -n \"${1}\" ]\n+then\n+    SERVICENAMES=( \"$@\" )\n+else\n+    SERVICENAMES=( $(busctl --no-legend \"--${BUS}\" --acquired | awk -e '{print $1}') )\n+fi\n+\n+\n+for service_name in \"${SERVICENAMES[@]}\"\n+do\n+\n+    dbus_paths=$(busctl \"--${BUS}\" tree  --list \"${service_name}\")\n+    tree_exit_code=\"$?\"\n+\n+    if [ \"${tree_exit_code}\" -eq \"1\" ]\n+    then\n+        printf \"Service name %s not found on this bus.\" \"${service_name}\"\n+        continue\n+    fi\n+\n+    printf \"service name: %s\" \"${service_name}\"\n+    for dbus_path in ${dbus_paths}\n+    do\n+        declare -a intro_data\n+        printf \"  object path: %s\\n\" \"${dbus_path}\"\n+\n+        # pull the intro_data into an array, one line per array entry (split on newline via readarray -t)\n+        # So that we can indent it slightly for purely cosmetic reasons.\n+        intro_data_raw=$(busctl \"--${BUS}\" \"introspect\" \"--no-legend\" \"${service_name}\" \"${dbus_path}\")\n+        readarray -t intro_data <<<\"${intro_data_raw}\"\n+\n+        if [ -n \"${intro_data}\" ]\n+        then\n+            for intro_data_line in \"${intro_data[@]}\"\n+            do\n+                printf \"    %s\\n\" \"${intro_data_line}\"\n+            done\n+            echo\n+        fi\n+\n+    done\n+    echo\n+done\n+"
        },
        {
          "filename": "scripts/rhsm-register-service",
          "status": "added",
          "additions": 42,
          "deletions": 0,
          "patch": "@@ -0,0 +1,42 @@\n+#!/usr/bin/env python\n+#\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+\n+# Init logging very early so we can log any issues that occur at import time\n+logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)5s [%(name)s:%(lineno)s] %(message)s\")\n+log = logging.getLogger('')\n+log.setLevel(logging.INFO)\n+\n+import sys\n+import warnings\n+\n+from rhsmlib.dbus import service_wrapper\n+from rhsmlib.dbus import objects\n+\n+if __name__ == \"__main__\":\n+    try:\n+        warnings.warn(\"This script should only be used for testing purposes. It is not secure.\")\n+        object_classes = [\n+            objects.DomainSocketRegisterDBusObject\n+        ]\n+        sys.exit(service_wrapper.main(sys.argv, object_classes=object_classes))\n+    except Exception:\n+        log.exception(\"DBus service startup failed\")\n+else:\n+    # Importing this module would screw up the importer's logging configuration since\n+    # we're setting up logging very early in module scope to catch any log messages that\n+    # occur during the loading of the dependent modules.\n+    raise ImportError(\"This module is not meant to be imported\")"
        },
        {
          "filename": "scripts/smoke_dbus.sh",
          "status": "added",
          "additions": 46,
          "deletions": 0,
          "patch": "@@ -0,0 +1,46 @@\n+#!/bin/bash\n+\n+FACTS=\"com.redhat.Subscriptions1.Facts\"\n+FACTS_PATH=\"/com/redhat/Subscriptions1/Facts/Host\"\n+FACTS_INTF=\"com.redhat.Subscriptions1.Facts\"\n+PROPS_INTF=\"org.freedesktop.DBus.Properties\"\n+INTRO_INTF=\"org.freedesktop.DBus.Introspectable\"\n+\n+busctl | grep 'rhsm'\n+busctl status \"${FACTS}\"\n+\n+pkaction | grep 'Subscriptions1'\n+\n+busctl tree \"${FACTS}\"\n+SERVICE=\"${FACTS}\"\n+OBJECT_PATH=\"${FACTS_PATH}\"\n+\n+# yes, it is using global variables and args\n+dbus_call () {\n+    local the_rest=$*\n+\n+    local CALL_ARGS=\"${SERVICE} ${OBJECT_PATH} ${INTF}\"\n+\n+    busctl call ${CALL_ARGS} ${the_rest}\n+}\n+\n+per_fact_object () {\n+    OBJECT_PATH=\"${1}\"\n+        \n+    busctl introspect \"${SERVICE}\" \"${OBJECT_PATH}\"\n+    \n+    INTF=\"${PROPS_INTF}\"\n+    dbus_call GetAll s \"${FACTS_INTF}\"\n+    dbus_call Get ss \"${FACTS_INTF}\" \"version\"\n+    dbus_call Get ss \"${FACTS_INTF}\" \"some_prop_that_doesnt_exist\"\n+\n+    INTF=\"${FACTS_INTF}\"\n+    dbus_call GetFacts\n+\n+    INTF=\"${INTRO_INTF}\"\n+    dbus_call Introspect\n+}\n+\n+per_fact_object \"${FACTS_PATH}\"\n+\n+#CALL_ARGS=\"${SERVICE} ${OBJECT_PATH} ${INTF}\""
        },
        {
          "filename": "setup.cfg",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -2,6 +2,7 @@\n [nosetests]\n with-xvfb=True\n with-randomly=True\n+cover-html=True\n # exclude tests with names that match regex .*ga_impls.*\n # Even though there are no tests with that name, but nose tries\n # to load modules that would have that name, and seems to ignore 'ignore-files'"
        },
        {
          "filename": "setup.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -274,15 +274,15 @@ def add_icons(self):\n     author=\"Adrian Likins\",\n     author_email=\"alikins@redhat.com\",\n     cmdclass=cmdclass,\n-    packages=find_packages('src', exclude=['subscription_manager.gui.firstboot.*', '*.ga_impls', '*.ga_impls.*', '*.plugin.ostree']),\n+    packages=find_packages('src', exclude=['subscription_manager.gui.firstboot.*', '*.ga_impls', '*.ga_impls.*', '*.plugin.ostree', '*.services.examples']),\n     package_dir={'': 'src'},\n     package_data={\n         'subscription_manager.gui': ['data/glade/*.glade', 'data/ui/*.ui', 'data/icons/*.svg'],\n     },\n     data_files=[\n         ('sbin', ['bin/subscription-manager', 'bin/subscription-manager-gui', 'bin/rhn-migrate-classic-to-rhsm']),\n         ('bin', ['bin/rct', 'bin/rhsm-debug']),\n-        (libexecdir, ['src/daemons/rhsmcertd-worker.py']),\n+        (libexecdir, ['src/daemons/rhsmcertd-worker.py', 'bin/rhsm-facts-service', 'bin/rhsm-service']),\n         # sat5to6 is packaged separately\n         ('share/man/man8', set(glob('man/*.8')) - set(['man/sat5to6.8'])),\n         ('share/man/man5', glob('man/*.5')),"
        },
        {
          "filename": "src/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "src/daemons/rhsm_d.py",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -78,8 +78,10 @@ def excepthook_logging(exc_type, exc_value, exc_traceback):\n         RHN_CLASSIC, RHSM_REGISTRATION_REQUIRED\n from subscription_manager.utils import print_error\n \n-import rhsm.config\n-CFG = rhsm.config.initConfig()\n+from rhsm.config import initConfig\n+from rhsmlib.services import config\n+\n+conf = config.Config(initConfig())\n \n enable_debug = False\n "
        },
        {
          "filename": "src/dnf-plugins/product-id.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -95,7 +95,7 @@ def get_enabled(self):\n                     # We have to look in all repos for productids, not just\n                     # the ones we create, or anaconda doesn't install it.\n                     self.meta_data_errors.append(repo.id)\n-            except Exception, e:\n+            except Exception as e:\n                 log.warn(\"Error loading productid metadata for %s.\" % repo)\n                 log.exception(e)\n                 self.meta_data_errors.append(repo.id)"
        },
        {
          "filename": "src/initial-setup/com_redhat_subscription_manager/gui/spokes/rhsm_gui.py",
          "status": "modified",
          "additions": 5,
          "deletions": 6,
          "patch": "@@ -35,7 +35,6 @@\n from subscription_manager.ga import Gtk as ga_Gtk\n from subscription_manager.gui import managergui\n from subscription_manager.injectioninit import init_dep_injection\n-from subscription_manager import injection as inj\n from subscription_manager.gui import registergui\n from subscription_manager import utils\n from subscription_manager.gui import utils as gui_utils\n@@ -65,16 +64,16 @@ def initialize(self):\n \n         init_dep_injection()\n \n-        facts = inj.require(inj.FACTS)\n-\n         backend = managergui.Backend()\n         self.info = registergui.RegisterInfo()\n         self.info.connect('notify::register-status', self._on_register_status_change)\n         self._status = self.info.get_property('register-status')\n \n-        self.register_widget = registergui.RegisterWidget(backend, facts,\n-                                                          reg_info=self.info,\n-                                                          parent_window=self.main_window)\n+        self.register_widget = registergui.RegisterWidget(\n+            backend,\n+            reg_info=self.info,\n+            parent_window=self.main_window\n+        )\n \n         self.register_box = self.builder.get_object(\"register_box\")\n         self.button_box = self.builder.get_object('navigation_button_box')"
        },
        {
          "filename": "src/plugins/product-id.py",
          "status": "modified",
          "additions": 4,
          "deletions": 4,
          "patch": "@@ -41,7 +41,7 @@ def posttrans_hook(conduit):\n \n     try:\n         init_dep_injection()\n-    except ImportError, e:\n+    except ImportError as e:\n         conduit.error(3, str(e))\n         return\n \n@@ -53,7 +53,7 @@ def posttrans_hook(conduit):\n         pm = YumProductManager(conduit._base)\n         pm.update_all()\n         conduit.info(3, 'Installed products updated.')\n-    except Exception, e:\n+    except Exception as e:\n         conduit.error(3, str(e))\n \n \n@@ -80,11 +80,11 @@ def get_enabled(self):\n                 if cert is None:\n                     continue\n                 lst.append((cert, repo.id))\n-            except yum.Errors.RepoMDError, e:\n+            except yum.Errors.RepoMDError as e:\n                 # We have to look in all repos for productids, not just\n                 # the ones we create, or anaconda doesn't install it.\n                 self.meta_data_errors.append(repo.id)\n-            except Exception, e:\n+            except Exception as e:\n                 log.warn(\"Error loading productid metadata for %s.\" % repo)\n                 log.exception(e)\n                 self.meta_data_errors.append(repo.id)"
        },
        {
          "filename": "src/plugins/subscription-manager.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -97,7 +97,7 @@ def update(conduit, cache_only):\n     if identity.is_valid():\n         try:\n             connection.UEPConnection(cert_file=cert_file, key_file=key_file)\n-        #FIXME: catchall exception\n+        # FIXME: catchall exception\n         except Exception:\n             # log\n             conduit.info(2, \"Unable to connect to Subscription Management Service\")\n@@ -174,5 +174,5 @@ def postconfig_hook(conduit):\n         update(conduit, cache_only)\n         warnOrGiveUsageMessage(conduit)\n         warnExpired(conduit)\n-    except Exception, e:\n+    except Exception as e:\n         conduit.error(2, str(e))"
        },
        {
          "filename": "src/rhsm_debug/debug_commands.py",
          "status": "modified",
          "additions": 13,
          "deletions": 12,
          "patch": "@@ -30,10 +30,11 @@\n from subscription_manager.cli import InvalidCLIOptionError, system_exit\n from rhsm import ourjson as json\n from rhsm.config import initConfig\n+from rhsmlib.services import config\n \n _ = gettext.gettext\n \n-cfg = initConfig()\n+conf = config.Config(initConfig())\n \n log = logging.getLogger('rhsm-app.' + __name__)\n \n@@ -134,7 +135,7 @@ def _do_command(self):\n \n             # FIXME: we need to anon proxy passwords?\n             sos = self.options.sos\n-            defaults = cfg.defaults()\n+            defaults = conf.defaults()\n             # sosreport collects /etc/rhsm/* and /var/*/rhsm/*, so these would\n             # be redundant for sos\n             if not sos:\n@@ -146,28 +147,28 @@ def _do_command(self):\n             if not sos:\n                 self._copy_cert_directory('/etc/pki/product-default', content_path)\n \n-            if defaults['productcertdir'] != cfg.get('rhsm', 'productCertDir') or not sos:\n-                self._copy_cert_directory(cfg.get('rhsm', 'productCertDir'), content_path)\n+            if defaults['productcertdir'] != conf['rhsm']['productCertDir'] or not sos:\n+                self._copy_cert_directory(conf['rhsm']['productCertDir'], content_path)\n \n-            if defaults['entitlementcertdir'] != cfg.get('rhsm', 'entitlementCertDir') or not sos:\n-                self._copy_cert_directory(cfg.get('rhsm', 'entitlementCertDir'), content_path)\n+            if defaults['entitlementcertdir'] != conf['rhsm']['entitlementCertDir'] or not sos:\n+                self._copy_cert_directory(conf['rhsm']['entitlementCertDir'], content_path)\n \n-            if defaults['consumercertdir'] != cfg.get('rhsm', 'consumerCertDir') or not sos:\n-                self._copy_cert_directory(cfg.get('rhsm', 'consumerCertDir'), content_path)\n+            if defaults['consumercertdir'] != conf['rhsm']['consumerCertDir'] or not sos:\n+                self._copy_cert_directory(conf['rhsm']['consumerCertDir'], content_path)\n \n             # If ca_cert_dir and pluginconfdif are configured as subdirs of /etc/rhsm\n             # (as is the default) we will have already copied there contents,\n             # so ignore directory exists errors\n             try:\n-                if defaults['ca_cert_dir'] != cfg.get('rhsm', 'ca_cert_dir') or not sos:\n-                    self._copy_cert_directory(cfg.get('rhsm', 'ca_cert_dir'), content_path)\n+                if defaults['ca_cert_dir'] != conf['rhsm']['ca_cert_dir'] or not sos:\n+                    self._copy_cert_directory(conf['rhsm']['ca_cert_dir'], content_path)\n             except EnvironmentError, e:\n                 if e.errno != errno.EEXIST:\n                     raise\n \n             try:\n-                if defaults['pluginconfdir'] != cfg.get('rhsm', 'pluginconfdir') or not sos:\n-                    self._copy_directory(cfg.get('rhsm', 'pluginconfdir'), content_path)\n+                if defaults['pluginconfdir'] != conf['rhsm']['pluginconfdir'] or not sos:\n+                    self._copy_directory(conf['rhsm']['pluginconfdir'], content_path)\n             except EnvironmentError, e:\n                 if e.errno != errno.EEXIST:\n                     raise"
        },
        {
          "filename": "src/rhsmlib/__init__.py",
          "status": "added",
          "additions": 27,
          "deletions": 0,
          "patch": "@@ -0,0 +1,27 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+import imp\n+\n+\n+def import_class(name):\n+    \"\"\"Load a class from a string.  Thanks http://stackoverflow.com/a/547867/61248 \"\"\"\n+    components = name.split('.')\n+    current_level = components[0]\n+    module_tuple = imp.find_module(current_level)\n+    module = imp.load_module(current_level, *module_tuple)\n+    for comp in components[1:-1]:\n+        # import all the way down to the class\n+        module_tuple = imp.find_module(comp, module.__path__)\n+        module = imp.load_module(comp, *module_tuple)\n+    # the class will be an attribute on the lowest level module\n+    return getattr(module, components[-1])"
        },
        {
          "filename": "src/rhsmlib/candlepin/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "src/rhsmlib/candlepin/api.py",
          "status": "added",
          "additions": 116,
          "deletions": 0,
          "patch": "@@ -0,0 +1,116 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+import socket\n+from M2Crypto import SSL\n+\n+import rhsm.connection\n+\n+log = logging.getLogger(__name__)\n+\n+\n+# Likely needs to subclass RestlibException etc\n+class CandlepinApiError(Exception):\n+    pass\n+\n+\n+class CandlepinApiSSLError(CandlepinApiError):\n+    pass\n+\n+\n+class CandlepinApiRestlibError(CandlepinApiError):\n+    pass\n+\n+\n+class CandlepinApiAuthenticationError(CandlepinApiError):\n+    pass\n+\n+\n+class CandlepinApiExpiredIDCertError(CandlepinApiError):\n+    pass\n+\n+\n+class CandlepinApiNetworkError(CandlepinApiError):\n+    pass\n+\n+\n+class Candlepin(object):\n+    def __init__(self, uep):\n+        self.uep = uep\n+        self._default_args = ()\n+        self.last_error = None\n+\n+    @property\n+    def default_args(self):\n+        # could include the default success/error callbacks\n+        return self._default_args\n+\n+    @property\n+    def default_kwargs(self):\n+        return self._default_kwargs\n+\n+    def call(self, rest_method, *args, **kwargs):\n+        success_callback = kwargs.get('success_callback', None)\n+        error_callback = kwargs.get('error_callback', None)\n+\n+        log.debug('success_cb=%s', success_callback)\n+        log.debug('error_callback=%s', error_callback)\n+        log.debug('rest_method=%s %s', rest_method, type(rest_method))\n+\n+        try:\n+            args = self.default_args + args\n+            return rest_method(*args, **kwargs)\n+        except AttributeError as e:\n+            log.exception(e)\n+            raise\n+        except SSL.SSLError as ex:\n+            log.exception(ex)\n+            self.last_error = ex\n+            log.error(\"Consumer certificate is invalid\")\n+            raise CandlepinApiSSLError('SSL related error (consumer identity cert is invalid?): %s' % ex)\n+        except rhsm.connection.RestlibException as ex:\n+            # Indicates we may be talking to a very old candlepin server\n+            # which does not have the necessary API call.\n+            log.exception(ex)\n+            self.last_error = ex\n+            raise CandlepinApiRestlibError('Error from candlepin: %s' % ex)\n+        except rhsm.connection.AuthenticationException as ex:\n+            log.error(\"Could not authenticate with server. Check registration status.\")\n+            log.exception(ex)\n+            self.last_error = ex\n+            raise CandlepinApiAuthenticationError(\"Could not authenticate with server. \"\n+                  \"Check registration status.: %s\" % ex)\n+        except rhsm.connection.ExpiredIdentityCertException as ex:\n+            log.exception(ex)\n+            self.last_error = ex\n+            msg = \"Bad identity, unable to connect to server\"\n+            raise CandlepinApiExpiredIDCertError(\"%s: %s\" % (msg, ex))\n+        except rhsm.connection.GoneException:\n+            raise\n+        # Most of the above are subclasses of ConnectionException that\n+        # get handled first\n+        except (rhsm.connection.ConnectionException, socket.error) as ex:\n+            log.error(ex)\n+            self.last_error = ex\n+\n+            msg = \"Unable to reach server.\"\n+            log.warn(msg)\n+            raise CandlepinApiNetworkError('%s: %s' % (msg, ex))\n+\n+\n+class CandlepinConsumer(Candlepin):\n+    def __init__(self, uep, uuid):\n+        self.uep = uep\n+        self.uuid = uuid\n+        self._default_args = (self.uuid,)"
        },
        {
          "filename": "src/rhsmlib/compat/__init__.py",
          "status": "added",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -0,0 +1 @@\n+from rhsmlib.compat.subprocess_compat import check_output  # NOQA"
        },
        {
          "filename": "src/rhsmlib/compat/subprocess_compat.py",
          "status": "added",
          "additions": 43,
          "deletions": 0,
          "patch": "@@ -0,0 +1,43 @@\n+# Copyright (c) 2010-2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+#\n+# Compat module that implements a subprocess.check_output work-a-like for\n+# python 2.6.\n+\n+import logging\n+import subprocess\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def check_output_2_6(*args, **kwargs):\n+    cmd_args = kwargs.get('args', None) or args[0]\n+\n+    log.debug(\"Running '%s'\" % cmd_args)\n+\n+    process = subprocess.Popen(*args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)\n+    (std_output, std_error) = process.communicate()\n+\n+    output = std_output.strip()\n+\n+    returncode = process.poll()\n+    if returncode:\n+        raise subprocess.CalledProcessError(returncode, cmd_args)\n+\n+    return output\n+\n+check_output = check_output_2_6\n+\n+if hasattr(subprocess, 'check_output'):\n+    check_output = subprocess.check_output"
        },
        {
          "filename": "src/rhsmlib/dbus/__init__.py",
          "status": "added",
          "additions": 3,
          "deletions": 0,
          "patch": "@@ -0,0 +1,3 @@\n+from rhsmlib.dbus.constants import *  # NOQA\n+from rhsmlib.dbus.exceptions import *  # NOQA\n+from rhsmlib.dbus.util import *  # NOQA"
        },
        {
          "filename": "src/rhsmlib/dbus/base_object.py",
          "status": "added",
          "additions": 35,
          "deletions": 0,
          "patch": "@@ -0,0 +1,35 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+\n+import logging\n+import dbus.service\n+\n+from rhsmlib.dbus import constants, exceptions\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class BaseObject(dbus.service.Object):\n+    # Name of the DBus interface provided by this object\n+    interface_name = constants.INTERFACE_BASE\n+    default_dbus_path = constants.ROOT_DBUS_PATH\n+\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        if object_path is None:\n+            object_path = self.default_dbus_path\n+        super(BaseObject, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+\n+    def _check_interface(self, interface_name):\n+        if interface_name != self.interface_name:\n+            raise exceptions.UnknownInterface(interface_name)"
        },
        {
          "filename": "src/rhsmlib/dbus/constants.py",
          "status": "added",
          "additions": 56,
          "deletions": 0,
          "patch": "@@ -0,0 +1,56 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+import string\n+\n+__all__ = [\n+    'NAME_BASE',\n+    'VERSION',\n+    'BUS_NAME',\n+    'INTERFACE_BASE',\n+    'ROOT_DBUS_PATH',\n+    'MAIN_INTERFACE',\n+    'MAIN_DBUS_PATH',\n+    'REGISTER_INTERFACE',\n+    'REGISTER_DBUS_PATH',\n+    'CONFIG_INTERFACE',\n+    'CONFIG_DBUS_PATH',\n+]\n+\n+# The base of the 'well known name' used for bus and service names, as well\n+# as interface names and object paths.\n+#\n+# \"com.redhat.RHSM1\"\n+NAME_BASE = \"com.redhat.RHSM\"\n+VERSION = \"1\"\n+BUS_NAME = NAME_BASE + VERSION\n+\n+# The default interface name for objects we share on this service.\n+INTERFACE_BASE = BUS_NAME\n+\n+# The root of the objectpath tree for our services.\n+# Note: No trailing '/'\n+#\n+# /com/redhat/RHSM1\n+ROOT_DBUS_PATH = '/' + string.replace(BUS_NAME, '.', '/')\n+\n+MAIN_INTERFACE = INTERFACE_BASE\n+MAIN_DBUS_PATH = ROOT_DBUS_PATH\n+\n+REGISTER_INTERFACE = '%s.%s' % (INTERFACE_BASE, 'RegisterServer')\n+REGISTER_DBUS_PATH = '%s/%s' % (ROOT_DBUS_PATH, 'RegisterServer')\n+\n+PRIVATE_REGISTER_INTERFACE = '%s.%s' % (INTERFACE_BASE, 'Register')\n+PRIVATE_REGISTER_DBUS_PATH = '%s/%s' % (ROOT_DBUS_PATH, 'Register')\n+\n+CONFIG_INTERFACE = '%s.%s' % (INTERFACE_BASE, 'Config')\n+CONFIG_DBUS_PATH = '%s/%s' % (ROOT_DBUS_PATH, 'Config')"
        },
        {
          "filename": "src/rhsmlib/dbus/dbus_utils.py",
          "status": "added",
          "additions": 244,
          "deletions": 0,
          "patch": "@@ -0,0 +1,244 @@\n+# -*- coding: utf-8 -*-\n+#\n+# Copyright (C) 2011,2012 Red Hat, Inc.\n+#\n+# Authors:\n+# Thomas Woerner <twoerner@redhat.com>\n+#\n+# This program is free software; you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation; either version 2 of the License, or\n+# (at your option) any later version.\n+#\n+# This program is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n+#\n+\n+import logging\n+import pwd\n+import sys\n+import xml.etree.ElementTree as Et\n+\n+import dbus\n+\n+PY2 = sys.version < '3'\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def command_of_pid(pid):\n+    \"\"\" Get command for pid from /proc \"\"\"\n+    try:\n+        with open(\"/proc/%d/cmdline\" % pid, \"r\") as f:\n+            cmd = f.readlines()[0].replace('\\0', \" \").strip()\n+    except:\n+        return None\n+    return cmd\n+\n+\n+def pid_of_sender(bus, sender):\n+    \"\"\" Get pid from sender string using\n+    org.freedesktop.DBus.GetConnectionUnixProcessID \"\"\"\n+\n+    dbus_obj = bus.get_object('org.freedesktop.DBus', '/org/freedesktop/DBus')\n+    dbus_iface = dbus.Interface(dbus_obj, 'org.freedesktop.DBus')\n+\n+    try:\n+        pid = int(dbus_iface.GetConnectionUnixProcessID(sender))\n+    except ValueError:\n+        return None\n+    return pid\n+\n+\n+def uid_of_sender(bus, sender):\n+    \"\"\" Get user id from sender string using\n+    org.freedesktop.DBus.GetConnectionUnixUser \"\"\"\n+\n+    dbus_obj = bus.get_object('org.freedesktop.DBus', '/org/freedesktop/DBus')\n+    dbus_iface = dbus.Interface(dbus_obj, 'org.freedesktop.DBus')\n+\n+    try:\n+        uid = int(dbus_iface.GetConnectionUnixUser(sender))\n+    except ValueError:\n+        return None\n+    return uid\n+\n+\n+def user_of_uid(uid):\n+    \"\"\" Get user for uid from pwd \"\"\"\n+\n+    try:\n+        pws = pwd.getpwuid(uid)\n+    except Exception:\n+        return None\n+    return pws[0]\n+\n+\n+def context_of_sender(bus, sender):\n+    \"\"\" Get SELinux context from sender string using\n+    org.freedesktop.DBus.GetConnectionSELinuxSecurityContext \"\"\"\n+\n+    dbus_obj = bus.get_object('org.freedesktop.DBus', '/org/freedesktop/DBus')\n+    dbus_iface = dbus.Interface(dbus_obj, 'org.freedesktop.DBus')\n+\n+    try:\n+        context = dbus_iface.GetConnectionSELinuxSecurityContext(sender)\n+    except:\n+        return None\n+\n+    return \"\".join(map(chr, dbus_to_python(context)))\n+\n+\n+def command_of_sender(bus, sender):\n+    \"\"\" Return command of D-Bus sender \"\"\"\n+\n+    return command_of_pid(pid_of_sender(bus, sender))\n+\n+\n+def user_of_sender(bus, sender):\n+    return user_of_uid(uid_of_sender(bus, sender))\n+\n+\n+def dbus_to_python(obj, expected_type=None):\n+    if obj is None:\n+        python_obj = obj\n+    elif isinstance(obj, dbus.Boolean):\n+        python_obj = bool(obj)\n+    elif isinstance(obj, dbus.String):\n+        python_obj = obj.encode('utf-8') if PY2 else str(obj)\n+    elif PY2 and isinstance(obj, dbus.UTF8String):  # Python3 has no UTF8String\n+        python_obj = str(obj)\n+    elif isinstance(obj, dbus.ObjectPath):\n+        python_obj = str(obj)\n+    elif isinstance(obj, dbus.Byte) or \\\n+            isinstance(obj, dbus.Int16) or \\\n+            isinstance(obj, dbus.Int32) or \\\n+            isinstance(obj, dbus.Int64) or \\\n+            isinstance(obj, dbus.UInt16) or \\\n+            isinstance(obj, dbus.UInt32) or \\\n+            isinstance(obj, dbus.UInt64):\n+        python_obj = int(obj)\n+    elif isinstance(obj, dbus.Double):\n+        python_obj = float(obj)\n+    elif isinstance(obj, dbus.Array):\n+        python_obj = [dbus_to_python(x) for x in obj]\n+    elif isinstance(obj, dbus.Struct):\n+        python_obj = tuple([dbus_to_python(x) for x in obj])\n+    elif isinstance(obj, dbus.Dictionary):\n+        #python_obj = {dbus_to_python(k): dbus_to_python(v) for k, v in obj.items()}\n+        python_obj = dict([dbus_to_python(k), dbus_to_python(v)] for k, v in obj.items())\n+    elif isinstance(obj, bool) or \\\n+         isinstance(obj, str) or isinstance(obj, bytes) or \\\n+         isinstance(obj, int) or isinstance(obj, float) or \\\n+         isinstance(obj, list) or isinstance(obj, tuple) or \\\n+         isinstance(obj, dict):\n+        python_obj = obj\n+    else:\n+        raise TypeError(\"Unhandled %s\" % obj)\n+\n+    if expected_type is not None:\n+        if (expected_type == bool and not isinstance(python_obj, bool)) or \\\n+           (expected_type == str and not isinstance(python_obj, str)) or \\\n+           (expected_type == int and not isinstance(python_obj, int)) or \\\n+           (expected_type == float and not isinstance(python_obj, float)) or \\\n+           (expected_type == list and not isinstance(python_obj, list)) or \\\n+           (expected_type == tuple and not isinstance(python_obj, tuple)) or \\\n+           (expected_type == dict and not isinstance(python_obj, dict)):\n+            raise TypeError(\"%s is %s, expected %s\" % (python_obj, type(python_obj), expected_type))\n+\n+    return python_obj\n+\n+# From lvm-dubstep/lvmdbus/utils.py  (GPLv2, copyright Red Hat)\n+# https://github.com/tasleson/lvm-dubstep\n+_type_map = dict(\n+    s=dbus.String,\n+    o=dbus.ObjectPath,\n+    t=dbus.UInt64,\n+    x=dbus.Int64,\n+    u=dbus.UInt32,\n+    i=dbus.Int32,\n+    n=dbus.Int16,\n+    q=dbus.UInt16,\n+    d=dbus.Double,\n+    y=dbus.Byte,\n+    b=dbus.Boolean)\n+\n+\n+def _pass_through(v):\n+    \"\"\"\n+    If we have something which is not a simple type we return the original\n+    value un-wrapped.\n+    :param v:\n+    :return:\"\"\"\n+    return v\n+\n+\n+def _dbus_type(t, value):\n+    return _type_map.get(t, _pass_through)(value)\n+\n+\n+def add_properties(xml, interface, props):\n+    \"\"\"\n+    Given xml that describes the interface, add property values to the XML\n+    for the specified interface.\n+    :param xml:         XML to edit\n+    :param interface:   Interface to add the properties too\n+    :param props:       Output from get_properties\n+    :return: updated XML string\n+    \"\"\"\n+    root = Et.fromstring(xml)\n+\n+    if props:\n+\n+        for c in root:\n+            # print c.attrib['name']\n+            if c.attrib['name'] == interface:\n+                for p in props:\n+                    temp = '<property type=\"%s\" name=\"%s\" access=\"%s\"/>\\n' % \\\n+                        (p['p_t'], p['p_name'], p['p_access'])\n+                    log.debug(\"intro xml temp buf=%s\", temp)\n+                    c.append(Et.fromstring(temp))\n+\n+        return Et.tostring(root, encoding='utf8')\n+    return xml\n+\n+\n+def dict_to_variant_dict(in_dict):\n+    # Handle creating dbus.Dictionaries with signatures of 'sv'\n+    for key, value in in_dict.iteritems():\n+        if isinstance(value, dict):\n+            in_dict[key] = dict_to_variant_dict(value)\n+    return dbus.Dictionary(in_dict, signature=\"sv\")\n+\n+\n+def _decode_dict(data):\n+    rv = {}\n+    for key, value in data.iteritems():\n+        if isinstance(key, unicode):\n+            key = key.encode('utf-8')\n+        if isinstance(value, unicode):\n+            value = value.encode('utf-8')\n+        elif isinstance(value, list):\n+            value = _decode_list(value)\n+        elif isinstance(value, dict):\n+            value = _decode_dict(value)\n+        rv[key] = value\n+    return rv\n+\n+\n+def _decode_list(data):\n+    rv = []\n+    for item in data:\n+        if isinstance(item, unicode):\n+            item = item.encode('utf-8')\n+        elif isinstance(item, list):\n+            item = _decode_list(item)\n+        elif isinstance(item, dict):\n+            item = _decode_dict(item)\n+        rv.append(item)\n+    return rv"
        },
        {
          "filename": "src/rhsmlib/dbus/exceptions.py",
          "status": "added",
          "additions": 90,
          "deletions": 0,
          "patch": "@@ -0,0 +1,90 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+import dbus\n+from rhsmlib.dbus import constants\n+\n+__all__ = [\n+    'RHSM1DBusException',\n+    'UnknownProperty',\n+    'UnknownInterface',\n+    'InvalidArguments',\n+    'AccessDenied',\n+    'PropertyMissing',\n+    'Failed',\n+]\n+\n+\n+class RHSM1DBusException(dbus.DBusException):\n+    \"\"\"Base exceptions.\"\"\"\n+    include_traceback = True\n+    _dbus_error_name = \"%s.Error\" % constants.INTERFACE_BASE\n+\n+\n+class UnknownProperty(dbus.DBusException):\n+    include_traceback = True\n+\n+    def __init__(self, property_name):\n+        super(UnknownProperty, self).__init__(\n+            \"Property '%s' does not exist\" % property_name,\n+            name=\"org.freedesktop.DBus.Error.UnknownProperty\"\n+        )\n+\n+\n+class UnknownInterface(dbus.DBusException):\n+    include_traceback = True\n+\n+    def __init__(self, interface_name):\n+        super(UnknownInterface, self).__init__(\n+            \"Interface '%s' is unknown\" % interface_name,\n+            name=\"org.freedesktop.DBus.Error.UnknownInterface\"\n+        )\n+\n+\n+class InvalidArguments(dbus.DBusException):\n+    include_traceback = True\n+\n+    def __init__(self, argument):\n+        super(InvalidArguments, self).__init__(\n+            \"Argument '%s' is invalid\" % argument,\n+            name=\"org.freedesktop.DBus.Error.InvalidArgs\"\n+        )\n+\n+\n+class AccessDenied(dbus.DBusException):\n+    include_traceback = True\n+\n+    def __init__(self, prop, interface):\n+        super(AccessDenied, self).__init__(\n+            \"Property '%s' isn't exported (or does not exist) on interface: %s\" % (prop, interface),\n+            name=\"org.freedesktop.DBus.Error.AccessDenied\"\n+        )\n+\n+\n+class PropertyMissing(dbus.DBusException):\n+    include_traceback = True\n+\n+    def __init__(self, prop, interface):\n+        super(PropertyMissing, self).__init__(\n+            \"Property '%s' does not exist on interface: %s\" % (prop, interface),\n+            name=\"org.freedesktop.DBus.Error.AccessDenied\"\n+        )\n+\n+\n+class Failed(dbus.DBusException):\n+    include_traceback = True\n+\n+    def __init__(self, msg=None):\n+        super(Failed, self).__init__(\n+            msg or \"Operation failed\",\n+            name=\"org.freedesktop.DBus.Error.Failed\"\n+        )"
        },
        {
          "filename": "src/rhsmlib/dbus/facts/__init__.py",
          "status": "added",
          "additions": 3,
          "deletions": 0,
          "patch": "@@ -0,0 +1,3 @@\n+from rhsmlib.dbus.facts.base import HostFacts  # noqa: F401\n+from rhsmlib.dbus.facts.client import FactsClient, FactsClientAuthenticationError  # noqa: F401\n+from rhsmlib.dbus.facts.constants import FACTS_DBUS_NAME, FACTS_DBUS_INTERFACE, FACTS_DBUS_PATH  # noqa: F401"
        },
        {
          "filename": "src/rhsmlib/dbus/facts/base.py",
          "status": "added",
          "additions": 120,
          "deletions": 0,
          "patch": "@@ -0,0 +1,120 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import os\n+import logging\n+import dbus\n+\n+import rhsm.config\n+\n+from rhsmlib.facts import collector, host_collector, hwprobe, custom\n+from rhsmlib.dbus import util, base_object\n+from rhsmlib.dbus.facts import constants\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class BaseFacts(base_object.BaseObject):\n+    interface_name = constants.FACTS_DBUS_INTERFACE\n+    default_props_data = {}\n+    facts_collector_class = collector.FactsCollector\n+\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        super(BaseFacts, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+\n+        # Default is an empty FactsCollector\n+        self.facts_collector = self.facts_collector_class()\n+\n+    @util.dbus_service_method(\n+        dbus_interface=constants.FACTS_DBUS_INTERFACE,\n+        out_signature='a{ss}')\n+    @util.dbus_handle_exceptions\n+    def GetFacts(self, sender=None):\n+        collection = self.facts_collector.collect()\n+        cleaned = dict([(str(key), str(value)) for key, value in collection.data.items()])\n+        return dbus.Dictionary(cleaned, signature=\"ss\")\n+\n+\n+class AllFacts(base_object.BaseObject):\n+    interface_name = constants.FACTS_DBUS_INTERFACE\n+    default_dbus_path = constants.FACTS_DBUS_PATH\n+\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        super(AllFacts, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+\n+        # Why aren't we using a dictionary here? Because we want to control the order and OrderedDict\n+        # isn't in Python 2.6.  By controlling the order and putting CustomFacts last, we can ensure\n+        # that users can override any fact.\n+        collector_definitions = [\n+            (\"Host\", HostFacts),\n+            (\"Hardware\", HardwareFacts),\n+            (\"Static\", StaticFacts),\n+            (\"Custom\", CustomFacts),\n+        ]\n+\n+        self.collectors = []\n+        for path, clazz in collector_definitions:\n+            sub_path = self.default_dbus_path + \"/\" + path\n+            self.collectors.append(\n+                (path, clazz(conn=conn, object_path=sub_path, bus_name=bus_name))\n+            )\n+\n+    @util.dbus_service_method(\n+        dbus_interface=constants.FACTS_DBUS_INTERFACE,\n+        out_signature='a{ss}')\n+    @util.dbus_handle_exceptions\n+    def GetFacts(self, sender=None):\n+        results = {}\n+        for name, fact_collector in self.collectors:\n+            results.update(fact_collector.GetFacts())\n+        return dbus.Dictionary(results, signature=\"ss\")\n+\n+    def remove_from_connection(self, connection=None, path=None):\n+        # Call remove_from_connection on all the child objects first\n+        for sub_path, obj in self.collectors:\n+            if path:\n+                child_path = path + \"/\" + sub_path\n+            else:\n+                child_path = None\n+            obj.remove_from_connection(connection, child_path)\n+        super(AllFacts, self).remove_from_connection(connection, path)\n+\n+\n+class HostFacts(BaseFacts):\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        super(HostFacts, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+        self.facts_collector = host_collector.HostCollector()\n+\n+\n+class HardwareFacts(BaseFacts):\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        super(HardwareFacts, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+        self.facts_collector = hwprobe.HardwareCollector()\n+\n+\n+class CustomFacts(BaseFacts):\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        super(CustomFacts, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+\n+        paths_and_globs = [\n+            (os.path.join(rhsm.config.DEFAULT_CONFIG_DIR, 'facts'), '*.facts'),\n+        ]\n+        self.facts_collector = custom.CustomFactsCollector(path_and_globs=paths_and_globs)\n+\n+\n+class StaticFacts(BaseFacts):\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        super(StaticFacts, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+        self.facts_collector = collector.StaticFactsCollector({\n+            \"system.certificate_version\": constants.CERT_VERSION\n+        })"
        },
        {
          "filename": "src/rhsmlib/dbus/facts/client.py",
          "status": "added",
          "additions": 61,
          "deletions": 0,
          "patch": "@@ -0,0 +1,61 @@\n+# Copyright (c) 2010-2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+\n+import logging\n+import dbus\n+\n+from rhsmlib.dbus.facts import constants as facts_constants\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class FactsClientAuthenticationError(Exception):\n+    def __init__(self, *args, **kwargs):\n+        action_id = kwargs.pop(\"action_id\")\n+        super(FactsClientAuthenticationError, self).__init__(*args, **kwargs)\n+        log.debug(\"FactsClientAuthenticationError created for %s\", action_id)\n+        self.action_id = action_id\n+\n+\n+class FactsClient(object):\n+    bus_name = facts_constants.FACTS_DBUS_NAME\n+    object_path = facts_constants.FACTS_DBUS_PATH\n+    interface_name = facts_constants.FACTS_DBUS_INTERFACE\n+\n+    def __init__(self, bus=None, bus_name=None, object_path=None, interface_name=None):\n+        self.bus = bus or dbus.SystemBus()\n+\n+        if bus_name:\n+            self.bus_name = bus_name\n+\n+        if object_path:\n+            self.object_path = object_path\n+\n+        if interface_name:\n+            self.interface_name = interface_name\n+\n+        self.dbus_proxy_object = self.bus.get_object(self.bus_name, self.object_path,\n+            follow_name_owner_changes=True)\n+\n+        self.interface = dbus.Interface(self.dbus_proxy_object,\n+            dbus_interface=self.interface_name)\n+\n+        self.bus.call_on_disconnection(self._on_bus_disconnect)\n+\n+    def GetFacts(self, *args, **kwargs):\n+        return self.interface.GetFacts(*args, **kwargs)\n+\n+    def _on_bus_disconnect(self, connection):\n+        self.dbus_proxy_object = None\n+        log.debug(\"Disconnected from FactsService\")"
        },
        {
          "filename": "src/rhsmlib/dbus/facts/constants.py",
          "status": "added",
          "additions": 38,
          "deletions": 0,
          "patch": "@@ -0,0 +1,38 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+from rhsmlib.dbus import constants\n+\n+__all__ = [\n+    'SUB_SERVICE_NAME',\n+    'FACTS_DBUS_NAME',\n+    'FACTS_DBUS_INTERFACE',\n+    'FACTS_DBUS_PATH',\n+    'FACTS_VERSION',\n+    'FACTS_NAME',\n+]\n+\n+SUB_SERVICE_NAME = \"Facts\"\n+\n+# com.redhat.RHSM1.Facts\n+FACTS_DBUS_NAME = constants.BUS_NAME + '.' + SUB_SERVICE_NAME\n+\n+# also, com.redhat.RHSM1.Facts\n+FACTS_DBUS_INTERFACE = constants.BUS_NAME + '.' + SUB_SERVICE_NAME\n+\n+# /com/redhat/RHSM1/Facts\n+FACTS_DBUS_PATH = constants.ROOT_DBUS_PATH + '/' + SUB_SERVICE_NAME\n+\n+FACTS_VERSION = \"1.1e1\"\n+FACTS_NAME = \"Red Hat Subscription Manager facts.\"\n+\n+CERT_VERSION = \"3.2\""
        },
        {
          "filename": "src/rhsmlib/dbus/objects/__init__.py",
          "status": "added",
          "additions": 16,
          "deletions": 0,
          "patch": "@@ -0,0 +1,16 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+from rhsmlib.dbus.objects.config import ConfigDBusObject  # NOQA\n+from rhsmlib.dbus.objects.main import Main  # NOQA\n+from rhsmlib.dbus.objects.register import RegisterDBusObject, DomainSocketRegisterDBusObject  # NOQA"
        },
        {
          "filename": "src/rhsmlib/dbus/objects/config.py",
          "status": "added",
          "additions": 72,
          "deletions": 0,
          "patch": "@@ -0,0 +1,72 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import dbus\n+import logging\n+\n+from rhsmlib.dbus import constants, base_object, util, dbus_utils\n+from rhsmlib.services.config import Config\n+\n+from dbus import DBusException\n+log = logging.getLogger(__name__)\n+\n+\n+class ConfigDBusObject(base_object.BaseObject):\n+    default_dbus_path = constants.CONFIG_DBUS_PATH\n+    interface_name = constants.CONFIG_INTERFACE\n+\n+    def __init__(self, conn=None, object_path=None, bus_name=None, parser=None):\n+        self.config = Config(parser)\n+        super(ConfigDBusObject, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+\n+    @util.dbus_service_method(\n+        constants.CONFIG_INTERFACE,\n+        in_signature='sv')\n+    @util.dbus_handle_exceptions\n+    def Set(self, property_name, new_value, sender=None):\n+        property_name = dbus_utils.dbus_to_python(property_name, str)\n+        new_value = dbus_utils.dbus_to_python(new_value, str)\n+        section, _dot, property_name = property_name.partition('.')\n+\n+        if not property_name:\n+            raise DBusException(\"Setting an entire section is not supported.  Use 'section.property' format.\")\n+\n+        self.config[section][property_name] = new_value\n+        self.config.persist()\n+\n+    @util.dbus_service_method(\n+        constants.CONFIG_INTERFACE,\n+        in_signature='',\n+        out_signature='a{sv}')\n+    @util.dbus_handle_exceptions\n+    def GetAll(self, sender=None):\n+        d = dbus.Dictionary({}, signature='sv')\n+        for k, v in self.config.iteritems():\n+            d[k] = dbus.Dictionary({}, signature='ss')\n+            for kk, vv in v.iteritems():\n+                d[k][kk] = vv\n+\n+        return d\n+\n+    @util.dbus_service_method(\n+        constants.CONFIG_INTERFACE,\n+        in_signature='s',\n+        out_signature='v')\n+    @util.dbus_handle_exceptions\n+    def Get(self, property_name, sender=None):\n+        section, _dot, property_name = property_name.partition('.')\n+\n+        if property_name:\n+            return self.config[section][property_name]\n+        else:\n+            return dbus.Dictionary(self.config[section], signature='sv')"
        },
        {
          "filename": "src/rhsmlib/dbus/objects/main.py",
          "status": "added",
          "additions": 25,
          "deletions": 0,
          "patch": "@@ -0,0 +1,25 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+\n+from rhsmlib.dbus import base_object\n+from subscription_manager.injectioninit import init_dep_injection\n+\n+log = logging.getLogger(__name__)\n+init_dep_injection()\n+\n+\n+class Main(base_object.BaseObject):\n+    \"\"\"Just a place holder for now\"\"\"\n+    pass"
        },
        {
          "filename": "src/rhsmlib/dbus/objects/register.py",
          "status": "added",
          "additions": 201,
          "deletions": 0,
          "patch": "@@ -0,0 +1,201 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import gettext\n+import socket\n+import json\n+import logging\n+import dbus.service\n+import threading\n+\n+from rhsmlib.dbus import constants, exceptions, dbus_utils, base_object, server, util, facts\n+\n+from subscription_manager import managerlib\n+from rhsm import connection\n+\n+from subscription_manager import injection as inj\n+from subscription_manager.injectioninit import init_dep_injection\n+\n+init_dep_injection()\n+\n+_ = gettext.gettext\n+log = logging.getLogger(__name__)\n+\n+\n+class RegisterDBusObject(base_object.BaseObject):\n+    default_dbus_path = constants.REGISTER_DBUS_PATH\n+    interface_name = constants.REGISTER_INTERFACE\n+\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        super(RegisterDBusObject, self).__init__(conn=conn, object_path=object_path, bus_name=bus_name)\n+        self.started_event = threading.Event()\n+        self.stopped_event = threading.Event()\n+        self.server = None\n+        self.lock = threading.Lock()\n+\n+    @util.dbus_service_method(\n+        constants.REGISTER_INTERFACE,\n+        in_signature='',\n+        out_signature='s')\n+    @util.dbus_handle_exceptions\n+    def Start(self, sender=None):\n+        with self.lock:\n+            if self.server:\n+                return self.server.address\n+\n+            log.debug('Attempting to create new domain socket server')\n+            self.server = server.DomainSocketServer(\n+                object_classes=[DomainSocketRegisterDBusObject],\n+            )\n+            address = self.server.run()\n+            log.debug('DomainSocketServer created and listening on \"%s\"', address)\n+            return address\n+\n+    @util.dbus_service_method(\n+        constants.REGISTER_INTERFACE,\n+        in_signature='',\n+        out_signature='b')\n+    @util.dbus_handle_exceptions\n+    def Stop(self, sender=None):\n+        with self.lock:\n+            if self.server:\n+                self.server.shutdown()\n+                self.server = None\n+                log.debug(\"Stopped DomainSocketServer\")\n+                return True\n+            else:\n+                raise exceptions.Failed(\"No domain socket server is running\")\n+\n+\n+class DomainSocketRegisterDBusObject(base_object.BaseObject):\n+    interface_name = constants.PRIVATE_REGISTER_INTERFACE\n+    default_dbus_path = constants.PRIVATE_REGISTER_DBUS_PATH\n+\n+    def __init__(self, conn=None, object_path=None, bus_name=None):\n+        # On our DomainSocket DBus server since a private connection is not a \"bus\", we have to treat\n+        # it slightly differently. In particular there are no names, no discovery and so on.\n+        super(DomainSocketRegisterDBusObject, self).__init__(\n+            conn=conn,\n+            object_path=object_path,\n+            bus_name=bus_name\n+        )\n+        self.installed_mgr = inj.require(inj.INSTALLED_PRODUCTS_MANAGER)\n+\n+    @dbus.service.method(\n+        dbus_interface=constants.PRIVATE_REGISTER_INTERFACE,\n+        in_signature='sssa{sv}',\n+        out_signature='a{sv}'\n+    )\n+    def Register(self, org, username, password, options):\n+        \"\"\"\n+        This method registers the system using basic auth\n+        (username and password for a given org).\n+        For any option that is required but not included the default will be\n+        used.\n+\n+        Options is a dict of strings that modify the outcome of this method.\n+\n+        Note this method is registration ONLY.  Auto-attach is a separate process.\n+        \"\"\"\n+        options['username'] = username\n+        options['password'] = password\n+\n+        result = self._register(org, None, options)\n+        return dbus_utils.dict_to_variant_dict(result)\n+\n+    @dbus.service.method(dbus_interface=constants.PRIVATE_REGISTER_INTERFACE,\n+        in_signature='sa(s)a{ss}',\n+        out_signature='a{sv}')\n+    def RegisterWithActivationKeys(self, org, activation_keys, options):\n+        \"\"\"\n+        Note this method is registration ONLY.  Auto-attach is a separate process.\n+        \"\"\"\n+        result = self._register(org, activation_keys, options)\n+        return dbus_utils.dict_to_variant_dict(result)\n+\n+    def _register(self, org, activation_keys, options):\n+        options = dbus_utils.dbus_to_python(options)\n+        options = self.validate_options(options)\n+\n+        environment = options.get('environment')\n+        facts_client = facts.FactsClient()\n+\n+        cp = self.build_uep(options)\n+        registration_output = cp.registerConsumer(\n+            name=options['name'],\n+            facts=facts_client.GetFacts(),\n+            owner=org,\n+            environment=environment,\n+            keys=activation_keys,\n+            installed_products=self.installed_mgr.format_for_server(),\n+            content_tags=self.installed_mgr.tags\n+        )\n+        self.installed_mgr.write_cache()\n+\n+        consumer = json.loads(registration_output['content'], object_hook=dbus_utils._decode_dict)\n+        managerlib.persist_consumer_cert(consumer)\n+\n+        if 'idCert' in consumer:\n+            del consumer['idCert']\n+\n+        registration_output['content'] = json.dumps(consumer)\n+        return registration_output\n+\n+    def build_uep(self, options):\n+        return connection.UEPConnection(\n+            username=options.get('username', None),\n+            password=options.get('password', None),\n+            host=options.get('host', None),\n+            ssl_port=connection.safe_int(options.get('port', None)),\n+            handler=options.get('handler', None),\n+            insecure=options.get('insecure', None),\n+            proxy_hostname=options.get('proxy_hostname', None),\n+            proxy_port=options.get('proxy_port', None),\n+            proxy_user=options.get('proxy_user', None),\n+            proxy_password=options.get('proxy_password', None),\n+            restlib_class=connection.BaseRestLib\n+        )\n+\n+    def is_registered(self):\n+        return inj.require(inj.IDENTITY).is_valid()\n+\n+    def validate_options(self, options):\n+        # TODO: Rewrite the error messages to be more dbus specific\n+        error_msg = None\n+        if self.is_registered() and not options.get('force', False):\n+            error_msg = _(\"This system is already registered. Add force to options to override.\")\n+        elif options.get('name') == '':\n+            error_msg = _(\"Error: system name can not be empty.\")\n+        elif 'consumerid' in options and 'force' in options:\n+            error_msg = _(\"Error: Can not force registration while attempting to recover registration with consumerid. Please use --force without --consumerid to re-register or use the clean command and try again without --force.\")\n+\n+        if 'activation_keys' in options:\n+            # 746259: Don't allow the user to pass in an empty string as an activation key\n+            if '' == options['activation_keys']:\n+                error_msg = _(\"Error: Must specify an activation key\")\n+            elif 'username' in options:\n+                error_msg = _(\"Error: Activation keys do not require user credentials.\")\n+            elif 'consumerid' in options:\n+                error_msg = _(\"Error: Activation keys can not be used with previously registered IDs.\")\n+            elif 'environment' in options:\n+                error_msg = _(\"Error: Activation keys do not allow environments to be specified.\")\n+            elif 'org' not in options:\n+                error_msg = _(\"Error: Must provide --org with activation keys.\")\n+\n+        if error_msg:\n+            raise exceptions.Failed(msg=error_msg)\n+\n+        if 'name' not in options:\n+            options['name'] = socket.gethostname()\n+\n+        return options"
        },
        {
          "filename": "src/rhsmlib/dbus/server.py",
          "status": "added",
          "additions": 179,
          "deletions": 0,
          "patch": "@@ -0,0 +1,179 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+import dbus.service\n+import dbus.server\n+import dbus.mainloop.glib\n+import threading\n+\n+from rhsmlib.dbus import constants\n+\n+from subscription_manager import ga_loader\n+ga_loader.init_ga()\n+from subscription_manager.ga import GLib\n+\n+from functools import partial\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class Server(object):\n+    def __init__(self, bus_class=None, bus_name=None, object_classes=None, bus_kwargs=None):\n+        \"\"\"Create a connection to a bus defined by bus_class and bus_kwargs; instantiate objects in\n+        object_classes; expose them under bus_name and enter a GLib mainloop.  bus_kwargs are generally\n+        only necessary if you're using dbus.bus.BusConnection\n+\n+        The object_classes argument is a list.  The list can contain either a class or a tuple consisting\n+        of a class and a dictionary of arguments to send that class's constructor.\n+        \"\"\"\n+\n+        # Configure mainloop for threading.  We must do so in GLib and python-dbus.\n+        GLib.threads_init()\n+        dbus.mainloop.glib.threads_init()\n+\n+        self.bus_name = bus_name or constants.BUS_NAME\n+        bus_class = bus_class or dbus.SystemBus\n+        bus_kwargs = bus_kwargs or {}\n+        object_classes = object_classes or []\n+        self.objects = []\n+\n+        try:\n+            self.bus = bus_class(**bus_kwargs)\n+        except dbus.exceptions.DBusException:\n+            log.exception(\"Could not create bus class\")\n+            raise\n+\n+        self.connection_name = dbus.service.BusName(self.bus_name, self.bus)\n+        self.mainloop = GLib.MainLoop()\n+\n+        for item in object_classes:\n+            try:\n+                clazz, kwargs = item[0], item[1]\n+            except TypeError:\n+                clazz = item\n+                kwargs = {}\n+\n+            self.objects.append(\n+                clazz(object_path=clazz.default_dbus_path, bus_name=self.connection_name, **kwargs)\n+            )\n+\n+    def run(self, started_event=None, stopped_event=None):\n+        \"\"\"The two arguments, started_event and stopped_event, should be instances of threading.Event that\n+        will be set when the mainloop has finished starting and stopping.\"\"\"\n+        try:\n+            GLib.idle_add(self.notify_started, started_event)\n+            self.mainloop.run()\n+        except KeyboardInterrupt as e:\n+            log.exception(e)\n+        except SystemExit as e:\n+            log.exception(e)\n+        except Exception as e:\n+            log.exception(e)\n+        finally:\n+            if stopped_event:\n+                stopped_event.set()\n+\n+    def notify_started(self, started_event):\n+        \"\"\"This callback will be run once the mainloop is up and running.  It's only purpose is to alert\n+        other blocked threads that the mainloop is ready.\"\"\"\n+        log.debug(\"Start notification sent\")\n+        if started_event:\n+            started_event.set()\n+        # Only run this callback once\n+        return False\n+\n+    def shutdown(self):\n+        \"\"\"This method is primarily intended for uses of Server in a thread such as during testing since\n+        in a single-threaded program, the execution would be blocked on the mainloop and therefore\n+        preclude even calling this method.\"\"\"\n+        self.mainloop.quit()\n+\n+        # Unregister/remove everything.  Note that if you used dbus.SessionBus or dbus.SystemBus,\n+        # python-dbus will keep a cache of your old BusName objects even though we are releasing the name\n+        # here.  This will create a problem if you attempt to reacquire the BusName since python-dbus will\n+        # give you a stale reference.  Use dbus.Connection.BusConnection to avoid this problem.\n+        # See http://stackoverflow.com/questions/17446414/dbus-object-lifecycle\n+        map(lambda x: x.remove_from_connection(), self.objects)\n+        self.bus.release_name(self.bus_name)\n+\n+\n+class DomainSocketServer(object):\n+    \"\"\"This class sets up a DBus server on a domain socket. That server can then be used to perform\n+    registration. The issue is that we can't send registration credentials over the regular system or\n+    session bus since those aren't really locked down. The work-around is the client asks our service\n+    to open another server on a domain socket, gets socket information back, and then connects and sends\n+    the register command (with the credentials) to the server on the domain socket.\"\"\"\n+    @staticmethod\n+    def connection_added(domain_socket_server, service_class, object_list, conn):\n+        object_list.append(service_class(conn=conn))\n+        with domain_socket_server.lock:\n+            domain_socket_server.connection_count += 1\n+        log.info(\"New connection: %s\", conn)\n+\n+    @staticmethod\n+    def connection_removed(domain_socket_server, conn):\n+        log.debug(\"Closed connection: %s\", conn)\n+        with domain_socket_server.lock:\n+            domain_socket_server.connection_count -= 1\n+            if domain_socket_server.connection_count == 0:\n+                log.debug('No connections remain')\n+            else:\n+                log.debug('Server still has connections')\n+\n+    @property\n+    def address(self):\n+        if self._server:\n+            return self._server.address\n+        else:\n+            return None\n+\n+    def __init__(self, object_classes=None):\n+        \"\"\"Create a connection to a bus defined by bus_class and bus_kwargs; instantiate objects in\n+        object_classes; expose them under bus_name and enter a GLib mainloop.  bus_kwargs are generally\n+        only necessary if you're using dbus.bus.BusConnection\n+\n+        The object_classes argument is a list.  The list can contain either a class or a tuple consisting\n+        of a class and a dictionary of arguments to send that class's constructor.\n+        \"\"\"\n+        self.object_classes = object_classes or []\n+        self.objects = []\n+\n+        self.lock = threading.Lock()\n+        with self.lock:\n+            self.connection_count = 0\n+\n+    def shutdown(self):\n+        map(lambda x: x.remove_from_connection(), self.objects)\n+        self._server.disconnect()\n+\n+        # Allow self.objects and self._server to get GCed\n+        self.objects = None\n+        self._server = None\n+\n+    def run(self):\n+        try:\n+            self._server = dbus.server.Server(\"unix:tmpdir=/var/run\")\n+\n+            for clazz in self.object_classes:\n+                self._server.on_connection_added.append(\n+                    partial(DomainSocketServer.connection_added, self, clazz, self.objects)\n+                )\n+\n+            self._server.on_connection_removed.append(\n+                partial(DomainSocketServer.connection_removed, self)\n+            )\n+\n+            return self.address\n+        except Exception as e:\n+            log.exception(e)"
        },
        {
          "filename": "src/rhsmlib/dbus/service_wrapper.py",
          "status": "added",
          "additions": 73,
          "deletions": 0,
          "patch": "@@ -0,0 +1,73 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+import sys\n+import optparse\n+import dbus\n+import dbus.mainloop.glib\n+import rhsmlib\n+import logging\n+\n+from rhsmlib.dbus import server\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def load_bus_class(option, opt_str, value, parser):\n+    \"\"\"OptionParser callback method to load a class from a string\"\"\"\n+    clazz = rhsmlib.import_class(value)\n+    parser.values.bus = clazz\n+\n+\n+def parse_argv(argv, default_dbus_name):\n+    parser = optparse.OptionParser(usage=\"usage: %prog [options] [class name]\")\n+    parser.add_option(\"-b\", \"--bus\",\n+        action=\"callback\", callback=load_bus_class,\n+        type=\"string\", default=dbus.SystemBus,\n+        help=\"Bus to use (defaults to dbus.SystemBus)\")\n+    parser.add_option(\"-n\", \"--bus-name\", default=default_dbus_name)\n+    parser.add_option(\"-v\", \"--verbose\", action=\"store_true\")\n+    (opts, args) = parser.parse_args(argv[1:])\n+    return opts, args\n+\n+\n+def main(argv=sys.argv, object_classes=None, default_bus_name=None):\n+    # Set default mainloop\n+    dbus.mainloop.glib.DBusGMainLoop(set_as_default=True)\n+\n+    if not default_bus_name:\n+        default_bus_name = rhsmlib.dbus.constants.BUS_NAME\n+\n+    options, args = parse_argv(argv, default_bus_name)\n+\n+    if options.verbose:\n+        logger = logging.getLogger('')\n+        logger.setLevel(logging.DEBUG)\n+\n+    if not object_classes:\n+        # Read the object classes from the command-line if we don't\n+        # get anything as a parameter\n+        object_classes = []\n+        for clazz in args:\n+            object_classes.append(rhsmlib.import_class(clazz))\n+\n+    try:\n+        log.debug('Starting DBus service with name %s' % options.bus_name)\n+        server.Server(\n+            bus_class=options.bus,\n+            bus_name=options.bus_name,\n+            object_classes=object_classes).run()\n+    except dbus.exceptions.DBusException as e:\n+        if e._dbus_error_name == \"org.freedesktop.DBus.Error.AccessDenied\":\n+            print(\"Access to DBus denied.  You need to edit /etc/dbus-1/system.conf to allow %s or run with \"\n+                \"dbus-daemon and a custom config file.\" % options.bus_name)\n+    return 0"
        },
        {
          "filename": "src/rhsmlib/dbus/util.py",
          "status": "added",
          "additions": 47,
          "deletions": 0,
          "patch": "@@ -0,0 +1,47 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+import logging\n+import sys\n+import decorator\n+import dbus.service\n+\n+from rhsmlib.dbus import exceptions\n+\n+log = logging.getLogger(__name__)\n+\n+__all__ = [\n+    'dbus_handle_exceptions',\n+    'dbus_service_method',\n+]\n+\n+\n+@decorator.decorator\n+def dbus_handle_exceptions(func, *args, **kwargs):\n+    \"\"\"Decorator to handle exceptions, log them, and wrap them if necessary\"\"\"\n+    try:\n+        ret = func(*args, **kwargs)\n+        return ret\n+    except dbus.DBusException as e:\n+        log.exception(e)\n+        raise\n+    except Exception as e:\n+        log.exception(e)\n+        trace = sys.exc_info()[2]\n+        raise exceptions.RHSM1DBusException(\"%s: %s\" % (type(e).__name__, str(e))), None, trace\n+\n+\n+def dbus_service_method(*args, **kwargs):\n+    # Tell python-dbus that \"sender\" will be the keyword to use for the sender unless otherwise\n+    # defined.\n+    kwargs.setdefault(\"sender_keyword\", \"sender\")\n+    return dbus.service.method(*args, **kwargs)"
        },
        {
          "filename": "src/rhsmlib/facts/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "src/rhsmlib/facts/cleanup.py",
          "status": "added",
          "additions": 72,
          "deletions": 0,
          "patch": "@@ -0,0 +1,72 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+\n+from rhsmlib.facts import collector\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class CleanupCollector(collector.FactsCollector):\n+    no_uuid_platforms = ['powervm_lx86', 'xen-dom0', 'ibm_systemz']\n+\n+    def get_all(self):\n+        cleanup_facts = {}\n+        dmi_socket_info = self.replace_socket_count_with_dmi()\n+        cleanup_facts.update(dmi_socket_info)\n+        return cleanup_facts\n+\n+    def explain_lack_of_virt_uuid(self):\n+        # No virt.uuid equiv is available for guests on these hypervisors\n+        #virt_is_guest = self._collected_hw_info['virt.is_guest']\n+        if not self._is_a_virt_host_type_with_virt_uuids():\n+            log.debug(\"we don't sell virt uuids here\")\n+\n+    def _is_a_virt_host_type_with_virt_uuids(self):\n+        virt_host_type = self._collected_hw_info['virt.host_type']\n+        for no_uuid_platform in self.no_uuid_platforms:\n+            if virt_host_type.find(no_uuid_platform) > -1:\n+                return False\n+        return True\n+\n+    def replace_socket_count_with_dmi(self):\n+        cleanup_info = {}\n+        # cpu topology reporting on xen dom0 machines is wrong. So\n+        # if we are a xen dom0, and we found socket info in dmiinfo,\n+        # replace our normal cpu socket calculation with the dmiinfo one\n+        # we have to do it after the virt data and cpu data collection\n+        if 'virt.host_type' not in self._collected_hw_info:\n+            return cleanup_info\n+\n+        if not self._host_is_xen_dom0():\n+            return cleanup_info\n+\n+        if 'dmi.meta.cpu_socket_count' not in self._collected_hw_info:\n+            return cleanup_info\n+\n+        # Alright, lets munge up cpu socket info based on the dmi info.\n+        socket_count = int(self._collected_hw_info['dmi.meta.cpu_socket_count'])\n+        cleanup_info['cpu.cpu_socket(s)'] = socket_count\n+\n+        if 'cpu.cpu(s)' not in self._collected_hw_info:\n+            return cleanup_info\n+\n+        # And the cores per socket count as well\n+        dmi_cpu_cores_per_cpu = int(self._collected_hw_info['cpu.cpu(s)']) / socket_count\n+        cleanup_info['cpu.core(s)_per_socket'] = dmi_cpu_cores_per_cpu\n+\n+        return cleanup_info\n+\n+    def _host_is_xen_dom0(self):\n+        return self._collected_hw_info['virt.host_type'].find('dom0') > -1"
        },
        {
          "filename": "src/rhsmlib/facts/collection.py",
          "status": "added",
          "additions": 91,
          "deletions": 0,
          "patch": "@@ -0,0 +1,91 @@\n+#\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public\n+# License as published by the Free Software Foundation; either version\n+# 2 of the License (GPLv2) or (at your option) any later version.\n+# There is NO WARRANTY for this software, express or implied,\n+# including the implied warranties of MERCHANTABILITY,\n+# NON-INFRINGEMENT, or FITNESS FOR A PARTICULAR PURPOSE. You should\n+# have received a copy of GPLv2 along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+from datetime import datetime\n+import collections\n+import logging\n+\n+log = logging.getLogger(__name__)\n+\n+\n+# TODO: Likely a bit much for this case\n+class FactsDict(collections.MutableMapping):\n+    \"\"\"A dict for facts that ignores items in 'graylist' on compares.\"\"\"\n+\n+    graylist = set(['cpu.cpu_mhz', 'lscpu.cpu_mhz'])\n+\n+    def __init__(self, *args, **kwargs):\n+        super(FactsDict, self).__init__(*args, **kwargs)\n+        self.data = {}\n+\n+    def __getitem__(self, key):\n+        return self.data[key]\n+\n+    def __setitem__(self, key, value):\n+        self.data[key] = value\n+\n+    def __delitem__(self, key):\n+        del self.data[key]\n+\n+    def __iter__(self):\n+        return iter(self.data)\n+\n+    def __len__(self):\n+        return len(self.data)\n+\n+    def __eq__(self, other):\n+        \"\"\"Compares all of the items in self.data, except it ignores keys in self.graylist.\"\"\"\n+        if not isinstance(other, FactsDict):\n+            return NotImplemented\n+\n+        keys_self = set(self.data).difference(self.graylist)\n+        keys_other = set(other.data).difference(self.graylist)\n+        if keys_self == keys_other:\n+            if all(self.data[k] == other.data[k] for k in keys_self):\n+                return True\n+\n+        return False\n+\n+    # Maybe total_ordering is a bit overkill for just a custom compare\n+    def __lt__(self, other):\n+        return len(self) < len(other)\n+\n+    def __repr__(self):\n+        return '%s(%r)' % (self.__class__.__name__, self.items())\n+\n+\n+def compare_with_graylist(dict_a, dict_b, graylist):\n+    ka = set(dict_a).difference(graylist)\n+    kb = set(dict_b).difference(graylist)\n+    return ka == kb and all(dict_a[k] == dict_b[k] for k in ka)\n+\n+\n+class FactsCollection(object):\n+    def __init__(self, facts_dict=None):\n+        self.data = facts_dict or FactsDict()\n+        self.collection_datetime = datetime.now()\n+\n+    def __repr__(self):\n+        buf = \"%s(facts_dict=%s, collection_datetime=%s)\" % \\\n+            (self.__class__.__name__, self.data, self.collection_datetime)\n+        return buf\n+\n+    @classmethod\n+    def from_facts_collection(cls, facts_collection):\n+        \"\"\"Create a FactsCollection with the data from facts_collection, but new timestamps.\n+        ie, a copy(), more or less.\"\"\"\n+        fc = cls()\n+        fc.data.update(facts_collection.data)\n+        return fc\n+\n+    def __iter__(self):\n+        return self.data"
        },
        {
          "filename": "src/rhsmlib/facts/collector.py",
          "status": "added",
          "additions": 110,
          "deletions": 0,
          "patch": "@@ -0,0 +1,110 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+import os\n+import platform\n+\n+from rhsmlib.facts import collection\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def get_arch(prefix=None):\n+    \"\"\"Get the systems architecture.\n+\n+    This relies on portable means, like uname to determine\n+    a high level system arch (ie, x86_64, ppx64,etc).\n+\n+    We need that so we can decide how to collect the\n+    arch specific hardware information.\n+\n+    Also support a 'prefix' arg that allows us to override\n+    the results. The contents of the '/prefix/arch' will\n+    override the arch. The 'prefix' arg defaults to None,\n+    equiv to '/'. This is intended only for test purposes.\n+\n+    Returns a string containing the arch.\"\"\"\n+\n+    DEFAULT_PREFIX = '/'\n+    ARCH_FILE_NAME = 'arch'\n+    prefix = prefix or DEFAULT_PREFIX\n+\n+    if prefix == DEFAULT_PREFIX:\n+        return platform.machine()\n+\n+    arch_file = os.path.join(prefix, ARCH_FILE_NAME)\n+    try:\n+        with open(arch_file, 'r') as arch_fd:\n+            return arch_fd.read().strip()\n+    except IOError as e:\n+        # If we specify a prefix, and there is no 'arch' file,\n+        # consider that fatal.\n+        log.exception(e)\n+        raise\n+\n+# An empty FactsCollector should just return an empty dict on get_all()\n+\n+\n+class FactsCollector(object):\n+    def __init__(self, arch=None, prefix=None, testing=None,\n+                 hardware_methods=None, collected_hw_info=None):\n+        \"\"\"Base class for facts collecting classes.\n+\n+        self._collected_hw_info will reference the passed collected_hw_info\n+        arg. When possible this should be a reference (or copy) to all of the facts\n+        collected in this run. Some collection methods need to alter behavior\n+        based on facts collector from other modules/classes.\n+        self._collected_hw_info isn't meant to be altered as a side effect, but\n+        no promises.\"\"\"\n+        self.allhw = {}\n+        self.prefix = prefix or ''\n+        self.testing = testing or False\n+\n+        self._collected_hw_info = collected_hw_info\n+        # we need this so we can decide which of the\n+        # arch specific code bases to follow\n+        self.arch = arch or get_arch(prefix=self.prefix)\n+\n+        self.hardware_methods = hardware_methods or []\n+\n+    def collect(self):\n+        \"\"\"Return a FactsCollection iterable.\"\"\"\n+        facts_dict = collection.FactsDict()\n+        facts_dict.update(self.get_all())\n+        facts_collection = collection.FactsCollection(facts_dict=facts_dict)\n+        return facts_collection\n+\n+    def get_all(self):\n+        # try each hardware method, and try/except around, since\n+        # these tend to be fragile\n+        all_hw_info = {}\n+        for hardware_method in self.hardware_methods:\n+            info_dict = {}\n+            try:\n+                info_dict = hardware_method()\n+            except Exception as e:\n+                log.warn(\"Hardware detection [%s] failed: %s\" % (hardware_method.__name__, e))\n+\n+            all_hw_info.update(info_dict)\n+\n+        return all_hw_info\n+\n+\n+class StaticFactsCollector(FactsCollector):\n+    def __init__(self, static_facts, **kwargs):\n+        super(FactsCollector, self).__init__(**kwargs)\n+        self.static_facts = static_facts\n+\n+    def get_all(self):\n+        return self.static_facts"
        },
        {
          "filename": "src/rhsmlib/facts/cpuinfo.py",
          "status": "added",
          "additions": 489,
          "deletions": 0,
          "patch": "@@ -0,0 +1,489 @@\n+#\n+# Read and parse /proc/cpuinfo\n+#\n+# Copyright (c) 2015 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+\n+# needs to be able to provide a data object for each cpu blob and\n+# for the system\n+#\n+# needs to be able to aggregate multiple cpu data objects and create\n+# an exemplar cpuinfo  (ie, if we want to ignore cpus at different\n+# speeds, this may be where)\n+#\n+# needs to work non-root if possible, possibly as a standalone module/cli\n+#\n+# Expect the info available in cpuinfo to very across arches, and across\n+# kernel and cpu versions. Some arches have almost no info. Some have tons.\n+# Some provide hex values, most decimal.\n+#\n+# Expect the field names to change often. Don't expect field names to\n+# be unique.\n+#\n+# Expect some fields to disappear without warning at any oppurtunity\n+# (This includes changing arch, version, kernel, os vendor. It also includes\n+#  no reason at all. cpus can disappear. cpus can remove fields. they can\n+#  reappear).\n+#\n+# Expect values of cpuinfo fields to change, somethings constantly. cpu speed\n+# for example, can actually vary _every_ time it is read.\n+#\n+# GOTCHAS: the field names are non consistent across arches, and can conflict\n+#          semantically.\n+#\n+#         surprise, some are not even one key: value per line (see s390x)\n+#\n+# context manager?\n+# class CpuinfoFile()\n+#     .read()\n+#  handle io errors\n+#\n+# can take file like object or\n+# class BaseParseCpuInfo()\n+#\n+# class FieldNameCanonicalizer()\n+#  ie, convert 'model name' to model_name\n+#  and 'proccesor' processor\n+#  and 'Processor' to... seriously aarch64?\n+#    ('Processor' and 'processor' fields...)\n+#\n+# class CpuInfo() the main interface class\n+#     arch = None\n+#     cpuinfo_class = None\n+#     # avoid, count of cpus/sockets/etc\n+#\n+#\n+# class X86_64():\n+#\n+# class S390X():\n+#   with fun \"multiple values per line\"\n+#\n+# class Aarch64():\n+#   with hex values and a system stanza\n+#\n+# class Ppc64():\n+#    system stanza and system model\n+# factory to init proper one based... uname.machine? 'arch' file?\n+\n+import collections\n+import itertools\n+import logging\n+import os\n+\n+# mostly populated from the arm CPUID instruction\n+# http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0432c/Bhccjgga.html\n+# http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0432c/Chdeaeij.html\n+#\n+# aarch \"version\" info\n+# CPU implementer : 0x50\n+# CPU architecture: AArch64\n+# CPU variant : 0x0\n+# CPU part    : 0x000\n+# CPU revision    : 0\n+\n+# Mostly info from intel CPUID instruction\n+# http://en.wikipedia.org/wiki/CPUID\n+#\n+# intel \"version\" info\n+# processor   : 22\n+# vendor_id   : GenuineIntel\n+# cpu family  : 6\n+# model       : 45\n+# model name  : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz\n+# stepping    : 7\n+# microcode   : 0x710\n+\n+log = logging.getLogger('rhsm-app.' + __name__)\n+\n+\n+class DefaultCpuFields(object):\n+    \"\"\"Maps generic cpuinfo fields to the corresponding field from ProcessorModel.\n+\n+    For, a cpu MODEL (a number or string that the cpu vendor assigns to that model of\n+    cpu, '45' for an intel Xeon for example)\n+    is in the 'model' field in /proc/cpuinfo, and the\n+    'model' in the sluggified field in X86_64ProcessorModel. For aarch64,\n+    the field 'cpu_part' is it's MODEL.\n+    \"\"\"\n+    MODEL_NAME = \"model_name\"\n+    MODEL = \"model\"\n+\n+\n+class X86_64Fields(object):\n+    MODEL_NAME = 'model_name'\n+    MODEL = 'model'\n+\n+\n+class Aarch64Fields(object):\n+    MODEL = 'cpu_part'\n+    MODEL_NAME = 'model_name'\n+\n+\n+class Ppc64Fields(object):\n+    MODEL = 'model'\n+    MODEL_NAME = 'machine'\n+\n+\n+# represent the data in /proc/cpuinfo, which may include multiple processors\n+class CpuinfoModel(object):\n+    fields_class = DefaultCpuFields\n+\n+    def __init__(self, cpuinfo_data=None):\n+        # The contents of /proc/cpuinfo\n+        self.cpuinfo_data = cpuinfo_data\n+\n+        # A iterable of CpuInfoModels, one for each processor in cpuinfo\n+        self.processors = []\n+\n+        # prologues or footnotes not associated with a particular processor\n+        self.other = []\n+\n+        # If were going to pretend all the cpus are the same,\n+        # what do they all have in common.\n+        self.common = {}\n+\n+        # model name    : Intel(R) Core(TM) i5 CPU       M 560  @ 2.67GHz\n+        self._model_name = None\n+\n+        # a model number\n+        # \"45\" for intel processor example above\n+        self._model = None\n+\n+    @property\n+    def count(self):\n+        return len(self.processors)\n+\n+    @property\n+    def model_name(self):\n+        if self._model_name:\n+            return self._model_name\n+\n+        if not self.common:\n+            return None\n+\n+        return self.common.get(self.fields_class.MODEL_NAME, None)\n+\n+    @property\n+    def model(self):\n+        if self._model:\n+            return self._model\n+\n+        if not self.common:\n+            return None\n+\n+        return self.common.get(self.fields_class.MODEL, None)\n+\n+    def __str__(self):\n+        lines = []\n+        lines.append(\"Processor count: %s\" % self.count)\n+        lines.append('model_name: %s' % self.model_name)\n+        lines.append(\"\")\n+        for k in sorted(self.common.keys()):\n+            lines.append(\"%s: %s\" % (k, self.common[k]))\n+        lines.append(\"\")\n+        for k, v in self.other:\n+            lines.append(\"%s: %s\" % (k, v))\n+        lines.append(\"\")\n+        return \"\\n\".join(lines)\n+\n+\n+class Aarch64ProcessorModel(dict):\n+    \"The info corresponding to the info about each aarch64 processor entry in cpuinfo\"\n+    pass\n+\n+\n+class X86_64ProcessorModel(dict):\n+    \"The info corresponding to the info about each X86_64 processor entry in cpuinfo\"\n+    pass\n+\n+\n+class Ppc64ProcessorModel(dict):\n+    \"The info corresponding to the info about each ppc64 processor entry in cpuinfo\"\n+    @classmethod\n+    def from_stanza(cls, stanza):\n+        cpu_data = cls()\n+        cpu_data.update(dict([fact_sluggify_item(item) for item in stanza]))\n+        return cpu_data\n+\n+\n+class X86_64CpuinfoModel(CpuinfoModel):\n+    \"\"\"The model for all the cpuinfo data for all processors on the machine.\n+\n+    ie, all the data in /proc/cpuinfo field as opposed to X86_64ProcessModel which\n+    is the info for 1 processor.\"\"\"\n+    fields_class = X86_64Fields\n+\n+\n+class Ppc64CpuinfoModel(CpuinfoModel):\n+    fields_class = Ppc64Fields\n+\n+\n+class Aarch64CpuinfoModel(CpuinfoModel):\n+    fields_class = Aarch64Fields\n+\n+\n+def fact_sluggify(key):\n+    \"\"\"Encodes an arbitrary string to something that can be used as a fact name.\n+\n+    ie, 'model_name' instead of 'Model name'\n+    whitespace -> _\n+    lowercase\n+    utf8\n+    escape quotes\n+\n+    In theory, any utf8 would work\n+    \"\"\"\n+    # yeah, terrible...\n+    return key.lower().strip().replace(' ', '_').replace('.', '_')\n+\n+\n+def fact_sluggify_item(item_tuple):\n+    newkey = fact_sluggify(item_tuple[0])\n+    return (newkey, item_tuple[1])\n+\n+\n+def split_key_value_generator(file_contents, line_splitter):\n+    for line in file_contents.splitlines():\n+        parts = line_splitter(line)\n+        if parts:\n+            yield parts\n+\n+\n+def line_splitter(line):\n+    # cpu family    : 6\n+    # model name    : Intel(R) Core(TM) i5 CPU       M 560  @ 2.67GHz\n+    parts = line.split(':', 1)\n+    if parts[0]:\n+        parts = [part.strip() for part in parts]\n+        return parts\n+    return None\n+\n+\n+def accumulate_fields(fields_accum, fields):\n+    for field in fields:\n+        fields_accum.add(field)\n+    return fields_accum\n+\n+\n+def find_shared_key_value_pairs(all_fields, processors):\n+    # smashem, last one wins\n+    smashed = collections.defaultdict(set)\n+\n+    # build a dict of fieldname -> list of all the different values\n+    # so we can dump the variant ones.\n+    for field in all_fields:\n+        for k, v in [(field, processor.get(field)) for processor in processors]:\n+            if v is None:\n+                continue\n+            smashed[k].add(v)\n+\n+    # remove fields that can't be smashed to one value\n+    common_cpu_info = dict([(x, smashed[x].pop()) for x in smashed if len(smashed[x]) == 1])\n+    return common_cpu_info\n+\n+\n+def split_kv_list_by_field(kv_list, field):\n+    \"\"\"Split the iterable kv_list into chunks by field.\n+\n+    For a list with repeating stanzas in it, this will\n+    return a generate that will return each chunk.\n+\n+    For something like /proc/cpuinfo, called with\n+    field 'processor', each stanza is a different cpu.\n+    \"\"\"\n+    current_stanza = None\n+    for key, value in kv_list:\n+        if key == field:\n+            if current_stanza:\n+                yield current_stanza\n+            current_stanza = [(key, value)]\n+            continue\n+\n+        # if we have garbage in and no start to processor info\n+        if current_stanza:\n+            current_stanza.append((key, value))\n+\n+    # end of kv_list\n+    if current_stanza:\n+        yield current_stanza\n+\n+\"\"\"\n+Processor   : AArch64 Processor rev 0 (aarch64)\n+processor   : 0\n+processor   : 1\n+processor   : 2\n+processor   : 3\n+processor   : 4\n+processor   : 5\n+processor   : 6\n+processor   : 7\n+Features    : fp asimd evtstrm\n+CPU implementer : 0x50\n+CPU architecture: AArch64\n+CPU variant : 0x0\n+CPU part    : 0x000\n+CPU revision    : 0\n+\n+Hardware    : APM X-Gene Mustang board\n+\"\"\"\n+\n+\n+class BaseCpuInfo(object):\n+    @classmethod\n+    def from_proc_cpuinfo_string(cls, proc_cpuinfo_string):\n+        \"\"\"Return a BaseCpuInfo subclass based on proc_cpuinfo_string.\n+\n+        proc_cpuinfo_string is the string resulting from reading\n+        the entire contents of /proc/cpuinfo.\"\"\"\n+        cpu_info = cls()\n+        cpu_info._parse(proc_cpuinfo_string)\n+\n+        return cpu_info\n+\n+\n+class Aarch64CpuInfo(BaseCpuInfo):\n+    def __init__(self):\n+        self.cpu_info = Aarch64CpuinfoModel()\n+\n+    def _parse(self, cpuinfo_data):\n+        raw_kv_iter = split_key_value_generator(cpuinfo_data, line_splitter)\n+\n+        # Yes, there is a 'Processor' field and multiple lower case 'processor'\n+        # fields.\n+        kv_iter = (self._capital_processor_to_model_name(item)\n+                   for item in raw_kv_iter)\n+\n+        slugged_kv_list = [fact_sluggify_item(item) for item in kv_iter]\n+\n+        # kind of duplicated\n+        self.cpu_info.common = self.gather_cpu_info_model(slugged_kv_list)\n+        self.cpu_info.processors = self.gather_processor_list(slugged_kv_list)\n+\n+        # For now, 'hardware' is per\n+        self.cpu_info.other = self.gather_cpu_info_other(slugged_kv_list)\n+\n+    def _capital_processor_to_model_name(self, item):\n+        \"\"\"Use the uppercase Processor field value as the model name.\n+\n+        For aarch64, the 'Processor' field is the closest to model name,\n+        so we sub it in now.\"\"\"\n+        if item[0] == 'Processor':\n+            item[0] = \"model_name\"\n+        return item\n+\n+    def gather_processor_list(self, kv_list):\n+        processor_list = []\n+        for k, v in kv_list:\n+            if k != 'processor':\n+                continue\n+            # build a ProcessorModel subclass for each processor\n+            # to add to CpuInfoModel.processors list\n+            cpu_info_model = self.gather_cpu_info_model(kv_list)\n+            cpu_info_model['processor'] = v\n+            processor_list.append(cpu_info_model)\n+        return processor_list\n+\n+    # FIXME: more generic would be to split the stanzas by empty lines in the\n+    # first pass\n+    def gather_cpu_info_other(self, kv_list):\n+        other_list = []\n+        for k, v in kv_list:\n+            if k == 'hardware':\n+                other_list.append([k, v])\n+        return other_list\n+\n+    def gather_cpu_info_model(self, kv_list):\n+        cpu_data = Aarch64ProcessorModel()\n+        for k, v in kv_list:\n+            if k == 'processor' or k == 'hardware':\n+                continue\n+            cpu_data[k] = v\n+        return cpu_data\n+\n+\n+class X86_64CpuInfo(BaseCpuInfo):\n+    def __init__(self):\n+        self.cpu_info = X86_64CpuinfoModel()\n+\n+    def _parse(self, cpuinfo_data):\n+        # ordered list\n+        kv_iter = split_key_value_generator(cpuinfo_data, line_splitter)\n+\n+        processors = []\n+        all_fields = set()\n+        for processor_stanza in split_kv_list_by_field(kv_iter, 'processor'):\n+            proc_dict = self.processor_stanza_to_processor_data(processor_stanza)\n+            processors.append(proc_dict)\n+            # keep track of fields as we see them\n+            all_fields = accumulate_fields(all_fields, proc_dict.keys())\n+\n+        self.cpu_info.common = find_shared_key_value_pairs(all_fields, processors)\n+        self.cpu_info.processors = processors\n+        self.cpu_info.cpuinfo_data = cpuinfo_data\n+\n+    def processor_stanza_to_processor_data(self, stanza):\n+        \"Take a list of k,v tuples, sluggify name, and add to a dict.\"\n+        cpu_data = X86_64ProcessorModel()\n+        cpu_data.update(dict([fact_sluggify_item(item) for item in stanza]))\n+        return cpu_data\n+\n+\n+class Ppc64CpuInfo(BaseCpuInfo):\n+    def __init__(self):\n+        self.cpu_info = Ppc64CpuinfoModel()\n+\n+    def _parse(self, cpuinfo_data):\n+        kv_iter = split_key_value_generator(cpuinfo_data, line_splitter)\n+\n+        processor_iter = itertools.takewhile(self._not_timebase_key, kv_iter)\n+        for processor_stanza in split_kv_list_by_field(processor_iter, 'processor'):\n+            proc_dict = Ppc64ProcessorModel.from_stanza(processor_stanza)\n+            self.cpu_info.processors.append(proc_dict)\n+\n+        # Treat the rest of the info as shared between all of the processor entries\n+        # kv_iter is the rest of cpuinfo that isn't processor stanzas\n+        self.cpu_info.common = dict([fact_sluggify_item(item) for item in kv_iter])\n+        self.cpu_info.cpuinfo_data = cpuinfo_data\n+\n+    def _not_timebase_key(self, item):\n+        return item[0] != 'timebase'\n+\n+\n+class SystemCpuInfoFactory(object):\n+    uname_to_cpuinfo = {'x86_64': X86_64CpuInfo,\n+                        'aarch64': Aarch64CpuInfo,\n+                        'ppc64': Ppc64CpuInfo,\n+                        'ppc64le': Ppc64CpuInfo}\n+    proc_cpuinfo_path = '/proc/cpuinfo'\n+\n+    @classmethod\n+    def from_uname_machine(cls, uname_machine, prefix=None):\n+        if uname_machine not in SystemCpuInfoFactory.uname_to_cpuinfo:\n+            # er?\n+            raise NotImplementedError\n+\n+        proc_cpuinfo_string = cls.open_proc_cpuinfo(prefix)\n+\n+        arch_class = cls.uname_to_cpuinfo[uname_machine]\n+        return arch_class.from_proc_cpuinfo_string(proc_cpuinfo_string)\n+\n+    @classmethod\n+    def open_proc_cpuinfo(cls, prefix=None):\n+        proc_cpuinfo_path = cls.proc_cpuinfo_path\n+        if prefix:\n+            proc_cpuinfo_path = os.path.join(prefix, cls.proc_cpuinfo_path[1:])\n+        proc_cpuinfo_buf = ''\n+        with open(proc_cpuinfo_path, 'r') as proc_cpuinfo_f:\n+            proc_cpuinfo_buf = proc_cpuinfo_f.read()\n+        return proc_cpuinfo_buf"
        },
        {
          "filename": "src/rhsmlib/facts/custom.py",
          "status": "added",
          "additions": 113,
          "deletions": 0,
          "patch": "@@ -0,0 +1,113 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import os\n+import glob\n+import logging\n+\n+from rhsm import ourjson\n+from rhsmlib.facts.collector import FactsCollector\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class CustomFacts(object):\n+    def __init__(self, data=None):\n+        self.data = data\n+\n+    @classmethod\n+    def from_json(cls, json_blob):\n+        custom_facts = cls\n+\n+        try:\n+            data = ourjson.loads(json_blob)\n+        except ValueError:\n+            log.warn(\"Unable to load custom facts file.\")\n+\n+        custom_facts.data = data\n+        return custom_facts\n+\n+    def __iter__(self):\n+        return iter(self.data.items())\n+\n+\n+class CustomFactsFileError(Exception):\n+    pass\n+\n+\n+class CustomFactsFile(object):\n+    def __init__(self, path=None):\n+        self.path = path\n+        self.buf = None\n+\n+    def _open_and_read(self):\n+        try:\n+            with open(self.path, 'r') as fd:\n+                return fd.read()\n+        except IOError:\n+            log.warn(\"Unable to open custom facts file: %s\" % self.path)\n+            raise\n+\n+    def read(self):\n+        custom_facts_data = self._open_and_read()\n+        return custom_facts_data\n+\n+    def close(self):\n+        pass\n+\n+\n+class CustomFactsDirectory(object):\n+    def __init__(self, path=None, glob_pattern=None):\n+        self.path = path\n+        self.glob_pattern = glob_pattern\n+\n+    def fact_file_path_iterator(self):\n+        facts_file_glob = os.path.join(self.path, self.glob_pattern)\n+        return glob.iglob(facts_file_glob)\n+\n+    def fact_file_iterator(self, fact_file_path_iterator):\n+        for fact_file_path in fact_file_path_iterator:\n+            log.info(\"Loading custom facts from: %s\" % fact_file_path)\n+            yield CustomFactsFile(fact_file_path)\n+\n+    def __iter__(self):\n+        for fact_file in self.fact_file_iterator(self.fact_file_path_iterator()):\n+            yield CustomFacts.from_json(fact_file.read())\n+\n+\n+class CustomFactsDirectories(object):\n+    def __init__(self, path_and_globs):\n+        self.path_and_globs = path_and_globs\n+\n+    def __iter__(self):\n+        for path, glob_pattern in self.path_and_globs:\n+            yield CustomFactsDirectory(path, glob_pattern)\n+\n+\n+class CustomFactsCollector(FactsCollector):\n+    def __init__(self, prefix=None, testing=None, collected_hw_info=None,\n+                 path_and_globs=None):\n+        super(CustomFactsCollector, self).__init__(\n+            prefix=prefix,\n+            testing=testing,\n+            collected_hw_info=collected_hw_info\n+        )\n+        self.path_and_globs = path_and_globs\n+        self.facts_directories = CustomFactsDirectories(self.path_and_globs)\n+\n+    def get_all(self):\n+        facts_dict = {}\n+        for facts_dir in self.facts_directories:\n+            for custom_facts in facts_dir:\n+                facts_dict.update(custom_facts.data)\n+        return facts_dict"
        },
        {
          "filename": "src/rhsmlib/facts/dmiinfo.py",
          "status": "added",
          "additions": 119,
          "deletions": 0,
          "patch": "@@ -0,0 +1,119 @@\n+# Copyright (c) 2010-2013 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+\"\"\"Load and collect DMI data.\n+\n+Note: This module will fail to import if dmidecode fails to import.\n+      firmware_info.py expects that and handles it, and any other\n+      module that imports it should handle an import error as well.\"\"\"\n+\n+import gettext\n+import logging\n+import os\n+\n+log = logging.getLogger(__name__)\n+\n+try:\n+    import dmidecode\n+except ImportError:\n+    log.warn(\"Unable to load dmidecode module. No DMI info will be collected\")\n+    raise\n+\n+from rhsmlib.facts import collector\n+\n+_ = gettext.gettext\n+\n+FIRMWARE_DUMP_FILENAME = \"dmi.dump\"\n+\n+\n+class DmiFirmwareInfoCollector(collector.FactsCollector):\n+    def __init__(self, prefix=None, testing=None, collected_hw_info=None):\n+        super(DmiFirmwareInfoCollector, self).__init__(\n+            prefix=prefix,\n+            testing=testing,\n+            collected_hw_info=collected_hw_info\n+        )\n+\n+        self._socket_designation = []\n+        self._socket_counter = 0\n+\n+        self.dump_file = None\n+        if testing and prefix:\n+            self.dump_file = os.path.join(prefix, FIRMWARE_DUMP_FILENAME)\n+            self.use_dump_file()\n+\n+    def use_dump_file(self):\n+        \"\"\"Set this instances to use a dmidecode dump file.\n+\n+        WARNING: This involves settings a module global\n+        attribute in 'dmidecode', not just for this class\n+        or object, but for the lifetime of the dmidecode module.\n+\n+        To 'unset' it, it can be set back to '/dev/mem', or\n+        re set it to another dump file.\"\"\"\n+        if os.access(self.dump_file, os.R_OK):\n+            dmidecode.set_dev(self.dump_file)\n+\n+    # This needs all of the previously collected hwinfo, so it can decide\n+    # what is bogus enough that the DMI info is better.\n+    def get_all(self):\n+        dmiinfo = {}\n+        dmi_data = {\n+            \"dmi.bios.\": self._read_dmi(dmidecode.bios),\n+            \"dmi.processor.\": self._read_dmi(dmidecode.processor),\n+            \"dmi.baseboard.\": self._read_dmi(dmidecode.baseboard),\n+            \"dmi.chassis.\": self._read_dmi(dmidecode.chassis),\n+            \"dmi.slot.\": self._read_dmi(dmidecode.slot),\n+            \"dmi.system.\": self._read_dmi(dmidecode.system),\n+            \"dmi.memory.\": self._read_dmi(dmidecode.memory),\n+            \"dmi.connector.\": self._read_dmi(dmidecode.connector),\n+        }\n+\n+        try:\n+            for tag, func in dmi_data.items():\n+                dmiinfo = self._get_dmi_data(func, tag, dmiinfo)\n+        except Exception as e:\n+            log.warn(_(\"Error reading system DMI information: %s\"), e)\n+\n+        return dmiinfo\n+\n+    def _read_dmi(self, func):\n+        try:\n+            return func()\n+        except Exception as e:\n+            log.warn(_(\"Error reading system DMI information with %s: %s\"), func, e)\n+            return None\n+\n+    def _get_dmi_data(self, func, tag, ddict):\n+        for key, value in func.items():\n+            for key1, value1 in value['data'].items():\n+                # FIXME: this loses useful data...\n+                if not isinstance(value1, str):\n+                    # we are skipping things like int and bool values, as\n+                    # well as lists and dicts\n+                    continue\n+\n+                # keep track of any cpu socket info we find, we have to do\n+                # it here, since we flatten it and lose the info creating nkey\n+                if tag == 'dmi.processor.' and key1 == 'Socket Designation':\n+                    self._socket_designation.append(value1)\n+\n+                nkey = ''.join([tag, key1.lower()]).replace(\" \", \"_\")\n+                ddict[nkey] = str(value1)\n+\n+        # Populate how many socket descriptions we saw in a faux-fact, so we can\n+        # use it to munge lscpu info later if needed.\n+        if self._socket_designation:\n+            ddict['dmi.meta.cpu_socket_count'] = str(len(self._socket_designation))\n+\n+        return ddict"
        },
        {
          "filename": "src/rhsmlib/facts/firmware_info.py",
          "status": "added",
          "additions": 111,
          "deletions": 0,
          "patch": "@@ -0,0 +1,111 @@\n+#\n+# Get the right platform specific provider or a null provider\n+#\n+# Copyright (c) 2010-2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import logging\n+\n+# The dmiinfo module will raise a ImportError if the 'dmidecode' module\n+# fails to import. So expect that.\n+try:\n+    from rhsmlib.facts import dmiinfo\n+except ImportError as e:\n+    dmiinfo = None\n+\n+from rhsmlib.facts import collector\n+\n+ARCHES_WITHOUT_DMI = [\"ppc64\", \"ppc64le\", \"s390x\"]\n+\n+log = logging.getLogger(__name__)\n+\n+\n+# This doesn't really do anything other than provide a null/noop provider for\n+# non-DMI platforms.\n+class NullFirmwareInfoCollector(object):\n+    \"\"\"Default provider for platform without a specific platform info provider.\n+\n+    ie, all platforms except those with DMI (ie, intel platforms)\"\"\"\n+    def __init__(self, *args, **kwargs):\n+        self.info = {}\n+\n+    def get_all(self):\n+        return self.info\n+\n+\n+class FirmwareCollector(collector.FactsCollector):\n+    def __init__(self, prefix=None, testing=None, collected_hw_info=None):\n+        super(FirmwareCollector, self).__init__(\n+            prefix=prefix,\n+            testing=testing,\n+            collected_hw_info=collected_hw_info\n+        )\n+\n+    def get_firmware_info(self):\n+        \"\"\"Read and parse data that comes from platform specific interfaces.\n+\n+        This is only dmi/smbios data for now (which isn't on ppc/s390).\n+        \"\"\"\n+        firmware_info_collector = get_firmware_collector(\n+            arch=self.arch,\n+            prefix=self.prefix,\n+            testing=self.testing\n+        )\n+\n+        # Pass in collected hardware so DMI etc can potentially override it\n+        firmware_info_dict = firmware_info_collector.get_all()\n+        # This can potentially clobber facts that already existed in self.allhw\n+        # (and is supposed to).\n+        return firmware_info_dict\n+\n+    def get_all(self):\n+        virt_info = {}\n+        firmware_info = self.get_firmware_info()\n+\n+        virt_info.update(firmware_info)\n+        return virt_info\n+\n+\n+# TODO/FIXME: As a first pass, move dmi and the generic firmware code here,\n+#             even though with kernels with sysfs dmi support, and a recent\n+#             version of dmidecode (> 3.0), most of the dmi info is readable\n+#             by a user. However, the system-uuid is not readable by a user,\n+#             and that is pretty much the only thing from DMI we care about,\n+def get_firmware_collector(arch, prefix=None, testing=None,\n+                           collected_hw_info=None):\n+    \"\"\"\n+    Return a class that can be used to get firmware info specific to\n+    this systems platform.\n+\n+    ie, DmiFirmwareInfoProvider on intel platforms, and a\n+    NullFirmwareInfoProvider otherwise.\n+    \"\"\"\n+    # we could potential consider /proc/sysinfo as a FirmwareInfoProvider\n+    # but at the moment, it is just firmware/dmi stuff.\n+\n+    if arch in ARCHES_WITHOUT_DMI:\n+        log.debug(\"Not looking for DMI info since it is not available on '%s'\" % arch)\n+        firmware_provider_class = NullFirmwareInfoCollector\n+    else:\n+        if dmiinfo:\n+            firmware_provider_class = dmiinfo.DmiFirmwareInfoCollector\n+        else:\n+            firmware_provider_class = NullFirmwareInfoCollector\n+\n+    firmware_provider = firmware_provider_class(\n+        prefix=prefix,\n+        testing=testing,\n+        collected_hw_info=collected_hw_info\n+    )\n+\n+    return firmware_provider"
        },
        {
          "filename": "src/rhsmlib/facts/host_collector.py",
          "status": "added",
          "additions": 105,
          "deletions": 0,
          "patch": "@@ -0,0 +1,105 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import os\n+\n+import rhsm.config\n+\n+from rhsmlib.facts import hwprobe\n+from rhsmlib.facts import cleanup\n+from rhsmlib.facts import custom\n+from rhsmlib.facts import virt\n+from rhsmlib.facts import firmware_info\n+\n+\n+from rhsmlib.facts import collector\n+\n+\n+class HostCollector(collector.FactsCollector):\n+    \"\"\"Collect facts for a host system.\n+\n+    'host' in this case means approx something running\n+    a single kernel image. ie, regular x86_64 hardware, a KVM\n+    virt guest, a ppc64 lpar guest. And not a cluster, or\n+    a container, or an installroot/chroot/mock, or an application,\n+    or a data center, or a distributed computing framework, or\n+    a non-linux hypervisor, etc.\n+\n+    This in turns runs:\n+        hwprobe.HardwareCollector()      [regular hardware facts]\n+        virt.VirtCollector()    [virt facts, results from virt-what etc]\n+        firmware_info.FirmwareCollector()  [dmiinfo, devicetree, etc]\n+        cleanup.CleanupCollector()  [Collapse redundant facts, alter any\n+                                     facts that depend on output of other facts, etc]\n+\n+\n+    Facts collected include DMI info and virt status and virt.uuid.\"\"\"\n+\n+    facts_sub_dir = 'facts'\n+    facts_glob = '*.facts'\n+\n+    def get_all(self):\n+        host_facts = {}\n+        hardware_collector = hwprobe.HardwareCollector(\n+            prefix=self.prefix,\n+            testing=self.testing,\n+            collected_hw_info=self._collected_hw_info\n+        )\n+        hardware_info = hardware_collector.get_all()\n+\n+        virt_collector = virt.VirtCollector(\n+            prefix=self.prefix,\n+            testing=self.testing,\n+            collected_hw_info=self._collected_hw_info\n+        )\n+\n+        virt_collector_info = virt_collector.get_all()\n+\n+        firmware_collector = firmware_info.FirmwareCollector(\n+            prefix=self.prefix,\n+            testing=self.testing,\n+            collected_hw_info=virt_collector_info\n+        )\n+\n+        # rename firmware.py\n+        firmware_info_dict = firmware_collector.get_all()\n+\n+        host_facts.update(hardware_info)\n+        host_facts.update(virt_collector_info)\n+        host_facts.update(firmware_info_dict)\n+\n+        default_rhsm_dir = rhsm.config.DEFAULT_CONFIG_DIR.rstrip('/')\n+        custom_facts_dir = os.path.join(default_rhsm_dir, self.facts_sub_dir)\n+        path_and_globs = [(custom_facts_dir, self.facts_glob)]\n+\n+        custom_facts = custom.CustomFactsCollector(\n+            prefix=self.prefix,\n+            testing=self.testing,\n+            collected_hw_info=self._collected_hw_info,\n+            path_and_globs=path_and_globs\n+        )\n+        custom_facts_dict = custom_facts.get_all()\n+        host_facts.update(custom_facts_dict)\n+\n+        # Now, munging, kluges, special cases, etc\n+        # NOTE: we are passing the facts we've already collected into\n+        # cleanup_collector.\n+        cleanup_collector = cleanup.CleanupCollector(\n+            prefix=self.prefix,\n+            testing=self.testing,\n+            collected_hw_info=host_facts\n+        )\n+        cleanup_info = cleanup_collector.get_all()\n+\n+        host_facts.update(cleanup_info)\n+        return host_facts"
        },
        {
          "filename": "src/rhsmlib/facts/hwprobe.py",
          "status": "added",
          "additions": 778,
          "deletions": 0,
          "patch": "@@ -0,0 +1,778 @@\n+#\n+# Module to probe Hardware info from the system\n+#\n+# Copyright (c) 2010 Red Hat, Inc.\n+#\n+# Authors: Pradeep Kilambi <pkilambi@redhat.com>\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+from __future__ import print_function\n+\n+import gettext\n+import logging\n+import os\n+import platform\n+import re\n+import socket\n+import sys\n+\n+from rhsmlib.facts import cpuinfo\n+from rhsmlib.facts import collector\n+\n+_ = gettext.gettext\n+\n+log = logging.getLogger(__name__)\n+\n+# There is no python3 version of python-ethtool\n+ethtool = None\n+try:\n+    import ethtool\n+except ImportError as e:\n+    log.warn(\"Unable to import the 'ethtool' module.\")\n+\n+# For python2.6 that doesn't have subprocess.check_output\n+from rhsmlib.compat import check_output as compat_check_output\n+from subprocess import CalledProcessError\n+\n+\n+class ClassicCheck:\n+    def is_registered_with_classic(self):\n+        try:\n+            sys.path.append('/usr/share/rhn')\n+            from up2date_client import up2dateAuth\n+        except ImportError:\n+            return False\n+\n+        return up2dateAuth.getSystemId() is not None\n+\n+\n+# take a string like '1-4' and returns a list of\n+# ints like [1,2,3,4]\n+# 31-37 return [31,32,33,34,35,36,37]\n+def parse_range(range_str):\n+    range_list = range_str.split('-')\n+    start = int(range_list[0])\n+    end = int(range_list[-1])\n+\n+    return range(start, end + 1)\n+\n+\n+# util to total up the values represented by a cpu siblings list\n+# ala /sys/devices/cpu/cpu0/topology/core_siblings_list\n+#\n+# which can be a comma separated list of ranges\n+#  1,2,3,4\n+#  1-2, 4-6, 8-10, 12-14\n+#\n+def gather_entries(entries_string):\n+    entries = []\n+    entry_parts = entries_string.split(',')\n+    for entry_part in entry_parts:\n+        # return a list of enumerated items\n+        entry_range = parse_range(entry_part)\n+        for entry in entry_range:\n+            entries.append(entry)\n+    return entries\n+\n+\n+class GenericPlatformSpecificInfoProvider(object):\n+    \"\"\"Default provider for platform without a specific platform info provider.\n+\n+    ie, all platforms except those with DMI (ie, intel platforms)\"\"\"\n+    def __init__(self, hardware_info, dump_file=None):\n+        self.info = {}\n+\n+    @staticmethod\n+    def log_warnings():\n+        pass\n+\n+\n+class HardwareCollector(collector.FactsCollector):\n+\n+    def __init__(self, arch=None, prefix=None, testing=None,\n+                 collected_hw_info=None):\n+        super(HardwareCollector, self).__init__(arch=arch, prefix=prefix,\n+                                       testing=testing,\n+                                       collected_hw_info=None)\n+\n+        self.hardware_methods = [\n+            self.get_uname_info,\n+            self.get_release_info,\n+            self.get_mem_info,\n+            self.get_proc_cpuinfo,\n+            self.get_cpu_info,\n+            self.get_ls_cpu_info,\n+            self.get_network_info,\n+            self.get_network_interfaces,\n+        ]\n+\n+    def get_uname_info(self):\n+        uname_info = {}\n+        uname_data = os.uname()\n+        uname_keys = (\n+            'uname.sysname',\n+            'uname.nodename',\n+            'uname.release',\n+            'uname.version',\n+            'uname.machine',\n+        )\n+        uname_info = dict(zip(uname_keys, uname_data))\n+        return uname_info\n+\n+    def get_release_info(self):\n+        distro_info = self.get_distribution()\n+        release_info = {\n+            'distribution.name': distro_info[0],\n+            'distribution.version': distro_info[1],\n+            'distribution.id': distro_info[2],\n+            'distribution.version.modifier': distro_info[3],\n+        }\n+        return release_info\n+\n+    def _open_release(self, filename):\n+        return open(filename, 'r')\n+\n+    # this version os very RHEL/Fedora specific...\n+    def get_distribution(self):\n+\n+        version = 'Unknown'\n+        distname = 'Unknown'\n+        dist_id = 'Unknown'\n+        version_modifier = ''\n+\n+        if os.path.exists('/etc/os-release'):\n+            f = open('/etc/os-release', 'r')\n+            os_release = f.readlines()\n+            f.close()\n+            data = {\n+                'PRETTY_NAME': 'Unknown',\n+                'NAME': distname,\n+                'ID': 'Unknown',\n+                'VERSION': dist_id,\n+                'VERSION_ID': version,\n+                'CPE_NAME': 'Unknown',\n+            }\n+            for line in os_release:\n+                split = map(lambda piece: piece.strip('\"\\n '), line.split('='))\n+                if len(split) != 2:\n+                    continue\n+                data[split[0]] = split[1]\n+\n+            version = data['VERSION_ID']\n+            distname = data['NAME']\n+            dist_id = data['VERSION']\n+            dist_id_search = re.search('\\((.*?)\\)', dist_id)\n+            if dist_id_search:\n+                dist_id = dist_id_search.group(1)\n+            # Split on ':' that is not preceded by '\\'\n+            vers_mod_data = re.split('(?<!\\\\\\):', data['CPE_NAME'])\n+            if len(vers_mod_data) >= 6:\n+                version_modifier = vers_mod_data[5].lower().replace('\\\\:', ':')\n+        elif os.path.exists('/etc/redhat-release'):\n+            # from platform.py from python2.\n+            _lsb_release_version = re.compile(r'(.+) release ([\\d.]+)\\s*(?!\\()(\\S*)\\s*[^(]*(?:\\((.+)\\))?')\n+            f = self._open_release('/etc/redhat-release')\n+            firstline = f.readline()\n+            f.close()\n+\n+            m = _lsb_release_version.match(firstline)\n+\n+            if m is not None:\n+                (distname, version, tmp_modifier, dist_id) = tuple(m.groups())\n+                if tmp_modifier:\n+                    version_modifier = tmp_modifier.lower()\n+\n+        elif hasattr(platform, 'linux_distribution'):\n+            (distname, version, dist_id) = platform.linux_distribution()\n+            version_modifier = 'Unknown'\n+\n+        return distname, version, dist_id, version_modifier\n+\n+    def get_mem_info(self):\n+        meminfo = {}\n+\n+        # most of this mem info changes constantly, which makes decding\n+        # when to update facts painful, so lets try to just collect the\n+        # useful bits\n+\n+        useful = [\"MemTotal\", \"SwapTotal\"]\n+        try:\n+            parser = re.compile(r'^(?P<key>\\S*):\\s*(?P<value>\\d*)\\s*kB')\n+            memdata = open('/proc/meminfo')\n+            for info in memdata:\n+                match = parser.match(info)\n+                if not match:\n+                    continue\n+                key, value = match.groups(['key', 'value'])\n+                if key in useful:\n+                    nkey = '.'.join([\"memory\", key.lower()])\n+                    meminfo[nkey] = \"%s\" % int(value)\n+        except Exception as e:\n+            log.warn(\"Error reading system memory information: %s\", e)\n+        return meminfo\n+\n+    def count_cpumask_entries(self, cpu, field):\n+        try:\n+            f = open(\"%s/topology/%s\" % (cpu, field), 'r')\n+        except IOError:\n+            return None\n+\n+        # ia64 entries seem to be null padded, or perhaps\n+        # that's a collection error\n+        # FIXME\n+        entries = f.read().rstrip('\\n\\x00')\n+        f.close()\n+        # these fields can exist, but be empty. For example,\n+        # thread_siblings_list from s390x-rhel64-zvm-2cpu-has-topo\n+        # test data\n+\n+        if len(entries):\n+            cpumask_entries = gather_entries(entries)\n+            return len(cpumask_entries)\n+        # that field was empty\n+        return None\n+\n+    # replace/add with getting CPU Totals for s390x\n+    def _parse_s390x_sysinfo_topology(self, cpu_count, sysinfo):\n+        # to quote lscpu.c:\n+        # CPU Topology SW:      0 0 0 4 6 4\n+        # /* s390 detects its cpu topology via /proc/sysinfo, if present.\n+        # * Using simply the cpu topology masks in sysfs will not give\n+        # * usable results since everything is virtualized. E.g.\n+        # * virtual core 0 may have only 1 cpu, but virtual core 2 may\n+        # * five cpus.\n+        # * If the cpu topology is not exported (e.g. 2nd level guest)\n+        # * fall back to old calculation scheme.\n+        # */\n+        for line in sysinfo:\n+            if line.startswith(\"CPU Topology SW:\"):\n+                parts = line.split(':', 1)\n+                s390_topo_str = parts[1]\n+                topo_parts = s390_topo_str.split()\n+\n+                # indexes 3/4/5 being books/sockets_per_book,\n+                # and cores_per_socket based on lscpu.c\n+                book_count = int(topo_parts[3])\n+                sockets_per_book = int(topo_parts[4])\n+                cores_per_socket = int(topo_parts[5])\n+\n+                socket_count = book_count * sockets_per_book\n+                cores_count = socket_count * cores_per_socket\n+\n+                return {\n+                    'socket_count': socket_count,\n+                    'cores_count': cores_count,\n+                    'book_count': book_count,\n+                    'sockets_per_book': sockets_per_book,\n+                    'cores_per_socket': cores_per_socket\n+                }\n+        log.debug(\"Looking for 'CPU Topology SW' in sysinfo, but it was not found\")\n+        return None\n+\n+    def has_s390x_sysinfo(self, proc_sysinfo):\n+        return os.access(proc_sysinfo, os.R_OK)\n+\n+    def read_s390x_sysinfo(self, cpu_count, proc_sysinfo):\n+        lines = []\n+        try:\n+            f = open(proc_sysinfo, 'r')\n+        except IOError:\n+            return lines\n+\n+        lines = f.readlines()\n+        f.close()\n+        return lines\n+\n+    def read_physical_id(self, cpu_file):\n+        try:\n+            f = open(\"%s/physical_id\" % cpu_file, 'r')\n+        except IOError:\n+            return None\n+\n+        buf = f.read().strip()\n+        f.close()\n+        return buf\n+\n+    def _ppc64_fallback(self, cpu_files):\n+\n+        # ppc64, particular POWER5/POWER6 machines, show almost\n+        # no cpu information on rhel5. There is a \"physical_id\"\n+        # associated with each cpu that seems to map to a\n+        # cpu, in a socket\n+        log.debug(\"trying ppc64 specific cpu topology detection\")\n+        # try to find cpuN/physical_id\n+        physical_ids = set()\n+        for cpu_file in cpu_files:\n+            physical_id = self.read_physical_id(cpu_file)\n+            # offline cpu's show physical id of -1. Normally\n+            # we count all present cpu's even if offline, but\n+            # in this case, we can't get any cpu info from the\n+            # cpu since it is offline, so don't count it\n+            if physical_id != '-1':\n+                physical_ids.add(physical_id)\n+\n+        if physical_ids:\n+            # For rhel6 or newer, we have more cpu topology info\n+            # exposed by the kernel which will override this\n+            socket_count = len(physical_ids)\n+            # add marker here so we know we fail back to this\n+            log.debug(\"Using /sys/devices/system/cpu/cpu*/physical_id for cpu info on ppc64\")\n+            return socket_count\n+\n+        return None\n+\n+    def check_for_cpu_topo(self, cpu_topo_dir):\n+        return os.access(cpu_topo_dir, os.R_OK)\n+\n+    def get_proc_cpuinfo(self):\n+        proc_cpuinfo = {}\n+        fact_namespace = 'proc_cpuinfo'\n+\n+        proc_cpuinfo_source = cpuinfo.SystemCpuInfoFactory.from_uname_machine(\n+            self.arch,\n+            prefix=self.prefix\n+        )\n+\n+        for key, value in proc_cpuinfo_source.cpu_info.common.items():\n+            proc_cpuinfo['%s.common.%s' % (fact_namespace, key)] = value\n+\n+        # NOTE: cpu_info.other is a potentially ordered non-uniq list, so may\n+        # not make sense for shoving into a list.\n+        for key, value in proc_cpuinfo_source.cpu_info.other:\n+            proc_cpuinfo['%s.system.%s' % (fact_namespace, key)] = value\n+\n+        # we could enumerate each processor here as proc_cpuinfo.cpu.3.key =\n+        # value, but that is a lot of fact table entries\n+        return proc_cpuinfo\n+\n+    def get_cpu_info(self):\n+        cpu_info = {}\n+        # we also have cpufreq, etc in this dir, so match just the numbs\n+        cpu_re = r'cpu([0-9]+$)'\n+\n+        cpu_files = []\n+        sys_cpu_path = self.prefix + \"/sys/devices/system/cpu/\"\n+        for cpu in os.listdir(sys_cpu_path):\n+            if re.match(cpu_re, cpu):\n+                cpu_topo_dir = os.path.join(sys_cpu_path, cpu, \"topology\")\n+\n+                # see rhbz#1070908\n+                # ppc64 machines running on LPARs will add\n+                # a sys cpu entry for every cpu thread on the\n+                # physical machine, regardless of how many are\n+                # allocated to the LPAR. This throws off the cpu\n+                # thread count, which throws off the cpu socket count.\n+                # The entries for the unallocated or offline cpus\n+                # do not have topology info however.\n+                # So, skip sys cpu entries without topology info.\n+                #\n+                # NOTE: this assumes RHEL6+, prior to rhel5, on\n+                # some arches like ppc and s390, there is no topology\n+                # info ever, so this will break.\n+                if self.check_for_cpu_topo(cpu_topo_dir):\n+                    cpu_files.append(\"%s/%s\" % (sys_cpu_path, cpu))\n+\n+        # for systems with no cpus\n+        if not cpu_files:\n+            return cpu_info\n+\n+        cpu_count = len(cpu_files)\n+\n+        # see if we have a /proc/sysinfo ala s390, if so\n+        # prefer that info\n+        proc_sysinfo = self.prefix + \"/proc/sysinfo\"\n+        has_sysinfo = self.has_s390x_sysinfo(proc_sysinfo)\n+\n+        # s390x can have cpu 'books'\n+        books = False\n+\n+        cores_per_socket = None\n+\n+        # assume each socket has the same number of cores, and\n+        # each core has the same number of threads.\n+        #\n+        # This is not actually true sometimes... *cough*s390x*cough*\n+        # but lscpu makes the same assumption\n+\n+        threads_per_core = self.count_cpumask_entries(cpu_files[0], 'thread_siblings_list')\n+        cores_per_cpu = self.count_cpumask_entries(cpu_files[0], 'core_siblings_list')\n+\n+        # if we find valid values in cpu/cpuN/topology/*siblings_list\n+        # sometimes it's not there, particularly on rhel5\n+        if threads_per_core and cores_per_cpu:\n+            cores_per_socket = cores_per_cpu / threads_per_core\n+            cpu_info[\"cpu.topology_source\"] = \"kernel /sys cpu sibling lists\"\n+\n+            # rhel6 s390x can have /sys cpu topo, but we can't make assumption\n+            # about it being evenly distributed, so if we also have topo info\n+            # in sysinfo, prefer that\n+            if self.arch == \"s390x\" and has_sysinfo:\n+                # for s390x on lpar, try to see if /proc/sysinfo has any\n+                # topo info\n+                log.debug(\"/proc/sysinfo found, attempting to gather cpu topology info\")\n+                sysinfo_lines = self.read_s390x_sysinfo(cpu_count, proc_sysinfo)\n+                if sysinfo_lines:\n+                    sysinfo = self._parse_s390x_sysinfo_topology(cpu_count, sysinfo_lines)\n+\n+                    # verify the sysinfo has system level virt info\n+                    if sysinfo:\n+                        cpu_info[\"cpu.topology_source\"] = \"s390x sysinfo\"\n+                        socket_count = sysinfo['socket_count']\n+                        book_count = sysinfo['book_count']\n+                        sockets_per_book = sysinfo['sockets_per_book']\n+                        cores_per_socket = sysinfo['cores_per_socket']\n+                        threads_per_core = 1\n+\n+                        # we can have a mismatch between /sys and /sysinfo. We\n+                        # defer to sysinfo in this case even for cpu_count\n+                        # cpu_count = sysinfo['cores_count'] * threads_per_core\n+                        books = True\n+\n+        else:\n+            # we have found no valid socket information, I only know\n+            # the number of cpu's, but no threads, no cores, no sockets\n+            log.debug(\"No cpu socket information found\")\n+\n+            # how do we get here?\n+            #   no cpu topology info, ala s390x on rhel5,\n+            #   no sysinfo topology info, ala s390x with zvm on rhel5\n+            # we have no great topo info here,\n+            # assume each cpu thread = 1 core = 1 socket\n+            threads_per_core = 1\n+            cores_per_cpu = 1\n+            cores_per_socket = 1\n+            socket_count = None\n+\n+            # lets try some arch/platform specific approaches\n+            if self.arch == \"ppc64\":\n+                socket_count = self._ppc64_fallback(cpu_files)\n+\n+                if socket_count:\n+                    log.debug(\"Using ppc64 cpu physical id for cpu topology info\")\n+                    cpu_info[\"cpu.topology_source\"] = \"ppc64 physical_package_id\"\n+\n+            else:\n+                # all of our usual methods failed us...\n+                log.debug(\"No cpu socket info found for real or virtual hardware\")\n+                # so we can track if we get this far\n+                cpu_info[\"cpu.topology_source\"] = \"fallback one socket\"\n+                socket_count = cpu_count\n+\n+            # for some odd cases where there are offline ppc64 cpu's,\n+            # this can end up not being a whole number...\n+            cores_per_socket = cpu_count / socket_count\n+\n+        if cores_per_socket and threads_per_core:\n+            # for s390x with sysinfo topo, we use the sysinfo numbers except\n+            # for cpu_count, which takes offline cpus into account. This is\n+            # mostly just to match lscpu behaviour here\n+            if cpu_info[\"cpu.topology_source\"] != \"s390x sysinfo\":\n+                socket_count = cpu_count / cores_per_socket / threads_per_core\n+\n+        # s390 etc\n+        # for s390, socket calculations are per book, and we can have multiple\n+        # books, so multiply socket count by book count\n+        # see if we are on a s390 with book info\n+        # all s390 platforms show book siblings, even the ones that also\n+        # show sysinfo (lpar)... Except on rhel5, where there is no\n+        # cpu topology info with lpar\n+        #\n+        # if we got book info from sysinfo, prefer it\n+        book_siblings_per_cpu = None\n+        if not books:\n+            book_siblings_per_cpu = self.count_cpumask_entries(cpu_files[0], 'book_siblings_list')\n+            if book_siblings_per_cpu:\n+                book_count = cpu_count / book_siblings_per_cpu\n+                sockets_per_book = book_count / socket_count\n+                cpu_info[\"cpu.topology_source\"] = \"s390 book_siblings_list\"\n+                books = True\n+\n+        # we should always know this...\n+        cpu_info[\"cpu.cpu(s)\"] = cpu_count\n+\n+        # these may be unknown...\n+        if socket_count:\n+            cpu_info['cpu.cpu_socket(s)'] = socket_count\n+        if cores_per_socket:\n+            cpu_info['cpu.core(s)_per_socket'] = cores_per_socket\n+        if threads_per_core:\n+            cpu_info[\"cpu.thread(s)_per_core\"] = threads_per_core\n+\n+        if book_siblings_per_cpu:\n+            cpu_info[\"cpu.book(s)_per_cpu\"] = book_siblings_per_cpu\n+\n+        if books:\n+            cpu_info[\"cpu.socket(s)_per_book\"] = sockets_per_book\n+            cpu_info[\"cpu.book(s)\"] = book_count\n+\n+        return cpu_info\n+\n+    def get_ls_cpu_info(self):\n+        lscpu_info = {}\n+\n+        LSCPU_CMD = '/usr/bin/lscpu'\n+\n+        # if we have `lscpu`, let's use it for facts as well, under\n+        # the `lscpu` name space\n+        if not os.access(LSCPU_CMD, os.R_OK):\n+            return lscpu_info\n+\n+        # copy of parent process environment\n+        parent_env = dict(os.environ)\n+\n+        # let us specify a test dir of /sys info for testing\n+        # If the user env sets LC_ALL, it overrides a LANG here, so\n+        # use LC_ALL here. See rhbz#1225435\n+        lscpu_env = parent_env.update({'LC_ALL': 'en_US.UTF-8'})\n+        lscpu_cmd = [LSCPU_CMD]\n+\n+        if self.testing:\n+            lscpu_cmd += ['-s', self.prefix]\n+\n+        # For display/message only\n+        lscpu_cmd_string = ' '.join(lscpu_cmd)\n+\n+        try:\n+            lscpu_out = compat_check_output(lscpu_cmd,\n+                                            env=lscpu_env)\n+        except CalledProcessError as e:\n+            log.exception(e)\n+            log.warn('Error with lscpu (%s) subprocess: %s', lscpu_cmd_string, e)\n+            return lscpu_info\n+\n+        errors = []\n+        try:\n+            cpu_data = lscpu_out.strip().split('\\n')\n+            for info in cpu_data:\n+                try:\n+                    key, value = info.split(\":\")\n+                    nkey = '.'.join([\"lscpu\", key.lower().strip().replace(\" \", \"_\")])\n+                    lscpu_info[nkey] = \"%s\" % value.strip()\n+                except ValueError as e:\n+                    # sometimes lscpu outputs weird things. Or fails.\n+                    # But this is per line, so keep track but let it pass.\n+                    errors.append(e)\n+\n+        except Exception as e:\n+            log.warn('Error reading system CPU information: %s', e)\n+        if errors:\n+            log.debug('Errors while parsing lscpu output: %s', errors)\n+\n+        return lscpu_info\n+\n+    def get_network_info(self):\n+        netinfo = {}\n+        try:\n+            host = socket.gethostname()\n+            netinfo['network.hostname'] = host\n+\n+            try:\n+                info = socket.getaddrinfo(host, None, socket.AF_INET, socket.SOCK_STREAM)\n+                ip_list = set([x[4][0] for x in info])\n+                netinfo['network.ipv4_address'] = ', '.join(ip_list)\n+            except Exception:\n+                netinfo['network.ipv4_address'] = \"127.0.0.1\"\n+\n+            try:\n+                info = socket.getaddrinfo(host, None, socket.AF_INET6, socket.SOCK_STREAM)\n+                ip_list = set([x[4][0] for x in info])\n+                netinfo['network.ipv6_address'] = ', '.join(ip_list)\n+            except Exception:\n+                netinfo['network.ipv6_address'] = \"::1\"\n+\n+        except Exception as e:\n+            log.warn('Error reading networking information: %s', e)\n+\n+        return netinfo\n+\n+    def _should_get_mac_address(self, device):\n+        return not (device.startswith('sit') or device.startswith('lo'))\n+\n+    def get_network_interfaces(self):\n+        netinfdict = {}\n+        old_ipv4_metakeys = ['ipv4_address', 'ipv4_netmask', 'ipv4_broadcast']\n+        ipv4_metakeys = ['address', 'netmask', 'broadcast']\n+        ipv6_metakeys = ['address', 'netmask']\n+        try:\n+            interfaces_info = ethtool.get_interfaces_info(ethtool.get_devices())\n+            for info in interfaces_info:\n+                master = None\n+                mac_address = info.mac_address\n+                device = info.device\n+                # Omit mac addresses for sit and lo device types. See BZ838123\n+                # mac address are per interface, not per address\n+                if self._should_get_mac_address(device):\n+                    key = '.'.join(['net.interface', device, 'mac_address'])\n+                    netinfdict[key] = mac_address\n+\n+                # all of our supported versions of python-ethtool support\n+                # get_ipv6_addresses\n+                for addr in info.get_ipv6_addresses():\n+                    # ethtool returns a different scope for \"public\" IPv6 addresses\n+                    # on different versions of RHEL.  EL5 is \"global\", while EL6 is\n+                    # \"universe\".  Make them consistent.\n+                    scope = addr.scope\n+                    if scope == 'universe':\n+                        scope = 'global'\n+\n+                    # FIXME: this doesn't support multiple addresses per interface\n+                    # (it finds them, but collides on the key name and loses all\n+                    # but the last write). See bz #874735\n+                    for mkey in ipv6_metakeys:\n+                        key = '.'.join(['net.interface', info.device, 'ipv6_%s' % (mkey), scope])\n+                        # we could specify a default here... that could hide\n+                        # api breakage though and unit testing hw detect is... meh\n+                        attr = getattr(addr, mkey) or 'Unknown'\n+                        netinfdict[key] = attr\n+\n+                # However, old version of python-ethtool do not support\n+                # get_ipv4_address\n+                #\n+                # python-ethtool's api changed between rhel6.3 and rhel6.4\n+                # (0.6-1.el6 to 0.6-2.el6)\n+                # (without revving the upstream version... bad python-ethtool!)\n+                # note that 0.6-5.el5 (from rhel5.9) has the old api\n+                #\n+                # previously, we got the 'ipv4_address' from the etherinfo object\n+                # directly. In the new api, that isn't exposed, so we get the list\n+                # of addresses on the interface, and populate the info from there.\n+                #\n+                # That api change as to address bz #759150. The bug there was that\n+                # python-ethtool only showed one ip address per interface. To\n+                # accomdate the finer grained info, the api changed...\n+                #\n+                # FIXME: see FIXME for get_ipv6_address, we don't record multiple\n+                # addresses per interface\n+                if hasattr(info, 'get_ipv4_addresses'):\n+                    for addr in info.get_ipv4_addresses():\n+                        for mkey in ipv4_metakeys:\n+                            # append 'ipv4_' to match the older interface and keeps facts\n+                            # consistent\n+                            key = '.'.join(['net.interface', info.device, 'ipv4_%s' % (mkey)])\n+                            attr = getattr(addr, mkey) or 'Unknown'\n+                            netinfdict[key] = attr\n+                # check to see if we are actually an ipv4 interface\n+                elif hasattr(info, 'ipv4_address'):\n+                    for mkey in old_ipv4_metakeys:\n+                        key = '.'.join(['net.interface', device, mkey])\n+                        attr = getattr(info, mkey) or 'Unknown'\n+                        netinfdict[key] = attr\n+                # otherwise we are ipv6 and we handled that already\n+\n+                # bonded slave devices can have their hwaddr changed\n+                #\n+                # \"master\" here refers to the slave's master device.\n+                # If we find a master link, we are a  slave, and we need\n+                # to check the /proc/net/bonding info to see what the\n+                # \"permanent\" hw address are for this slave\n+                try:\n+                    master = os.readlink('/sys/class/net/%s/master' % info.device)\n+                #FIXME\n+                except Exception:\n+                    master = None\n+\n+                if master:\n+                    master_interface = os.path.basename(master)\n+                    permanent_mac_addr = self._get_slave_hwaddr(master_interface, info.device)\n+                    key = '.'.join(['net.interface', info.device, \"permanent_mac_address\"])\n+                    netinfdict[key] = permanent_mac_addr\n+\n+        except Exception as e:\n+            log.exception(e)\n+            log.warn(\"Error reading network interface information: %s\", e)\n+        return netinfdict\n+\n+    # from rhn-client-tools  hardware.py\n+    # see bz#785666\n+    def _get_slave_hwaddr(self, master, slave):\n+        hwaddr = \"\"\n+        try:\n+            bonding = open('/proc/net/bonding/%s' % master, \"r\")\n+        except:\n+            return hwaddr\n+\n+        slave_found = False\n+        for line in bonding.readlines():\n+            if slave_found and line.find(\"Permanent HW addr: \") != -1:\n+                hwaddr = line.split()[3].upper()\n+                break\n+\n+            if line.find(\"Slave Interface: \") != -1:\n+                ifname = line.split()[2]\n+                if ifname == slave:\n+                    slave_found = True\n+\n+        bonding.close()\n+        return hwaddr\n+\n+\n+if __name__ == '__main__':\n+    _LIBPATH = \"/usr/share/rhsm\"\n+    # add to the path if need be\n+    if _LIBPATH not in sys.path:\n+        sys.path.append(_LIBPATH)\n+\n+    from subscription_manager import logutil\n+    logutil.init_logger()\n+\n+    hw = HardwareCollector(prefix=sys.argv[1], testing=True)\n+\n+    if len(sys.argv) > 1:\n+        hw.prefix = sys.argv[1]\n+        hw.testing = True\n+    hw_dict = hw.get_all()\n+\n+    # just show the facts collected, unless we specify data dir and well,\n+    # anything else\n+    if len(sys.argv) > 2:\n+        for hkey, hvalue in sorted(hw_dict.items()):\n+            print(\"'%s' : '%s'\" % (hkey, hvalue))\n+\n+    if not hw.testing:\n+        sys.exit(0)\n+\n+    # verify the cpu socket info collection we use for rhel5 matches lscpu\n+    cpu_items = [\n+        ('cpu.core(s)_per_socket', 'lscpu.core(s)_per_socket'),\n+        ('cpu.cpu(s)', 'lscpu.cpu(s)'),\n+        # NOTE: the substring is different for these two folks...\n+        # FIXME: follow up to see if this has changed\n+        ('cpu.cpu_socket(s)', 'lscpu.socket(s)'),\n+        ('cpu.book(s)', 'lscpu.book(s)'),\n+        ('cpu.thread(s)_per_core', 'lscpu.thread(s)_per_core'),\n+        ('cpu.socket(s)_per_book', 'lscpu.socket(s)_per_book')\n+    ]\n+    failed = False\n+    failed_list = []\n+    for cpu_item in cpu_items:\n+        value_0 = int(hw_dict.get(cpu_item[0], -1))\n+        value_1 = int(hw_dict.get(cpu_item[1], -1))\n+\n+        #print \"%s/%s: %s %s\" % (cpu_item[0], cpu_item[1], value_0, value_1)\n+\n+        if value_0 != value_1 and ((value_0 != -1) and (value_1 != -1)):\n+            failed_list.append((cpu_item[0], cpu_item[1], value_0, value_1))\n+\n+    must_haves = ['cpu.cpu_socket(s)', 'cpu.cpu(s)', 'cpu.core(s)_per_socket', 'cpu.thread(s)_per_core']\n+    missing_set = set(must_haves).difference(set(hw_dict))\n+\n+    if failed:\n+        print(\"cpu detection error\")\n+    for failed in failed_list:\n+        print(\"The values %s %s do not match (|%s| != |%s|)\" % (failed[0], failed[1], failed[2], failed[3]))\n+    if missing_set:\n+        for missing in missing_set:\n+            print(\"cpu info fact: %s was missing\" % missing)\n+\n+    if failed:\n+        sys.exit(1)"
        },
        {
          "filename": "src/rhsmlib/facts/virt.py",
          "status": "added",
          "additions": 156,
          "deletions": 0,
          "patch": "@@ -0,0 +1,156 @@\n+#\n+# Probe hardware info that requires root\n+#\n+# Copyright (c) 2010-2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+\n+import gettext\n+import logging\n+\n+from rhsmlib.facts import collector\n+\n+# For python2.6 that doesn't have subprocess.check_output\n+from rhsmlib.compat import check_output as compat_check_output\n+\n+log = logging.getLogger(__name__)\n+\n+_ = gettext.gettext\n+\n+\n+class VirtWhatCollector(collector.FactsCollector):\n+    def get_all(self):\n+        return self.get_virt_info()\n+\n+    # NOTE/TODO/FIXME: Not all platforms require admin privs to determine virt type or uuid\n+    def get_virt_info(self):\n+        virt_dict = {}\n+\n+        try:\n+            host_type = compat_check_output('virt-what')\n+            # BZ1018807 xen can report xen and xen-hvm.\n+            # Force a single line\n+            host_type = \", \".join(host_type.splitlines())\n+\n+            # If this is blank, then not a guest\n+            virt_dict['virt.is_guest'] = bool(host_type)\n+            if bool(host_type):\n+                virt_dict['virt.is_guest'] = True\n+                virt_dict['virt.host_type'] = host_type\n+            else:\n+                virt_dict['virt.is_guest'] = False\n+                virt_dict['virt.host_type'] = \"Not Applicable\"\n+        # TODO:  Should this only catch OSErrors?\n+        except Exception as e:\n+            # Otherwise there was an error running virt-what - who knows\n+            log.exception(e)\n+            virt_dict['virt.is_guest'] = 'Unknown'\n+\n+        # xen dom0 is a guest for virt-what's purposes, but is a host for\n+        # our purposes. Adjust is_guest accordingly. (#757697)\n+        try:\n+            if virt_dict['virt.host_type'].find('dom0') > -1:\n+                virt_dict['virt.is_guest'] = False\n+        except KeyError:\n+            # if host_type is not defined, do nothing (#768397)\n+            pass\n+\n+        return virt_dict\n+\n+\n+class VirtUuidCollector(collector.FactsCollector):\n+    # Note: unlike system uuid in DMI info, the virt.uuid is\n+    # available to non-root users on ppc64*\n+    # ppc64 LPAR has it's virt.uuid in /proc/devicetree\n+    # so parts of this don't need to be in AdminHardware\n+    devicetree_vm_uuid_arches = ['ppc64', 'ppc64le']\n+\n+    # No virt.uuid equiv is available for guests on these hypervisors\n+    no_uuid_platforms = ['powervm_lx86', 'xen-dom0', 'ibm_systemz']\n+\n+    def get_all(self):\n+        return self.get_virt_uuid()\n+\n+    def get_virt_uuid(self):\n+        \"\"\"\n+        Given a populated fact list, add on a virt.uuid fact if appropriate.\n+        Partially adapted from Spacewalk's rhnreg.py, example hardware reporting\n+        found in virt-what tests\n+        \"\"\"\n+\n+        # For 99% of uses, virt.uuid will actually be from dmi info\n+        # See dmiinfo.py and cleanup.py\n+\n+        virt_uuid_dict = {}\n+\n+        # For ppc64, virt uuid is in /proc/device-tree/vm,uuid\n+        # just the uuid in txt, one line\n+\n+        # ie, ppc64/ppc64le\n+        if self.arch in self.devicetree_vm_uuid_arches:\n+            virt_uuid_dict.update(self._get_devicetree_vm_uuid())\n+\n+        # potentially override DMI-determined UUID with\n+        # what is on the file system (xen para-virt)\n+        # Does this need root access?\n+        try:\n+            uuid_file = open('/sys/hypervisor/uuid', 'r')\n+            uuid = uuid_file.read()\n+            uuid_file.close()\n+            virt_uuid_dict['virt.uuid'] = uuid.rstrip(\"\\r\\n\")\n+        except IOError:\n+            pass\n+\n+        return virt_uuid_dict\n+\n+    def _get_devicetree_vm_uuid(self):\n+        \"\"\"Collect the virt.uuid fact from device-tree/vm,uuid\n+\n+        For ppc64/ppc64le systems running KVM or PowerKVM, the\n+        virt uuid is found in /proc/device-tree/vm,uuid.\n+\n+        (In contrast to use of DMI on x86_64).\"\"\"\n+\n+        virt_dict = {}\n+\n+        vm_uuid_path = \"%s/proc/device-tree/vm,uuid\" % self.prefix\n+\n+        try:\n+            with open(vm_uuid_path) as fo:\n+                contents = fo.read()\n+                vm_uuid = contents.strip()\n+                virt_dict['virt.uuid'] = vm_uuid\n+        except IOError as e:\n+            log.warn(\"Tried to read %s but there was an error: %s\", vm_uuid_path, e)\n+\n+        return virt_dict\n+\n+\n+class VirtCollector(collector.FactsCollector):\n+    def get_all(self):\n+        virt_what_collector = VirtWhatCollector(prefix=self.prefix, testing=self.testing)\n+\n+        virt_what_info = virt_what_collector.get_all()\n+\n+        # Pass virt_what_info to the uuid collector since it needs to know\n+        # what hypervisor the host is.\n+        virt_uuid_collector = VirtUuidCollector(\n+            prefix=self.prefix,\n+            testing=self.testing,\n+            collected_hw_info=virt_what_info\n+        )\n+        virt_uuid_info = virt_uuid_collector.get_all()\n+        virt_info = {}\n+        virt_info.update(virt_what_info)\n+        virt_info.update(virt_uuid_info)\n+        return virt_info"
        },
        {
          "filename": "src/rhsmlib/services/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "src/rhsmlib/services/config.py",
          "status": "added",
          "additions": 141,
          "deletions": 0,
          "patch": "@@ -0,0 +1,141 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+import rhsm.config\n+import collections\n+\n+\n+class Config(collections.MutableMapping):\n+    def __init__(self, parser=None, auto_persist=False):\n+        if parser:\n+            self._parser = parser\n+        else:\n+            self._parser = rhsm.config.initConfig()\n+\n+        self.auto_persist = auto_persist\n+\n+        self._sections = {}\n+        for s in self._parser.sections():\n+            self._sections[s] = ConfigSection(self, self._parser, s, self.auto_persist)\n+        super(Config, self).__init__()\n+\n+    def persist(self):\n+        self._parser.save()\n+\n+    def defaults(self):\n+        return self._parser.defaults()\n+\n+    def __getitem__(self, name):\n+        if name in self:\n+            return self._sections[name]\n+        raise KeyError(\"No configuration section '%s' exists\" % name)\n+\n+    def __setitem__(self, key, value):\n+        try:\n+            value.iteritems()\n+        except Exception:\n+            raise\n+\n+        if key in self:\n+            # Similar to __delitem__ but with no persistence\n+            self._parser.remove_section(key)\n+            # Be aware that RhsmConfigParser is very diligent about keeping\n+            # default values in the configuration.  Deleting the section will result\n+            # in all the section's values being reset to the defaults.\n+            del self._sections[key]\n+\n+        self._parser.add_section(key)\n+        self._sections[key] = ConfigSection(self, self._parser, key, self.auto_persist)\n+\n+        for k, v in value.iteritems():\n+            self._sections[key][k] = v\n+\n+        if self.auto_persist:\n+            self.persist()\n+\n+    def __delitem__(self, key):\n+        self._parser.remove_section(key)\n+        del self._sections[key]\n+        if self.auto_persist:\n+            self.persist()\n+\n+    def __contains__(self, key):\n+        return key in self._sections\n+\n+    def __iter__(self):\n+        return iter(self._parser.sections())\n+\n+    def iter_sections(self):\n+        \"\"\"An iterator that yields the actual ConfigSection objects instead of just\n+        the names of the sections.\"\"\"\n+        for s in self._parser.sections():\n+            yield self[s]\n+\n+    def __len__(self):\n+        return len(self._parser.sections())\n+\n+    def __repr__(self):\n+        result = {}\n+        for name, s in self._sections.items():\n+            result[name] = repr(s)\n+        return \"%s\" % result\n+\n+\n+class ConfigSection(collections.MutableMapping):\n+    def __init__(self, wrapper, parser, section, auto_persist=False):\n+        self._wrapper = wrapper\n+        self._parser = parser\n+        self._section = section\n+        self.auto_persist = auto_persist\n+\n+    def __iter__(self):\n+        return iter(self._parser.options(self._section))\n+\n+    def __getitem__(self, key):\n+        if key in self:\n+            return self._parser.get(self._section, key)\n+        raise KeyError(\"Property '%s' does not exist in section '%s'\" % (key, self._section))\n+\n+    def __setitem__(self, key, value):\n+        self._parser.set(self._section, key, value)\n+        if self.auto_persist:\n+            self._wrapper.persist()\n+\n+    def __delitem__(self, key):\n+        if key in self:\n+            self._parser.remove_option(self._section, key)\n+            if self.auto_persist:\n+                self._persist()\n+        else:\n+            raise KeyError(\"Property '%s' does not exist in section '%s'\" % (key, self._section))\n+\n+    def __contains__(self, key):\n+        return self._parser.has_option(self._section, key)\n+\n+    def __len__(self):\n+        return len(self._parser.options(self._section))\n+\n+    def _persist(self):\n+        self._wrapper.persist()\n+\n+    def __repr__(self):\n+        return \"%s\" % self._parser.items(self._section)\n+\n+    def get_int(self, key):\n+        return self._parser.get_int(self._section, key)\n+\n+    def get_default(self, key):\n+        return self._parser.get_default(self._section, key)\n+\n+    def has_default(self, key):\n+        return self._parser.has_default(self._section, key)"
        },
        {
          "filename": "src/subscription_manager/action_client.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -39,6 +39,7 @@ class ActionClient(base_action_client.BaseActionClient):\n \n     def _get_libset(self):\n \n+        # TODO: replace with FSM thats progress through this async and wait/joins if needed\n         self.entcertlib = EntCertActionInvoker()\n         self.content_client = ContentActionClient()\n         self.factlib = FactsActionInvoker()"
        },
        {
          "filename": "src/subscription_manager/async.py",
          "status": "modified",
          "additions": 14,
          "deletions": 13,
          "patch": "@@ -18,6 +18,7 @@\n import Queue\n import threading\n import gettext\n+import sys\n \n from subscription_manager.ga import GObject as ga_GObject\n from subscription_manager.entcertlib import Disconnected\n@@ -41,8 +42,8 @@ def _run_refresh(self, active_on, callback, data):\n         try:\n             self.pool.refresh(active_on)\n             self.queue.put((callback, data, None))\n-        except Exception, e:\n-            self.queue.put((callback, data, e))\n+        except Exception:\n+            self.queue.put((callback, data, sys.exc_info()))\n \n     def _watch_thread(self):\n         \"\"\"\n@@ -84,8 +85,8 @@ def _run_bind(self, pool, quantity, bind_callback, cert_callback, except_callbac\n             fetch_certificates(self.certlib)\n             if cert_callback:\n                 ga_GObject.idle_add(cert_callback)\n-        except Exception, e:\n-            ga_GObject.idle_add(except_callback, e)\n+        except Exception:\n+            ga_GObject.idle_add(except_callback, sys.exc_info())\n \n     def _run_unbind(self, serial, selection, callback, except_callback):\n         \"\"\"\n@@ -96,13 +97,13 @@ def _run_unbind(self, serial, selection, callback, except_callback):\n             self.cp_provider.get_consumer_auth_cp().unbindBySerial(self.identity.uuid, serial)\n             try:\n                 self.certlib.update()\n-            except Disconnected, e:\n+            except Disconnected:\n                 pass\n \n             if callback:\n                 ga_GObject.idle_add(callback)\n-        except Exception, e:\n-            ga_GObject.idle_add(except_callback, e, selection)\n+        except Exception:\n+            ga_GObject.idle_add(except_callback, sys.exc_info(), selection)\n \n     def bind(self, pool, quantity, except_callback, bind_callback=None, cert_callback=None):\n         threading.Thread(target=self._run_bind, name=\"AsyncBindBindThread\",\n@@ -130,8 +131,8 @@ def _load_data(self, success_callback, except_callback):\n             current_repos = self.overrides_api.repo_lib.get_repos(apply_overrides=False)\n \n             self._process_callback(success_callback, current_overrides, current_repos)\n-        except Exception, e:\n-            self._process_callback(except_callback, e)\n+        except Exception:\n+            self._process_callback(except_callback, sys.exc_info())\n \n     def _update(self, to_add, to_remove, success_callback, except_callback):\n         '''\n@@ -156,8 +157,8 @@ def _update(self, to_add, to_remove, success_callback, except_callback):\n             current_repos = self.overrides_api.repo_lib.get_repos(apply_overrides=False)\n \n             self._process_callback(success_callback, current_overrides, current_repos)\n-        except Exception, e:\n-            self._process_callback(except_callback, e)\n+        except Exception:\n+            self._process_callback(except_callback, sys.exc_info())\n \n     def _remove_all(self, repo_ids, success_callback, except_callback):\n         try:\n@@ -170,8 +171,8 @@ def _remove_all(self, repo_ids, success_callback, except_callback):\n             current_repos = self.overrides_api.repo_lib.get_repos(apply_overrides=False)\n \n             self._process_callback(success_callback, current_overrides, current_repos)\n-        except Exception, e:\n-            self._process_callback(except_callback, e)\n+        except Exception:\n+            self._process_callback(except_callback, sys.exc_info())\n \n     def _process_callback(self, callback, *args):\n         ga_GObject.idle_add(callback, *args)"
        },
        {
          "filename": "src/subscription_manager/base_action_client.py",
          "status": "modified",
          "additions": 1,
          "deletions": 4,
          "patch": "@@ -27,10 +27,7 @@ class BaseActionClient(object):\n     An object used to update the certficates, yum repos, and facts for the system.\n     \"\"\"\n \n-    # can we inject both of these?\n-    def __init__(self, facts=None):\n-\n-        self.facts = facts\n+    def __init__(self):\n \n         self._libset = self._get_libset()\n         self.lock = inj.require(inj.ACTION_LOCK)"
        },
        {
          "filename": "src/subscription_manager/cache.py",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -33,12 +33,14 @@\n from subscription_manager.jsonwrapper import PoolWrapper\n from rhsm import ourjson as json\n \n+from rhsmlib.services import config\n+\n _ = gettext.gettext\n log = logging.getLogger(__name__)\n \n PACKAGES_RESOURCE = \"packages\"\n \n-cfg = initConfig()\n+conf = config.Config(initConfig())\n \n \n class CacheManager(object):\n@@ -362,7 +364,7 @@ def __init__(self, current_profile=None):\n         # Could be None, we'll read the system's current profile later once\n         # we're sure we actually need the data.\n         self._current_profile = current_profile\n-        self._report_package_profile = cfg.get_int('rhsm', 'report_package_profile')\n+        self._report_package_profile = conf['rhsm'].get_int('report_package_profile')\n \n     # give tests a chance to use something other than RPMProfile\n     def _get_profile(self, profile_type):"
        },
        {
          "filename": "src/subscription_manager/certdirectory.py",
          "status": "modified",
          "additions": 7,
          "deletions": 6,
          "patch": "@@ -23,11 +23,12 @@\n from rhsm.config import initConfig\n from subscription_manager.injection import require, ENT_DIR\n \n-log = logging.getLogger(__name__)\n+from rhsmlib.services import config\n \n+log = logging.getLogger(__name__)\n _ = gettext.gettext\n \n-cfg = initConfig()\n+conf = config.Config(initConfig())\n \n DEFAULT_PRODUCT_CERT_DIR = \"/etc/pki/product-default\"\n \n@@ -59,7 +60,7 @@ def list(self):\n \n     def listdirs(self):\n         dirs = []\n-        for p, fn in self.list_all():\n+        for _p, fn in self.list_all():\n             path = self.abspath(fn)\n             if Path.isdir(path):\n                 dirs.append(Directory(path))\n@@ -114,7 +115,7 @@ def list(self):\n         if self._listing is not None:\n             return self._listing\n         listing = []\n-        for p, fn in Directory.list(self):\n+        for _p, fn in Directory.list(self):\n             if not fn.endswith('.pem') or fn.endswith(self.KEY):\n                 continue\n             path = self.abspath(fn)\n@@ -219,7 +220,7 @@ def get_installed_products(self):\n \n class ProductDirectory(ProductCertificateDirectory):\n     def __init__(self, path=None, default_path=None):\n-        installed_prod_path = path or cfg.get('rhsm', 'productCertDir')\n+        installed_prod_path = path or conf['rhsm']['productCertDir']\n         default_prod_path = default_path or DEFAULT_PRODUCT_CERT_DIR\n         self.installed_prod_dir = ProductCertificateDirectory(path=installed_prod_path)\n         self.default_prod_dir = ProductCertificateDirectory(path=default_prod_path)\n@@ -252,7 +253,7 @@ def path(self):\n \n class EntitlementDirectory(CertificateDirectory):\n \n-    PATH = cfg.get('rhsm', 'entitlementCertDir')\n+    PATH = conf['rhsm']['entitlementCertDir']\n     PRODUCT = 'product'\n \n     @classmethod"
        },
        {
          "filename": "src/subscription_manager/cpuinfo.py",
          "status": "modified",
          "additions": 1,
          "deletions": 3,
          "patch": "@@ -483,7 +483,5 @@ def open_proc_cpuinfo(cls, prefix=None):\n         proc_cpuinfo_path = cls.proc_cpuinfo_path\n         if prefix:\n             proc_cpuinfo_path = os.path.join(prefix, cls.proc_cpuinfo_path[1:])\n-        proc_cpuinfo_buf = ''\n         with open(proc_cpuinfo_path, 'r') as proc_cpuinfo_f:\n-            proc_cpuinfo_buf = proc_cpuinfo_f.read()\n-        return proc_cpuinfo_buf\n+            return proc_cpuinfo_f.read()"
        },
        {
          "filename": "src/subscription_manager/dbus_interface.py",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "patch": "@@ -14,6 +14,9 @@\n #\n \n import dbus\n+import dbus.mainloop\n+import dbus.mainloop.glib\n+\n import inspect\n import logging\n import subscription_manager.injection as inj\n@@ -29,6 +32,8 @@ def __init__(self):\n         try:\n             # Only follow names if there is a default main loop\n             self.has_main_loop = self._get_main_loop() is not None\n+            log.debug(\"self.has_main_loop=%s\", self.has_main_loop)\n+            dbus.mainloop.glib.DBusGMainLoop(set_as_default=True)\n \n             self.bus = dbus.SystemBus()\n             validity_obj = self._get_validity_object(self.service_name,"
        },
        {
          "filename": "src/subscription_manager/entbranding.py",
          "status": "modified",
          "additions": 1,
          "deletions": 3,
          "patch": "@@ -174,10 +174,8 @@ def write(self, brand_info):\n             brand_file.write(brand_info)\n \n     def read(self):\n-        brand_info = None\n         with open(self.path, 'r') as brand_file:\n-            brand_info = brand_file.read()\n-        return brand_info\n+            return brand_file.read()\n \n     def __str__(self):\n         return \"<BrandFile path=%s>\" % self.path"
        },
        {
          "filename": "src/subscription_manager/entcertlib.py",
          "status": "modified",
          "additions": 0,
          "deletions": 5,
          "patch": "@@ -17,7 +17,6 @@\n import logging\n import socket\n \n-from rhsm.config import initConfig\n from rhsm.certificate import Key, create_from_pem\n \n from subscription_manager.certdirectory import Writer\n@@ -28,13 +27,9 @@\n from subscription_manager import rhelentbranding\n import subscription_manager.injection as inj\n \n-\n log = logging.getLogger(__name__)\n-\n _ = gettext.gettext\n \n-cfg = initConfig()\n-\n \n class EntCertActionInvoker(certlib.BaseActionInvoker):\n     \"\"\"Invoker for entitlement certificate updating actions.\"\"\""
        },
        {
          "filename": "src/subscription_manager/factlib.py",
          "status": "modified",
          "additions": 25,
          "deletions": 21,
          "patch": "@@ -21,6 +21,9 @@\n from certlib import Locker, ActionReport\n from subscription_manager import injection as inj\n \n+import rhsmlib.dbus.facts as facts\n+import rhsmlib.candlepin.api as candlepin_api\n+\n _ = gettext.gettext\n \n log = logging.getLogger(__name__)\n@@ -78,31 +81,32 @@ class FactsActionCommand(object):\n     Returns a FactsActionReport.\n     \"\"\"\n     def __init__(self):\n-        self.cp_provider = inj.require(inj.CP_PROVIDER)\n-        self.uep = self.cp_provider.get_consumer_auth_cp()\n         self.report = FactsActionReport()\n-        self.facts = inj.require(inj.FACTS)\n-\n-    def perform(self):\n+        self.facts_client = facts.FactsClient()\n \n-        # figure out the diff between latest facts and\n-        # report that as updates\n+    def collect_facts(self):\n+        return self.facts_client.GetFacts()\n \n-        if self.facts.has_changed():\n-            fact_updates = self.facts.get_facts()\n-            self.report.fact_updates = fact_updates\n+    def sync_facts_to_server(self, fact_updates):\n+        consumer_identity = inj.require(inj.IDENTITY)\n+        if not consumer_identity.is_valid():\n+            return self.report\n \n-            consumer_identity = inj.require(inj.IDENTITY)\n-            if not consumer_identity.is_valid():\n-                # FIXME: more info\n-                return self.report\n+        cp_provider = inj.require(inj.CP_PROVIDER)\n+        uep = cp_provider.get_consumer_auth_cp()\n \n-            # CacheManager.update_check calls self.has_changed,\n-            # is the self.facts.has_changed above redundant?\n-            self.facts.update_check(self.uep, consumer_identity.uuid)\n-            log.info(\"Facts have been updated.\")\n-        else:\n-            log.debug(\"Facts have not changed, skipping upload.\")\n+        consumer_api = candlepin_api.CandlepinConsumer(uep, consumer_identity.uuid)\n+        res = consumer_api.call(uep.updateConsumer, fact_updates)\n+        log.debug(\"sync_facts_to_server candlepin api res=%s\", res)\n \n-        # FIXME: can populate this with more info later\n+    def perform(self):\n+        # figure out the diff between latest facts and\n+        # report that as updates\n+        self.update()\n         return self.report\n+\n+    def update(self):\n+        \"\"\"This will collect the facts from the dbus service and push them to the server.\"\"\"\n+        collected_facts = self.collect_facts()\n+        self.report.fact_updates = collected_facts\n+        self.sync_facts_to_server(collected_facts)"
        },
        {
          "filename": "src/subscription_manager/facts.py",
          "status": "modified",
          "additions": 8,
          "deletions": 75,
          "patch": "@@ -13,25 +13,18 @@\n \n from datetime import datetime\n import gettext\n-import glob\n import logging\n import os\n \n-import rhsm.config\n-\n from subscription_manager.injection import PLUGIN_MANAGER, require\n from subscription_manager.cache import CacheManager\n-import subscription_manager.injection as inj\n from rhsm import ourjson as json\n \n-_ = gettext.gettext\n+from rhsmlib.dbus.facts import FactsClient\n \n+_ = gettext.gettext\n log = logging.getLogger(__name__)\n \n-# Hardcoded value for the version of certificates this version of the client\n-# prefers:\n-CERT_VERSION = \"3.2\"\n-\n \n class Facts(CacheManager):\n     \"\"\"\n@@ -43,11 +36,9 @@ class Facts(CacheManager):\n     \"\"\"\n     CACHE_FILE = \"/var/lib/rhsm/facts/facts.json\"\n \n-    def __init__(self, ent_dir=None, prod_dir=None):\n+    def __init__(self):\n         self.facts = {}\n \n-        self.entitlement_dir = ent_dir or inj.require(inj.ENT_DIR)\n-        self.product_dir = prod_dir or inj.require(inj.PROD_DIR)\n         # see bz #627962\n         # we would like to have this info, but for now, since it\n         # can change constantly on laptops, it makes for a lot of\n@@ -75,80 +66,22 @@ def has_changed(self):\n \n         cached_facts = self._read_cache() or {}\n         # In order to accurately check for changes, we must refresh local data\n-        self.facts = self.get_facts(True)\n+        self.facts = self.get_facts()\n \n         for key in (set(self.facts) | set(cached_facts)) - set(self.graylist):\n             if self.facts.get(key) != cached_facts.get(key):\n                 return True\n         return False\n \n-    def get_facts(self, refresh=False):\n-        if ((len(self.facts) == 0) or refresh):\n-            facts = {}\n-            facts.update(self._load_hw_facts())\n-\n-            # Set the preferred entitlement certificate version:\n-            facts.update({\"system.certificate_version\": CERT_VERSION})\n-\n-            facts.update(self._load_custom_facts())\n-            self.plugin_manager.run('post_facts_collection', facts=facts)\n-            self.facts = facts\n+    def get_facts(self):\n+        facts_dbus_client = FactsClient()\n+        self.facts = facts_dbus_client.GetFacts()\n+        self.plugin_manager.run('post_facts_collection', facts=self.facts)\n         return self.facts\n \n     def to_dict(self):\n         return self.get_facts()\n \n-    def _load_hw_facts(self):\n-        import hwprobe\n-        return hwprobe.Hardware().get_all()\n-\n-    def _parse_facts_json(self, json_buffer, file_path):\n-        custom_facts = None\n-\n-        try:\n-            custom_facts = json.loads(json_buffer)\n-        except ValueError:\n-            log.warn(\"Unable to load custom facts file: %s\" % file_path)\n-\n-        return custom_facts\n-\n-    def _open_custom_facts(self, file_path):\n-        if not os.access(file_path, os.R_OK):\n-            log.warn(\"Unable to access custom facts file: %s\" % file_path)\n-            return None\n-\n-        try:\n-            f = open(file_path)\n-        except IOError:\n-            log.warn(\"Unable to open custom facts file: %s\" % file_path)\n-            return None\n-\n-        json_buffer = f.read()\n-        f.close()\n-\n-        return json_buffer\n-\n-    def _load_custom_facts(self):\n-        \"\"\"\n-        Load custom facts from .facts files in /etc/rhsm/facts.\n-        \"\"\"\n-        # BZ 1112326 don't double the '/'\n-        facts_file_glob = \"%s/facts/*.facts\" % rhsm.config.DEFAULT_CONFIG_DIR.rstrip('/')\n-        file_facts = {}\n-        for file_path in glob.glob(facts_file_glob):\n-            log.info(\"Loading custom facts from: %s\" % file_path)\n-            json_buffer = self._open_custom_facts(file_path)\n-\n-            if json_buffer is None:\n-                continue\n-\n-            custom_facts = self._parse_facts_json(json_buffer, file_path)\n-\n-            if custom_facts:\n-                file_facts.update(custom_facts)\n-\n-        return file_facts\n-\n     def _sync_with_server(self, uep, consumer_uuid):\n         log.debug(\"Updating facts on server\")\n         uep.updateConsumer(consumer_uuid, facts=self.get_facts())"
        },
        {
          "filename": "src/subscription_manager/ga_impls/ga_gtk2/GLib.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -2,5 +2,7 @@\n \n timeout_add = gobject.timeout_add\n idle_add = gobject.idle_add\n+MainLoop = gobject.MainLoop\n+threads_init = gobject.threads_init\n \n-__all__ = [timeout_add, idle_add]\n+__all__ = [timeout_add, idle_add, MainLoop, threads_init]"
        },
        {
          "filename": "src/subscription_manager/gui/allsubs.py",
          "status": "modified",
          "additions": 2,
          "deletions": 3,
          "patch": "@@ -44,7 +44,7 @@ class AllSubscriptionsTab(widgets.SubscriptionManagerTab):\n                         'filter_options_button', 'applied_filters_label']\n     gui_file = \"allsubs\"\n \n-    def __init__(self, backend, facts, parent_win):\n+    def __init__(self, backend, parent_win):\n \n         super(AllSubscriptionsTab, self).__init__()\n \n@@ -59,13 +59,12 @@ def __init__(self, backend, facts, parent_win):\n         self.parent_win = parent_win\n         self.backend = backend\n         self.identity = require(IDENTITY)\n-        self.facts = facts\n \n         # Progress bar\n         self.pb = None\n         self.timer = 0\n \n-        self.pool_stash = managerlib.PoolStash(self.facts)\n+        self.pool_stash = managerlib.PoolStash()\n \n         self.async_bind = async.AsyncBind(self.backend.certlib)\n "
        },
        {
          "filename": "src/subscription_manager/gui/factsgui.py",
          "status": "modified",
          "additions": 29,
          "deletions": 15,
          "patch": "@@ -22,6 +22,8 @@\n from subscription_manager.gui.utils import handle_gui_exception, linkify\n from subscription_manager import injection as inj\n \n+import rhsmlib.dbus.facts as facts\n+\n _ = gettext.gettext\n \n log = logging.getLogger(__name__)\n@@ -38,15 +40,16 @@ class SystemFactsDialog(widgets.SubmanBaseWidget):\n                     'system_id_label', 'system_id_title']\n     gui_file = \"factsdialog\"\n \n-    def __init__(self, facts, update_callback=None):\n-\n+    def __init__(self, update_callback=None):\n         super(SystemFactsDialog, self).__init__()\n \n         #self.consumer = consumer\n         self.update_callback = update_callback\n         self.identity = inj.require(inj.IDENTITY)\n         self.cp_provider = inj.require(inj.CP_PROVIDER)\n-        self.facts = facts\n+\n+        self.facts = facts.FactsClient()\n+\n         self.connect_signals({\n                 \"on_system_facts_dialog_delete_event\": self._hide_callback,\n                 \"on_close_button_clicked\": self._hide_callback,\n@@ -93,23 +96,20 @@ def _display_system_id(self, identity):\n             self.system_id_title.hide()\n             self.system_id_label.hide()\n \n-    def display_facts(self):\n-        \"\"\"Updates the list store with the current system facts.\"\"\"\n-        self.facts_store.clear()\n-\n-        last_update = self.facts.get_last_update()\n-        if last_update:\n-            self.last_update_label.set_text(last_update.strftime(\"%c\"))\n-        else:\n-            self.last_update_label.set_text(_('No previous update'))\n+    def _on_get_facts_error_handler(self, exception):\n+        log.debug(exception)\n+        raise exception\n \n-        # make sure we get fresh facts, since entitlement validity status could         # change\n-        system_facts_dict = self.facts.get_facts()\n+    def _on_get_facts_reply_handler(self, facts_dict):\n+        self.update_facts_store(facts_dict)\n \n-        system_facts = system_facts_dict.items()\n+    def update_facts_store(self, facts_dict):\n+        self.facts_store.clear()\n+        system_facts = facts_dict.items()\n \n         system_facts.sort()\n         group = None\n+\n         for fact, value in system_facts:\n             new_group = fact.split(\".\", 1)[0]\n             if new_group != group:\n@@ -119,6 +119,20 @@ def display_facts(self):\n                 value = _(\"Unknown\")\n             self.facts_store.append(parent, [str(fact), str(value)])\n \n+    def display_facts(self):\n+        \"\"\"Updates the list store with the current system facts.\"\"\"\n+        # make sure we get fresh facts, since entitlement validity status could\n+        self.facts.GetFacts(reply_handler=self._on_get_facts_reply_handler,\n+                            error_handler=self._on_get_facts_error_handler)\n+\n+        # make last_update a Facts dbus property?\n+        #last_update = self.facts.get_last_update()\n+        last_update = None\n+        if last_update:\n+            self.last_update_label.set_text(last_update.strftime(\"%c\"))\n+        else:\n+            self.last_update_label.set_text(_('No previous update'))\n+\n         identity = inj.require(inj.IDENTITY)\n         self._display_system_id(identity)\n "
        },
        {
          "filename": "src/subscription_manager/gui/firstboot/rhsm_login.py",
          "status": "modified",
          "additions": 7,
          "deletions": 3,
          "patch": "@@ -2,14 +2,15 @@\n import gettext\n import sys\n import logging\n+import dbus.mainloop.glib\n \n _ = lambda x: gettext.ldgettext(\"rhsm\", x)\n \n from subscription_manager import ga_loader\n ga_loader.init_ga()\n \n from subscription_manager.ga import Gtk as ga_Gtk\n-from subscription_manager.ga import gtk_compat\n+from subscription_manager.ga import gtk_compat, GLib\n \n gtk_compat.threads_init()\n \n@@ -30,7 +31,6 @@\n \n from subscription_manager import injection as inj\n \n-from subscription_manager.facts import Facts\n from subscription_manager.hwprobe import Hardware\n from subscription_manager.gui import managergui\n from subscription_manager.gui import registergui\n@@ -61,6 +61,10 @@ def __init__(self):\n         \"\"\"\n         super(moduleClass, self).__init__()\n \n+        dbus.mainloop.glib.DBusGMainLoop(set_as_default=True)\n+        GLib.threads_init()\n+        dbus.mainloop.glib.threads_init()\n+\n         self.mode = constants.MODE_REGULAR\n         self.title = _(\"Subscription Management Registration\")\n         self.sidebarTitle = _(\"Subscription Registration\")\n@@ -74,7 +78,7 @@ def __init__(self):\n         reg_info = registergui.RegisterInfo()\n         self.backend = managergui.Backend()\n         self.plugin_manager = inj.require(inj.PLUGIN_MANAGER)\n-        self.register_widget = registergui.FirstbootWidget(self.backend, Facts(), reg_info)\n+        self.register_widget = registergui.FirstbootWidget(self.backend, reg_info)\n         self.register_widget.connect(\"notify::screen-ready\", self._on_screen_ready_change)\n \n         # Will be False if we are on an older RHEL version where"
        },
        {
          "filename": "src/subscription_manager/gui/installedtab.py",
          "status": "modified",
          "additions": 1,
          "deletions": 2,
          "patch": "@@ -59,7 +59,7 @@ class InstalledProductsTab(widgets.SubscriptionManagerTab):\n                  'update_certificates_button', 'register_button']\n     gui_file = \"installed\"\n \n-    def __init__(self, backend, facts, tab_icon,\n+    def __init__(self, backend, tab_icon,\n                  parent, ent_dir, prod_dir):\n         # The row striping in this TreeView is handled automatically\n         # because we have the rules_hint set to True in the Glade file.\n@@ -71,7 +71,6 @@ def __init__(self, backend, facts, tab_icon,\n         self.product_dir = prod_dir\n         self.entitlement_dir = ent_dir\n \n-        self.facts = facts\n         self.backend = backend\n \n         # Product column"
        },
        {
          "filename": "src/subscription_manager/gui/managergui.py",
          "status": "modified",
          "additions": 5,
          "deletions": 15,
          "patch": "@@ -38,7 +38,6 @@\n \n from subscription_manager.branding import get_branding\n from subscription_manager.entcertlib import EntCertActionInvoker\n-from subscription_manager.facts import Facts\n from subscription_manager.hwprobe import ClassicCheck\n from subscription_manager import managerlib\n from subscription_manager.utils import get_client_versions, get_server_versions, parse_baseurl_info, restart_virt_who\n@@ -176,7 +175,7 @@ def _on_proxy_error_dialog_response(self, window, response):\n     def _exit(self, *args):\n         system_exit(0)\n \n-    def __init__(self, backend=None, facts=None,\n+    def __init__(self, backend=None,\n                  ent_dir=None, prod_dir=None,\n                  auto_launch_registration=False):\n         super(MainWindow, self).__init__()\n@@ -193,14 +192,6 @@ def __init__(self, backend=None, facts=None,\n         self.backend = backend or Backend()\n         self.identity = require(IDENTITY)\n \n-        self.facts = facts or Facts(self.backend.entitlement_dir,\n-                self.backend.product_dir)\n-        # We need to make sure facts are loaded immediately, some GUI operations\n-        # are done in separate threads, and if facts try to load in another\n-        # thread the virt guest detection code breaks due to hwprobe's use of\n-        # signals.\n-        self.facts.get_facts()\n-\n         log.debug(\"Client Versions: %s \" % get_client_versions())\n         # Log the server version asynchronously\n         ga_GLib.idle_add(self.log_server_version, self.backend.cp_provider.get_consumer_auth_cp())\n@@ -220,7 +211,7 @@ def __init__(self, backend=None, facts=None,\n         self.product_dir = prod_dir or self.backend.product_dir\n         self.entitlement_dir = ent_dir or self.backend.entitlement_dir\n \n-        self.system_facts_dialog = factsgui.SystemFactsDialog(self.facts, update_callback=self._handle_facts_updated)\n+        self.system_facts_dialog = factsgui.SystemFactsDialog(update_callback=self._handle_facts_updated)\n \n         self.preferences_dialog = PreferencesDialog(self.backend,\n                                                     self._get_window())\n@@ -239,18 +230,17 @@ def __init__(self, backend=None, facts=None,\n                 ga_Gtk.IconSize.MENU)\n \n         self.installed_tab = InstalledProductsTab(self.backend,\n-                                                  self.facts,\n                                                   self.installed_tab_icon,\n                                                   self,\n                                                   ent_dir=self.entitlement_dir,\n                                                   prod_dir=self.product_dir)\n+\n         self.my_subs_tab = MySubscriptionsTab(self.backend,\n                                               self.main_window,\n                                               ent_dir=self.entitlement_dir,\n                                               prod_dir=self.product_dir)\n \n         self.all_subs_tab = AllSubscriptionsTab(self.backend,\n-                                                self.facts,\n                                                 self.main_window)\n \n         hbox = ga_Gtk.HBox(spacing=6)\n@@ -432,7 +422,7 @@ def _should_show_redeem(self):\n         return can_redeem\n \n     def _register_item_clicked(self, widget):\n-        registration_dialog = registergui.RegisterDialog(self.backend, self.facts)\n+        registration_dialog = registergui.RegisterDialog(self.backend)\n         registration_dialog.register_dialog.connect('destroy',\n                                                     self._on_dialog_destroy,\n                                                     widget)\n@@ -513,7 +503,7 @@ def _import_cert_item_clicked(self, widget):\n         self.import_sub_dialog.show()\n \n     def _update_certificates_button_clicked(self, widget):\n-        autobind_wizard = registergui.AutobindWizardDialog(self.backend, self.facts)\n+        autobind_wizard = registergui.AutobindWizardDialog(self.backend)\n         autobind_wizard.register_dialog.connect('destroy',\n                                                 self._on_dialog_destroy,\n                                                 widget)"
        },
        {
          "filename": "src/subscription_manager/gui/registergui.py",
          "status": "modified",
          "additions": 146,
          "deletions": 114,
          "patch": "@@ -23,10 +23,13 @@\n import sys\n import threading\n \n+from subscription_manager import ga_loader\n+ga_loader.init_ga()\n+\n from subscription_manager.ga import Gtk as ga_Gtk\n from subscription_manager.ga import GObject as ga_GObject\n \n-import rhsm.config as config\n+import rhsm.config as base_config\n from rhsm.utils import ServerUrlParseError\n from rhsm.connection import GoneException, RestlibException, UEPConnection, \\\n         ProxyException\n@@ -47,13 +50,15 @@\n         NoProductsException\n from subscription_manager.jsonwrapper import PoolWrapper\n \n+import rhsmlib.dbus.facts as facts\n+\n _ = lambda x: gettext.ldgettext(\"rhsm\", x)\n \n gettext.textdomain(\"rhsm\")\n-\n log = logging.getLogger(__name__)\n \n-CFG = config.initConfig()\n+from rhsmlib.services import config\n+conf = config.Config(base_config.initConfig())\n \n \n class RegisterState(object):\n@@ -122,13 +127,13 @@ def reset_resolver():\n \n def server_info_from_config(config):\n     return {\n-            \"host\": config.get('server', 'hostname'),\n-            \"ssl_port\": config.get_int('server', 'port'),\n-            \"handler\": config.get('server', 'prefix'),\n-            \"proxy_hostname\": config.get('server', 'proxy_hostname'),\n-            \"proxy_port\": config.get_int('server', 'proxy_port'),\n-            \"proxy_user\": config.get('server', 'proxy_user'),\n-            \"proxy_password\": config.get('server', 'proxy_password')\n+            \"host\": conf['server']['hostname'],\n+            \"ssl_port\": conf['server'].get_int('port'),\n+            \"handler\": conf['server']['prefix'],\n+            \"proxy_hostname\": conf['server']['proxy_hostname'],\n+            \"proxy_port\": conf['server'].get_int('proxy_port'),\n+            \"proxy_user\": conf['server']['proxy_user'],\n+            \"proxy_password\": conf['server']['proxy_password']\n            }\n \n \n@@ -219,9 +224,9 @@ def __init__(self):\n \n     def _defaults_from_config(self):\n         \"\"\"Load the current server values from configuration (rhsm.conf).\"\"\"\n-        self.set_property('hostname', CFG.get('server', 'hostname'))\n-        self.set_property('port', CFG.get('server', 'port'))\n-        self.set_property('prefix', CFG.get('server', 'prefix'))\n+        self.set_property('hostname', conf['server']['hostname'])\n+        self.set_property('port', conf['server']['port'])\n+        self.set_property('prefix', conf['server']['prefix'])\n \n     def _initial_registration_status(self):\n         msg = _(\"This system is currently not registered.\")\n@@ -259,12 +264,10 @@ class RegisterWidget(widgets.SubmanBaseWidget):\n     register_button_label = ga_GObject.property(type=str, default=_('Register'))\n     # TODO: a prop equivalent to initial-setups 'completed' and 'status' props\n \n-    def __init__(self, backend, facts, reg_info=None,\n-                 parent_window=None):\n+    def __init__(self, backend, reg_info=None, parent_window=None):\n         super(RegisterWidget, self).__init__()\n \n         self.backend = backend\n-        self.facts = facts\n \n         self.async = AsyncBackend(self.backend)\n \n@@ -340,7 +343,6 @@ def __init__(self, backend, facts, reg_info=None,\n     def add_screen(self, idx, screen_class):\n         screen = screen_class(reg_info=self.info,\n                               async_backend=self.async,\n-                              facts=self.facts,\n                               parent_window=self.parent_window)\n \n         # add the index of the screen in self._screens to the class itself\n@@ -402,8 +404,8 @@ def do_register_message(self, msg, msg_type=None):\n     def do_register_finished(self):\n         msg = _(\"System '%s' successfully registered.\\n\") % self.info.identity.name\n         self.info.set_property('register-status', msg)\n-        CFG.save()\n-        last_server_info = server_info_from_config(CFG)\n+        conf.persist()\n+        last_server_info = server_info_from_config(conf)\n         last_server_info['cert_file'] = self.backend.cp_provider.cert_file\n         last_server_info['key_file'] = self.backend.cp_provider.key_file\n         self.info.set_property('server-info', last_server_info)\n@@ -700,7 +702,7 @@ class RegisterDialog(widgets.SubmanBaseWidget):\n     gui_file = \"register_dialog\"\n     __gtype_name__ = 'RegisterDialog'\n \n-    def __init__(self, backend, facts=None, callbacks=None):\n+    def __init__(self, backend, callbacks=None):\n         \"\"\"\n         Callbacks will be executed when registration status changes.\n         \"\"\"\n@@ -711,7 +713,7 @@ def __init__(self, backend, facts=None, callbacks=None):\n         self.reg_info = RegisterInfo()\n \n         # RegisterWidget is a oect, but not a Gtk.Widget\n-        self.register_widget = self.create_wizard_widget(backend, facts, self.reg_info,\n+        self.register_widget = self.create_wizard_widget(backend, self.reg_info,\n                                                          self.register_dialog)\n \n         # But RegisterWidget.register_widget is a Gtk.Widget, so add it to\n@@ -746,15 +748,14 @@ def __init__(self, backend, facts=None, callbacks=None):\n         self.window = self.register_dialog\n         self.back_button.set_sensitive(False)\n \n-    def create_wizard_widget(self, backend, facts, reg_info, parent_window):\n+    def create_wizard_widget(self, backend, reg_info, parent_window):\n         \"\"\"Create a RegisterWidget or subclass and use it for our content.\"\"\"\n \n         # FIXME: Need better error handling in general, but it's kind of\n         # annoying to have to pass the top level widget all over the place\n         register_widget = RegisterWidget(backend=backend,\n-                                       facts=facts,\n-                                       reg_info=reg_info,\n-                                       parent_window=parent_window)\n+                                         reg_info=reg_info,\n+                                         parent_window=parent_window)\n \n         return register_widget\n \n@@ -827,9 +828,9 @@ class AutoBindWidget(RegisterWidget):\n \n     initial_screen = SELECT_SLA_PAGE\n \n-    def __init__(self, backend, facts, reg_info=None,\n+    def __init__(self, backend, reg_info=None,\n                  parent_window=None):\n-        super(AutoBindWidget, self).__init__(backend, facts, reg_info,\n+        super(AutoBindWidget, self).__init__(backend, reg_info,\n                                              parent_window)\n \n     def choose_initial_screen(self):\n@@ -855,15 +856,14 @@ def choose_initial_screen(self):\n class AutobindWizardDialog(RegisterDialog):\n     __gtype_name__ = \"AutobindWizardDialog\"\n \n-    def __init__(self, backend, facts):\n-        super(AutobindWizardDialog, self).__init__(backend, facts)\n+    def __init__(self, backend):\n+        super(AutobindWizardDialog, self).__init__(backend)\n \n-    def create_wizard_widget(self, backend, facts, reg_info, parent_window):\n+    def create_wizard_widget(self, backend, reg_info, parent_window):\n \n         # FIXME: Need better error handling in general, but it's kind of\n         # annoying to have to pass the top level widget all over the place\n         autobind_widget = AutoBindWidget(backend=backend,\n-                                         facts=facts,\n                                          reg_info=reg_info,\n                                          parent_window=parent_window)\n \n@@ -899,7 +899,7 @@ class Screen(widgets.SubmanBaseWidget):\n \n     ready = ga_GObject.property(type=bool, default=True)\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n+    def __init__(self, reg_info, async_backend, parent_window):\n         super(Screen, self).__init__()\n \n         self.pre_message = \"\"\n@@ -911,7 +911,6 @@ def __init__(self, reg_info, async_backend, facts, parent_window):\n         self.parent_window = parent_window\n         self.info = reg_info\n         self.async = async_backend\n-        self.facts = facts\n \n     def stay(self):\n         self.emit('stay-on-screen')\n@@ -966,13 +965,12 @@ class NoGuiScreen(ga_GObject.GObject):\n \n     ready = ga_GObject.property(type=bool, default=True)\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n+    def __init__(self, reg_info, async_backend, parent_window):\n         ga_GObject.GObject.__init__(self)\n \n         self.parent_window = parent_window\n         self.info = reg_info\n         self.async = async_backend\n-        self.facts = facts\n \n         self.button_label = None\n         self.needs_gui = False\n@@ -1012,8 +1010,8 @@ def stay(self):\n class PerformRegisterScreen(NoGuiScreen):\n     screen_enum = PERFORM_REGISTER_PAGE\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(PerformRegisterScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(PerformRegisterScreen, self).__init__(reg_info, async_backend, parent_window)\n \n     def _on_registration_finished_cb(self, new_account, error=None):\n         if error is not None:\n@@ -1051,7 +1049,6 @@ def pre(self):\n         self.info.set_property('register-status', msg)\n \n         self.async.register_consumer(self.info.get_property('consumername'),\n-                                     self.facts,\n                                      self.info.get_property('owner-key'),\n                                      self.info.get_property('environment'),\n                                      self.info.get_property('activation-keys'),\n@@ -1110,8 +1107,8 @@ def back_handler(self):\n class PerformPackageProfileSyncScreen(NoGuiScreen):\n     screen_enum = UPLOAD_PACKAGE_PROFILE_PAGE\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(PerformPackageProfileSyncScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(PerformPackageProfileSyncScreen, self).__init__(reg_info, async_backend, parent_window)\n         self.pre_message = _(\"Uploading package profile\")\n \n     def _on_update_package_profile_finished_cb(self, result, error=None):\n@@ -1151,8 +1148,8 @@ def pre(self):\n class PerformSubscribeScreen(NoGuiScreen):\n     screen_enum = PERFORM_SUBSCRIBE_PAGE\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(PerformSubscribeScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(PerformSubscribeScreen, self).__init__(reg_info, async_backend, parent_window)\n         self.pre_message = _(\"Attaching subscriptions\")\n \n     def _on_subscribing_finished_cb(self, unused, error=None):\n@@ -1185,8 +1182,8 @@ class ConfirmSubscriptionsScreen(Screen):\n \n     gui_file = \"confirmsubs\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(ConfirmSubscriptionsScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(ConfirmSubscriptionsScreen, self).__init__(reg_info, async_backend, parent_window)\n         self.button_label = _(\"_Attach\")\n \n         self.store = ga_Gtk.ListStore(str, bool, str)\n@@ -1249,8 +1246,8 @@ class SelectSLAScreen(Screen):\n                                           'owner_treeview']\n     gui_file = \"selectsla\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(SelectSLAScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(SelectSLAScreen, self).__init__(reg_info, async_backend, parent_window)\n \n         self.pre_message = _(\"Finding suitable service levels\")\n         self.button_label = _(\"_Next\")\n@@ -1409,7 +1406,6 @@ def pre(self):\n         self.info.identity.reload()\n \n         self.async.find_service_levels(self.info.identity.uuid,\n-                                       self.facts,\n                                        self._on_get_service_levels_cb)\n         return True\n \n@@ -1428,8 +1424,8 @@ class EnvironmentScreen(Screen):\n     widget_names = Screen.widget_names + ['environment_treeview']\n     gui_file = \"environment\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(EnvironmentScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(EnvironmentScreen, self).__init__(reg_info, async_backend, parent_window)\n \n         self.pre_message = _(\"Fetching list of possible environments\")\n         renderer = ga_Gtk.CellRendererText()\n@@ -1497,8 +1493,8 @@ class OrganizationScreen(Screen):\n     widget_names = Screen.widget_names + ['owner_treeview']\n     gui_file = \"organization\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(OrganizationScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(OrganizationScreen, self).__init__(reg_info, async_backend, parent_window)\n \n         self.pre_message = _(\"Fetching list of possible organizations\")\n \n@@ -1575,8 +1571,8 @@ class CredentialsScreen(Screen):\n     gui_file = \"credentials\"\n     screen_enum = CREDENTIALS_PAGE\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(CredentialsScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(CredentialsScreen, self).__init__(reg_info, async_backend, parent_window)\n \n         self._initialize_consumer_name()\n         self.registration_tip_label.set_label(\"<small>%s</small>\" %\n@@ -1673,14 +1669,14 @@ def back_handler(self):\n class ActivationKeyScreen(Screen):\n     screen_enum = ACTIVATION_KEY_PAGE\n     widget_names = Screen.widget_names + [\n-                'activation_key_entry',\n-                'organization_entry',\n-                'consumer_entry',\n-        ]\n+        'activation_key_entry',\n+        'organization_entry',\n+        'consumer_entry',\n+    ]\n     gui_file = \"activation_key\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(ActivationKeyScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(ActivationKeyScreen, self).__init__(reg_info, async_backend, parent_window)\n         self._initialize_consumer_name()\n \n     def _initialize_consumer_name(self):\n@@ -1756,8 +1752,8 @@ def back_handler(self):\n \n class RefreshSubscriptionsScreen(NoGuiScreen):\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(RefreshSubscriptionsScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(RefreshSubscriptionsScreen, self).__init__(reg_info, async_backend, parent_window)\n         self.pre_message = _(\"Attaching subscriptions\")\n \n     def _on_refresh_cb(self, msg, error=None):\n@@ -1785,15 +1781,15 @@ class ChooseServerScreen(Screen):\n                                           'activation_key_checkbox']\n     gui_file = \"choose_server\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(ChooseServerScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(ChooseServerScreen, self).__init__(reg_info, async_backend, parent_window)\n \n         self.button_label = _(\"_Next\")\n \n         callbacks = {\n-                \"on_default_button_clicked\": self._on_default_button_clicked,\n-                \"on_proxy_button_clicked\": self._on_proxy_button_clicked,\n-            }\n+            \"on_default_button_clicked\": self._on_default_button_clicked,\n+            \"on_proxy_button_clicked\": self._on_proxy_button_clicked,\n+        }\n \n         self.connect_signals(callbacks)\n \n@@ -1802,9 +1798,9 @@ def __init__(self, reg_info, async_backend, facts, parent_window):\n     def _on_default_button_clicked(self, widget):\n         # Default port and prefix are fine, so we can be concise and just\n         # put the hostname for RHN:\n-        self.server_entry.set_text(\"%s:%s%s\" % (config.DEFAULT_HOSTNAME,\n-            config.DEFAULT_PORT,\n-            config.DEFAULT_PREFIX))\n+        self.server_entry.set_text(\"%s:%s%s\" % (base_config.DEFAULT_HOSTNAME,\n+            base_config.DEFAULT_PORT,\n+            base_config.DEFAULT_PREFIX))\n \n     def _on_proxy_button_clicked(self, widget):\n         # proxy dialog may attempt to resolve proxy and server names, so\n@@ -1840,7 +1836,7 @@ def apply(self):\n         self.stay()\n         server = self.server_entry.get_text()\n         try:\n-            (hostname, port, prefix) = parse_server_info(server, CFG)\n+            (hostname, port, prefix) = parse_server_info(server, conf)\n             self.info.set_property('hostname', hostname)\n             self.info.set_property('port', port)\n             self.info.set_property('prefix', prefix)\n@@ -1856,8 +1852,8 @@ def apply(self):\n \n     def set_server_entry(self, hostname, port, prefix):\n         # No need to show port and prefix for hosted:\n-        if hostname == config.DEFAULT_HOSTNAME:\n-            self.server_entry.set_text(config.DEFAULT_HOSTNAME)\n+        if hostname == base_config.DEFAULT_HOSTNAME:\n+            self.server_entry.set_text(base_config.DEFAULT_HOSTNAME)\n         else:\n             self.server_entry.set_text(\"%s:%s%s\" % (hostname,\n                                        port, prefix))\n@@ -1896,9 +1892,9 @@ def _on_validate_server(self, info, error=None):\n                       None)\n             self.pre_done()\n             return\n-        CFG.set('server', 'hostname', hostname)\n-        CFG.set('server', 'port', port)\n-        CFG.set('server', 'prefix', prefix)\n+        conf['server']['hostname'] = hostname\n+        conf['server']['port'] = port\n+        conf['server']['prefix'] = prefix\n \n         self.pre_done()\n         if self.info.get_property('use_activation_keys'):\n@@ -1996,36 +1992,70 @@ def _get_environment_list(self, owner_key, callback):\n         except Exception:\n             self.queue.put((callback, None, sys.exc_info()))\n \n-    def _register_consumer(self, name, facts, owner, env, activation_keys, callback):\n+    def _register_consumer(self, name, owner, env, activation_keys, callback):\n         \"\"\"\n         method run in the worker thread.\n         \"\"\"\n         try:\n+            # We've got several steps here that all happen in this thread\n+            #\n+            # Behing a 'gather system info' screen?\n+            #  get installed prods\n+            #  get facts (local collection or facts dbus service)\n+            #\n+            # run pre_register plugin (in main?)\n+            # ACTUALLY REGISTER (the network call)\n+            # run post_register plugin (in main?)\n+            #\n+            # persist identity\n+            #  # These could move to call back\n+            # reload identity\n+            # persist new installed products info ?\n+            # persist facts cache (for now)\n+            # persist new consumer cert\n+            # # already branch to make this a seperate page/thread\n+            # update package profile (ie, read rpmdb, slow...)\n+            #   which can make a package profile upload request\n+            # restart virt-who   (wat?)\n+            #\n+            # We should probably split that up some.\n+            #\n             installed_mgr = require(INSTALLED_PRODUCTS_MANAGER)\n \n-            # TODO: not sure why we pass in a facts.Facts, and call it's\n-            #       get_facts() three times. The two bracketing plugin calls\n-            #       are meant to be able to enhance/tweak facts\n-            #\n-            # TODO: plugin hooks could run in the main thread\n-            #       Really anything that doesn't use retval.\n+            # A proxy to the dbus service\n+            facts_dbus_client = facts.FactsClient()\n+\n+            # Note: for now, this is blocking. Maybe we should do it\n+            #       in the gui mainthread async and pass it in?\n+\n+            log.debug(\"about to dbus GetFacts\")\n+            facts_dict = facts_dbus_client.GetFacts()\n+            log.debug(\"finished doing dbus GetFacts\")\n+\n+            # TODO: We end up calling plugins from threads, which is a little weird.\n+            #       Seems like a reasonable place to go back to main thread, run the\n+            #       plugin, run the network request in a thread, come back to main, run post\n+            #       plugin, etc.\n+\n             self.plugin_manager.run(\"pre_register_consumer\", name=name,\n-                                    facts=facts.get_facts())\n+                                    facts=facts_dict)\n \n             cp = self.backend.cp_provider.get_basic_auth_cp()\n-            retval = cp.registerConsumer(name=name, facts=facts.get_facts(),\n+            retval = cp.registerConsumer(name=name, facts=facts_dict,\n                                          owner=owner, environment=env,\n                                          keys=activation_keys,\n                                          installed_products=installed_mgr.format_for_server())\n \n             self.plugin_manager.run(\"post_register_consumer\", consumer=retval,\n-                                    facts=facts.get_facts())\n+                                    facts=facts_dict)\n \n             # TODO: split persisting info into it's own thread\n             require(IDENTITY).reload()\n+\n+            # TODO: This will be rhsm-facts.services problem now\n             # Facts and installed products went out with the registration\n             # request, manually write caches to disk:\n-            facts.write_cache()\n+            # facts.write_cache()\n             installed_mgr.write_cache()\n \n             # Write the identity cert to disk\n@@ -2119,7 +2149,7 @@ def _subscribe(self, uuid, current_sla, dry_run_result, callback):\n     #    update_other_action_client_stuff\n     # for sla in available_slas:\n     #   get_dry_run_bind for sla\n-    def _find_suitable_service_levels(self, consumer_uuid, facts):\n+    def _find_suitable_service_levels(self, consumer_uuid):\n \n         # FIXME:\n         self.backend.update()\n@@ -2155,7 +2185,7 @@ def _find_suitable_service_levels(self, consumer_uuid, facts):\n         suitable_slas = {}\n \n         # eek, in a thread\n-        action_client = ActionClient(facts=facts)\n+        action_client = ActionClient()\n         action_client.update()\n \n         for sla in available_slas:\n@@ -2178,12 +2208,12 @@ def _find_suitable_service_levels(self, consumer_uuid, facts):\n         # why do we call cert_sorter stuff in the return?\n         return (current_sla, self.backend.cs.unentitled_products.values(), suitable_slas)\n \n-    def _find_service_levels(self, consumer_uuid, facts, callback):\n+    def _find_service_levels(self, consumer_uuid, callback):\n         \"\"\"\n         method run in the worker thread.\n         \"\"\"\n         try:\n-            suitable_slas = self._find_suitable_service_levels(consumer_uuid, facts)\n+            suitable_slas = self._find_suitable_service_levels(consumer_uuid)\n             self.queue.put((callback, suitable_slas, None))\n         except Exception:\n             self.queue.put((callback, None, sys.exc_info()))\n@@ -2237,34 +2267,36 @@ def get_environment_list(self, owner_key, callback):\n                                             name=\"GetEnvironmentListThread\",\n                                             args=(owner_key, callback)))\n \n-    def register_consumer(self, name, facts, owner, env, activation_keys, callback):\n+    def register_consumer(self, name, owner, env, activation_keys, callback):\n         \"\"\"\n         Run consumer registration asyncronously\n         \"\"\"\n         ga_GObject.idle_add(self._watch_thread)\n-        self._start_thread(threading.Thread(target=self._register_consumer,\n-                                            name=\"RegisterConsumerThread\",\n-                                            args=(name, facts, owner,\n-                                                  env, activation_keys, callback)))\n+        self._start_thread(threading.Thread(\n+            target=self._register_consumer,\n+            name=\"RegisterConsumerThread\",\n+            args=(name, owner, env, activation_keys, callback)))\n \n     def update_package_profile(self, uuid, callback):\n         ga_GObject.idle_add(self._watch_thread)\n-        self._start_thread(threading.Thread(target=self._update_package_profile,\n-                                            name=\"UpdatePackageProfileThread\",\n-                                            args=(uuid, callback)))\n+        self._start_thread(threading.Thread(\n+            target=self._update_package_profile,\n+            name=\"UpdatePackageProfileThread\",\n+            args=(uuid, callback)))\n \n     def subscribe(self, uuid, current_sla, dry_run_result, callback):\n         ga_GObject.idle_add(self._watch_thread)\n-        self._start_thread(threading.Thread(target=self._subscribe,\n-                                            name=\"SubscribeThread\",\n-                                            args=(uuid, current_sla,\n-                                                  dry_run_result, callback)))\n+        self._start_thread(threading.Thread(\n+            target=self._subscribe,\n+            name=\"SubscribeThread\",\n+            args=(uuid, current_sla, dry_run_result, callback)))\n \n-    def find_service_levels(self, consumer_uuid, facts, callback):\n+    def find_service_levels(self, consumer_uuid, callback):\n         ga_GObject.idle_add(self._watch_thread)\n-        self._start_thread(threading.Thread(target=self._find_service_levels,\n-                                            name=\"FindServiceLevelsThread\",\n-                                            args=(consumer_uuid, facts, callback)))\n+        self._start_thread(threading.Thread(\n+            target=self._find_service_levels,\n+            name=\"FindServiceLevelsThread\",\n+            args=(consumer_uuid, callback)))\n \n     def refresh(self, callback):\n         ga_GObject.idle_add(self._watch_thread)\n@@ -2291,8 +2323,8 @@ def validate_server(self, hostname, port, prefix, callback):\n class DoneScreen(Screen):\n     gui_file = \"done_box\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(DoneScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(DoneScreen, self).__init__(reg_info, async_backend, parent_window)\n         self.pre_message = \"We are done.\"\n \n     def pre(self):\n@@ -2310,14 +2342,14 @@ class InfoScreen(Screen):\n     Also allows the user to skip registration if they wish.\n     \"\"\"\n     widget_names = Screen.widget_names + [\n-                'register_radio',\n-                'skip_radio',\n-                'why_register_dialog'\n-        ]\n+        'register_radio',\n+        'skip_radio',\n+        'why_register_dialog'\n+    ]\n     gui_file = \"registration_info\"\n \n-    def __init__(self, reg_info, async_backend, facts, parent_window):\n-        super(InfoScreen, self).__init__(reg_info, async_backend, facts, parent_window)\n+    def __init__(self, reg_info, async_backend, parent_window):\n+        super(InfoScreen, self).__init__(reg_info, async_backend, parent_window)\n         self.button_label = _(\"_Next\")\n         callbacks = {\"on_why_register_button_clicked\":\n                      self._on_why_register_button_clicked,"
        },
        {
          "filename": "src/subscription_manager/gui/utils.py",
          "status": "modified",
          "additions": 3,
          "deletions": 0,
          "patch": "@@ -63,6 +63,9 @@ def handle_gui_exception(e, msg, parent, format_msg=True, log_msg=None):\n     format_msg = if true, string sub the exception error in the msg\n     \"\"\"\n     if isinstance(e, tuple):\n+        if not log_msg:\n+            log_msg = str(e[1])\n+\n         log.error(log_msg, exc_info=e)\n         # Get the class instance of the exception\n         e = e[1]"
        },
        {
          "filename": "src/subscription_manager/identity.py",
          "status": "modified",
          "additions": 4,
          "deletions": 3,
          "patch": "@@ -20,8 +20,9 @@\n from rhsm.config import initConfig\n from subscription_manager.certdirectory import Path\n \n-CFG = initConfig()\n+from rhsmlib.services import config\n \n+conf = config.Config(initConfig())\n log = logging.getLogger(__name__)\n \n \n@@ -31,7 +32,7 @@ class ConsumerIdentity:\n     Includes helpers for reading/writing consumer identity certificates\n     from disk.\"\"\"\n \n-    PATH = CFG.get('rhsm', 'consumerCertDir')\n+    PATH = conf['rhsm']['consumerCertDir']\n     KEY = 'key.pem'\n     CERT = 'cert.pem'\n \n@@ -147,7 +148,7 @@ def reload(self):\n             self.consumer = None\n             self.name = None\n             self.uuid = None\n-            self.cert_dir_path = CFG.get('rhsm', 'consumerCertDir')\n+            self.cert_dir_path = conf['rhsm']['consumerCertDir']\n \n     def _get_consumer_identity(self):\n         # FIXME: wrap in exceptions, catch IOErrors etc, raise anything else"
        },
        {
          "filename": "src/subscription_manager/lock.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -101,7 +101,7 @@ def __init__(self, path):\n         self.lockdir = None\n         self.blocking = None\n \n-        lock_dir, fn = os.path.split(self.path)\n+        lock_dir, _fn = os.path.split(self.path)\n         try:\n             if not os.path.exists(lock_dir):\n                 os.makedirs(lock_dir)"
        },
        {
          "filename": "src/subscription_manager/managercli.py",
          "status": "modified",
          "additions": 92,
          "deletions": 75,
          "patch": "@@ -41,6 +41,7 @@\n \n from subscription_manager.branding import get_branding\n from subscription_manager.entcertlib import EntCertActionInvoker\n+from subscription_manager.factlib import FactsActionCommand\n from subscription_manager.action_client import ActionClient, UnregisterActionClient\n from subscription_manager.cert_sorter import ComplianceManager, FUTURE_SUBSCRIBED, \\\n         SUBSCRIBED, NOT_SUBSCRIBED, EXPIRED, PARTIALLY_SUBSCRIBED, UNKNOWN\n@@ -63,11 +64,14 @@\n from subscription_manager.printing_utils import columnize, format_name, \\\n         none_wrap_columnize_callback, echo_columnize_callback, highlight_by_filter_string_columnize_callback\n \n+import rhsmlib.dbus.facts as facts\n+\n _ = gettext.gettext\n \n log = logging.getLogger(__name__)\n \n-cfg = rhsm.config.initConfig()\n+from rhsmlib.services import config\n+conf = config.Config(rhsm.config.initConfig())\n \n SM = \"subscription-manager\"\n ERR_NOT_REGISTERED_MSG = _(\"This system is not yet registered. Try 'subscription-manager register --help' for more information.\")\n@@ -315,12 +319,12 @@ def _get_logger(self):\n \n     def test_proxy_connection(self):\n         result = None\n-        if not self.proxy_hostname and not cfg.get(\"server\", \"proxy_hostname\"):\n+        if not self.proxy_hostname and not conf[\"server\"][\"proxy_hostname\"]:\n             return True\n         try:\n             s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n             s.settimeout(10)\n-            result = s.connect_ex((self.proxy_hostname or cfg.get(\"server\", \"proxy_hostname\"), int(self.proxy_port or rhsm.config.DEFAULT_PROXY_PORT)))\n+            result = s.connect_ex((self.proxy_hostname or conf[\"server\"][\"proxy_hostname\"], int(self.proxy_port or rhsm.config.DEFAULT_PROXY_PORT)))\n         except Exception as e:\n             log.info(\"Attempted bad proxy: %s\" % e)\n             return False\n@@ -422,21 +426,21 @@ def main(self, args=None):\n             system_exit(os.EX_USAGE)\n \n         if hasattr(self.options, \"insecure\") and self.options.insecure:\n-            cfg.set(\"server\", \"insecure\", \"1\")\n+            conf[\"server\"][\"insecure\"] = \"1\"\n             config_changed = True\n \n         if hasattr(self.options, \"server_url\") and self.options.server_url:\n             try:\n                 (self.server_hostname,\n                  self.server_port,\n-                 self.server_prefix) = parse_server_info(self.options.server_url, cfg)\n+                 self.server_prefix) = parse_server_info(self.options.server_url, conf)\n             except ServerUrlParseError, e:\n                 print _(\"Error parsing serverurl:\")\n                 handle_exception(\"Error parsing serverurl:\", e)\n \n-            cfg.set(\"server\", \"hostname\", self.server_hostname)\n-            cfg.set(\"server\", \"port\", self.server_port)\n-            cfg.set(\"server\", \"prefix\", self.server_prefix)\n+            conf[\"server\"][\"hostname\"] = self.server_hostname\n+            conf[\"server\"][\"port\"] = self.server_port\n+            conf[\"server\"][\"prefix\"] = self.server_prefix\n             if self.server_port:\n                 self.server_port = int(self.server_port)\n             config_changed = True\n@@ -450,9 +454,10 @@ def main(self, args=None):\n                 print _(\"Error parsing baseurl:\")\n                 handle_exception(\"Error parsing baseurl:\", e)\n \n-            cfg.set(\"rhsm\", \"baseurl\", format_baseurl(baseurl_server_hostname,\n-                                                      baseurl_server_port,\n-                                                      baseurl_server_prefix))\n+            conf[\"rhsm\"][\"baseurl\"] = format_baseurl(\n+                baseurl_server_hostname,\n+                baseurl_server_port,\n+                baseurl_server_prefix)\n             config_changed = True\n \n         # support foo.example.com:3128 format\n@@ -464,7 +469,7 @@ def main(self, args=None):\n                 self.proxy_port = int(parts[1])\n             else:\n                 # if no port specified, use the one from the config, or fallback to the default\n-                self.proxy_port = cfg.get_int('server', 'proxy_port') or rhsm.config.DEFAULT_PROXY_PORT\n+                self.proxy_port = conf['server'].get_int('proxy_port') or rhsm.config.DEFAULT_PROXY_PORT\n             config_changed = True\n \n         if hasattr(self.options, \"proxy_user\") and self.options.proxy_user:\n@@ -538,7 +543,7 @@ def main(self, args=None):\n \n             # Only persist the config changes if there was no exception\n             if config_changed and self.persist_server_options():\n-                cfg.save()\n+                conf.persist()\n \n             if return_code is not None:\n                 return return_code\n@@ -1104,22 +1109,27 @@ def _do_command(self):\n \n         self.cp_provider.clean()\n \n-        facts = inj.require(inj.FACTS)\n+        # A proxy to the dbus service\n+        facts_dbus_client = facts.FactsClient()\n \n         # Proceed with new registration:\n         try:\n             if not self.options.activation_keys:\n                 print _(\"Registering to: %s:%s%s\") % \\\n-                    (cfg.get(\"server\", \"hostname\"), cfg.get(\"server\", \"port\"), cfg.get(\"server\", \"prefix\"))\n+                    (conf[\"server\"][\"hostname\"], conf[\"server\"][\"port\"], conf[\"server\"][\"prefix\"])\n                 self.cp_provider.set_user_pass(self.username, self.password)\n                 admin_cp = self.cp_provider.get_basic_auth_cp()\n             else:\n                 admin_cp = self.cp_provider.get_no_auth_cp()\n \n-            facts_dic = facts.get_facts()\n+            # TODO/FIXME: revisit register cli refactor branch and combine it and\n+            #             AsyncBackend to run this as a series of states triggered by events\n+            # This is blocking and not async, which aside from blocking here, also\n+            # means things like following name owner changes gets weird.\n+            facts_dict = facts_dbus_client.GetFacts()\n \n             self.plugin_manager.run(\"pre_register_consumer\", name=consumername,\n-                                    facts=facts_dic)\n+                                    facts=facts_dict)\n \n             if self.options.consumerid:\n                 # TODO remove the username/password\n@@ -1142,14 +1152,14 @@ def _do_command(self):\n                         self.options.environment)\n \n                 consumer = admin_cp.registerConsumer(name=consumername,\n-                     type=self.options.consumertype, facts=facts_dic,\n+                     type=self.options.consumertype, facts=facts_dict,\n                      owner=owner_key, environment=environment_id,\n                      keys=self.options.activation_keys,\n                      installed_products=self.installed_mgr.format_for_server(),\n                      content_tags=self.installed_mgr.tags)\n                 self.installed_mgr.write_cache()\n             self.plugin_manager.run(\"post_register_consumer\", consumer=consumer,\n-                                    facts=facts_dic)\n+                                    facts=facts_dict)\n         except connection.RestlibException, re:\n             log.exception(re)\n             system_exit(os.EX_SOFTWARE, re.msg)\n@@ -1177,15 +1187,20 @@ def _do_command(self):\n         # Must update facts to clear out the old ones:\n         if self.options.consumerid:\n             log.info(\"Updating facts\")\n-            facts.update_check(self.cp, consumer['uuid'], force=True)\n+            #\n+            # FIXME: Need a ConsumerFacts.sync or update or something\n+            # TODO: We register, with facts, then update facts again...?\n+            #       Are we trying to sync potential new or dynamic facts?\n+            #facts.update_check(self.cp, consumer['uuid'], force=True)\n \n         profile_mgr = inj.require(inj.PROFILE_MANAGER)\n         # 767265: always force an upload of the packages when registering\n         profile_mgr.update_check(self.cp, consumer['uuid'], True)\n \n         # Facts and installed products went out with the registration request,\n         # manually write caches to disk:\n-        facts.write_cache()\n+        # facts service job now(soon)\n+        #facts.write_cache()\n         self.installed_mgr.update_check(self.cp, consumer['uuid'])\n \n         if self.options.release:\n@@ -1369,9 +1384,14 @@ def _do_command(self):\n         try:\n             # FIXME: why just facts and package profile update here?\n             # update facts first, if we need to\n-            facts = inj.require(inj.FACTS)\n-            facts.update_check(self.cp, self.identity.uuid)\n \n+            # FIXME: either do above, or add a ConsumerFacts model and populate\n+            #        it from dbus call, and update_check() to send to candlepin\n+            #facts = inj.require(inj.FACTS)\n+            #facts.update_check(self.cp, self.identity.uuid)\n+\n+            # ie, don't forget to fix me\n+            raise Exception('facts syncing not implemented yet, so this should fail tests.')\n             profile_mgr = inj.require(inj.PROFILE_MANAGER)\n             profile_mgr.update_check(self.cp, self.identity.uuid)\n \n@@ -1426,9 +1446,9 @@ def show_current_release(self):\n \n     def _do_command(self):\n \n-        cdn_url = cfg.get('rhsm', 'baseurl')\n+        cdn_url = conf['rhsm']['baseurl']\n         # note: parse_baseurl_info will populate with defaults if not found\n-        (cdn_hostname, cdn_port, cdn_prefix) = parse_baseurl_info(cdn_url)\n+        (cdn_hostname, cdn_port, _cdn_prefix) = parse_baseurl_info(cdn_url)\n \n         # Base CliCommand has already setup proxy info etc\n         self.cp_provider.set_content_connection_info(cdn_hostname=cdn_hostname,\n@@ -1495,11 +1515,11 @@ def __init__(self):\n         _(\"All installed products are covered by valid entitlements.\")\n         _(\"No need to update subscriptions at this time.\")\n \n-    def _read_pool_ids(self, file):\n+    def _read_pool_ids(self, f):\n         if not self.options.pool:\n             self.options.pool = []\n \n-        for line in fileinput.input(file):\n+        for line in fileinput.input(f):\n             for pool in filter(bool, re.split(r\"\\s+\", line.strip())):\n                 self.options.pool.append(pool)\n \n@@ -1848,31 +1868,26 @@ def _validate_options(self):\n     def _do_command(self):\n         self._validate_options()\n \n-        identity = inj.require(inj.IDENTITY)\n         if self.options.list:\n-            facts = inj.require(inj.FACTS)\n-            fact_dict = facts.get_facts()\n-            fact_keys = fact_dict.keys()\n-            fact_keys.sort()\n-            for key in fact_keys:\n-                value = fact_dict[key]\n+            # A proxy to the dbus service\n+            facts_dbus_client = facts.FactsClient()\n+\n+            facts_dict = facts_dbus_client.GetFacts()\n+            facts_keys = facts_dict.keys()\n+            facts_keys.sort()\n+\n+            for key in facts_keys:\n+                value = facts_dict[key]\n                 if str(value).strip() == \"\":\n                     value = _(\"Unknown\")\n                 print \"%s: %s\" % (key, value)\n \n         if self.options.update:\n-            facts = inj.require(inj.FACTS)\n-            try:\n-                facts.update_check(self.cp, identity.uuid, force=True)\n-            except connection.RestlibException, re:\n-                log.exception(re)\n-                system_exit(os.EX_SOFTWARE, re.msg)\n-            log.info(\"Succesfully updated the system facts.\")\n-            print _(\"Successfully updated the system facts.\")\n+            facts_action_command = FactsActionCommand()\n+            facts_action_command.update()\n \n \n class ImportCertCommand(CliCommand):\n-\n     def __init__(self):\n         shortdesc = _(\"Import certificates which were provided outside of the tool\")\n         super(ImportCertCommand, self).__init__(\"import\", shortdesc, False)\n@@ -2169,45 +2184,48 @@ def __init__(self):\n                                help=_(\"list the configuration for this system\"))\n         self.parser.add_option(\"--remove\", dest=\"remove\", action=\"append\",\n                                help=_(\"remove configuration entry by section.name\"))\n-        for section in cfg.sections():\n-            for name, value in cfg.items(section):\n-                self.parser.add_option(\"--\" + section + \".\" + name, dest=(section + \".\" + name),\n-                    help=_(\"Section: %s, Name: %s\") % (section, name))\n+        for s in conf.keys():\n+            section = conf[s]\n+            for name, _value in section.items():\n+                self.parser.add_option(\"--\" + s + \".\" + name, dest=(s + \".\" + name),\n+                    help=_(\"Section: %s, Name: %s\") % (s, name))\n \n     def _validate_options(self):\n         if self.options.list:\n             too_many = False\n             if self.options.remove:\n                 too_many = True\n             else:\n-                for section in cfg.sections():\n-                    for name, value in cfg.items(section):\n-                        if getattr(self.options, section + \".\" + name):\n+                for s in conf.keys():\n+                    section = conf[s]\n+                    for name, _value in section.items():\n+                        if getattr(self.options, s + \".\" + name):\n                             too_many = True\n                             break\n             if too_many:\n                 system_exit(os.EX_USAGE, _(\"Error: --list should not be used with any other options for setting or removing configurations.\"))\n \n         if not (self.options.list or self.options.remove):\n             has = False\n-            for section in cfg.sections():\n-                for name, value in cfg.items(section):\n-                    test = \"%s\" % getattr(self.options, section + \".\" + name)\n+            for s in conf.keys():\n+                section = conf[s]\n+                for name, _value in section.items():\n+                    test = \"%s\" % getattr(self.options, s + \".\" + name)\n                     has = has or (test != 'None')\n             if not has:\n                 # if no options are given, default to --list\n                 self.options.list = True\n \n         if self.options.remove:\n             for r in self.options.remove:\n-                if not \".\" in r:\n+                if not \".\" in r:  # pragma: noqa\n                     system_exit(os.EX_USAGE, _(\"Error: configuration entry designation for removal must be of format [section.name]\"))\n \n                 section = r.split('.')[0]\n                 name = r.split('.')[1]\n                 found = False\n-                if cfg.has_section(section):\n-                    for key, value in cfg.items(section):\n+                if section in conf.keys():\n+                    for key, _value in conf[section].items():\n                         if name == key:\n                             found = True\n                 if not found:\n@@ -2217,14 +2235,15 @@ def _do_command(self):\n         self._validate_options()\n \n         if self.options.list:\n-            for section in cfg.sections():\n-                print '[%s]' % (section)\n-                source_list = cfg.items(section)\n+            for s in conf.keys():\n+                section = conf[s]\n+                print '[%s]' % s\n+                source_list = section.items()\n                 source_list.sort()\n                 for (name, value) in source_list:\n                     indicator1 = ''\n                     indicator2 = ''\n-                    if (value == cfg.get_default(section, name)):\n+                    if value == section.get_default(name):\n                         indicator1 = '['\n                         indicator2 = ']'\n                     print '   %s = %s%s%s' % (name, indicator1, value, indicator2)\n@@ -2236,23 +2255,24 @@ def _do_command(self):\n                 section = r.split('.')[0]\n                 name = r.split('.')[1]\n                 try:\n-                    if not cfg.has_default(section, name):\n-                        cfg.set(section, name, '')\n+                    if not conf[section].has_default(name):\n+                        conf[section][name] = ''\n                         print _(\"You have removed the value for section %s and name %s.\") % (section, name)\n                     else:\n-                        cfg.set(section, name, cfg.get_default(section, name))\n+                        conf[section][name] = conf[section].get_default(name)\n                         print _(\"You have removed the value for section %s and name %s.\") % (section, name)\n                         print _(\"The default value for %s will now be used.\") % (name)\n                 except Exception:\n                     print _(\"Section %s and name %s cannot be removed.\") % (section, name)\n-            cfg.save()\n+            conf.persist()\n         else:\n-            for section in cfg.sections():\n-                for name, value in cfg.items(section):\n-                    value = \"%s\" % getattr(self.options, section + \".\" + name)\n+            for s in conf.keys():\n+                section = conf[s]\n+                for name, value in section.items():\n+                    value = \"%s\" % getattr(self.options, s + \".\" + name)\n                     if not value == 'None':\n-                        cfg.set(section, name, value)\n-            cfg.save()\n+                        section[name] = value\n+            conf.persist()\n \n     def require_connection(self):\n         return False\n@@ -2342,9 +2362,7 @@ def _do_command(self):\n                     system_exit(os.EX_DATAERR,\n                                 msg.format(dateexample=dateexample))\n \n-            facts = inj.require(inj.FACTS)\n-            epools = managerlib.get_available_entitlements(facts=facts,\n-                                                           get_all=self.options.all,\n+            epools = managerlib.get_available_entitlements(get_all=self.options.all,\n                                                            active_on=on_date,\n                                                            overlapping=self.options.no_overlap,\n                                                            uninstalled=self.options.match_installed,\n@@ -2532,8 +2550,7 @@ def print_consumed(self, service_level=None, filter_string=None, pid_only=False)\n                             pool_type,\n                             managerlib.format_date(cert.valid_range.begin()),\n                             managerlib.format_date(cert.valid_range.end()),\n-                            system_type, **kwargs\n-                        ) + \"\\n\"\n+                            system_type, **kwargs) + \"\\n\"\n             elif not pid_only:\n                 if filter_string and service_level:\n                     print(\n@@ -2578,7 +2595,7 @@ def _colon_split(self, option, opt_str, value, parser):\n         if value.strip() == '':\n             raise OptionValueError(_(\"You must specify an override in the form of \\\"name:value\\\" with --add.\"))\n \n-        k, colon, v = value.partition(':')\n+        k, _colon, v = value.partition(':')\n         if not v or not k:\n             raise OptionValueError(_(\"--add arguments should be in the form of \\\"name:value\\\"\"))\n "
        },
        {
          "filename": "src/subscription_manager/managerlib.py",
          "status": "modified",
          "additions": 31,
          "deletions": 19,
          "patch": "@@ -31,11 +31,10 @@\n import subscription_manager.cache as cache\n from subscription_manager.cert_sorter import StackingGroupSorter, ComplianceManager\n from subscription_manager import identity\n-from subscription_manager.facts import Facts\n from subscription_manager.injection import require, CERT_SORTER, \\\n         IDENTITY, ENTITLEMENT_STATUS_CACHE, \\\n         PROD_STATUS_CACHE, ENT_DIR, PROD_DIR, CP_PROVIDER, OVERRIDE_STATUS_CACHE, \\\n-        POOLTYPE_CACHE, RELEASE_STATUS_CACHE\n+        POOLTYPE_CACHE, RELEASE_STATUS_CACHE, FACTS\n from subscription_manager import isodate\n from subscription_manager.jsonwrapper import PoolWrapper\n from subscription_manager.repolib import RepoActionInvoker\n@@ -84,8 +83,7 @@ def __init__(self, errors):\n         self.errors = errors\n \n     def __str__(self, reason=\"\"):\n-        msg = 'Entitlement Certificate(s) update failed due to the following reasons:\\n' + \\\n-        '\\n'.join(self.errors)\n+        msg = 'Entitlement Certificate(s) update failed due to the following reasons:\\n' + '\\n'.join(self.errors)\n         return msg\n \n \n@@ -259,14 +257,30 @@ def filter_subscribed_pools(self, pools, subscribed_pool_ids,\n         return filtered_pools\n \n \n-def list_pools(uep, consumer_uuid, facts, list_all=False, active_on=None, filter_string=None):\n+def list_pools(uep, consumer_uuid, list_all=False, active_on=None, filter_string=None):\n     \"\"\"\n     Wrapper around the UEP call to fetch pools, which forces a facts update\n     if anything has changed before making the request. This ensures the\n     rule checks server side will have the most up to date info about the\n     consumer possible.\n     \"\"\"\n-    facts.update_check(uep, consumer_uuid)\n+\n+    # client tells service 'look for facts again'\n+    # if service finds new facts:\n+    #     -emit a signal?\n+    #     - or just update properties\n+    #       - and set a 'been_synced' property to False\n+    # client waits for facts check to finish\n+    # if no changes or been_synced=True, continue\n+    # if changes or unsynced:\n+    #    subman updates candlepin with the latest version of services GetFacts() [blocking]\n+    #    when finished, subman emit's 'factsSyncFinished'\n+    #        - then service flops 'been_synced' property\n+    #    -or- subman calls 'here_are_the_latest_facts_to_the_server()' on service\n+    #         then service flops 'been_synced' property\n+    # subman gets signal that props changed, and that been_synced is now true\n+    # since it's been synced, then subman continues\n+    require(FACTS).update_check(uep, consumer_uuid)\n \n     profile_mgr = cache.ProfileManager()\n     profile_mgr.update_check(uep, consumer_uuid)\n@@ -281,14 +295,11 @@ def list_pools(uep, consumer_uuid, facts, list_all=False, active_on=None, filter\n # TODO: This method is morphing the actual pool json and returning a new\n # dict which does not contain all the pool info. Not sure if this is really\n # necessary. Also some \"view\" specific things going on in here.\n-def get_available_entitlements(facts, get_all=False, active_on=None,\n-        overlapping=False, uninstalled=False, text=None, filter_string=None):\n+def get_available_entitlements(get_all=False, active_on=None, overlapping=False,\n+                               uninstalled=False, text=None, filter_string=None):\n     \"\"\"\n     Returns a list of entitlement pools from the server.\n \n-    Facts will be updated if appropriate before making the request, to ensure\n-    the rules on the server will pass if appropriate.\n-\n     The 'all' setting can be used to return all pools, even if the rules do\n     not pass. (i.e. show pools that are incompatible for your hardware)\n     \"\"\"\n@@ -309,7 +320,7 @@ def get_available_entitlements(facts, get_all=False, active_on=None,\n         'management_enabled'\n     ]\n \n-    pool_stash = PoolStash(Facts(require(ENT_DIR), require(PROD_DIR)))\n+    pool_stash = PoolStash()\n     dlist = pool_stash.get_filtered_pools_list(active_on, not get_all,\n            overlapping, uninstalled, text, filter_string)\n \n@@ -444,9 +455,8 @@ class PoolStash(object):\n     Object used to fetch pools from the server, sort them into compatible,\n     incompatible, and installed lists. Also does filtering based on name.\n     \"\"\"\n-    def __init__(self, facts):\n+    def __init__(self):\n         self.identity = require(IDENTITY)\n-        self.facts = facts\n         self.sorter = None\n \n         # Pools which passed rules server side for this consumer:\n@@ -477,15 +487,15 @@ def refresh(self, active_on):\n         self.compatible_pools = {}\n         log.debug(\"Refreshing pools from server...\")\n         for pool in list_pools(require(CP_PROVIDER).get_consumer_auth_cp(),\n-                self.identity.uuid, self.facts, active_on=active_on):\n+                self.identity.uuid, active_on=active_on):\n             self.compatible_pools[pool['id']] = pool\n             self.all_pools[pool['id']] = pool\n \n         # Filter the list of all pools, removing those we know are compatible.\n         # Sadly this currently requires a second query to the server.\n         self.incompatible_pools = {}\n         for pool in list_pools(require(CP_PROVIDER).get_consumer_auth_cp(),\n-                self.identity.uuid, self.facts, list_all=True, active_on=active_on):\n+                self.identity.uuid, list_all=True, active_on=active_on):\n             if not pool['id'] in self.compatible_pools:\n                 self.incompatible_pools[pool['id']] = pool\n                 self.all_pools[pool['id']] = pool\n@@ -516,11 +526,11 @@ def get_filtered_pools_list(self, active_on, incompatible,\n \n         if incompatible:\n             for pool in list_pools(require(CP_PROVIDER).get_consumer_auth_cp(),\n-                    self.identity.uuid, self.facts, active_on=active_on, filter_string=filter_string):\n+                    self.identity.uuid, active_on=active_on, filter_string=filter_string):\n                 self.compatible_pools[pool['id']] = pool\n         else:  # --all has been used\n             for pool in list_pools(require(CP_PROVIDER).get_consumer_auth_cp(),\n-                    self.identity.uuid, self.facts, list_all=True, active_on=active_on, filter_string=filter_string):\n+                    self.identity.uuid, list_all=True, active_on=active_on, filter_string=filter_string):\n                 self.all_pools[pool['id']] = pool\n \n         return self._filter_pools(incompatible, overlapping, uninstalled, False, text)\n@@ -862,7 +872,9 @@ def clean_all_data(backup=True):\n     # for deleting persistent caches\n     cache.ProfileManager.delete_cache()\n     cache.InstalledProductsManager.delete_cache()\n-    Facts.delete_cache()\n+\n+    # FIXME: implement as dbus client to facts service DeleteCache() once implemented\n+    #Facts.delete_cache()\n \n     # WrittenOverridesCache is also a subclass of cache.CacheManager, but\n     # it is deleted in RepoActionInvoker.delete_repo_file() below."
        },
        {
          "filename": "src/subscription_manager/migrate/migrate.py",
          "status": "modified",
          "additions": 18,
          "deletions": 17,
          "patch": "@@ -25,8 +25,6 @@\n import subprocess\n import sys\n \n-import rhsm.config\n-\n from datetime import datetime\n from rhsm.https import ssl\n \n@@ -46,6 +44,9 @@\n from rhsm.utils import parse_url\n from rhsm import ourjson as json\n \n+from rhsm.config import initConfig\n+from rhsmlib.services import config\n+\n _RHNLIBPATH = \"/usr/share/rhn\"\n if _RHNLIBPATH not in sys.path:\n     sys.path.append(_RHNLIBPATH)\n@@ -157,7 +158,7 @@ def __init__(self, username, password):\n class MigrationEngine(object):\n     def __init__(self, options):\n         self.rhncfg = initUp2dateConfig()\n-        self.rhsmcfg = rhsm.config.initConfig()\n+        self.rhsmcfg = config.Config(initConfig())\n \n         # Sometimes we need to send up the entire contents of the system id file\n         # which is referred to in Satellite 5 nomenclature as a \"certificate\"\n@@ -226,25 +227,25 @@ def transfer_http_proxy_settings(self):\n             if self.options.noproxy:\n                 # If the user doesn't want to use a proxy to connect to their subscription\n                 # management server, then remove any proxy information that may have crept in.\n-                self.rhsmcfg.set('server', 'proxy_hostname', '')\n-                self.rhsmcfg.set('server', 'proxy_port', '')\n-                self.rhsmcfg.set('server', 'proxy_user', '')\n-                self.rhsmcfg.set('server', 'proxy_password', '')\n+                self.rhsmcfg['server']['proxy_hostname'] = ''\n+                self.rhsmcfg['server']['proxy_port'] = ''\n+                self.rhsmcfg['server']['proxy_user'] = ''\n+                self.rhsmcfg['server']['proxy_password'] = ''\n             else:\n-                self.rhsmcfg.set('server', 'proxy_hostname', self.proxy_host)\n-                self.rhsmcfg.set('server', 'proxy_port', self.proxy_port)\n-                self.rhsmcfg.set('server', 'proxy_user', self.proxy_user or '')\n-                self.rhsmcfg.set('server', 'proxy_password', self.proxy_pass or '')\n-            self.rhsmcfg.save()\n+                self.rhsmcfg['server']['proxy_hostname'] = self.proxy_host\n+                self.rhsmcfg['server']['proxy_port'] = self.proxy_port\n+                self.rhsmcfg['server']['proxy_user'] = self.proxy_user or ''\n+                self.rhsmcfg['server']['proxy_password'] = self.proxy_pass or ''\n+            self.rhsmcfg.persist()\n \n     def _get_connection_info(self):\n         url_parse_error = os.EX_USAGE\n         try:\n             if self.options.destination_url is None:\n                 url_parse_error = os.EX_CONFIG\n-                hostname = self.rhsmcfg.get('server', 'hostname')\n-                port = self.rhsmcfg.get_int('server', 'port')\n-                prefix = self.rhsmcfg.get('server', 'prefix')\n+                hostname = self.rhsmcfg['server']['hostname']\n+                port = self.rhsmcfg['server'].get_int('port')\n+                prefix = self.rhsmcfg['server']['prefix']\n             else:\n                 (_user, _password, hostname, port, prefix) = parse_url(self.options.destination_url, default_port=443)\n         except ServerUrlParseError, e:\n@@ -961,8 +962,8 @@ def validate_options(options):\n \n \n def is_hosted():\n-    rhsmcfg = rhsm.config.initConfig()\n-    hostname = rhsmcfg.get('server', 'hostname')\n+    rhsmcfg = config.Config(initConfig())\n+    hostname = rhsmcfg['server']['hostname']\n     return bool(re.search('subscription\\.rhn\\.(.*\\.)*redhat\\.com', hostname) or\n                 re.search('subscription\\.rhsm\\.(.*\\.)*redhat\\.com', hostname))\n "
        },
        {
          "filename": "src/subscription_manager/plugin/ostree/config.py",
          "status": "modified",
          "additions": 13,
          "deletions": 11,
          "patch": "@@ -16,8 +16,10 @@\n import logging\n import os\n \n-from rhsm import config\n+from rhsm.config import initConfig, RhsmConfigParser\n+from rhsm.config import NoSectionError  # noqa: F401 Imported so we can catch elsewhere\n from subscription_manager import utils\n+from rhsmlib.services import config\n \n # iniparse.utils isn't in old versions\n # but it's always there if ostree is\n@@ -29,7 +31,7 @@\n \n log = logging.getLogger(__name__)\n \n-CFG = config.initConfig()\n+conf = config.Config(initConfig())\n \n \"\"\"Ostree has two config files, both based on the freedesktop.org\n Desktop Entry spec. This defines a file format based on \"ini\" style\n@@ -63,7 +65,7 @@ class RefspecFormatException(Exception):\n \n \n # KeyFile is the desktop.org name for ini files, more or less\n-class KeyFileConfigParser(config.RhsmConfigParser):\n+class KeyFileConfigParser(RhsmConfigParser):\n     \"\"\"A ini/ConfigParser subclass based on RhsmConfigParser.\n \n     This impl removes the RhsmConfigParser support for rhsm.config.DEFAULTS.\n@@ -76,8 +78,8 @@ class KeyFileConfigParser(config.RhsmConfigParser):\n     # but we also don't want to monkeypatch our config module.\n     #\n     # why? useful convience methods on RhsmConfigParser\n-    # why not pyxdg xdg.IniFile? woudl still have to add type casts\n-    # why noy pyxdg xdg.DesktopEntry?  the repo configs are not actually\n+    # why not pyxdg xdg.IniFile? would still have to add type casts\n+    # why not pyxdg xdg.DesktopEntry?  the repo configs are not actually\n     #    desktop files, just that format, so that class throws exceptions\n     #    in that case.\n     def defaults(self):\n@@ -173,11 +175,11 @@ def set(self, section, key, value):\n         return self.config_parser.set(section, key, value)\n \n     def get_proxy(self):\n-        proxy_host = CFG.get('server', 'proxy_hostname')\n+        proxy_host = conf['server']['proxy_hostname']\n         # proxy_port as string is fine here\n-        proxy_port = CFG.get('server', 'proxy_port')\n-        proxy_user = CFG.get('server', 'proxy_user')\n-        proxy_password = CFG.get('server', 'proxy_password')\n+        proxy_port = conf['server']['proxy_port']\n+        proxy_user = conf['server']['proxy_user']\n+        proxy_password = conf['server']['proxy_password']\n \n         proxy_uri = None\n         if proxy_host == \"\":\n@@ -205,8 +207,8 @@ def set_remote(self, ostree_remote):\n         # Assume all remotes will share the same cdn\n         # This is really info about a particular CDN, we just happen\n         # to only support one at the moment.\n-        baseurl = CFG.get('rhsm', 'baseurl')\n-        ca_cert = CFG.get('rhsm', 'repo_ca_cert')\n+        baseurl = conf['rhsm']['baseurl']\n+        ca_cert = conf['rhsm']['repo_ca_cert']\n \n         full_url = utils.url_base_join(baseurl, ostree_remote.url)\n "
        },
        {
          "filename": "src/subscription_manager/plugins.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -631,7 +631,7 @@ def is_plugin(c):\n         # find all the plugin classes with valid configs first\n         # then add them, so we skip the module if a class has a bad config\n         found_plugin_classes = []\n-        for name, clazz in sorted(plugin_classes):\n+        for _name, clazz in sorted(plugin_classes):\n \n             # We could have the module conf here, and check in that\n             # instead of a per class config. We would not be able to"
        },
        {
          "filename": "src/subscription_manager/productid.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -604,7 +604,7 @@ def _list_has_workstation_and_desktop_cert(self, product_cert_list):\n         \"\"\"determine if product cert list has desktop and workstation certs\"\"\"\n         has_workstation = False\n         has_desktop = False\n-        for product, product_cert in product_cert_list:\n+        for product, _product_cert in product_cert_list:\n             if self._is_workstation(product):\n                 has_workstation = True\n             if self._is_desktop(product):"
        },
        {
          "filename": "src/subscription_manager/repolib.py",
          "status": "modified",
          "additions": 12,
          "deletions": 11,
          "patch": "@@ -36,10 +36,11 @@\n \n from subscription_manager.certlib import ActionReport, BaseActionInvoker\n from subscription_manager.certdirectory import Path\n+from rhsmlib.services import config\n \n log = logging.getLogger(__name__)\n \n-CFG = initConfig()\n+conf = config.Config(initConfig())\n \n ALLOWED_CONTENT_TYPES = [\"yum\"]\n \n@@ -49,7 +50,7 @@\n def manage_repos_enabled():\n     manage_repos = True\n     try:\n-        manage_repos = CFG.get_int('rhsm', 'manage_repos')\n+        manage_repos = conf['rhsm'].get_int('manage_repos')\n     except ValueError, e:\n         log.exception(e)\n         return True\n@@ -316,16 +317,16 @@ def get_unique_content(self):\n \n         # baseurl and ca_cert could be \"CDNInfo\" or\n         # bundle with \"ConnectionInfo\" etc\n-        baseurl = CFG.get('rhsm', 'baseurl')\n-        ca_cert = CFG.get('rhsm', 'repo_ca_cert')\n+        baseurl = conf['rhsm']['baseurl']\n+        ca_cert = conf['rhsm']['repo_ca_cert']\n \n         content_list = self.get_all_content(baseurl, ca_cert)\n \n         # assumes items in content_list are hashable\n         return set(content_list)\n \n     # Expose as public API for RepoActionInvoker.is_managed, since that\n-    # is used by openshift tooling.\n+    # is used by Openshift tooling.\n     # See https://bugzilla.redhat.com/show_bug.cgi?id=1223038\n     def matching_content(self):\n         return model.find_content(self.ent_source,\n@@ -390,7 +391,7 @@ def update_repo(self, old_repo, new_repo):\n         \"\"\"\n         changes_made = 0\n \n-        for key, (mutable, default) in self._build_props(old_repo, new_repo).items():\n+        for key, (mutable, _default) in self._build_props(old_repo, new_repo).items():\n             new_val = new_repo.get(key)\n \n             # Mutable properties should be added if not currently defined,\n@@ -522,7 +523,7 @@ def __init__(self, repo_id, existing_values=None):\n         # NOTE: This sets the above properties to the default values even if\n         # they are not defined on disk. i.e. these properties will always\n         # appear in this dict, but their values may be None.\n-        for k, (m, d) in self.PROPERTIES.items():\n+        for k, (_m, d) in self.PROPERTIES.items():\n             if k not in self.keys():\n                 self[k] = d\n \n@@ -575,9 +576,9 @@ def _set_proxy_info(repo):\n \n         # Worth passing in proxy config info to from_ent_cert_content()?\n         # That would decouple Repo some\n-        proxy_host = CFG.get('server', 'proxy_hostname')\n+        proxy_host = conf['server']['proxy_hostname']\n         # proxy_port as string is fine here\n-        proxy_port = CFG.get('server', 'proxy_port')\n+        proxy_port = conf['server']['proxy_port']\n         if proxy_host != \"\":\n             proxy = \"https://%s\" % proxy_host\n             if proxy_port != \"\":\n@@ -586,8 +587,8 @@ def _set_proxy_info(repo):\n         # These could be empty string, in which case they will not be\n         # set in the yum repo file:\n         repo['proxy'] = proxy\n-        repo['proxy_username'] = CFG.get('server', 'proxy_user')\n-        repo['proxy_password'] = CFG.get('server', 'proxy_password')\n+        repo['proxy_username'] = conf['server']['proxy_user']\n+        repo['proxy_password'] = conf['server']['proxy_password']\n \n         return repo\n "
        },
        {
          "filename": "src/subscription_manager/rhelentbranding.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -79,7 +79,7 @@ def _get_branded_cert_product(self):\n         # same product, with the same branded name.\n \n         branded_name_set = set([])\n-        for cert, product in branded_certs:\n+        for _cert, product in branded_certs:\n             # uniq on product id and product name\n             branded_name_set.add(product.brand_name)\n "
        },
        {
          "filename": "src/subscription_manager/utils.py",
          "status": "modified",
          "additions": 5,
          "deletions": 5,
          "patch": "@@ -67,9 +67,9 @@ def parse_server_info(local_server_entry, config=None):\n     port = ''\n     prefix = ''\n     if config is not None:\n-        hostname = config.get(\"server\", \"hostname\")\n-        port = config.get(\"server\", \"port\")\n-        prefix = config.get(\"server\", \"prefix\")\n+        hostname = config[\"server\"][\"hostname\"]\n+        port = config[\"server\"][\"port\"]\n+        prefix = config[\"server\"][\"prefix\"]\n     return parse_url(local_server_entry,\n                       hostname or DEFAULT_HOSTNAME,\n                       port or DEFAULT_PORT,\n@@ -501,15 +501,15 @@ def print_error(message):\n     sys.stderr.write(\"\\n\")\n \n \n-def unique_list_items(list, hash_function=lambda x: x):\n+def unique_list_items(l, hash_function=lambda x: x):\n     \"\"\"\n     Accepts a list of items.\n     Returns a list of the unique items in the input.\n     Maintains order.\n     \"\"\"\n     observed = set()\n     unique_items = []\n-    for item in list:\n+    for item in l:\n         item_key = hash_function(item)\n         if item_key in observed:\n             continue"
        },
        {
          "filename": "subscription-manager.spec",
          "status": "modified",
          "additions": 28,
          "deletions": 26,
          "patch": "@@ -77,8 +77,10 @@ BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root-%(%{__id_u} -n)\n \n Requires:  python-ethtool\n Requires:  python-iniparse\n+Requires:  python-decorator\n+Requires:  dbus-x11\n Requires:  virt-what\n-Requires:  python-rhsm >= 1.18.1\n+Requires:  python-rhsm > 1.18.4\n Requires:  python-decorator\n \n %if 0%{?sles_version}\n@@ -218,6 +220,7 @@ Summary: Subscription Manager plugins for DNF\n Group: System Environment/Base\n Requires: %{name} = %{version}-%{release}\n Requires: dnf >= 1.0.0\n+Requires: python2-dnf-plugins-core\n \n %description -n dnf-plugin-subscription-manager\n This package provides plugins to interact with repositories and subscriptions\n@@ -330,14 +333,6 @@ rm -rf %{buildroot}\n %attr(755,root,root) %{_libexecdir}/rhsmcertd-worker\n %attr(755,root,root) %{_libexecdir}/rhsmd\n \n-# init scripts and systemd services\n-%if %use_systemd\n-    %attr(644,root,root) %{_unitdir}/rhsmcertd.service\n-    %attr(644,root,root) %{_tmpfilesdir}/%{name}.conf\n-%else\n-    %attr(755,root,root) %{_initrddir}/rhsmcertd\n-%endif\n-\n # our config dirs and files\n %attr(755,root,root) %dir %{_sysconfdir}/pki/consumer\n %attr(755,root,root) %dir %{_sysconfdir}/pki/entitlement\n@@ -346,8 +341,6 @@ rm -rf %{buildroot}\n %attr(644,root,root) %config(noreplace) %{_sysconfdir}/rhsm/rhsm.conf\n %config %attr(644,root,root) %{_sysconfdir}/rhsm/logging.conf\n \n-%config(noreplace) %{_sysconfdir}/dbus-1/system.d/com.redhat.SubscriptionManager.conf\n-\n # PAM config\n %if !0%{?sles_version}\n %{_sysconfdir}/pam.d/subscription-manager\n@@ -365,7 +358,6 @@ rm -rf %{buildroot}\n # misc system config\n %config(noreplace) %attr(644,root,root) %{_sysconfdir}/logrotate.d/subscription-manager\n %attr(700,root,root) %{_sysconfdir}/cron.daily/rhsmd\n-%{_datadir}/dbus-1/system-services/com.redhat.SubscriptionManager.service\n \n %attr(755,root,root) %dir %{_var}/log/rhsm\n %attr(755,root,root) %dir %{_var}/spool/rhsm/debug\n@@ -382,12 +374,7 @@ rm -rf %{buildroot}\n %{_sysconfdir}/bash_completion.d/rhsm-icon\n %{_sysconfdir}/bash_completion.d/rhsmcertd\n \n-%dir %{python_sitelib}/\n %dir %{python_sitelib}/subscription_manager\n-%dir %{python_sitelib}/subscription_manager/api\n-%dir %{python_sitelib}/subscription_manager/branding\n-%dir %{python_sitelib}/subscription_manager/model\n-%dir %{python_sitelib}/subscription_manager/plugin\n \n # code, python modules and packages\n %{python_sitelib}/subscription_manager-*.egg-info/*\n@@ -419,6 +406,30 @@ rm -rf %{buildroot}\n %{_prefix}/lib/yum-plugins/product-id.py*\n %{_prefix}/lib/yum-plugins/search-disabled-repos.py*\n \n+# rhsmlib\n+%dir %{python_sitelib}/rhsmlib\n+%{python_sitelib}/rhsmlib/*.py*\n+%{python_sitelib}/rhsmlib/candlepin/*.py*\n+%{python_sitelib}/rhsmlib/compat/*.py*\n+%{python_sitelib}/rhsmlib/facts/*.py*\n+%{python_sitelib}/rhsmlib/services/*.py*\n+%{python_sitelib}/rhsmlib/dbus/*.py*\n+%{python_sitelib}/rhsmlib/dbus/facts/*.py*\n+%{python_sitelib}/rhsmlib/dbus/objects/*.py*\n+\n+%{_datadir}/polkit-1/actions/com.redhat.*.policy\n+%{_datadir}/dbus-1/system-services/com.redhat.*.service\n+%attr(755,root,root) %{_libexecdir}/rhsm*-service\n+\n+# Despite the name similarity dbus-1/system.d has nothing to do with systemd\n+%config(noreplace) %{_sysconfdir}/dbus-1/system.d/com.redhat.*.conf\n+%if %use_systemd\n+    %attr(644,root,root) %{_unitdir}/*.service\n+    %attr(644,root,root) %{_tmpfilesdir}/%{name}.conf\n+%else\n+    %attr(755,root,root) %{_initrddir}/rhsmcertd\n+%endif\n+\n # Incude rt CLI tool\n %dir %{python_sitelib}/rct\n %{python_sitelib}/rct/*.py*\n@@ -448,11 +459,6 @@ rm -rf %{buildroot}\n %{_bindir}/rhsm-icon\n \n %dir %{python_sitelib}/subscription_manager/gui\n-%dir %{python_sitelib}/subscription_manager/gui/data\n-%dir %{python_sitelib}/subscription_manager/gui/data/ui\n-%dir %{python_sitelib}/subscription_manager/gui/data/glade\n-%dir %{python_sitelib}/subscription_manager/gui/data/icons\n-\n %{python_sitelib}/subscription_manager/gui/*.py*\n %{python_sitelib}/subscription_manager/gui/data/ui/*.ui\n %{python_sitelib}/subscription_manager/gui/data/glade/*.glade\n@@ -521,10 +527,6 @@ rm -rf %{buildroot}\n %files -n subscription-manager-initial-setup-addon\n %defattr(-,root,root,-)\n %dir %{_datadir}/anaconda/addons/com_redhat_subscription_manager/\n-%dir %{_datadir}/anaconda/addons/com_redhat_subscription_manager/gui/\n-%dir %{_datadir}/anaconda/addons/com_redhat_subscription_manager/gui/spokes/\n-%dir %{_datadir}/anaconda/addons/com_redhat_subscription_manager/categories/\n-%dir %{_datadir}/anaconda/addons/com_redhat_subscription_manager/ks/\n %{_datadir}/anaconda/addons/com_redhat_subscription_manager/*.py*\n %{_datadir}/anaconda/addons/com_redhat_subscription_manager/gui/*.py*\n %{_datadir}/anaconda/addons/com_redhat_subscription_manager/gui/spokes/*.ui"
        },
        {
          "filename": "test-requirements.txt",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -1,3 +1,4 @@\n+-r requirements.txt\n lxml\n nose\n nose-capturestderr\n@@ -6,7 +7,7 @@ coverage\n polib\n pep8==1.5.7\n pyflakes\n-flake8==3.0.2\n+flake8==3.0.4\n freezegun\n simplejson\n mock"
        },
        {
          "filename": "test/fixture.py",
          "status": "modified",
          "additions": 33,
          "deletions": 6,
          "patch": "@@ -26,16 +26,23 @@\n import stubs\n import subscription_manager.injection as inj\n import subscription_manager.managercli\n+from rhsmlib.services import config\n \n # use instead of the normal pid file based ActionLock\n from threading import RLock\n \n \n @contextmanager\n-def open_mock(content, **kwargs):\n+def open_mock(content=None, **kwargs):\n+    content_out = StringIO.StringIO()\n     m = mock_open(read_data=content)\n-    with patch('__builtin__.open', m, create=True, **kwargs) as m:\n-        yield m\n+    with patch('__builtin__.open', m, create=True, **kwargs) as mo:\n+        stream = StringIO.StringIO(content)\n+        rv = mo.return_value\n+        rv.write = lambda x: content_out.write(x)\n+        rv.content_out = lambda: content_out.getvalue()\n+        rv.__iter__ = lambda x: iter(stream.readlines())\n+        yield rv\n \n \n @contextmanager\n@@ -108,18 +115,38 @@ def __eq__(self, other):\n \n \n class SubManFixture(unittest.TestCase):\n+    def set_facts(self):\n+        \"\"\"Override if you need to set facts for a test.\"\"\"\n+        return {\"mock.facts\": \"true\"}\n+\n     \"\"\"\n     Can be extended by any subscription manager test case to make\n     sure nothing on the actual system is read/touched, and appropriate\n     mocks/stubs are in place.\n     \"\"\"\n     def setUp(self):\n+        # No matter what, stop all patching (even if we have a failure in setUp itself)\n         self.addCleanup(patch.stopall)\n \n         # Never attempt to use the actual managercli.cfg which points to a\n         # real file in etc.\n-        cfg_patcher = patch.object(subscription_manager.managercli, 'cfg', new=stubs.config.CFG)\n-        self.mock_cfg = cfg_patcher.start()\n+\n+        self.mock_cfg_parser = stubs.StubConfig()\n+\n+        original_conf = subscription_manager.managercli.conf\n+\n+        def unstub_conf():\n+            subscription_manager.managercli.conf = original_conf\n+\n+        # Mock makes it damn near impossible to mock a module attribute (which we shouldn't be using\n+        # in the first place because it's terrible) so we monkey-patch it ourselves.\n+        # TODO Fix this idiocy by not reading the damn config on module import\n+        subscription_manager.managercli.conf = config.Config(self.mock_cfg_parser)\n+        self.addCleanup(unstub_conf)\n+\n+        facts_host_patcher = patch('rhsmlib.dbus.facts.FactsClient', auto_spec=True)\n+        self.mock_facts_host = facts_host_patcher.start()\n+        self.mock_facts_host.return_value.GetFacts.return_value = self.set_facts()\n \n         # By default mock that we are registered. Individual test cases\n         # can override if they are testing disconnected scenario.\n@@ -244,7 +271,7 @@ def _inject_mock_valid_consumer(self, uuid=None):\n         return identity\n \n     def _inject_mock_invalid_consumer(self, uuid=None):\n-        \"\"\"For chaning injected consumer identity to one that fails is_valid()\n+        \"\"\"For chaining injected consumer identity to one that fails is_valid()\n \n         Returns the injected identity if it need to be examined.\n         \"\"\""
        },
        {
          "filename": "test/rhsmlib_test/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "test/rhsmlib_test/base.py",
          "status": "added",
          "additions": 233,
          "deletions": 0,
          "patch": "@@ -0,0 +1,233 @@\n+#! /usr/bin/env python\n+#\n+# Copyright (c) 2011 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public\n+# License as published by the Free Software Foundation; either version\n+# 2 of the License (GPLv2) or (at your option) any later version.\n+# There is NO WARRANTY for this software, express or implied,\n+# including the implied warranties of MERCHANTABILITY,\n+# NON-INFRINGEMENT, or FITNESS FOR A PARTICULAR PURPOSE. You should\n+# have received a copy of GPLv2 along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+from tempfile import NamedTemporaryFile\n+\n+try:\n+    import unittest2 as unittest\n+except ImportError:\n+    import unittest\n+\n+import os\n+import sys\n+import dbus\n+import dbus.lowlevel\n+import dbus.bus\n+import dbus.mainloop.glib\n+import functools\n+import logging\n+import threading\n+import Queue\n+\n+from rhsmlib.dbus import constants, server\n+\n+# Set DBus mainloop early in test run (test import time!)\n+dbus.mainloop.glib.DBusGMainLoop(set_as_default=True)\n+logger = logging.getLogger(__name__)\n+\n+\n+class TestUtilsMixin(object):\n+    def assert_items_equals(self, a, b):\n+        \"\"\"Assert that two lists contain the same items regardless of order.\"\"\"\n+        if sorted(a) != sorted(b):\n+            self.fail(\"%s != %s\" % (a, b))\n+        return True\n+\n+    def write_temp_file(self, data):\n+        # create a temp file for use as a config file. This should get cleaned\n+        # up magically when it is closed so make sure to close it!\n+        fid = NamedTemporaryFile(mode='w+b', suffix='.tmp')\n+        fid.write(data)\n+        fid.seek(0)\n+        return fid\n+\n+\n+class DBusObjectTest(unittest.TestCase):\n+    '''Subclass of unittest.TestCase use for testing DBus methods in the same process.  During setUp this\n+    class starts a thread that makes a DBus connection and exposes some objects on the bus.  The main thread\n+    blocks until the connection has completed.\n+\n+    When the main thread reaches a test case, the test case needs to call dbus_request() and pass in a DBus\n+    proxy object and a function to run after the DBus call has received a response.  The dbus_request method\n+    starts another thread that makes an async call to the thread serving the objects under test and then\n+    blocks the main thread (the DBus call must be asynchronous to avoid deadlock).  The function passed to\n+    dbus_request is run and then dbus_request unblocks the main thread.\n+    '''\n+    def setUp(self):\n+        self.started_event = threading.Event()\n+        self.stopped_event = threading.Event()\n+        self.handler_complete_event = threading.Event()\n+\n+        # If we don't use a BusConnection and use say dbus.SessionBus() directly, each test will end up\n+        # getting old bus connections since the dbus bindings cache bus connections.  You can use the private\n+        # kwarg to tell dbus.SessionBus/SystemBus not to cache, but that's deprecated.\n+        self.server_thread = ServerThread(kwargs={\n+            'object_classes': self.dbus_objects(),\n+            'bus_name': self.bus_name(),\n+            'bus_class': dbus.bus.BusConnection,\n+            'bus_kwargs': self.bus_kwargs,\n+            'started_event': self.started_event,\n+            'stopped_event': self.stopped_event,\n+        })\n+        self.started_event.wait()\n+        self.result_queue = Queue.Queue(maxsize=1)\n+        self.addCleanup(self.stop_server)\n+\n+    def stop_server(self):\n+        self.server_thread.stop()\n+        self.stopped_event.wait()\n+\n+    @property\n+    def bus_kwargs(self):\n+        if os.geteuid() == 0:\n+            return {'address_or_type': dbus.Bus.TYPE_SYSTEM}\n+        else:\n+            return {'address_or_type': dbus.Bus.TYPE_SESSION}\n+\n+    def proxy_for(self, path):\n+        return dbus.bus.BusConnection(**self.bus_kwargs).get_object(self.bus_name(), path)\n+\n+    def dbus_request(self, reply_handler, proxy, proxy_args=None, error_handler=None):\n+        '''This method makes an async request to the server thread and *does not* block.  It is\n+        unlikely that you will want to use this since not blocking means that the rest of your test case\n+        will run potentially before the async callback finishes.  If you do need this method,\n+        you will still almost certainly need to call self.handler_complete_event.wait() at the end of your\n+        test so that the test runner itself will block until the async callback finishes.'''\n+\n+        DBusRequestThread(kwargs={\n+            'proxy': proxy,\n+            'proxy_args': proxy_args,\n+            'reply_handler': reply_handler,\n+            'error_handler': error_handler,\n+            'handler_complete_event': self.handler_complete_event,\n+            'queue': self.result_queue\n+        })\n+        self.handler_complete_event.wait()\n+        # Raise any exception generated by the handlers.  I.e. actually fail a test if assertions failed in\n+        # the DBusRequestThread\n+        if not self.result_queue.empty():\n+            result = self.result_queue.get()\n+            raise result[0], result[1], result[2]\n+\n+    def dbus_objects(self):\n+        '''This method should return a list of DBus service classes that need to be instantiated in the\n+        server thread.  Generally this should just be a list containing the class under test.\n+        In that list, you can also pass in a tuple composed of the object class and a dictionary of keyword\n+        arguments for the object's constructor\n+        '''\n+        raise NotImplementedError('Subclasses should define what DBus objects to test')\n+\n+    def bus_name(self):\n+        '''This method should return the bus name that the server thread should use'''\n+        return constants.BUS_NAME\n+\n+\n+class ServerThread(threading.Thread):\n+    def __init__(self, **kwds):\n+        super(ServerThread, self).__init__(name=self.__class__.__name__, **kwds)\n+        kwargs = kwds['kwargs']\n+        self.bus_class = kwargs.get('bus_class', dbus.bus.BusConnection(dbus.bus.BUS_SESSION))\n+        self.bus_name = kwargs.get('bus_name', constants.BUS_NAME)\n+        self.object_classes = kwargs.get('object_classes', [])\n+        self.started_event = kwargs['started_event']\n+        self.stopped_event = kwargs['stopped_event']\n+        self.bus_kwargs = kwargs['bus_kwargs']\n+        self.server = None\n+        self.start()\n+\n+    def run(self):\n+        try:\n+            self.server = server.Server(\n+                bus_class=self.bus_class,\n+                bus_name=self.bus_name,\n+                object_classes=self.object_classes,\n+                bus_kwargs=self.bus_kwargs)\n+            self.server.run(self.started_event, self.stopped_event)\n+        except Exception as e:\n+            logger.exception(e)\n+            self.started_event.set()\n+\n+    def stop(self):\n+        if self.server:\n+            self.server.shutdown()\n+\n+\n+class DBusRequestThread(threading.Thread):\n+    def __init__(self, **kwds):\n+        super(DBusRequestThread, self).__init__(name=self.__class__.__name__, **kwds)\n+        kwargs = kwds['kwargs']\n+        self.queue = kwargs['queue']\n+        self.proxy = kwargs['proxy']\n+        self.proxy_args = kwargs['proxy_args']\n+\n+        if self.proxy_args is None:\n+            self.proxy_args = []\n+\n+        self.reply_handler = self.reply_wrap(kwargs['reply_handler'])\n+        # If no error_handler is given, error_wrap will just raise the DBus error\n+        self.error_handler = self.error_wrap(kwargs.get('error_handler'))\n+\n+        self.handler_complete_event = kwargs['handler_complete_event']\n+        self.start()\n+\n+    def reply_wrap(self, func):\n+        def dummy():\n+            pass\n+\n+        if func is None:\n+            func = dummy\n+\n+        @functools.wraps(func)\n+        def wrapper(*args, **kwargs):\n+            try:\n+                if func is not dummy:\n+                    func(*args, **kwargs)\n+            except Exception:\n+                self.queue.put(sys.exc_info())\n+            finally:\n+                self.handler_complete_event.set()\n+\n+        return wrapper\n+\n+    def error_wrap(self, func=None):\n+        def dummy():\n+            pass\n+\n+        if func is None:\n+            func = dummy\n+\n+        @functools.wraps(func)\n+        def wrapper(*args, **kwargs):\n+            try:\n+                if func is not dummy:\n+                    func(*args, **kwargs)\n+                else:\n+                    raise args[0]\n+            except Exception:\n+                self.queue.put(sys.exc_info())\n+            finally:\n+                self.handler_complete_event.set()\n+\n+        return wrapper\n+\n+    def run(self):\n+        try:\n+            self.proxy(\n+                *self.proxy_args,\n+                reply_handler=self.reply_handler,\n+                error_handler=self.error_handler)\n+        except Exception:\n+            # If the proxy is messed up some how, we still need to push the error to the main thread and\n+            # wake it.\n+            self.queue.put(sys.exc_info())\n+            self.handler_complete_event.set()"
        },
        {
          "filename": "test/rhsmlib_test/test_collector.py",
          "status": "added",
          "additions": 35,
          "deletions": 0,
          "patch": "@@ -0,0 +1,35 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+\n+try:\n+    import unittest2 as unittest\n+except ImportError:\n+    import unittest\n+\n+import mock\n+from test.fixture import open_mock\n+\n+from rhsmlib.facts import collector\n+\n+\n+class GetArchTest(unittest.TestCase):\n+    @mock.patch('platform.machine')\n+    def test_returns_arch(self, mock_machine):\n+        mock_machine.return_value = \"hello_arch\"\n+        arch = collector.get_arch()\n+        self.assertEqual(\"hello_arch\", arch)\n+\n+    def test_returns_arch_override(self):\n+        with open_mock(content=\"hello_arch\"):\n+            arch = collector.get_arch(prefix=\"/does/not/exist\")\n+            self.assertEqual(\"hello_arch\", arch)"
        },
        {
          "filename": "test/rhsmlib_test/test_config.py",
          "status": "added",
          "additions": 230,
          "deletions": 0,
          "patch": "@@ -0,0 +1,230 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+try:\n+    import unittest2 as unittest\n+except ImportError:\n+    import unittest\n+\n+import dbus\n+\n+from rhsm.config import RhsmConfigParser, NoOptionError\n+from rhsmlib.dbus import constants\n+from rhsmlib.dbus.objects.config import ConfigDBusObject\n+from rhsmlib.services.config import Config, ConfigSection\n+from test.rhsmlib_test.base import DBusObjectTest, TestUtilsMixin\n+\n+TEST_CONFIG = \"\"\"\n+[foo]\n+bar =\n+quux = baz\n+bigger_than_32_bit = 21474836470\n+bigger_than_64_bit = 123456789009876543211234567890\n+\n+[server]\n+hostname = server.example.com\n+prefix = /candlepin\n+port = 8443\n+insecure = 1\n+ssl_verify_depth = 3\n+proxy_hostname =\n+proxy_port =\n+proxy_user =\n+proxy_password =\n+\n+[rhsm]\n+ca_cert_dir = /etc/rhsm/ca-test/\n+baseurl= https://content.example.com\n+repo_ca_cert = %(ca_cert_dir)sredhat-uep-non-default.pem\n+productCertDir = /etc/pki/product\n+entitlementCertDir = /etc/pki/entitlement\n+consumerCertDir = /etc/pki/consumer\n+report_package_profile = 1\n+pluginDir = /usr/lib/rhsm-plugins\n+some_option = %(repo_ca_cert)stest\n+manage_repos =\n+\n+[rhsmcertd]\n+certCheckInterval = 245\n+\n+[logging]\n+default_log_level = DEBUG\n+\"\"\"\n+\n+\n+class BaseConfigTest(unittest.TestCase, TestUtilsMixin):\n+    expected_sections = ['foo', 'server', 'rhsm', 'rhsmcertd', 'logging']\n+\n+    def setUp(self):\n+        super(BaseConfigTest, self).setUp()\n+        self.fid = self.write_temp_file(TEST_CONFIG)\n+        self.parser = RhsmConfigParser(self.fid.name)\n+        self.config = Config(self.parser)\n+        self.addCleanup(self.fid.close)\n+\n+\n+class TestConfig(BaseConfigTest):\n+    def test_config_contains(self):\n+        self.assertTrue('server' in self.config)\n+        self.assertFalse('not_here' in self.config)\n+\n+    def test_config_len(self):\n+        self.assertEquals(len(self.expected_sections), len(self.config))\n+\n+    def test_keys(self):\n+        self.assert_items_equals(self.expected_sections, self.config.keys())\n+\n+    def test_values(self):\n+        values = self.config.values()\n+        for v in values:\n+            self.assertIsInstance(v, ConfigSection)\n+\n+    def test_set_new_section(self):\n+        self.config['new_section'] = {'hello': 'world'}\n+        self.assertEquals(['hello'], self.config._parser.options('new_section'))\n+        self.assertEquals('world', self.config._parser.get('new_section', 'hello'))\n+\n+    def test_set_old_section(self):\n+        self.config['foo'] = {'hello': 'world'}\n+        self.assertEquals(['hello'], self.config._parser.options('foo'))\n+        self.assertEquals('world', self.config._parser.get('foo', 'hello'))\n+        self.assertRaises(NoOptionError, self.config._parser.get, 'foo', 'quux')\n+\n+    def test_get_item(self):\n+        self.assertIsInstance(self.config['server'], ConfigSection)\n+\n+    def test_persist(self):\n+        self.config['foo'] = {'hello': 'world'}\n+        self.config.persist()\n+        reparsed = RhsmConfigParser(self.fid.name)\n+        self.assertEquals('world', reparsed.get('foo', 'hello'))\n+        self.assertRaises(NoOptionError, reparsed.get, 'foo', 'quux')\n+\n+    def test_auto_persists(self):\n+        config = Config(self.parser, auto_persist=True)\n+        config['foo'] = {'hello': 'world'}\n+        reparsed = RhsmConfigParser(self.fid.name)\n+        self.assertEquals('world', reparsed.get('foo', 'hello'))\n+        self.assertRaises(NoOptionError, reparsed.get, 'foo', 'quux')\n+\n+    def test_does_not_auto_persist_by_default(self):\n+        config = Config(self.parser, auto_persist=False)\n+        config['foo'] = {'hello': 'world'}\n+        reparsed = RhsmConfigParser(self.fid.name)\n+        self.assertEquals('baz', reparsed.get('foo', 'quux'))\n+        self.assertRaises(NoOptionError, reparsed.get, 'foo', 'hello')\n+\n+    def test_del_item(self):\n+        del self.config['foo']\n+        self.assertFalse(self.config._parser.has_section('foo'))\n+\n+    def test_iter(self):\n+        sections = [s for s in self.config]\n+        self.assert_items_equals(self.expected_sections, sections)\n+\n+\n+class TestConfigSection(BaseConfigTest):\n+    def test_get_value(self):\n+        self.assertEquals('1', self.config['server']['insecure'])\n+\n+    def test_get_missing_value(self):\n+        with self.assertRaises(KeyError):\n+            self.config['server']['missing']\n+\n+    def test_set_item(self):\n+        self.assertEquals('baz', self.config['foo']['quux'])\n+        self.config['foo']['quux'] = 'fizz'\n+        self.assertEquals('fizz', self.config['foo']['quux'])\n+\n+    def test_auto_persist(self):\n+        config = Config(self.parser, auto_persist=True)\n+        self.assertEquals('baz', config['foo']['quux'])\n+        config['foo']['quux'] = 'fizz'\n+        self.assertEquals('fizz', config['foo']['quux'])\n+\n+        reparsed = RhsmConfigParser(self.fid.name)\n+        self.assertEquals('fizz', reparsed.get('foo', 'quux'))\n+\n+    def test_persist_cascades(self):\n+        config = Config(self.parser, auto_persist=False)\n+        self.assertEquals('baz', config['foo']['quux'])\n+        config['foo']['quux'] = 'fizz'\n+        config.persist()\n+        self.assertEquals('fizz', config['foo']['quux'])\n+\n+        reparsed = RhsmConfigParser(self.fid.name)\n+        self.assertEquals('fizz', reparsed.get('foo', 'quux'))\n+\n+    def test_del_item(self):\n+        del self.config['foo']['quux']\n+        self.assertNotIn('quux', self.config['foo'])\n+\n+        with self.assertRaises(KeyError):\n+            del self.config['foo']['missing_key']\n+\n+    def test_len(self):\n+        self.assertEquals(4, len(self.config['foo']))\n+\n+    def test_in(self):\n+        self.assertIn(\"quux\", self.config['foo'])\n+        self.assertNotIn(\"missing\", self.config['foo'])\n+\n+\n+class TestConfigDBusObject(DBusObjectTest, TestUtilsMixin):\n+    def setUp(self):\n+        super(TestConfigDBusObject, self).setUp()\n+        self.proxy = self.proxy_for(ConfigDBusObject.default_dbus_path)\n+        self.interface = dbus.Interface(self.proxy, constants.CONFIG_INTERFACE)\n+\n+    def dbus_objects(self):\n+        self.fid = self.write_temp_file(TEST_CONFIG)\n+        self.addCleanup(self.fid.close)\n+        self.parser = RhsmConfigParser(self.fid.name)\n+        return [(ConfigDBusObject, {'parser': self.parser})]\n+\n+    def test_get_all(self):\n+        def assertions(*args):\n+            result = args[0]\n+            self.assertIn(\"server\", result)\n+\n+        dbus_method_args = []\n+        self.dbus_request(assertions, self.interface.GetAll, dbus_method_args)\n+\n+    def test_get_property(self):\n+        def assertions(*args):\n+            result = args[0]\n+            self.assertIn('server.example.com', result)\n+\n+        dbus_method_args = ['server.hostname']\n+        self.dbus_request(assertions, self.interface.Get, dbus_method_args)\n+\n+    def test_get_section(self):\n+        def assertions(*args):\n+            result = args[0]\n+            self.assertIn('hostname', result)\n+\n+        dbus_method_args = ['server']\n+        self.dbus_request(assertions, self.interface.Get, dbus_method_args)\n+\n+    def test_set(self):\n+        def assertions(*args):\n+            self.assertEqual('new', self.parser.get('server', 'hostname'))\n+\n+        dbus_method_args = ['server.hostname', 'new']\n+        self.dbus_request(assertions, self.interface.Set, dbus_method_args)\n+\n+    def test_set_section_fails(self):\n+        dbus_method_args = ['server', 'new']\n+\n+        with self.assertRaisesRegexp(dbus.DBusException, r'Setting an entire section is not.*'):\n+            self.dbus_request(None, self.interface.Set, dbus_method_args)"
        },
        {
          "filename": "test/rhsmlib_test/test_facts.py",
          "status": "added",
          "additions": 46,
          "deletions": 0,
          "patch": "@@ -0,0 +1,46 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+import dbus\n+import dbus.exceptions\n+\n+from rhsmlib.dbus.facts.base import AllFacts\n+from rhsmlib.dbus.facts import constants\n+\n+from test.rhsmlib_test.base import DBusObjectTest\n+\n+\n+class TestFactsDBusObject(DBusObjectTest):\n+    def setUp(self):\n+        super(TestFactsDBusObject, self).setUp()\n+        self.proxy = self.proxy_for(AllFacts.default_dbus_path)\n+        self.interface = dbus.Interface(self.proxy, constants.FACTS_DBUS_INTERFACE)\n+\n+    def dbus_objects(self):\n+        return [AllFacts]\n+\n+    def bus_name(self):\n+        return constants.FACTS_DBUS_NAME\n+\n+    def test_get_facts(self):\n+        def assertions(*args):\n+            result = args[0]\n+            self.assertIn(\"uname.machine\", result)\n+\n+        self.dbus_request(assertions, self.interface.GetFacts)\n+\n+    def test_missing_method(self):\n+        def assertions(*args):\n+            pass\n+\n+        with self.assertRaises(dbus.exceptions.DBusException):\n+            self.dbus_request(assertions, self.interface.MissingMethod)"
        },
        {
          "filename": "test/rhsmlib_test/test_register.py",
          "status": "added",
          "additions": 262,
          "deletions": 0,
          "patch": "@@ -0,0 +1,262 @@\n+#\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+#\n+\n+import errno\n+import mock\n+import json\n+import dbus.connection\n+import socket\n+\n+import rhsm.connection\n+import subscription_manager.injection as inj\n+\n+from test import stubs\n+from test.fixture import SubManFixture\n+from test.rhsmlib_test.base import DBusObjectTest\n+\n+from rhsmlib.dbus import dbus_utils, constants\n+from rhsmlib.dbus.objects import DomainSocketRegisterDBusObject, RegisterDBusObject\n+\n+CONTENT_JSON = '''{\"hypervisorId\": null,\n+        \"serviceLevel\": \"\",\n+        \"autoheal\": true,\n+        \"idCert\": \"FAKE_KEY\",\n+        \"owner\": {\"href\": \"/owners/admin\", \"displayName\": \"Admin Owner\",\n+        \"id\": \"ff808081550d997c01550d9adaf40003\", \"key\": \"admin\"},\n+        \"href\": \"/consumers/c1b8648c-6f0a-4aa5-b34e-b9e62c0e4364\",\n+        \"facts\": {}, \"id\": \"ff808081550d997c015511b0406d1065\",\n+        \"uuid\": \"c1b8648c-6f0a-4aa5-b34e-b9e62c0e4364\",\n+        \"guestIds\": null, \"capabilities\": null,\n+        \"environment\": null, \"installedProducts\": null,\n+        \"canActivate\": false, \"type\": {\"manifest\": false,\n+        \"id\": \"1000\", \"label\": \"system\"}, \"annotations\": null,\n+        \"username\": \"admin\", \"updated\": \"2016-06-02T15:16:51+0000\",\n+        \"lastCheckin\": null, \"entitlementCount\": 0, \"releaseVer\":\n+        {\"releaseVer\": null}, \"entitlementStatus\": \"valid\", \"name\":\n+        \"test.example.com\", \"created\": \"2016-06-02T15:16:51+0000\",\n+        \"contentTags\": null, \"dev\": false}'''\n+\n+SUCCESSFUL_REGISTRATION = {\n+    \"headers\": {\n+        'content-type': 'application/json',\n+        'date': 'Thu, 02 Jun 2016 15:16:51 GMT',\n+        'server': 'Apache-Coyote/1.1',\n+        'transfer-encoding': 'chunked',\n+        'x-candlepin-request-uuid': '01566658-137b-478c-84c0-38540daa8602',\n+        'x-version': '2.0.13-1'\n+    },\n+    \"content\": CONTENT_JSON,\n+    \"status\": \"200\"\n+}\n+\n+\n+class DomainSocketRegisterDBusObjectUnitTest(SubManFixture):\n+    def setUp(self):\n+        self.dbus_connection = mock.Mock(spec=dbus.connection.Connection)\n+        super(DomainSocketRegisterDBusObjectUnitTest, self).setUp()\n+\n+    @mock.patch(\"subscription_manager.managerlib.persist_consumer_cert\")\n+    @mock.patch(\"rhsm.connection.UEPConnection\")\n+    def test_register(self, stub_uep, mock_persist_consumer):\n+        self._inject_mock_invalid_consumer()\n+\n+        expected_consumer = json.loads(CONTENT_JSON, object_hook=dbus_utils._decode_dict)\n+        del expected_consumer['idCert']\n+\n+        stub_uep.return_value.registerConsumer = mock.Mock(return_value=SUCCESSFUL_REGISTRATION)\n+        register_service = DomainSocketRegisterDBusObject(conn=self.dbus_connection)\n+\n+        output = register_service.Register('admin', 'admin', 'admin', {\n+            'host': 'localhost',\n+            'port': '8443',\n+            'handler': '/candlepin'\n+        })\n+\n+        # Be sure we are persisting the consumer cert\n+        mock_persist_consumer.assert_called_once_with(expected_consumer)\n+        # Be sure we get the right output\n+        self.assertEquals(output, SUCCESSFUL_REGISTRATION)\n+\n+    @mock.patch(\"rhsm.connection.UEPConnection\")\n+    def test_get_uep_from_options(self, stub_uep):\n+        stub_uep.return_value = mock.Mock(spec=rhsm.connection.UEPConnection)\n+        options = {\n+            'username': 'test',\n+            'password': 'test_password',\n+            'host': 'localhost',\n+            'port': '8443',\n+            'handler': '/candlepin',\n+            'insecure': True\n+        }\n+\n+        self._inject_mock_invalid_consumer()\n+\n+        register_service = DomainSocketRegisterDBusObject(conn=self.dbus_connection)\n+        register_service.build_uep(options)\n+\n+        stub_uep.assert_called_once_with(\n+            username=options.get('username', None),\n+            password=options.get('password', None),\n+            host=options.get('host', None),\n+            ssl_port=rhsm.connection.safe_int(options.get('port', None)),\n+            handler=options.get('handler', None),\n+            insecure=options.get('insecure', None),\n+            proxy_hostname=options.get('proxy_hostname', None),\n+            proxy_port=options.get('proxy_port', None),\n+            proxy_user=options.get('proxy_user', None),\n+            proxy_password=options.get('proxy_password', None),\n+            restlib_class=rhsm.connection.BaseRestLib\n+        )\n+\n+    @mock.patch(\"subscription_manager.managerlib.persist_consumer_cert\")\n+    @mock.patch(\"rhsm.connection.UEPConnection\")\n+    def test_register_with_activation_keys(self, stub_uep, mock_persist_consumer):\n+        self._inject_mock_invalid_consumer()\n+\n+        expected_consumer = json.loads(CONTENT_JSON,\n+            object_hook=dbus_utils._decode_dict)\n+        del expected_consumer['idCert']\n+        stub_uep.return_value.registerConsumer = mock.Mock(return_value=SUCCESSFUL_REGISTRATION)\n+        register_service = DomainSocketRegisterDBusObject(self.dbus_connection)\n+\n+        output = register_service.RegisterWithActivationKeys('admin', ['default_key'], {\n+            'host': 'localhost',\n+            'port': '8443',\n+            'handler': '/candlepin'\n+        })\n+\n+        # Be sure we are persisting the consumer cert\n+        mock_persist_consumer.assert_called_once_with(expected_consumer)\n+        # Be sure we get the right output\n+        self.assertEquals(output, SUCCESSFUL_REGISTRATION)\n+\n+\n+class DomainSocketRegisterDBusObjectFunctionalTest(DBusObjectTest):\n+    def dbus_objects(self):\n+        return [RegisterDBusObject]\n+\n+    def setUp(self):\n+        inj.provide(inj.INSTALLED_PRODUCTS_MANAGER, stubs.StubInstalledProductsManager())\n+        facts_host_patcher = mock.patch('rhsmlib.dbus.facts.FactsClient', auto_spec=True)\n+        self.addCleanup(facts_host_patcher.stop)\n+        self.mock_facts_host = facts_host_patcher.start()\n+        self.mock_facts_host.return_value.GetFacts.return_value = {}\n+\n+        super(DomainSocketRegisterDBusObjectFunctionalTest, self).setUp()\n+        self.proxy = self.proxy_for(RegisterDBusObject.default_dbus_path)\n+        self.interface = dbus.Interface(self.proxy, constants.REGISTER_INTERFACE)\n+\n+    def test_open_domain_socket(self):\n+        dbus_method_args = []\n+\n+        def assertions(*args):\n+            result = args[0]\n+            self.assertRegexpMatches(result, r'/var/run/dbus.*')\n+\n+        self.dbus_request(assertions, self.interface.Start, dbus_method_args)\n+\n+    def test_same_socket_on_subsequent_opens(self):\n+        dbus_method_args = []\n+\n+        def assertions(*args):\n+            # Assign the result as an attribute to this function.\n+            # See http://stackoverflow.com/a/27910553/6124862\n+            assertions.result = args[0]\n+            self.assertRegexpMatches(assertions.result, r'/var/run/dbus.*')\n+\n+        self.dbus_request(assertions, self.interface.Start, dbus_method_args)\n+\n+        # Reset the handler_complete_event so we'll block for the second\n+        # dbus_request\n+        self.handler_complete_event.clear()\n+\n+        def assertions2(*args):\n+            result2 = args[0]\n+            self.assertEqual(assertions.result, result2)\n+\n+        self.dbus_request(assertions2, self.interface.Start, dbus_method_args)\n+\n+    def test_cannot_close_what_is_not_opened(self):\n+        with self.assertRaises(dbus.exceptions.DBusException):\n+            self.dbus_request(None, self.interface.Stop, [])\n+\n+    def test_closes_domain_socket(self):\n+        def get_address(*args):\n+            address = args[0]\n+            _prefix, _equal, address = address.partition('=')\n+            get_address.address, _equal, _suffix = address.partition(',')\n+\n+        self.dbus_request(get_address, self.interface.Start, [])\n+        self.handler_complete_event.clear()\n+\n+        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n+        try:\n+            # The socket returned for connection is an abstract socket so we have\n+            # to begin the name with a NUL byte to get into that namespace.  See\n+            # http://blog.eduardofleury.com/archives/2007/09/13\n+            sock.connect('\\0' + get_address.address)\n+        finally:\n+            sock.close()\n+\n+        self.dbus_request(None, self.interface.Stop, [])\n+        self.handler_complete_event.wait()\n+\n+        with self.assertRaises(socket.error) as serr:\n+            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n+            try:\n+                sock.connect('\\0' + get_address.address)\n+            finally:\n+                sock.close()\n+            self.assertEqual(serr.errno, errno.ECONNREFUSED)\n+\n+    def _inject_mock_invalid_consumer(self, uuid=None):\n+        invalid_identity = mock.NonCallableMock(name='InvalidIdentityMock')\n+        invalid_identity.is_valid = mock.Mock(return_value=False)\n+        invalid_identity.uuid = uuid or \"INVALIDCONSUMERUUID\"\n+        invalid_identity.cert_dir_path = \"/not/a/real/path/to/pki/consumer/\"\n+        inj.provide(inj.IDENTITY, invalid_identity)\n+        return invalid_identity\n+\n+    @mock.patch(\"subscription_manager.managerlib.persist_consumer_cert\")\n+    @mock.patch(\"rhsm.connection.UEPConnection\")\n+    def test_can_register_over_domain_socket(self, stub_uep, mock_persist_consumer):\n+        def get_address(*args):\n+            get_address.address = args[0]\n+\n+        self.dbus_request(get_address, self.interface.Start, [])\n+        self.handler_complete_event.clear()\n+\n+        socket_conn = dbus.connection.Connection(get_address.address)\n+        socket_proxy = socket_conn.get_object(constants.BUS_NAME, constants.PRIVATE_REGISTER_DBUS_PATH)\n+        socket_interface = dbus.Interface(socket_proxy, constants.PRIVATE_REGISTER_INTERFACE)\n+\n+        expected_consumer = json.loads(CONTENT_JSON, object_hook=dbus_utils._decode_dict)\n+        del expected_consumer['idCert']\n+\n+        def assertions(*args):\n+            # Be sure we are persisting the consumer cert\n+            mock_persist_consumer.assert_called_once_with(expected_consumer)\n+            self.assertEquals(args[0], SUCCESSFUL_REGISTRATION)\n+\n+        self._inject_mock_invalid_consumer()\n+        stub_uep.return_value.registerConsumer = mock.Mock(return_value=SUCCESSFUL_REGISTRATION)\n+\n+        register_opts = ['admin', 'admin', 'admin', {\n+            'host': 'localhost',\n+            'port': '8443',\n+            'handler': '/candlepin'\n+        }]\n+\n+        self.dbus_request(assertions, socket_interface.Register, register_opts)"
        },
        {
          "filename": "test/rhsmlib_test/test_service_wrapper.py",
          "status": "added",
          "additions": 60,
          "deletions": 0,
          "patch": "@@ -0,0 +1,60 @@\n+# Copyright (c) 2016 Red Hat, Inc.\n+#\n+# This software is licensed to you under the GNU General Public License,\n+# version 2 (GPLv2). There is NO WARRANTY for this software, express or\n+# implied, including the implied warranties of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2\n+# along with this software; if not, see\n+# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n+#\n+# Red Hat trademarks are not licensed under GPLv2. No permission is\n+# granted to use or replicate Red Hat trademarks that are incorporated\n+# in this software or its documentation.\n+try:\n+    import unittest2 as unittest\n+except ImportError:\n+    import unittest\n+\n+import dbus\n+import mock\n+\n+from rhsmlib.dbus import service_wrapper, constants\n+\n+\n+class ServiceWrapperTest(unittest.TestCase):\n+    def test_parse_argv(self):\n+        opts, args = service_wrapper.parse_argv(\n+            ['cmd_name', '--verbose', '--bus-name', 'Hello', 'Foo'], 'Default')\n+        self.assertTrue(opts.verbose)\n+        self.assertEqual(opts.bus_name, 'Hello')\n+        self.assertEqual(args, ['Foo'])\n+\n+    def test_uses_default_bus_name(self):\n+        opts, args = service_wrapper.parse_argv(['cmd_name', 'Foo'], 'Default')\n+        self.assertFalse(opts.verbose)\n+        self.assertEqual(opts.bus, dbus.SystemBus)\n+        self.assertEqual(opts.bus_name, 'Default')\n+        self.assertEqual(args, ['Foo'])\n+\n+    def test_loads_bus_given(self):\n+        opts, args = service_wrapper.parse_argv(['cmd_name', '--bus', 'dbus.SessionBus', 'Foo'], 'Default')\n+        self.assertEqual(opts.bus, dbus.SessionBus)\n+\n+    @mock.patch(\"rhsmlib.dbus.service_wrapper.server.Server\")\n+    def test_loads_an_object_class(self, mock_serve):\n+        # Just use some class we have available\n+        service_wrapper.main(['cmd_name', 'mock.MagicMock'])\n+        mock_serve.assert_called_with(\n+            bus_class=dbus.SystemBus,\n+            bus_name=constants.BUS_NAME,\n+            object_classes=[mock.MagicMock]\n+        )\n+\n+    @mock.patch(\"rhsmlib.dbus.service_wrapper.server.Server\")\n+    def test_loads_from_an_array_of_classes(self, mock_serve):\n+        service_wrapper.main(['cmd_name'], [mock.MagicMock])\n+        mock_serve.assert_called_with(\n+            bus_class=dbus.SystemBus,\n+            bus_name=constants.BUS_NAME,\n+            object_classes=[mock.MagicMock]\n+        )"
        },
        {
          "filename": "test/smoke.sh",
          "status": "modified",
          "additions": 13,
          "deletions": 7,
          "patch": "@@ -10,15 +10,18 @@ WRAPPER=\"\"\n # or \"SUBMAN_DEBUG=1 bin/subscription-manager\"\n #SM=\"SUBMAN_DEBUG=1 bin/subscription-manager\"\n #SM=\"subscription-manager\"\n-SM=\"PYTHONPATH=../python-rhsm/src/:src/ bin/subscription-manager\"\n-WORKER=\"PYTHONPATH=../python-rhsm/src/:src/ python src/daemons/rhsmcertd-worker.py\"\n-RHSMD=\"PYTHONPATH=../python-rhsm/src/:src/ src/daemons/rhsm_d.py\"\n+# To unset the default path and use the installed version (or system paths, etc)\n+# pass in PYPATH=, aka 'PYPATH= test/smoke.sh'\n+PYPATH=${PYPATH-PYTHONPATH=../python-rhsm/src:src/}\n+SM=\"${PYPATH} bin/subscription-manager\"\n+WORKER=\"${PYPATH} python src/daemons/rhsmcertd-worker.py\"\n+RHSMD=\"${PYPATH} src/daemons/rhsm_d.py\"\n RHSMCERTD=\"bin/rhsmcertd\"\n-RCT=\"PYTHONPATH=../python-rhsm/src/:src/ bin/rct\"\n-RHSM_DEBUG=\"PYTHONPATH=../python-rhsm/src/:src/ bin/rhsm-debug\"\n+RCT=\"${PYPATH} bin/rct\"\n+RHSM_DEBUG=\"${PYPATH} bin/rhsm-debug\"\n \n # assume we are testing installed version\n-VIRT_WHO=\"PYTHONPATH=../python-rhsm/src/:src/ /usr/bin/virt-who\"\n+VIRT_WHO=\"${PYPATH} /usr/bin/virt-who\"\n \n # where to store backup copies of rhsm related files\n # NOTE: we currently dont restore them\n@@ -29,7 +32,7 @@ CONF_BACKUP=\"${BACKUP_DIR}/${TIMESTAMP}/\"\n \n # running yum requires installing the pluings\n # Note: have to set CONF_BACKUP first\n-YUM=\"PYTHONPATH=../python-rhsm/src/:src/ yum -c ${CONF_BACKUP}/yum-smoke.conf\"\n+YUM=\"${PYPATH} yum -c ${CONF_BACKUP}/yum-smoke.conf\"\n \n \n # this script assumes it's running from top level of src checkout\n@@ -269,6 +272,9 @@ run_sm \"0\" refresh\n run_sm \"0\" redeem --email \"${REDEEM_EMAIL}\"\n \n run_sm \"0\" facts\n+run_sm \"0\" facts --list\n+run_sm \"0\" facts --update\n+\n run_sm \"0\" identity\n run_sm \"0\" orgs --username \"${USERNAME}\" --password \"${PASSWORD}\"\n "
        },
        {
          "filename": "test/stubs.py",
          "status": "modified",
          "additions": 4,
          "deletions": 5,
          "patch": "@@ -35,8 +35,6 @@\n from rhsm.certificate2 import EntitlementCertificate, ProductCertificate, \\\n         Product, Content, Order\n from rhsm import profile\n-\n-\n from rhsm import ourjson as json\n \n # config file is root only, so just fill in a stringbuffer\n@@ -62,6 +60,7 @@\n productCertDir = /etc/pki/product\n entitlementCertDir = /etc/pki/entitlement\n consumerCertDir = /etc/pki/consumer\n+ca_cert_dir = /etc/rhsm/ca/\n \n [rhsmcertd]\n certCheckInterval = 240\n@@ -116,8 +115,6 @@ def save(self, config_file=None):\n             raise IOError\n         return None\n \n-    # replace read with readfp on stringio\n-\n \n def stubInitConfig():\n     return StubConfig()\n@@ -390,7 +387,7 @@ def __init__(self, host=None, ssl_port=None, handler=None,\n                  username=None, password=None,\n                  proxy_hostname=None, proxy_port=None,\n                  proxy_user=None, proxy_password=None,\n-                 cert_file=None, key_file=None):\n+                 cert_file=None, key_file=None, restlib_class=None):\n         self.registered_consumer_info = {\"uuid\": 'dummy-consumer-uuid'}\n         self.environment_list = []\n         self.called_unregister_uuid = None\n@@ -455,6 +452,8 @@ def setConsumer(self, consumer):\n     def getConsumer(self, consumerId, username=None, password=None):\n         if hasattr(self, 'consumer') and self.consumer:\n             return self.consumer\n+        if callable(self.registered_consumer_info):\n+            return self.registered_consumer_info()\n         return self.registered_consumer_info\n \n     def unbindAll(self, consumer):"
        },
        {
          "filename": "test/test_async.py",
          "status": "modified",
          "additions": 4,
          "deletions": 3,
          "patch": "@@ -63,7 +63,7 @@ def _create_async_pool(self):\n         inj.provide(inj.CERT_SORTER, stubs.StubCertSorter())\n \n         self.pool_stash = \\\n-                managerlib.PoolStash(facts=self.stub_facts)\n+                managerlib.PoolStash()\n \n         self.ap = async.AsyncPool(self.pool_stash)\n \n@@ -88,5 +88,6 @@ def test_exception(self):\n \n         self.mainloop.run()\n         self.assertTrue(len(self.callbacks) > 3)\n-        # we should have an exception in the error from the callback\n-        self.assertTrue(isinstance(self.callbacks[0][1], IOError))\n+        # we should have an sys.exc_info tuple in the error from the callback\n+        self.assertTrue(isinstance(self.callbacks[0][1], tuple))\n+        self.assertTrue(isinstance(self.callbacks[0][1][1], IOError))"
        },
        {
          "filename": "test/test_dnf_content_plugin.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -10,7 +10,7 @@\n try:\n     import dnf\n     import librepo\n-except ImportError, e:\n+except ImportError as e:\n     raise SkipTest(e)\n \n \n@@ -33,6 +33,8 @@\n fp, pathname, description = imp.find_module(module_name, [dir_path])\n try:\n     dnf_product_id = imp.load_module('dnf_product_id', fp, pathname, description)\n+except ImportError as e:\n+    raise SkipTest(e)\n finally:\n     fp.close()\n "
        },
        {
          "filename": "test/test_factlib.py",
          "status": "modified",
          "additions": 2,
          "deletions": 13,
          "patch": "@@ -20,11 +20,10 @@\n \n \n class TestFactlib(fixture.SubManFixture):\n-\n     def setUp(self):\n         super(TestFactlib, self).setUp()\n-        #self.stub_uep = stubs.StubUEP()\n-        self.expected_facts = {'fact1': 'F1', 'fact2': 'F2'}\n+        # As set in fixture.py:\n+        self.expected_facts = {\"mock.facts\": \"true\"}\n \n         inj.provide(inj.FACTS, stubs.StubFacts(self.expected_facts))\n         self.fl = factlib.FactsActionInvoker()\n@@ -36,7 +35,6 @@ def test_factlib_updates_when_identity_does_not_exist(self):\n         self.assertEquals(len(self.expected_facts), count)\n \n     def test_factlib_updates_when_identity_exists(self):\n-\n         invalid_consumer = self._inject_mock_valid_consumer()\n         self.facts_passed_to_server = None\n         self.consumer_uuid_passed_to_server = None\n@@ -54,12 +52,3 @@ def track_facts_update(consumer_uuid, facts):\n         self.assertEquals(len(self.expected_facts), count)\n         self.assertEquals(self.expected_facts, self.facts_passed_to_server)\n         self.assertEquals(invalid_consumer.uuid, self.consumer_uuid_passed_to_server)\n-\n-\n-class ConsumerIdentityExistsStub(stubs.StubConsumerIdentity):\n-    def __init__(self, keystring, certstring):\n-        super(ConsumerIdentityExistsStub, self).__init__(keystring, certstring)\n-\n-    @classmethod\n-    def exists(cls):\n-        return True"
        },
        {
          "filename": "test/test_facts.py",
          "status": "modified",
          "additions": 18,
          "deletions": 117,
          "patch": "@@ -3,7 +3,6 @@\n from mock import patch\n \n import fixture\n-from stubs import StubEntitlementDirectory, StubProductDirectory\n from subscription_manager import facts\n from rhsm import ourjson as json\n \n@@ -133,8 +132,7 @@ def setUp(self):\n         fd = open(fact_cache, \"w\")\n         fd.write(facts_buf)\n         fd.close()\n-        self.f = facts.Facts(ent_dir=StubEntitlementDirectory([]),\n-                             prod_dir=StubProductDirectory([]))\n+        self.f = facts.Facts()\n         self.f.CACHE_FILE = fact_cache\n \n     def tearDown(self):\n@@ -149,154 +147,57 @@ def test_facts_last_update(self):\n         #FIXME: verify the date is correct\n         self.f.get_last_update()\n \n-    @patch('subscription_manager.facts.Facts._load_custom_facts',\n-           return_value={})\n-    @patch('subscription_manager.facts.Facts._load_hw_facts',\n+    @patch('subscription_manager.facts.Facts.get_facts',\n            return_value={'newstuff': 'a new_hope'})\n-    def test_facts_has_changed(self, mock_load_hw, mock_load_cf):\n+    def test_facts_has_changed(self, mock_collect):\n         self.assertTrue(self.f.has_changed())\n \n-    @patch('subscription_manager.facts.Facts._load_custom_facts',\n-           return_value={})\n-    @patch('subscription_manager.facts.Facts._load_hw_facts')\n-    def test_facts_has_changed_no_change(self, mock_load_hw, mock_load_cf):\n+    @patch('subscription_manager.facts.Facts.get_facts')\n+    def test_facts_has_changed_no_change(self, mock_collect):\n         test_facts = json.loads(facts_buf)\n-        mock_load_hw.return_value = test_facts\n+        mock_collect.return_value = test_facts\n         changed = self.f.has_changed()\n         self.assert_equal_dict(test_facts, self.f.facts)\n         self.assertFalse(changed)\n \n-    @patch('subscription_manager.facts.Facts._load_custom_facts',\n-           return_value={})\n-    @patch('subscription_manager.facts.Facts._load_hw_facts')\n-    def test_facts_has_changed_with_change(self, mock_load_hw, mock_load_cf):\n+    @patch('subscription_manager.facts.Facts.get_facts')\n+    def test_facts_has_changed_with_change(self, mock_collect):\n         test_facts = json.loads(facts_buf)\n         # change socket fact count from what is in the cache\n         test_facts['cpu.cpu_socket(s)'] = '16'\n-        mock_load_hw.return_value = test_facts\n+        mock_collect.return_value = test_facts\n \n         changed = self.f.has_changed()\n         self.assertEquals(self.f.facts['cpu.cpu_socket(s)'], '16')\n         self.assertTrue(changed)\n \n     @patch('subscription_manager.facts.Facts._read_cache',\n            return_value=None)\n-    @patch('subscription_manager.facts.Facts._load_custom_facts',\n-           return_value={})\n-    @patch('subscription_manager.facts.Facts._load_hw_facts')\n-    def test_facts_has_changed_cache_is_none(self, mock_load_hw,\n-                                             mock_load_cf,\n-                                             mock_read_cache):\n+    @patch('subscription_manager.facts.Facts.get_facts')\n+    def test_facts_has_changed_cache_is_none(self, mock_collect, mock_read_cache):\n         test_facts = json.loads(facts_buf)\n-        mock_load_hw.return_value = test_facts\n+        mock_collect.return_value = test_facts\n \n         changed = self.f.has_changed()\n         self.assert_equal_dict(test_facts, self.f.facts)\n         self.assertTrue(changed)\n \n     @patch('subscription_manager.facts.Facts._cache_exists',\n            return_value=False)\n-    @patch('subscription_manager.facts.Facts._load_custom_facts',\n-           return_value={})\n-    @patch('subscription_manager.facts.Facts._load_hw_facts')\n-    def test_facts_has_changed_cache_exists_false(self, mock_load_hw,\n-                                                  mock_load_cf,\n-                                                  mock_read_cache):\n-\n+    @patch('subscription_manager.facts.Facts.get_facts')\n+    def test_facts_has_changed_cache_exists_false(self, mock_collect, mock_read_cache):\n         test_facts = json.loads(facts_buf)\n-        mock_load_hw.return_value = test_facts\n+        mock_collect.return_value = test_facts\n \n         changed = self.f.has_changed()\n         self.assertTrue(changed)\n \n-    @patch('subscription_manager.facts.Facts._load_custom_facts')\n-    @patch('subscription_manager.facts.Facts._load_hw_facts')\n-    def test_get_facts(self, mock_load_hw, mock_load_cf):\n-        mock_load_hw.return_value = \\\n+    @patch('subscription_manager.facts.Facts.get_facts')\n+    def test_get_facts(self, mock_collect):\n+        mock_collect.return_value = \\\n             {'net.interface.lo.ipv4_address': '127.0.0.1'}\n \n-        mock_load_cf.return_value = \\\n-            {'some.custom_fact': 'foobar'}\n-\n         f = self.f.get_facts()\n \n         self.assertTrue(isinstance(f, dict))\n         self.assertEquals(f['net.interface.lo.ipv4_address'], '127.0.0.1')\n-        self.assertEquals(f['some.custom_fact'], 'foobar')\n-\n-    @patch('subscription_manager.facts.Facts._load_custom_facts')\n-    @patch('subscription_manager.facts.Facts._load_hw_facts')\n-    def test_custom_facts_override_hardware_facts(self, mock_load_hw, mock_load_cf):\n-        mock_load_hw.return_value = \\\n-            {'net.interface.lo.ipv4_address': '127.0.0.1'}\n-\n-        mock_load_cf.return_value = \\\n-            {'net.interface.lo.ipv4_address': 'foobar'}\n-\n-        f = self.f.get_facts()\n-        self.assertEquals(f['net.interface.lo.ipv4_address'], 'foobar')\n-\n-    # simulate an empty facts file\n-    @patch('subscription_manager.facts.Facts._open_custom_facts',\n-           return_value=\"\")\n-    @patch('subscription_manager.facts.Facts._load_hw_facts',\n-           return_value={})\n-    def test_empty_custom_facts(self, mock_load_hw, mock_open_cf):\n-        # dont load hardware info\n-        mock_load_hw.return_value = {}\n-        f = self.f.get_facts()\n-        # not much to check, just want to verify we dont\n-        # throw an exception\n-        # see rhbz #966747\n-        self.assertTrue(isinstance(f, dict))\n-\n-    @patch('glob.glob', return_value=\"/path/to/custom/facts/foo.fact\")\n-    @patch('subscription_manager.facts.Facts._open_custom_facts')\n-    @patch('subscription_manager.facts.Facts._load_hw_facts',\n-           return_value={})\n-    def test_custom_facts(self, mock_load_hw, mock_open_cf, mock_glob):\n-        mock_open_cf.return_value = facts_buf\n-        f = self.f.get_facts()\n-        self.assertTrue(isinstance(f, dict))\n-\n-    @patch('__builtin__.open',\n-           side_effect=IOError)\n-    @patch('subscription_manager.facts.Facts._load_hw_facts',\n-           return_value={'test_key': 'test_value'})\n-    def test_io_error_on_custom_facts(self, mock_load_hw, mock_open):\n-        # verify we handle ioerrors reading custom facts\n-        f = self.f.get_facts()\n-\n-        self.assertTrue(isinstance(f, dict))\n-        self.assertEquals(f['test_key'], 'test_value')\n-\n-    @patch('subscription_manager.facts.Facts._load_custom_facts')\n-    @patch('subscription_manager.facts.Facts._load_hw_facts')\n-    def test_write_facts(self, mock_load_hw, mock_load_cf):\n-        mock_load_hw.return_value = \\\n-            {'net.interface.lo.ipv4_address': '127.0.0.1',\n-             'cpu.cpu_socket(s)': '128',\n-             'newstuff': 'newstuff_is_true'}\n-        fact_cache_dir = tempfile.mkdtemp()\n-        fact_cache = fact_cache_dir + \"/facts.json\"\n-\n-        # write to a new file\n-        self.f.fact_cache_dir = fact_cache_dir\n-        self.f.CACHE_FILE = fact_cache\n-\n-        # mocking load_hw_facts and load_custom_facts neuters get_facts\n-        #self.f.get_facts = 'asdfadfasdfadf'\n-        self.f.write_cache()\n-\n-        new_facts_buf = open(fact_cache).read()\n-        new_facts = json.loads(new_facts_buf)\n-        self.assertEquals(new_facts['newstuff'], 'newstuff_is_true')\n-\n-    @patch('subscription_manager.facts.Facts._load_custom_facts',\n-           return_value={})\n-    @patch('subscription_manager.facts.Facts._load_hw_facts',\n-           return_value={})\n-    def test_entitlement_version(self, mock_load_hw, mock_load_cf):\n-        self.assertTrue(\"system.certificate_version\" in self.f.get_facts())\n-        self.assertEquals(facts.CERT_VERSION,\n-                self.f.get_facts()['system.certificate_version'])"
        },
        {
          "filename": "test/test_facts_gui.py",
          "status": "modified",
          "additions": 7,
          "deletions": 19,
          "patch": "@@ -20,26 +20,14 @@ def setUp(self):\n         self.expected_facts = expected_facts\n         self.stub_facts = StubFacts(expected_facts)\n \n-    def test_facts_are_displayed(self):\n-        found_facts = {}\n-\n-        def check_facts(parent, facts):\n-            found_facts[facts[0]] = facts[1]\n-\n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n-        dialog.facts_store.append = check_facts\n-        dialog.display_facts()\n-\n-        self.assertEquals(self.expected_facts, found_facts)\n-\n     def test_hides_environment_when_not_supported(self):\n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n+        dialog = factsgui.SystemFactsDialog()\n         dialog.display_facts()\n         self.assertEquals(False, dialog.environment_title.get_property(\"visible\"))\n         self.assertEquals(False, dialog.environment_label.get_property(\"visible\"))\n \n     def test_shows_unknown_for_no_org(self):\n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n+        dialog = factsgui.SystemFactsDialog()\n         dialog.display_facts()\n         #No owner id should show if we have no owner\n         self.assertEquals(False, dialog.owner_label.get_property(\"visible\"))\n@@ -48,7 +36,7 @@ def test_shows_unknown_for_no_org(self):\n     @patch.object(StubUEP, 'getOwner')\n     def test_shows_org_id(self, mock_getOwner):\n         mock_getOwner.return_value = {'displayName': 'foo', 'key': 'bar'}\n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n+        dialog = factsgui.SystemFactsDialog()\n         dialog.display_facts()\n         self.assertEquals(True, dialog.owner_label.get_property(\"visible\"))\n         self.assertEquals(True, dialog.owner_title.get_property(\"visible\"))\n@@ -59,7 +47,7 @@ def test_shows_org_id(self, mock_getOwner):\n     def test_shows_environment_when_supported(self, mock_getConsumer, mock_supports_resource):\n         mock_supports_resource.return_value = True\n         mock_getConsumer.return_value = {'environment': {'name': 'foobar'}}\n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n+        dialog = factsgui.SystemFactsDialog()\n         dialog.display_facts()\n         self.assertEquals(True, dialog.environment_title.get_property(\"visible\"))\n         self.assertEquals(True, dialog.environment_label.get_property(\"visible\"))\n@@ -70,7 +58,7 @@ def test_shows_environment_when_supported(self, mock_getConsumer, mock_supports_\n     def test_shows_environment_when_empty(self, mock_getConsumer, mock_supports_resource):\n         mock_supports_resource.return_value = True\n         mock_getConsumer.return_value = {'environment': None}\n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n+        dialog = factsgui.SystemFactsDialog()\n         dialog.display_facts()\n         self.assertEquals(True, dialog.environment_title.get_property(\"visible\"))\n         self.assertEquals(True, dialog.environment_label.get_property(\"visible\"))\n@@ -86,15 +74,15 @@ def new_identity():\n             return id_mock\n         provide(IDENTITY, new_identity)\n \n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n+        dialog = factsgui.SystemFactsDialog()\n         dialog.show()\n \n         enabled = dialog.update_button.get_property('sensitive')\n \n         self.assertFalse(enabled)\n \n     def test_update_button_enabled(self):\n-        dialog = factsgui.SystemFactsDialog(self.stub_facts)\n+        dialog = factsgui.SystemFactsDialog()\n         dialog.show()\n \n         enabled = dialog.update_button.get_property('sensitive')"
        },
        {
          "filename": "test/test_lock.py",
          "status": "modified",
          "additions": 3,
          "deletions": 6,
          "patch": "@@ -6,6 +6,7 @@\n import os\n import subprocess\n import sys\n+import shutil\n import tempfile\n import threading\n import time\n@@ -19,15 +20,11 @@ class TestLock(unittest.TestCase):\n     lf_name = \"lock.file\"\n \n     def setUp(self):\n-        self.tmp_dir = self._tmp_dir()\n         self.other_process = None\n \n-    def _tmp_dir(self):\n-        tmp_dir = tempfile.mkdtemp(suffix=\"lock\", prefix=\"subman-unit-tests-\")\n-        return tmp_dir\n-\n     def _lock_path(self):\n-        tmp_dir = self._tmp_dir()\n+        tmp_dir = tempfile.mkdtemp(suffix=\"-lock\", prefix=\"subman-unit-tests-\")\n+        self.addCleanup(shutil.rmtree, tmp_dir, ignore_errors=True)\n         return os.path.join(tmp_dir, self.lf_name)\n \n     # For thread.Timer()"
        },
        {
          "filename": "test/test_logutil.py",
          "status": "modified",
          "additions": 21,
          "deletions": 5,
          "patch": "@@ -38,17 +38,31 @@ def test_log_init(self):\n         logutil.init_logger()\n         sm_logger = logging.getLogger(\"subscription_manager\")\n         rhsm_logger = logging.getLogger(\"rhsm-app\")\n-        self.assertEqual(sm_logger.getEffectiveLevel(), logging.DEBUG)\n-        self.assertEqual(rhsm_logger.getEffectiveLevel(), logging.DEBUG)\n+        sm_effective = sm_logger.getEffectiveLevel()\n+        rhsm_effective = rhsm_logger.getEffectiveLevel()\n+        # Fun hack for 2.6/2.7 interoperability\n+        self.assertTrue(\n+            logging.DEBUG == sm_effective or\n+            logging._levelNames[sm_effective] == logging.DEBUG)\n+        self.assertTrue(\n+            logging.DEBUG == rhsm_effective or\n+            logging._levelNames[rhsm_effective] == logging.DEBUG)\n \n     def test_log_init_default_log_level(self):\n         self.rhsm_config.set(\"logging\", \"default_log_level\", \"WARNING\")\n \n         logutil.init_logger()\n         sm_logger = logging.getLogger(\"subscription_manager\")\n         rhsm_logger = logging.getLogger(\"rhsm-app\")\n-        self.assertEqual(sm_logger.getEffectiveLevel(), logging.WARNING)\n-        self.assertEqual(rhsm_logger.getEffectiveLevel(), logging.WARNING)\n+        sm_effective = sm_logger.getEffectiveLevel()\n+        rhsm_effective = rhsm_logger.getEffectiveLevel()\n+        # Fun hack for 2.6/2.7 interoperability\n+        self.assertTrue(\n+            logging.WARNING == sm_effective or\n+            logging._levelNames[sm_effective] == logging.WARNING)\n+        self.assertTrue(\n+            logging.WARNING == rhsm_effective or\n+            logging._levelNames[rhsm_effective] == logging.WARNING)\n \n     def test_init_logger_for_yum(self):\n         logutil.init_logger_for_yum()\n@@ -77,7 +91,9 @@ def test_set_valid_logger_level(self):\n \n         for logger_name, log_level in logging_conf:\n             real_log_level = logging.getLogger(logger_name).getEffectiveLevel()\n-            self.assertEqual(real_log_level, logging._checkLevel(log_level))\n+            self.assertTrue(\n+                logging._levelNames[log_level] == real_log_level or\n+                log_level == real_log_level)\n \n     def test_set_invalid_logger_level(self):\n         test_logger_name = 'foobar'"
        },
        {
          "filename": "test/test_managercli.py",
          "status": "modified",
          "additions": 11,
          "deletions": 8,
          "patch": "@@ -383,19 +383,19 @@ def test_no_commands(self):\n         self._test_no_exception([])\n \n     def test_main_server_url(self):\n-        with patch.object(self.mock_cfg, \"save\") as mock_save:\n+        with patch.object(self.mock_cfg_parser, \"save\") as mock_save:\n             server_url = \"https://subscription.rhsm.redhat.com/subscription\"\n             self._test_no_exception([\"--serverurl\", server_url])\n             mock_save.assert_called_with()\n \n     def test_main_base_url(self):\n-        with patch.object(self.mock_cfg, \"save\") as mock_save:\n+        with patch.object(self.mock_cfg_parser, \"save\") as mock_save:\n             base_url = \"https://cdn.redhat.com\"\n             self._test_no_exception([\"--baseurl\", base_url])\n             mock_save.assert_called_with()\n \n     def test_insecure(self):\n-        with patch.object(self.mock_cfg, \"save\") as mock_save:\n+        with patch.object(self.mock_cfg_parser, \"save\") as mock_save:\n             self._test_no_exception([\"--insecure\"])\n             mock_save.assert_called_with()\n \n@@ -940,7 +940,7 @@ def test_set_config(self):\n \n         baseurl = 'https://someserver.example.com/foo'\n         self.cc.main(['--rhsm.baseurl', baseurl])\n-        self.assertEquals(managercli.cfg.store['rhsm']['baseurl'], baseurl)\n+        self.assertEquals(managercli.conf['rhsm']['baseurl'], baseurl)\n \n     def test_remove_config_default(self):\n         with Capture() as cap:\n@@ -1171,6 +1171,12 @@ class TestFactsCommand(TestCliProxyCommand):\n class TestImportCertCommand(TestCliCommand):\n     command_class = managercli.ImportCertCommand\n \n+    def setUp(self):\n+        super(TestImportCertCommand, self).setUp()\n+        argv_patcher = patch.object(sys, 'argv', ['subscription-manager', 'import'])\n+        argv_patcher.start()\n+        self.addCleanup(argv_patcher.stop)\n+\n     def test_certificates(self):\n         self.cc.is_registered = Mock(return_value=False)\n         self.cc.main([\"--certificate\", \"one\", \"--certificate\", \"two\"])\n@@ -1239,10 +1245,7 @@ def test_service_level_not_supported(self):\n \n     def test_service_level_supported(self):\n         self.cc.cp.setConsumer({'serviceLevel': 'Jarjar'})\n-        try:\n-            self.cc.set_service_level('JRJAR')\n-        except SystemExit:\n-            self.fail(\"Exception Raised\")\n+        self.cc.set_service_level('JRJAR')\n \n \n class TestReleaseCommand(TestCliProxyCommand):"
        },
        {
          "filename": "test/test_managergui.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -13,7 +13,7 @@ def test_main_window(self):\n         provide(PROD_DIR, stubs.StubProductDirectory([]))\n         provide(PRODUCT_DATE_RANGE_CALCULATOR, mock.Mock())\n \n-        managergui.MainWindow(backend=stubs.StubBackend(), facts=stubs.StubFacts(),\n+        managergui.MainWindow(backend=stubs.StubBackend(),\n                               ent_dir=stubs.StubCertificateDirectory([]),\n                               prod_dir=stubs.StubProductDirectory([]))\n "
        },
        {
          "filename": "test/test_managerlib.py",
          "status": "modified",
          "additions": 7,
          "deletions": 7,
          "patch": "@@ -296,7 +296,7 @@ def test_multiple_pools(self):\n         self.assertEquals(5, merged_pools.consumed)\n \n \n-class PoolFilterTests(unittest.TestCase):\n+class PoolFilterTests(SubManFixture):\n \n     def test_uninstalled_filter_direct_match(self):\n         product1 = 'product1'\n@@ -1032,7 +1032,7 @@ def test_sort_virt_to_top(self):\n class PoolStashTest(unittest.TestCase):\n \n     def test_empty_stash_zero_length(self):\n-        my_stash = PoolStash(None)\n+        my_stash = PoolStash()\n         self.assertTrue(my_stash.all_pools_size() == 0)\n \n \n@@ -1106,7 +1106,7 @@ def test_no_pools(self):\n         # get the injected stub uep\n         cp = self.get_consumer_cp()\n         cp.getPoolsList = Mock(return_value=[])\n-        res = managerlib.get_available_entitlements(facts={})\n+        res = managerlib.get_available_entitlements()\n         self.assertEquals(0, len(res))\n \n     def test_incompatible(self):\n@@ -1122,10 +1122,10 @@ def get_pools_list(consumer=None, listAll=False, active_on=None, owner=None, fil\n \n         cp.getPoolsList = Mock(side_effect=get_pools_list)\n \n-        res = managerlib.get_available_entitlements(facts={}, get_all=True)\n+        res = managerlib.get_available_entitlements(get_all=True)\n         self.assertEquals(2, len(res))\n \n-        res = managerlib.get_available_entitlements(facts={}, get_all=False)\n+        res = managerlib.get_available_entitlements(get_all=False)\n         self.assertEquals(1, len(res))\n \n     def test_installed(self):\n@@ -1144,10 +1144,10 @@ def get_pools_list(consumer=None, listAll=False, active_on=None, owner=None, fil\n         product_directory = StubProductDirectory(pids=['some_product'])\n         provide(PROD_DIR, product_directory)\n \n-        res = managerlib.get_available_entitlements(facts={}, get_all=True, uninstalled=True)\n+        res = managerlib.get_available_entitlements(get_all=True, uninstalled=True)\n         self.assertEquals(2, len(res))\n \n-        res = managerlib.get_available_entitlements(facts={}, uninstalled=True)\n+        res = managerlib.get_available_entitlements(uninstalled=True)\n         self.assertEquals(1, len(res))\n \n     def build_pool_dict(self, pool_id, provided_products=[]):"
        },
        {
          "filename": "test/test_migration.py",
          "status": "modified",
          "additions": 58,
          "deletions": 48,
          "patch": "@@ -391,17 +391,23 @@ def test_setting_unauthenticated_proxy(self):\n             \"enableProxyAuth\": False,\n             }\n         self.engine.rhncfg = rhn_config\n+        section = MagicMock()\n+        self.engine.rhsmcfg.__getitem__.return_value = section\n+\n         self.engine.transfer_http_proxy_settings()\n-        expected = [call(\"server\", \"proxy_hostname\", \"proxy.example.com\"),\n-            call(\"server\", \"proxy_port\", \"123\"),\n-            call(\"server\", \"proxy_user\", \"\"),\n-            call(\"server\", \"proxy_password\", \"\"),\n-            ]\n-        self.assertTrue(self.engine.rhsmcfg.set.call_args_list == expected)\n-        self.engine.rhsmcfg.save.assert_called_once_with()\n+        expected = [call(\"proxy_hostname\", \"proxy.example.com\"),\n+            call(\"proxy_port\", \"123\"),\n+            call(\"proxy_user\", \"\"),\n+            call(\"proxy_password\", \"\"),\n+        ]\n+        self.assertTrue(section.__setitem__.call_args_list == expected)\n+        self.engine.rhsmcfg.persist.assert_called_once_with()\n \n     def test_setting_authenticated_proxy(self):\n         self.engine.rhsmcfg = MagicMock()\n+        section = MagicMock()\n+        self.engine.rhsmcfg.__getitem__.return_value = section\n+\n         self.engine.options = self.create_options(noproxy=False)\n \n         rhn_config = {\n@@ -410,16 +416,16 @@ def test_setting_authenticated_proxy(self):\n             \"enableProxyAuth\": True,\n             \"proxyUser\": \"foo\",\n             \"proxyPassword\": \"bar\",\n-            }\n+        }\n         self.engine.rhncfg = rhn_config\n         self.engine.transfer_http_proxy_settings()\n-        expected = [call(\"server\", \"proxy_hostname\", \"proxy.example.com\"),\n-            call(\"server\", \"proxy_port\", \"123\"),\n-            call(\"server\", \"proxy_user\", \"foo\"),\n-            call(\"server\", \"proxy_password\", \"bar\"),\n-            ]\n-        self.assertTrue(self.engine.rhsmcfg.set.call_args_list == expected)\n-        self.engine.rhsmcfg.save.assert_called_once_with()\n+        expected = [call(\"proxy_hostname\", \"proxy.example.com\"),\n+            call(\"proxy_port\", \"123\"),\n+            call(\"proxy_user\", \"foo\"),\n+            call(\"proxy_password\", \"bar\"),\n+        ]\n+        self.assertTrue(section.__setitem__.call_args_list == expected)\n+        self.engine.rhsmcfg.persist.assert_called_once_with()\n \n     def test_setting_prefixed_proxy(self):\n         self.engine.rhsmcfg = MagicMock()\n@@ -431,14 +437,17 @@ def test_setting_prefixed_proxy(self):\n             \"enableProxyAuth\": False,\n             }\n         self.engine.rhncfg = rhn_config\n+        section = MagicMock()\n+        self.engine.rhsmcfg.__getitem__.return_value = section\n         self.engine.transfer_http_proxy_settings()\n-        expected = [call(\"server\", \"proxy_hostname\", \"proxy.example.com\"),\n-            call(\"server\", \"proxy_port\", \"123\"),\n-            call(\"server\", \"proxy_user\", \"\"),\n-            call(\"server\", \"proxy_password\", \"\"),\n-            ]\n-        self.assertTrue(self.engine.rhsmcfg.set.call_args_list == expected)\n-        self.engine.rhsmcfg.save.assert_called_once_with()\n+        expected = [\n+            call(\"proxy_hostname\", \"proxy.example.com\"),\n+            call(\"proxy_port\", \"123\"),\n+            call(\"proxy_user\", \"\"),\n+            call(\"proxy_password\", \"\"),\n+        ]\n+        self.assertTrue(section.__setitem__.call_args_list == expected)\n+        self.engine.rhsmcfg.persist.assert_called_once_with()\n \n     def test_noproxy_option(self):\n         self.engine.rhsmcfg = MagicMock()\n@@ -450,36 +459,20 @@ def test_noproxy_option(self):\n             \"enableProxyAuth\": False,\n             }\n         self.engine.rhncfg = rhn_config\n+        section = MagicMock()\n+        self.engine.rhsmcfg.__getitem__.return_value = section\n         self.engine.transfer_http_proxy_settings()\n-        expected = [call(\"server\", \"proxy_hostname\", \"\"),\n-            call(\"server\", \"proxy_port\", \"\"),\n-            call(\"server\", \"proxy_user\", \"\"),\n-            call(\"server\", \"proxy_password\", \"\"),\n+        expected = [call(\"proxy_hostname\", \"\"),\n+            call(\"proxy_port\", \"\"),\n+            call(\"proxy_user\", \"\"),\n+            call(\"proxy_password\", \"\"),\n             ]\n-        self.assertTrue(self.engine.rhsmcfg.set.call_args_list == expected)\n+        self.assertTrue(section.__setitem__.call_args_list == expected)\n         self.assertEquals(\"proxy.example.com\", self.engine.proxy_host)\n         self.assertEquals(\"123\", self.engine.proxy_port)\n         self.assertEquals(None, self.engine.proxy_user)\n         self.assertEquals(None, self.engine.proxy_pass)\n \n-    def _setup_rhsmcfg_mocks(self):\n-        self.engine.options = self.create_options()\n-\n-        self.engine.rhsmcfg = MagicMock()\n-        self.engine.rhsmcfg.get = MagicMock(side_effect=[\n-            \"candlepin.example.com\",\n-            \"/candlepin\",\n-            ])\n-        self.engine.rhsmcfg.get_int = MagicMock(side_effect=[443])\n-\n-        expected = [call(\"server\", \"hostname\"),\n-            call(\"server\", \"prefix\"),\n-            ]\n-\n-        get_int_expected = [call(\"server\", \"port\")]\n-\n-        return expected, get_int_expected\n-\n     @patch(\"rhn.rpclib.Server\")\n     def test_load_transition_data(self, mock_server):\n         mock_server.system.transitionDataForSystem.return_value = {\"uuid\": \"1\"}\n@@ -541,10 +534,27 @@ def test_consumer_does_not_exist(self):\n         self.engine.cp.getConsumer.assert_called_once_with(\"123\")\n \n     def test_no_server_url_provided_basic_auth(self):\n-        expected, get_int_expected = self._setup_rhsmcfg_mocks()\n+        self.engine.options = self.create_options()\n+\n+        self.engine.rhsmcfg = MagicMock()\n+        section = MagicMock()\n+        self.engine.rhsmcfg.__getitem__.return_value = section\n+\n+        section.__getitem__.return_value = MagicMock(side_effect=[\n+            \"candlepin.example.com\",\n+            \"/candlepin\",\n+        ])\n+        section.get_int = MagicMock(side_effect=[443])\n+\n+        expected = [call(\"hostname\"),\n+            call(\"prefix\"),\n+        ]\n+\n+        int_expected = [call(\"port\")]\n+\n         self.engine.get_candlepin_connection(\"some_username\", \"some_password\")\n-        self.assertTrue(self.engine.rhsmcfg.get.call_args_list == expected)\n-        self.assertTrue(self.engine.rhsmcfg.get_int.call_args_list == get_int_expected)\n+        self.assertTrue(section.__getitem__.call_args_list == expected)\n+        self.assertTrue(section.get_int.call_args_list == int_expected)\n \n     def test_bad_server_url_basic_auth(self):\n         self.engine.options = self.create_options(destination_url='http://')"
        },
        {
          "filename": "test/test_registrationgui.py",
          "status": "modified",
          "additions": 6,
          "deletions": 12,
          "patch": "@@ -28,16 +28,13 @@ def setUp(self):\n         self.facts = StubFacts(fact_dict=expected_facts)\n \n         self.reg_info = RegisterInfo()\n-        self.rs = RegisterWidget(backend=self.backend,\n-                                 facts=self.facts,\n-                                 reg_info=self.reg_info)\n+        self.rs = RegisterWidget(backend=self.backend, reg_info=self.reg_info)\n \n         self.rs._screens[CHOOSE_SERVER_PAGE] = Mock()\n         self.rs._screens[CHOOSE_SERVER_PAGE].index = 0\n         self.rs._screens[CHOOSE_SERVER_PAGE].screens_index = 0\n         self.rs._screens[CHOOSE_SERVER_PAGE].button_label = \"Dummy\"\n-        self.rs._screens[CHOOSE_SERVER_PAGE].apply.return_value = \\\n-                CREDENTIALS_PAGE\n+        self.rs._screens[CHOOSE_SERVER_PAGE].apply.return_value = CREDENTIALS_PAGE\n \n     def test_show(self):\n         self.rs.initialize()\n@@ -154,7 +151,6 @@ def setUp(self):\n         stub_reg = StubReg()\n         self.screen = CredentialsScreen(reg_info=stub_reg.reg_info,\n                                         async_backend=stub_reg.async,\n-                                        facts=stub_reg.facts,\n                                         parent_window=stub_reg.parent_window)\n \n     def test_clear_credentials_dialog(self):\n@@ -178,7 +174,6 @@ def setUp(self):\n         stub_reg = StubReg()\n         self.screen = ActivationKeyScreen(reg_info=stub_reg.reg_info,\n                                           async_backend=stub_reg.async,\n-                                          facts=stub_reg.facts,\n                                           parent_window=stub_reg.parent_window)\n \n     def test_split_activation_keys(self):\n@@ -194,7 +189,6 @@ def setUp(self):\n         stub_reg = StubReg()\n         self.screen = ChooseServerScreen(reg_info=stub_reg.reg_info,\n                                          async_backend=stub_reg.async,\n-                                         facts=stub_reg.facts,\n                                          parent_window=stub_reg.parent_window)\n \n     def test_activation_key_checkbox_sensitive(self):\n@@ -236,20 +230,20 @@ def setUp(self):\n \n     def test_auto_system_complete(self):\n         self.backend.cp_provider.get_consumer_auth_cp().getConsumer = \\\n-           Mock(return_value={\"serviceLevel\": \"\", \"owner\": {\"key\": \"admin\"}})\n+            Mock(return_value={\"serviceLevel\": \"\", \"owner\": {\"key\": \"admin\"}})\n         self.backend.cs.valid_products = ['RH001', 'RH002']\n         self.backend.cs.installed_products = ['RH001', 'RH002']\n         self.backend.cs.partial_stacks = []\n         self.backend.cs.system_status = 'valid'\n         self.backend.cp_provider.get_consumer_auth_cp().getServiceLevelList = Mock(return_value=[])\n-        self.assertRaises(AllProductsCoveredException, self.asyncBackend._find_suitable_service_levels, '12345', {})\n+        self.assertRaises(AllProductsCoveredException, self.asyncBackend._find_suitable_service_levels, '12345')\n \n     def test_auto_system_partial(self):\n         self.backend.cp_provider.get_consumer_auth_cp().getConsumer = \\\n-           Mock(return_value={\"serviceLevel\": \"\", \"owner\": {\"key\": \"admin\"}})\n+            Mock(return_value={\"serviceLevel\": \"\", \"owner\": {\"key\": \"admin\"}})\n         self.backend.cs.valid_products = ['RH001', 'RH002']\n         self.backend.cs.installed_products = ['RH001', 'RH002']\n         self.backend.cs.partial_stacks = []\n         self.backend.cs.system_status = 'partial'\n         self.backend.cp_provider.get_consumer_auth_cp().getServiceLevelList = Mock(return_value=[])\n-        self.asyncBackend._find_suitable_service_levels('12345', {})\n+        self.asyncBackend._find_suitable_service_levels('12345')"
        },
        {
          "filename": "test/test_repolib.py",
          "status": "modified",
          "additions": 15,
          "deletions": 14,
          "patch": "@@ -31,7 +31,8 @@\n from subscription_manager.repolib import Repo, RepoActionInvoker, \\\n         RepoUpdateActionCommand, TidyWriter, RepoFile, YumReleaseverSource\n from subscription_manager import injection as inj\n-from rhsm import config\n+from rhsm.config import RhsmConfigParser\n+from rhsmlib.services import config\n \n from subscription_manager import repolib\n \n@@ -757,7 +758,13 @@ def test_configparsers_equal_int(self, tidy_writer, stub_create):\n \"\"\"\n \n \n-class RhsmConfigParserFromString(config.RhsmConfigParser):\n+class ConfigFromString(config.Config):\n+    def __init__(self, config_string):\n+        parser = RhsmConfigParserFromString(config_string)\n+        super(ConfigFromString, self).__init__(parser)\n+\n+\n+class RhsmConfigParserFromString(RhsmConfigParser):\n     def __init__(self, config_string):\n         SafeConfigParser.__init__(self)\n         self.stringio = StringIO(config_string)\n@@ -786,41 +793,35 @@ def __init__(self, config_string):\n \n \n class TestManageReposEnabled(fixture.SubManFixture):\n-    @patch.object(repolib, 'CFG',\n-                  RhsmConfigParserFromString(config_string=unset_config))\n+    @patch.object(repolib, 'conf', ConfigFromString(config_string=unset_config))\n     def test(self):\n         # default stub config, no manage_repo defined, uses default\n         manage_repos_enabled = repolib.manage_repos_enabled()\n         self.assertEquals(manage_repos_enabled, True)\n \n-    @patch.object(repolib, 'CFG',\n-                  RhsmConfigParserFromString(config_string=unset_manage_repos_cfg_buf))\n+    @patch.object(repolib, 'conf', ConfigFromString(config_string=unset_manage_repos_cfg_buf))\n     def test_empty_manage_repos(self):\n         manage_repos_enabled = repolib.manage_repos_enabled()\n         self.assertEquals(manage_repos_enabled, True)\n \n-    @patch.object(repolib, 'CFG',\n-                  RhsmConfigParserFromString(config_string=manage_repos_zero_config))\n+    @patch.object(repolib, 'conf', ConfigFromString(config_string=manage_repos_zero_config))\n     def test_empty_manage_repos_zero(self):\n         manage_repos_enabled = repolib.manage_repos_enabled()\n         self.assertEquals(manage_repos_enabled, False)\n \n-    @patch.object(repolib, 'CFG',\n-                  RhsmConfigParserFromString(config_string=manage_repos_bool_config))\n+    @patch.object(repolib, 'config', ConfigFromString(config_string=manage_repos_bool_config))\n     def test_empty_manage_repos_bool(self):\n         manage_repos_enabled = repolib.manage_repos_enabled()\n         # Should fail, and return default of 1\n         self.assertEquals(manage_repos_enabled, True)\n \n-    @patch.object(repolib, 'CFG',\n-                  RhsmConfigParserFromString(config_string=manage_repos_not_an_int))\n+    @patch.object(repolib, 'config', ConfigFromString(config_string=manage_repos_not_an_int))\n     def test_empty_manage_repos_not_an_int(self):\n         manage_repos_enabled = repolib.manage_repos_enabled()\n         # Should fail, and return default of 1\n         self.assertEquals(manage_repos_enabled, True)\n \n-    @patch.object(repolib, 'CFG',\n-                  RhsmConfigParserFromString(config_string=manage_repos_int_37))\n+    @patch.object(repolib, 'conf', ConfigFromString(config_string=manage_repos_int_37))\n     def test_empty_manage_repos_int_37(self):\n         manage_repos_enabled = repolib.manage_repos_enabled()\n         # Should fail, and return default of 1"
        },
        {
          "filename": "test/test_utils.py",
          "status": "modified",
          "additions": 5,
          "deletions": 3,
          "patch": "@@ -16,6 +16,8 @@\n from rhsm.config import DEFAULT_PORT, DEFAULT_PREFIX, DEFAULT_HOSTNAME, \\\n     DEFAULT_CDN_HOSTNAME, DEFAULT_CDN_PORT, DEFAULT_CDN_PREFIX\n \n+from rhsmlib.services import config\n+\n \n class TestParseServerInfo(SubManFixture):\n     def setUp(self):\n@@ -55,7 +57,7 @@ def test_hostname_only_config(self):\n         self.stubConfig.set(\"server\", \"prefix\", \"/test-prefix\")\n \n         local_url = \"myhost.example.com\"\n-        (hostname, port, prefix) = parse_server_info(local_url, self.stubConfig)\n+        (hostname, port, prefix) = parse_server_info(local_url, config.Config(self.stubConfig))\n         self.assertEquals(\"myhost.example.com\", hostname)\n         self.assertEquals(\"344\", port)\n         self.assertEquals(\"/test-prefix\", prefix)\n@@ -64,7 +66,7 @@ def test_hostname_port_config(self):\n         self.stubConfig.set(\"server\", \"port\", \"600\")\n \n         local_url = \"myhost.example.com/myapp\"\n-        (hostname, port, prefix) = parse_server_info(local_url, self.stubConfig)\n+        (hostname, port, prefix) = parse_server_info(local_url, config.Config(self.stubConfig))\n         self.assertEquals(\"myhost.example.com\", hostname)\n         self.assertEquals(\"600\", port)\n         self.assertEquals(\"/myapp\", prefix)\n@@ -73,7 +75,7 @@ def test_hostname_prefix_config(self):\n         self.stubConfig.set(\"server\", \"prefix\", \"/test-prefix\")\n \n         local_url = \"myhost.example.com:500\"\n-        (hostname, port, prefix) = parse_server_info(local_url, self.stubConfig)\n+        (hostname, port, prefix) = parse_server_info(local_url, config.Config(self.stubConfig))\n         self.assertEquals(\"myhost.example.com\", hostname)\n         self.assertEquals(\"500\", port)\n         self.assertEquals(\"/test-prefix\", prefix)"
        },
        {
          "filename": "tox.ini",
          "status": "modified",
          "additions": 3,
          "deletions": 2,
          "patch": "@@ -13,12 +13,13 @@\n # E402 module level import not at top of file\n # E731 do not assign a lambda expression, use a def\n \n+# TODO enable E198, E199, E122, E124, E121\n [pep8]\n-ignore=E121,E122,E123,E124,E125,E126,E127,E128,E129,E131,E265,E402,E501,E713,E714,E731\n+ignore=E123,E125,E126,E127,E128,E129,E265,E402,E501,E713,E714,E731,E198,E199,E122,E124,E121\n max-line-length=300\n \n [flake8]\n-ignore=E121,E122,E123,E124,E125,E126,E127,E128,E129,E131,E265,E402,E501,E713,E714,E731,V250,V260\n+ignore=E123,E125,E126,E127,E128,E129,E265,E402,E501,E713,E714,E731,V250,V260,E198,E199,E122,E124,E121\n exclude=*certdata*,*manifestdata*\n jobs=auto\n max-line-length=300"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 5,
        "dependency_files": 3,
        "test_files": 25,
        "unique_directories": 27,
        "max_directory_depth": 5
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "4da2473b0d4ae50868bbe498fc7cbe7dcf1fb810",
            "date": "2025-01-07T17:16:36Z",
            "author_login": "ptoscano"
          },
          {
            "sha": "50cb52bfb04a76b8e12c3cd282453f21b2a27462",
            "date": "2025-01-09T08:09:06Z",
            "author_login": "ptoscano"
          },
          {
            "sha": "c0c62d16b8d7e75633cb78704f6070656478b390",
            "date": "2024-12-23T23:38:39Z",
            "author_login": "salvatore-cocuzza"
          },
          {
            "sha": "c6adde0677e6ecae7dbf7c68443b7383e52027aa",
            "date": "2024-12-23T23:38:39Z",
            "author_login": "HarmfulBreeze"
          },
          {
            "sha": "82bd1f3a59d5d2a244355045a43ddd9dea1320ed",
            "date": "2024-12-23T23:38:39Z",
            "author_login": "Atalanttore"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": null,
    "cvss_vector": null,
    "cwe_id": "CWE-270",
    "description": "It was found that subscription-manager's DBus interface before 1.19.4 let unprivileged user access the com.redhat.RHSM1.Facts.GetFacts and com.redhat.RHSM1.Config.Set methods. An unprivileged local attacker could use these methods to gain access to private information, or launch a privilege escalation attack.",
    "attack_vector": null,
    "attack_complexity": null
  },
  "temporal_data": {
    "published_date": "2018-07-27T20:29:00.577",
    "last_modified": "2024-11-21T03:23:55.800",
    "fix_date": "2017-01-04T17:56:15Z"
  },
  "references": [
    {
      "url": "http://www.securityfocus.com/bid/97015",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory",
        "VDB Entry"
      ]
    },
    {
      "url": "https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2017-2663",
      "source": "secalert@redhat.com",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/candlepin/subscription-manager/commit/2aa48ef65",
      "source": "secalert@redhat.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://www.securityfocus.com/bid/97015",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory",
        "VDB Entry"
      ]
    },
    {
      "url": "https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2017-2663",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/candlepin/subscription-manager/commit/2aa48ef65",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T22:59:32.418129",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "subscription-manager",
    "owner": "candlepin",
    "created_at": "2012-05-17T16:13:30Z",
    "updated_at": "2025-01-09T14:14:59Z",
    "pushed_at": "2025-01-14T12:35:21Z",
    "size": 71925,
    "stars": 65,
    "forks": 120,
    "open_issues": 22,
    "watchers": 65,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [
      "SLE_11_SP4",
      "SLES-11.4",
      "SLES-15"
    ],
    "languages": {
      "Python": 2802398,
      "C": 144429,
      "Shell": 38664,
      "Euphoria": 16725,
      "Makefile": 15126,
      "CMake": 3681,
      "Roff": 1701,
      "Elixir": 1540
    },
    "commit_activity": {
      "total_commits_last_year": 118,
      "avg_commits_per_week": 2.269230769230769,
      "days_active_last_year": 68
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": false,
      "has_issues": false,
      "allow_forking": true,
      "is_template": false,
      "license": "gpl-2.0"
    },
    "collected_at": "2025-01-14T16:35:29.330646"
  }
}