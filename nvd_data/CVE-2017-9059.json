{
  "cve_id": "CVE-2017-9059",
  "github_data": {
    "repository": "torvalds/linux",
    "fix_commit": "c70422f760c120480fee4de6c38804c72aa26bc1",
    "related_commits": [
      "c70422f760c120480fee4de6c38804c72aa26bc1",
      "c70422f760c120480fee4de6c38804c72aa26bc1"
    ],
    "patch_url": null,
    "fix_commit_details": {
      "sha": "c70422f760c120480fee4de6c38804c72aa26bc1",
      "commit_date": "2017-05-10T20:29:23Z",
      "author": {
        "login": "torvalds",
        "type": "User",
        "stats": {
          "total_commits": 38557,
          "average_weekly_commits": 32.37363560033585,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 881
        }
      },
      "commit_message": {
        "title": "Merge tag 'nfsd-4.12' of git://linux-nfs.org/~bfields/linux",
        "length": 1210,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 2237,
        "additions": 1334,
        "deletions": 903
      },
      "files": [
        {
          "filename": "fs/lockd/svc.c",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -132,6 +132,8 @@ lockd(void *vrqstp)\n {\n \tint\t\terr = 0;\n \tstruct svc_rqst *rqstp = vrqstp;\n+\tstruct net *net = &init_net;\n+\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n \n \t/* try_to_freeze() is called from svc_recv() */\n \tset_freezable();\n@@ -176,6 +178,8 @@ lockd(void *vrqstp)\n \tif (nlmsvc_ops)\n \t\tnlmsvc_invalidate_all();\n \tnlm_shutdown_hosts();\n+\tcancel_delayed_work_sync(&ln->grace_period_end);\n+\tlocks_end_grace(&ln->lockd_manager);\n \treturn 0;\n }\n \n@@ -270,8 +274,6 @@ static void lockd_down_net(struct svc_serv *serv, struct net *net)\n \tif (ln->nlmsvc_users) {\n \t\tif (--ln->nlmsvc_users == 0) {\n \t\t\tnlm_shutdown_hosts_net(net);\n-\t\t\tcancel_delayed_work_sync(&ln->grace_period_end);\n-\t\t\tlocks_end_grace(&ln->lockd_manager);\n \t\t\tsvc_shutdown_net(serv, net);\n \t\t\tdprintk(\"lockd_down_net: per-net data destroyed; net=%p\\n\", net);\n \t\t}"
        },
        {
          "filename": "fs/lockd/svclock.c",
          "status": "modified",
          "additions": 9,
          "deletions": 9,
          "patch": "@@ -870,15 +870,15 @@ nlmsvc_grant_reply(struct nlm_cookie *cookie, __be32 status)\n \tif (!(block = nlmsvc_find_block(cookie)))\n \t\treturn;\n \n-\tif (block) {\n-\t\tif (status == nlm_lck_denied_grace_period) {\n-\t\t\t/* Try again in a couple of seconds */\n-\t\t\tnlmsvc_insert_block(block, 10 * HZ);\n-\t\t} else {\n-\t\t\t/* Lock is now held by client, or has been rejected.\n-\t\t\t * In both cases, the block should be removed. */\n-\t\t\tnlmsvc_unlink_block(block);\n-\t\t}\n+\tif (status == nlm_lck_denied_grace_period) {\n+\t\t/* Try again in a couple of seconds */\n+\t\tnlmsvc_insert_block(block, 10 * HZ);\n+\t} else {\n+\t\t/*\n+\t\t * Lock is now held by client, or has been rejected.\n+\t\t * In both cases, the block should be removed.\n+\t\t */\n+\t\tnlmsvc_unlink_block(block);\n \t}\n \tnlmsvc_release_block(block);\n }"
        },
        {
          "filename": "fs/nfs/callback.c",
          "status": "modified",
          "additions": 17,
          "deletions": 9,
          "patch": "@@ -76,7 +76,10 @@ nfs4_callback_svc(void *vrqstp)\n \n \tset_freezable();\n \n-\twhile (!kthread_should_stop()) {\n+\twhile (!kthread_freezable_should_stop(NULL)) {\n+\n+\t\tif (signal_pending(current))\n+\t\t\tflush_signals(current);\n \t\t/*\n \t\t * Listen for a request on the socket\n \t\t */\n@@ -85,6 +88,8 @@ nfs4_callback_svc(void *vrqstp)\n \t\t\tcontinue;\n \t\tsvc_process(rqstp);\n \t}\n+\tsvc_exit_thread(rqstp);\n+\tmodule_put_and_exit(0);\n \treturn 0;\n }\n \n@@ -103,9 +108,10 @@ nfs41_callback_svc(void *vrqstp)\n \n \tset_freezable();\n \n-\twhile (!kthread_should_stop()) {\n-\t\tif (try_to_freeze())\n-\t\t\tcontinue;\n+\twhile (!kthread_freezable_should_stop(NULL)) {\n+\n+\t\tif (signal_pending(current))\n+\t\t\tflush_signals(current);\n \n \t\tprepare_to_wait(&serv->sv_cb_waitq, &wq, TASK_INTERRUPTIBLE);\n \t\tspin_lock_bh(&serv->sv_cb_lock);\n@@ -121,11 +127,13 @@ nfs41_callback_svc(void *vrqstp)\n \t\t\t\terror);\n \t\t} else {\n \t\t\tspin_unlock_bh(&serv->sv_cb_lock);\n-\t\t\tschedule();\n+\t\t\tif (!kthread_should_stop())\n+\t\t\t\tschedule();\n \t\t\tfinish_wait(&serv->sv_cb_waitq, &wq);\n \t\t}\n-\t\tflush_signals(current);\n \t}\n+\tsvc_exit_thread(rqstp);\n+\tmodule_put_and_exit(0);\n \treturn 0;\n }\n \n@@ -221,14 +229,14 @@ static int nfs_callback_up_net(int minorversion, struct svc_serv *serv,\n static struct svc_serv_ops nfs40_cb_sv_ops = {\n \t.svo_function\t\t= nfs4_callback_svc,\n \t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n-\t.svo_setup\t\t= svc_set_num_threads,\n+\t.svo_setup\t\t= svc_set_num_threads_sync,\n \t.svo_module\t\t= THIS_MODULE,\n };\n #if defined(CONFIG_NFS_V4_1)\n static struct svc_serv_ops nfs41_cb_sv_ops = {\n \t.svo_function\t\t= nfs41_callback_svc,\n \t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n-\t.svo_setup\t\t= svc_set_num_threads,\n+\t.svo_setup\t\t= svc_set_num_threads_sync,\n \t.svo_module\t\t= THIS_MODULE,\n };\n \n@@ -280,7 +288,7 @@ static struct svc_serv *nfs_callback_create_svc(int minorversion)\n \t\tprintk(KERN_WARNING \"nfs_callback_create_svc: no kthread, %d users??\\n\",\n \t\t\tcb_info->users);\n \n-\tserv = svc_create(&nfs4_callback_program, NFS4_CALLBACK_BUFSIZE, sv_ops);\n+\tserv = svc_create_pooled(&nfs4_callback_program, NFS4_CALLBACK_BUFSIZE, sv_ops);\n \tif (!serv) {\n \t\tprintk(KERN_ERR \"nfs_callback_create_svc: create service failed\\n\");\n \t\treturn ERR_PTR(-ENOMEM);"
        },
        {
          "filename": "fs/nfsd/nfs3xdr.c",
          "status": "modified",
          "additions": 17,
          "deletions": 6,
          "patch": "@@ -334,8 +334,11 @@ nfs3svc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n \tif (!p)\n \t\treturn 0;\n \tp = xdr_decode_hyper(p, &args->offset);\n-\n \targs->count = ntohl(*p++);\n+\n+\tif (!xdr_argsize_check(rqstp, p))\n+\t\treturn 0;\n+\n \tlen = min(args->count, max_blocksize);\n \n \t/* set up the kvec */\n@@ -349,7 +352,7 @@ nfs3svc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n \t\tv++;\n \t}\n \targs->vlen = v;\n-\treturn xdr_argsize_check(rqstp, p);\n+\treturn 1;\n }\n \n int\n@@ -541,9 +544,11 @@ nfs3svc_decode_readlinkargs(struct svc_rqst *rqstp, __be32 *p,\n \tp = decode_fh(p, &args->fh);\n \tif (!p)\n \t\treturn 0;\n+\tif (!xdr_argsize_check(rqstp, p))\n+\t\treturn 0;\n \targs->buffer = page_address(*(rqstp->rq_next_page++));\n \n-\treturn xdr_argsize_check(rqstp, p);\n+\treturn 1;\n }\n \n int\n@@ -569,10 +574,14 @@ nfs3svc_decode_readdirargs(struct svc_rqst *rqstp, __be32 *p,\n \targs->verf   = p; p += 2;\n \targs->dircount = ~0;\n \targs->count  = ntohl(*p++);\n+\n+\tif (!xdr_argsize_check(rqstp, p))\n+\t\treturn 0;\n+\n \targs->count  = min_t(u32, args->count, PAGE_SIZE);\n \targs->buffer = page_address(*(rqstp->rq_next_page++));\n \n-\treturn xdr_argsize_check(rqstp, p);\n+\treturn 1;\n }\n \n int\n@@ -590,15 +599,17 @@ nfs3svc_decode_readdirplusargs(struct svc_rqst *rqstp, __be32 *p,\n \targs->dircount = ntohl(*p++);\n \targs->count    = ntohl(*p++);\n \n+\tif (!xdr_argsize_check(rqstp, p))\n+\t\treturn 0;\n+\n \tlen = args->count = min(args->count, max_blocksize);\n \twhile (len > 0) {\n \t\tstruct page *p = *(rqstp->rq_next_page++);\n \t\tif (!args->buffer)\n \t\t\targs->buffer = page_address(p);\n \t\tlen -= PAGE_SIZE;\n \t}\n-\n-\treturn xdr_argsize_check(rqstp, p);\n+\treturn 1;\n }\n \n int"
        },
        {
          "filename": "fs/nfsd/nfs4proc.c",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -1259,7 +1259,8 @@ nfsd4_layout_verify(struct svc_export *exp, unsigned int layout_type)\n \t\treturn NULL;\n \t}\n \n-\tif (!(exp->ex_layout_types & (1 << layout_type))) {\n+\tif (layout_type >= LAYOUT_TYPE_MAX ||\n+\t    !(exp->ex_layout_types & (1 << layout_type))) {\n \t\tdprintk(\"%s: layout type %d not supported\\n\",\n \t\t\t__func__, layout_type);\n \t\treturn NULL;"
        },
        {
          "filename": "fs/nfsd/nfs4state.c",
          "status": "modified",
          "additions": 6,
          "deletions": 19,
          "patch": "@@ -1912,28 +1912,15 @@ static void copy_clid(struct nfs4_client *target, struct nfs4_client *source)\n \ttarget->cl_clientid.cl_id = source->cl_clientid.cl_id; \n }\n \n-int strdup_if_nonnull(char **target, char *source)\n-{\n-\tif (source) {\n-\t\t*target = kstrdup(source, GFP_KERNEL);\n-\t\tif (!*target)\n-\t\t\treturn -ENOMEM;\n-\t} else\n-\t\t*target = NULL;\n-\treturn 0;\n-}\n-\n static int copy_cred(struct svc_cred *target, struct svc_cred *source)\n {\n-\tint ret;\n+\ttarget->cr_principal = kstrdup(source->cr_principal, GFP_KERNEL);\n+\ttarget->cr_raw_principal = kstrdup(source->cr_raw_principal,\n+\t\t\t\t\t\t\t\tGFP_KERNEL);\n+\tif ((source->cr_principal && ! target->cr_principal) ||\n+\t    (source->cr_raw_principal && ! target->cr_raw_principal))\n+\t\treturn -ENOMEM;\n \n-\tret = strdup_if_nonnull(&target->cr_principal, source->cr_principal);\n-\tif (ret)\n-\t\treturn ret;\n-\tret = strdup_if_nonnull(&target->cr_raw_principal,\n-\t\t\t\t\tsource->cr_raw_principal);\n-\tif (ret)\n-\t\treturn ret;\n \ttarget->cr_flavor = source->cr_flavor;\n \ttarget->cr_uid = source->cr_uid;\n \ttarget->cr_gid = source->cr_gid;"
        },
        {
          "filename": "fs/nfsd/nfs4xdr.c",
          "status": "modified",
          "additions": 12,
          "deletions": 7,
          "patch": "@@ -2831,9 +2831,14 @@ nfsd4_encode_fattr(struct xdr_stream *xdr, struct svc_fh *fhp,\n \t}\n #endif /* CONFIG_NFSD_PNFS */\n \tif (bmval2 & FATTR4_WORD2_SUPPATTR_EXCLCREAT) {\n-\t\tstatus = nfsd4_encode_bitmap(xdr, NFSD_SUPPATTR_EXCLCREAT_WORD0,\n-\t\t\t\t\t\t  NFSD_SUPPATTR_EXCLCREAT_WORD1,\n-\t\t\t\t\t\t  NFSD_SUPPATTR_EXCLCREAT_WORD2);\n+\t\tu32 supp[3];\n+\n+\t\tmemcpy(supp, nfsd_suppattrs[minorversion], sizeof(supp));\n+\t\tsupp[0] &= NFSD_SUPPATTR_EXCLCREAT_WORD0;\n+\t\tsupp[1] &= NFSD_SUPPATTR_EXCLCREAT_WORD1;\n+\t\tsupp[2] &= NFSD_SUPPATTR_EXCLCREAT_WORD2;\n+\n+\t\tstatus = nfsd4_encode_bitmap(xdr, supp[0], supp[1], supp[2]);\n \t\tif (status)\n \t\t\tgoto out;\n \t}\n@@ -4119,8 +4124,7 @@ nfsd4_encode_getdeviceinfo(struct nfsd4_compoundres *resp, __be32 nfserr,\n \t\tstruct nfsd4_getdeviceinfo *gdev)\n {\n \tstruct xdr_stream *xdr = &resp->xdr;\n-\tconst struct nfsd4_layout_ops *ops =\n-\t\tnfsd4_layout_ops[gdev->gd_layout_type];\n+\tconst struct nfsd4_layout_ops *ops;\n \tu32 starting_len = xdr->buf->len, needed_len;\n \t__be32 *p;\n \n@@ -4137,6 +4141,7 @@ nfsd4_encode_getdeviceinfo(struct nfsd4_compoundres *resp, __be32 nfserr,\n \n \t/* If maxcount is 0 then just update notifications */\n \tif (gdev->gd_maxcount != 0) {\n+\t\tops = nfsd4_layout_ops[gdev->gd_layout_type];\n \t\tnfserr = ops->encode_getdeviceinfo(xdr, gdev);\n \t\tif (nfserr) {\n \t\t\t/*\n@@ -4189,8 +4194,7 @@ nfsd4_encode_layoutget(struct nfsd4_compoundres *resp, __be32 nfserr,\n \t\tstruct nfsd4_layoutget *lgp)\n {\n \tstruct xdr_stream *xdr = &resp->xdr;\n-\tconst struct nfsd4_layout_ops *ops =\n-\t\tnfsd4_layout_ops[lgp->lg_layout_type];\n+\tconst struct nfsd4_layout_ops *ops;\n \t__be32 *p;\n \n \tdprintk(\"%s: err %d\\n\", __func__, nfserr);\n@@ -4213,6 +4217,7 @@ nfsd4_encode_layoutget(struct nfsd4_compoundres *resp, __be32 nfserr,\n \t*p++ = cpu_to_be32(lgp->lg_seg.iomode);\n \t*p++ = cpu_to_be32(lgp->lg_layout_type);\n \n+\tops = nfsd4_layout_ops[lgp->lg_layout_type];\n \tnfserr = ops->encode_layoutget(xdr, lgp);\n out:\n \tkfree(lgp->lg_content);"
        },
        {
          "filename": "fs/nfsd/nfsxdr.c",
          "status": "modified",
          "additions": 10,
          "deletions": 3,
          "patch": "@@ -257,6 +257,9 @@ nfssvc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n \tlen = args->count     = ntohl(*p++);\n \tp++; /* totalcount - unused */\n \n+\tif (!xdr_argsize_check(rqstp, p))\n+\t\treturn 0;\n+\n \tlen = min_t(unsigned int, len, NFSSVC_MAXBLKSIZE_V2);\n \n \t/* set up somewhere to store response.\n@@ -272,7 +275,7 @@ nfssvc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n \t\tv++;\n \t}\n \targs->vlen = v;\n-\treturn xdr_argsize_check(rqstp, p);\n+\treturn 1;\n }\n \n int\n@@ -362,9 +365,11 @@ nfssvc_decode_readlinkargs(struct svc_rqst *rqstp, __be32 *p, struct nfsd_readli\n \tp = decode_fh(p, &args->fh);\n \tif (!p)\n \t\treturn 0;\n+\tif (!xdr_argsize_check(rqstp, p))\n+\t\treturn 0;\n \targs->buffer = page_address(*(rqstp->rq_next_page++));\n \n-\treturn xdr_argsize_check(rqstp, p);\n+\treturn 1;\n }\n \n int\n@@ -402,9 +407,11 @@ nfssvc_decode_readdirargs(struct svc_rqst *rqstp, __be32 *p,\n \targs->cookie = ntohl(*p++);\n \targs->count  = ntohl(*p++);\n \targs->count  = min_t(u32, args->count, PAGE_SIZE);\n+\tif (!xdr_argsize_check(rqstp, p))\n+\t\treturn 0;\n \targs->buffer = page_address(*(rqstp->rq_next_page++));\n \n-\treturn xdr_argsize_check(rqstp, p);\n+\treturn 1;\n }\n \n /*"
        },
        {
          "filename": "fs/nfsd/vfs.c",
          "status": "modified",
          "additions": 20,
          "deletions": 4,
          "patch": "@@ -94,6 +94,12 @@ nfsd_cross_mnt(struct svc_rqst *rqstp, struct dentry **dpp,\n \terr = follow_down(&path);\n \tif (err < 0)\n \t\tgoto out;\n+\tif (path.mnt == exp->ex_path.mnt && path.dentry == dentry &&\n+\t    nfsd_mountpoint(dentry, exp) == 2) {\n+\t\t/* This is only a mountpoint in some other namespace */\n+\t\tpath_put(&path);\n+\t\tgoto out;\n+\t}\n \n \texp2 = rqst_exp_get_by_name(rqstp, &path);\n \tif (IS_ERR(exp2)) {\n@@ -167,16 +173,26 @@ static int nfsd_lookup_parent(struct svc_rqst *rqstp, struct dentry *dparent, st\n /*\n  * For nfsd purposes, we treat V4ROOT exports as though there was an\n  * export at *every* directory.\n+ * We return:\n+ * '1' if this dentry *must* be an export point,\n+ * '2' if it might be, if there is really a mount here, and\n+ * '0' if there is no chance of an export point here.\n  */\n int nfsd_mountpoint(struct dentry *dentry, struct svc_export *exp)\n {\n-\tif (d_mountpoint(dentry))\n+\tif (!d_inode(dentry))\n+\t\treturn 0;\n+\tif (exp->ex_flags & NFSEXP_V4ROOT)\n \t\treturn 1;\n \tif (nfsd4_is_junction(dentry))\n \t\treturn 1;\n-\tif (!(exp->ex_flags & NFSEXP_V4ROOT))\n-\t\treturn 0;\n-\treturn d_inode(dentry) != NULL;\n+\tif (d_mountpoint(dentry))\n+\t\t/*\n+\t\t * Might only be a mountpoint in a different namespace,\n+\t\t * but we need to check.\n+\t\t */\n+\t\treturn 2;\n+\treturn 0;\n }\n \n __be32"
        },
        {
          "filename": "include/linux/sunrpc/rpc_rdma.h",
          "status": "modified",
          "additions": 3,
          "deletions": 0,
          "patch": "@@ -143,6 +143,9 @@ enum rpcrdma_proc {\n #define rdma_done\tcpu_to_be32(RDMA_DONE)\n #define rdma_error\tcpu_to_be32(RDMA_ERROR)\n \n+#define err_vers\tcpu_to_be32(ERR_VERS)\n+#define err_chunk\tcpu_to_be32(ERR_CHUNK)\n+\n /*\n  * Private extension to RPC-over-RDMA Version One.\n  * Message passed during RDMA-CM connection set-up."
        },
        {
          "filename": "include/linux/sunrpc/svc.h",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -336,8 +336,7 @@ xdr_argsize_check(struct svc_rqst *rqstp, __be32 *p)\n {\n \tchar *cp = (char *)p;\n \tstruct kvec *vec = &rqstp->rq_arg.head[0];\n-\treturn cp >= (char*)vec->iov_base\n-\t\t&& cp <= (char*)vec->iov_base + vec->iov_len;\n+\treturn cp == (char *)vec->iov_base + vec->iov_len;\n }\n \n static inline int\n@@ -474,6 +473,7 @@ void\t\t   svc_pool_map_put(void);\n struct svc_serv *  svc_create_pooled(struct svc_program *, unsigned int,\n \t\t\tstruct svc_serv_ops *);\n int\t\t   svc_set_num_threads(struct svc_serv *, struct svc_pool *, int);\n+int\t\t   svc_set_num_threads_sync(struct svc_serv *, struct svc_pool *, int);\n int\t\t   svc_pool_stats_open(struct svc_serv *serv, struct file *file);\n void\t\t   svc_destroy(struct svc_serv *);\n void\t\t   svc_shutdown_net(struct svc_serv *, struct net *);"
        },
        {
          "filename": "include/linux/sunrpc/svc_rdma.h",
          "status": "modified",
          "additions": 27,
          "deletions": 48,
          "patch": "@@ -48,6 +48,12 @@\n #include <rdma/rdma_cm.h>\n #define SVCRDMA_DEBUG\n \n+/* Default and maximum inline threshold sizes */\n+enum {\n+\tRPCRDMA_DEF_INLINE_THRESH = 4096,\n+\tRPCRDMA_MAX_INLINE_THRESH = 65536\n+};\n+\n /* RPC/RDMA parameters and stats */\n extern unsigned int svcrdma_ord;\n extern unsigned int svcrdma_max_requests;\n@@ -85,27 +91,11 @@ struct svc_rdma_op_ctxt {\n \tenum dma_data_direction direction;\n \tint count;\n \tunsigned int mapped_sges;\n-\tstruct ib_sge sge[RPCSVC_MAXPAGES];\n+\tstruct ib_send_wr send_wr;\n+\tstruct ib_sge sge[1 + RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE];\n \tstruct page *pages[RPCSVC_MAXPAGES];\n };\n \n-/*\n- * NFS_ requests are mapped on the client side by the chunk lists in\n- * the RPCRDMA header. During the fetching of the RPC from the client\n- * and the writing of the reply to the client, the memory in the\n- * client and the memory in the server must be mapped as contiguous\n- * vaddr/len for access by the hardware. These data strucures keep\n- * these mappings.\n- *\n- * For an RDMA_WRITE, the 'sge' maps the RPC REPLY. For RDMA_READ, the\n- * 'sge' in the svc_rdma_req_map maps the server side RPC reply and the\n- * 'ch' field maps the read-list of the RPCRDMA header to the 'sge'\n- * mapping of the reply.\n- */\n-struct svc_rdma_chunk_sge {\n-\tint start;\t\t/* sge no for this chunk */\n-\tint count;\t\t/* sge count for this chunk */\n-};\n struct svc_rdma_fastreg_mr {\n \tstruct ib_mr *mr;\n \tstruct scatterlist *sg;\n@@ -114,15 +104,7 @@ struct svc_rdma_fastreg_mr {\n \tenum dma_data_direction direction;\n \tstruct list_head frmr_list;\n };\n-struct svc_rdma_req_map {\n-\tstruct list_head free;\n-\tunsigned long count;\n-\tunion {\n-\t\tstruct kvec sge[RPCSVC_MAXPAGES];\n-\t\tstruct svc_rdma_chunk_sge ch[RPCSVC_MAXPAGES];\n-\t\tunsigned long lkey[RPCSVC_MAXPAGES];\n-\t};\n-};\n+\n #define RDMACTXT_F_LAST_CTXT\t2\n \n #define\tSVCRDMA_DEVCAP_FAST_REG\t\t1\t/* fast mr registration */\n@@ -144,14 +126,15 @@ struct svcxprt_rdma {\n \tu32\t\t     sc_max_requests;\t/* Max requests */\n \tu32\t\t     sc_max_bc_requests;/* Backward credits */\n \tint                  sc_max_req_size;\t/* Size of each RQ WR buf */\n+\tu8\t\t     sc_port_num;\n \n \tstruct ib_pd         *sc_pd;\n \n \tspinlock_t\t     sc_ctxt_lock;\n \tstruct list_head     sc_ctxts;\n \tint\t\t     sc_ctxt_used;\n-\tspinlock_t\t     sc_map_lock;\n-\tstruct list_head     sc_maps;\n+\tspinlock_t\t     sc_rw_ctxt_lock;\n+\tstruct list_head     sc_rw_ctxts;\n \n \tstruct list_head     sc_rq_dto_q;\n \tspinlock_t\t     sc_rq_dto_lock;\n@@ -181,9 +164,7 @@ struct svcxprt_rdma {\n /* The default ORD value is based on two outstanding full-size writes with a\n  * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */\n #define RPCRDMA_ORD             (64/4)\n-#define RPCRDMA_SQ_DEPTH_MULT   8\n #define RPCRDMA_MAX_REQUESTS    32\n-#define RPCRDMA_MAX_REQ_SIZE    4096\n \n /* Typical ULP usage of BC requests is NFSv4.1 backchannel. Our\n  * current NFSv4.1 implementation supports one backchannel slot.\n@@ -201,19 +182,11 @@ static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,\n \n /* svc_rdma_backchannel.c */\n extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,\n-\t\t\t\t    struct rpcrdma_msg *rmsgp,\n+\t\t\t\t    __be32 *rdma_resp,\n \t\t\t\t    struct xdr_buf *rcvbuf);\n \n /* svc_rdma_marshal.c */\n extern int svc_rdma_xdr_decode_req(struct xdr_buf *);\n-extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,\n-\t\t\t\t     struct rpcrdma_msg *,\n-\t\t\t\t     enum rpcrdma_errcode, __be32 *);\n-extern void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *, int);\n-extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);\n-extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,\n-\t\t\t\t\t    __be32, __be64, u32);\n-extern unsigned int svc_rdma_xdr_get_reply_hdr_len(__be32 *rdma_resp);\n \n /* svc_rdma_recvfrom.c */\n extern int svc_rdma_recvfrom(struct svc_rqst *);\n@@ -224,16 +197,25 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,\n \t\t\t\tstruct svc_rdma_op_ctxt *, int *, u32 *,\n \t\t\t\tu32, u32, u64, bool);\n \n+/* svc_rdma_rw.c */\n+extern void svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma);\n+extern int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,\n+\t\t\t\t     __be32 *wr_ch, struct xdr_buf *xdr);\n+extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,\n+\t\t\t\t     __be32 *rp_ch, bool writelist,\n+\t\t\t\t     struct xdr_buf *xdr);\n+\n /* svc_rdma_sendto.c */\n-extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,\n-\t\t\t    struct svc_rdma_req_map *, bool);\n+extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,\n+\t\t\t\t  struct svc_rdma_op_ctxt *ctxt,\n+\t\t\t\t  __be32 *rdma_resp, unsigned int len);\n+extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,\n+\t\t\t\t struct svc_rdma_op_ctxt *ctxt,\n+\t\t\t\t int num_sge, u32 inv_rkey);\n extern int svc_rdma_sendto(struct svc_rqst *);\n-extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,\n-\t\t\t\tint);\n \n /* svc_rdma_transport.c */\n extern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);\n-extern void svc_rdma_wc_write(struct ib_cq *, struct ib_wc *);\n extern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);\n extern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);\n extern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);\n@@ -244,9 +226,6 @@ extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);\n extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);\n extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);\n extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);\n-extern struct svc_rdma_req_map *svc_rdma_get_req_map(struct svcxprt_rdma *);\n-extern void svc_rdma_put_req_map(struct svcxprt_rdma *,\n-\t\t\t\t struct svc_rdma_req_map *);\n extern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);\n extern void svc_rdma_put_frmr(struct svcxprt_rdma *,\n \t\t\t      struct svc_rdma_fastreg_mr *);"
        },
        {
          "filename": "include/uapi/linux/nfsd/cld.h",
          "status": "modified",
          "additions": 8,
          "deletions": 6,
          "patch": "@@ -22,6 +22,8 @@\n #ifndef _NFSD_CLD_H\n #define _NFSD_CLD_H\n \n+#include <linux/types.h>\n+\n /* latest upcall version available */\n #define CLD_UPCALL_VERSION 1\n \n@@ -37,18 +39,18 @@ enum cld_command {\n \n /* representation of long-form NFSv4 client ID */\n struct cld_name {\n-\tuint16_t\tcn_len;\t\t\t\t/* length of cm_id */\n+\t__u16\t\tcn_len;\t\t\t\t/* length of cm_id */\n \tunsigned char\tcn_id[NFS4_OPAQUE_LIMIT];\t/* client-provided */\n } __attribute__((packed));\n \n /* message struct for communication with userspace */\n struct cld_msg {\n-\tuint8_t\t\tcm_vers;\t\t/* upcall version */\n-\tuint8_t\t\tcm_cmd;\t\t\t/* upcall command */\n-\tint16_t\t\tcm_status;\t\t/* return code */\n-\tuint32_t\tcm_xid;\t\t\t/* transaction id */\n+\t__u8\t\tcm_vers;\t\t/* upcall version */\n+\t__u8\t\tcm_cmd;\t\t\t/* upcall command */\n+\t__s16\t\tcm_status;\t\t/* return code */\n+\t__u32\t\tcm_xid;\t\t\t/* transaction id */\n \tunion {\n-\t\tint64_t\t\tcm_gracetime;\t/* grace period start time */\n+\t\t__s64\t\tcm_gracetime;\t/* grace period start time */\n \t\tstruct cld_name\tcm_name;\n \t} __attribute__((packed)) cm_u;\n } __attribute__((packed));"
        },
        {
          "filename": "net/sunrpc/Kconfig",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -52,6 +52,7 @@ config SUNRPC_XPRT_RDMA\n \ttristate \"RPC-over-RDMA transport\"\n \tdepends on SUNRPC && INFINIBAND && INFINIBAND_ADDR_TRANS\n \tdefault SUNRPC && INFINIBAND\n+\tselect SG_POOL\n \thelp\n \t  This option allows the NFS client and server to use RDMA\n \t  transports (InfiniBand, iWARP, or RoCE)."
        },
        {
          "filename": "net/sunrpc/svc.c",
          "status": "modified",
          "additions": 96,
          "deletions": 38,
          "patch": "@@ -702,59 +702,32 @@ choose_victim(struct svc_serv *serv, struct svc_pool *pool, unsigned int *state)\n \treturn task;\n }\n \n-/*\n- * Create or destroy enough new threads to make the number\n- * of threads the given number.  If `pool' is non-NULL, applies\n- * only to threads in that pool, otherwise round-robins between\n- * all pools.  Caller must ensure that mutual exclusion between this and\n- * server startup or shutdown.\n- *\n- * Destroying threads relies on the service threads filling in\n- * rqstp->rq_task, which only the nfs ones do.  Assumes the serv\n- * has been created using svc_create_pooled().\n- *\n- * Based on code that used to be in nfsd_svc() but tweaked\n- * to be pool-aware.\n- */\n-int\n-svc_set_num_threads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n+/* create new threads */\n+static int\n+svc_start_kthreads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n {\n \tstruct svc_rqst\t*rqstp;\n \tstruct task_struct *task;\n \tstruct svc_pool *chosen_pool;\n-\tint error = 0;\n \tunsigned int state = serv->sv_nrthreads-1;\n \tint node;\n \n-\tif (pool == NULL) {\n-\t\t/* The -1 assumes caller has done a svc_get() */\n-\t\tnrservs -= (serv->sv_nrthreads-1);\n-\t} else {\n-\t\tspin_lock_bh(&pool->sp_lock);\n-\t\tnrservs -= pool->sp_nrthreads;\n-\t\tspin_unlock_bh(&pool->sp_lock);\n-\t}\n-\n-\t/* create new threads */\n-\twhile (nrservs > 0) {\n+\tdo {\n \t\tnrservs--;\n \t\tchosen_pool = choose_pool(serv, pool, &state);\n \n \t\tnode = svc_pool_map_get_node(chosen_pool->sp_id);\n \t\trqstp = svc_prepare_thread(serv, chosen_pool, node);\n-\t\tif (IS_ERR(rqstp)) {\n-\t\t\terror = PTR_ERR(rqstp);\n-\t\t\tbreak;\n-\t\t}\n+\t\tif (IS_ERR(rqstp))\n+\t\t\treturn PTR_ERR(rqstp);\n \n \t\t__module_get(serv->sv_ops->svo_module);\n \t\ttask = kthread_create_on_node(serv->sv_ops->svo_function, rqstp,\n \t\t\t\t\t      node, \"%s\", serv->sv_name);\n \t\tif (IS_ERR(task)) {\n-\t\t\terror = PTR_ERR(task);\n \t\t\tmodule_put(serv->sv_ops->svo_module);\n \t\t\tsvc_exit_thread(rqstp);\n-\t\t\tbreak;\n+\t\t\treturn PTR_ERR(task);\n \t\t}\n \n \t\trqstp->rq_task = task;\n@@ -763,18 +736,103 @@ svc_set_num_threads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n \n \t\tsvc_sock_update_bufs(serv);\n \t\twake_up_process(task);\n-\t}\n+\t} while (nrservs > 0);\n+\n+\treturn 0;\n+}\n+\n+\n+/* destroy old threads */\n+static int\n+svc_signal_kthreads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n+{\n+\tstruct task_struct *task;\n+\tunsigned int state = serv->sv_nrthreads-1;\n+\n \t/* destroy old threads */\n-\twhile (nrservs < 0 &&\n-\t       (task = choose_victim(serv, pool, &state)) != NULL) {\n+\tdo {\n+\t\ttask = choose_victim(serv, pool, &state);\n+\t\tif (task == NULL)\n+\t\t\tbreak;\n \t\tsend_sig(SIGINT, task, 1);\n \t\tnrservs++;\n+\t} while (nrservs < 0);\n+\n+\treturn 0;\n+}\n+\n+/*\n+ * Create or destroy enough new threads to make the number\n+ * of threads the given number.  If `pool' is non-NULL, applies\n+ * only to threads in that pool, otherwise round-robins between\n+ * all pools.  Caller must ensure that mutual exclusion between this and\n+ * server startup or shutdown.\n+ *\n+ * Destroying threads relies on the service threads filling in\n+ * rqstp->rq_task, which only the nfs ones do.  Assumes the serv\n+ * has been created using svc_create_pooled().\n+ *\n+ * Based on code that used to be in nfsd_svc() but tweaked\n+ * to be pool-aware.\n+ */\n+int\n+svc_set_num_threads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n+{\n+\tif (pool == NULL) {\n+\t\t/* The -1 assumes caller has done a svc_get() */\n+\t\tnrservs -= (serv->sv_nrthreads-1);\n+\t} else {\n+\t\tspin_lock_bh(&pool->sp_lock);\n+\t\tnrservs -= pool->sp_nrthreads;\n+\t\tspin_unlock_bh(&pool->sp_lock);\n \t}\n \n-\treturn error;\n+\tif (nrservs > 0)\n+\t\treturn svc_start_kthreads(serv, pool, nrservs);\n+\tif (nrservs < 0)\n+\t\treturn svc_signal_kthreads(serv, pool, nrservs);\n+\treturn 0;\n }\n EXPORT_SYMBOL_GPL(svc_set_num_threads);\n \n+/* destroy old threads */\n+static int\n+svc_stop_kthreads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n+{\n+\tstruct task_struct *task;\n+\tunsigned int state = serv->sv_nrthreads-1;\n+\n+\t/* destroy old threads */\n+\tdo {\n+\t\ttask = choose_victim(serv, pool, &state);\n+\t\tif (task == NULL)\n+\t\t\tbreak;\n+\t\tkthread_stop(task);\n+\t\tnrservs++;\n+\t} while (nrservs < 0);\n+\treturn 0;\n+}\n+\n+int\n+svc_set_num_threads_sync(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n+{\n+\tif (pool == NULL) {\n+\t\t/* The -1 assumes caller has done a svc_get() */\n+\t\tnrservs -= (serv->sv_nrthreads-1);\n+\t} else {\n+\t\tspin_lock_bh(&pool->sp_lock);\n+\t\tnrservs -= pool->sp_nrthreads;\n+\t\tspin_unlock_bh(&pool->sp_lock);\n+\t}\n+\n+\tif (nrservs > 0)\n+\t\treturn svc_start_kthreads(serv, pool, nrservs);\n+\tif (nrservs < 0)\n+\t\treturn svc_stop_kthreads(serv, pool, nrservs);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(svc_set_num_threads_sync);\n+\n /*\n  * Called from a server thread as it's exiting. Caller must hold the \"service\n  * mutex\" for the service."
        },
        {
          "filename": "net/sunrpc/xprtrdma/Makefile",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -4,5 +4,5 @@ rpcrdma-y := transport.o rpc_rdma.o verbs.o \\\n \tfmr_ops.o frwr_ops.o \\\n \tsvc_rdma.o svc_rdma_backchannel.o svc_rdma_transport.o \\\n \tsvc_rdma_marshal.o svc_rdma_sendto.o svc_rdma_recvfrom.o \\\n-\tmodule.o\n+\tsvc_rdma_rw.o module.o\n rpcrdma-$(CONFIG_SUNRPC_BACKCHANNEL) += backchannel.o"
        },
        {
          "filename": "net/sunrpc/xprtrdma/svc_rdma.c",
          "status": "modified",
          "additions": 3,
          "deletions": 5,
          "patch": "@@ -58,9 +58,9 @@ unsigned int svcrdma_max_requests = RPCRDMA_MAX_REQUESTS;\n unsigned int svcrdma_max_bc_requests = RPCRDMA_MAX_BC_REQUESTS;\n static unsigned int min_max_requests = 4;\n static unsigned int max_max_requests = 16384;\n-unsigned int svcrdma_max_req_size = RPCRDMA_MAX_REQ_SIZE;\n-static unsigned int min_max_inline = 4096;\n-static unsigned int max_max_inline = 65536;\n+unsigned int svcrdma_max_req_size = RPCRDMA_DEF_INLINE_THRESH;\n+static unsigned int min_max_inline = RPCRDMA_DEF_INLINE_THRESH;\n+static unsigned int max_max_inline = RPCRDMA_MAX_INLINE_THRESH;\n \n atomic_t rdma_stat_recv;\n atomic_t rdma_stat_read;\n@@ -247,8 +247,6 @@ int svc_rdma_init(void)\n \tdprintk(\"SVCRDMA Module Init, register RPC RDMA transport\\n\");\n \tdprintk(\"\\tsvcrdma_ord      : %d\\n\", svcrdma_ord);\n \tdprintk(\"\\tmax_requests     : %u\\n\", svcrdma_max_requests);\n-\tdprintk(\"\\tsq_depth         : %u\\n\",\n-\t\tsvcrdma_max_requests * RPCRDMA_SQ_DEPTH_MULT);\n \tdprintk(\"\\tmax_bc_requests  : %u\\n\", svcrdma_max_bc_requests);\n \tdprintk(\"\\tmax_inline       : %d\\n\", svcrdma_max_req_size);\n "
        },
        {
          "filename": "net/sunrpc/xprtrdma/svc_rdma_backchannel.c",
          "status": "modified",
          "additions": 29,
          "deletions": 42,
          "patch": "@@ -12,7 +12,17 @@\n \n #undef SVCRDMA_BACKCHANNEL_DEBUG\n \n-int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,\n+/**\n+ * svc_rdma_handle_bc_reply - Process incoming backchannel reply\n+ * @xprt: controlling backchannel transport\n+ * @rdma_resp: pointer to incoming transport header\n+ * @rcvbuf: XDR buffer into which to decode the reply\n+ *\n+ * Returns:\n+ *\t%0 if @rcvbuf is filled in, xprt_complete_rqst called,\n+ *\t%-EAGAIN if server should call ->recvfrom again.\n+ */\n+int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,\n \t\t\t     struct xdr_buf *rcvbuf)\n {\n \tstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\n@@ -27,13 +37,13 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,\n \n \tp = (__be32 *)src->iov_base;\n \tlen = src->iov_len;\n-\txid = rmsgp->rm_xid;\n+\txid = *rdma_resp;\n \n #ifdef SVCRDMA_BACKCHANNEL_DEBUG\n \tpr_info(\"%s: xid=%08x, length=%zu\\n\",\n \t\t__func__, be32_to_cpu(xid), len);\n \tpr_info(\"%s: RPC/RDMA: %*ph\\n\",\n-\t\t__func__, (int)RPCRDMA_HDRLEN_MIN, rmsgp);\n+\t\t__func__, (int)RPCRDMA_HDRLEN_MIN, rdma_resp);\n \tpr_info(\"%s:      RPC: %*ph\\n\",\n \t\t__func__, (int)len, p);\n #endif\n@@ -53,7 +63,7 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,\n \t\tgoto out_unlock;\n \tmemcpy(dst->iov_base, p, len);\n \n-\tcredits = be32_to_cpu(rmsgp->rm_credit);\n+\tcredits = be32_to_cpup(rdma_resp + 2);\n \tif (credits == 0)\n \t\tcredits = 1;\t/* don't deadlock */\n \telse if (credits > r_xprt->rx_buf.rb_bc_max_requests)\n@@ -90,9 +100,9 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,\n  * Caller holds the connection's mutex and has already marshaled\n  * the RPC/RDMA request.\n  *\n- * This is similar to svc_rdma_reply, but takes an rpc_rqst\n- * instead, does not support chunks, and avoids blocking memory\n- * allocation.\n+ * This is similar to svc_rdma_send_reply_msg, but takes a struct\n+ * rpc_rqst instead, does not support chunks, and avoids blocking\n+ * memory allocation.\n  *\n  * XXX: There is still an opportunity to block in svc_rdma_send()\n  * if there are no SQ entries to post the Send. This may occur if\n@@ -101,59 +111,36 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,\n static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,\n \t\t\t      struct rpc_rqst *rqst)\n {\n-\tstruct xdr_buf *sndbuf = &rqst->rq_snd_buf;\n \tstruct svc_rdma_op_ctxt *ctxt;\n-\tstruct svc_rdma_req_map *vec;\n-\tstruct ib_send_wr send_wr;\n \tint ret;\n \n-\tvec = svc_rdma_get_req_map(rdma);\n-\tret = svc_rdma_map_xdr(rdma, sndbuf, vec, false);\n-\tif (ret)\n+\tctxt = svc_rdma_get_context(rdma);\n+\n+\t/* rpcrdma_bc_send_request builds the transport header and\n+\t * the backchannel RPC message in the same buffer. Thus only\n+\t * one SGE is needed to send both.\n+\t */\n+\tret = svc_rdma_map_reply_hdr(rdma, ctxt, rqst->rq_buffer,\n+\t\t\t\t     rqst->rq_snd_buf.len);\n+\tif (ret < 0)\n \t\tgoto out_err;\n \n \tret = svc_rdma_repost_recv(rdma, GFP_NOIO);\n \tif (ret)\n \t\tgoto out_err;\n \n-\tctxt = svc_rdma_get_context(rdma);\n-\tctxt->pages[0] = virt_to_page(rqst->rq_buffer);\n-\tctxt->count = 1;\n-\n-\tctxt->direction = DMA_TO_DEVICE;\n-\tctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;\n-\tctxt->sge[0].length = sndbuf->len;\n-\tctxt->sge[0].addr =\n-\t    ib_dma_map_page(rdma->sc_cm_id->device, ctxt->pages[0], 0,\n-\t\t\t    sndbuf->len, DMA_TO_DEVICE);\n-\tif (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr)) {\n-\t\tret = -EIO;\n-\t\tgoto out_unmap;\n-\t}\n-\tsvc_rdma_count_mappings(rdma, ctxt);\n-\n-\tmemset(&send_wr, 0, sizeof(send_wr));\n-\tctxt->cqe.done = svc_rdma_wc_send;\n-\tsend_wr.wr_cqe = &ctxt->cqe;\n-\tsend_wr.sg_list = ctxt->sge;\n-\tsend_wr.num_sge = 1;\n-\tsend_wr.opcode = IB_WR_SEND;\n-\tsend_wr.send_flags = IB_SEND_SIGNALED;\n-\n-\tret = svc_rdma_send(rdma, &send_wr);\n-\tif (ret) {\n-\t\tret = -EIO;\n+\tret = svc_rdma_post_send_wr(rdma, ctxt, 1, 0);\n+\tif (ret)\n \t\tgoto out_unmap;\n-\t}\n \n out_err:\n-\tsvc_rdma_put_req_map(rdma, vec);\n \tdprintk(\"svcrdma: %s returns %d\\n\", __func__, ret);\n \treturn ret;\n \n out_unmap:\n \tsvc_rdma_unmap_dma(ctxt);\n \tsvc_rdma_put_context(ctxt, 1);\n+\tret = -EIO;\n \tgoto out_err;\n }\n "
        },
        {
          "filename": "net/sunrpc/xprtrdma/svc_rdma_marshal.c",
          "status": "modified",
          "additions": 0,
          "deletions": 89,
          "patch": "@@ -166,92 +166,3 @@ int svc_rdma_xdr_decode_req(struct xdr_buf *rq_arg)\n \tdprintk(\"svcrdma: failed to parse transport header\\n\");\n \treturn -EINVAL;\n }\n-\n-int svc_rdma_xdr_encode_error(struct svcxprt_rdma *xprt,\n-\t\t\t      struct rpcrdma_msg *rmsgp,\n-\t\t\t      enum rpcrdma_errcode err, __be32 *va)\n-{\n-\t__be32 *startp = va;\n-\n-\t*va++ = rmsgp->rm_xid;\n-\t*va++ = rmsgp->rm_vers;\n-\t*va++ = xprt->sc_fc_credits;\n-\t*va++ = rdma_error;\n-\t*va++ = cpu_to_be32(err);\n-\tif (err == ERR_VERS) {\n-\t\t*va++ = rpcrdma_version;\n-\t\t*va++ = rpcrdma_version;\n-\t}\n-\n-\treturn (int)((unsigned long)va - (unsigned long)startp);\n-}\n-\n-/**\n- * svc_rdma_xdr_get_reply_hdr_length - Get length of Reply transport header\n- * @rdma_resp: buffer containing Reply transport header\n- *\n- * Returns length of transport header, in bytes.\n- */\n-unsigned int svc_rdma_xdr_get_reply_hdr_len(__be32 *rdma_resp)\n-{\n-\tunsigned int nsegs;\n-\t__be32 *p;\n-\n-\tp = rdma_resp;\n-\n-\t/* RPC-over-RDMA V1 replies never have a Read list. */\n-\tp += rpcrdma_fixed_maxsz + 1;\n-\n-\t/* Skip Write list. */\n-\twhile (*p++ != xdr_zero) {\n-\t\tnsegs = be32_to_cpup(p++);\n-\t\tp += nsegs * rpcrdma_segment_maxsz;\n-\t}\n-\n-\t/* Skip Reply chunk. */\n-\tif (*p++ != xdr_zero) {\n-\t\tnsegs = be32_to_cpup(p++);\n-\t\tp += nsegs * rpcrdma_segment_maxsz;\n-\t}\n-\n-\treturn (unsigned long)p - (unsigned long)rdma_resp;\n-}\n-\n-void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *rmsgp, int chunks)\n-{\n-\tstruct rpcrdma_write_array *ary;\n-\n-\t/* no read-list */\n-\trmsgp->rm_body.rm_chunks[0] = xdr_zero;\n-\n-\t/* write-array discrim */\n-\tary = (struct rpcrdma_write_array *)\n-\t\t&rmsgp->rm_body.rm_chunks[1];\n-\tary->wc_discrim = xdr_one;\n-\tary->wc_nchunks = cpu_to_be32(chunks);\n-\n-\t/* write-list terminator */\n-\tary->wc_array[chunks].wc_target.rs_handle = xdr_zero;\n-\n-\t/* reply-array discriminator */\n-\tary->wc_array[chunks].wc_target.rs_length = xdr_zero;\n-}\n-\n-void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *ary,\n-\t\t\t\t int chunks)\n-{\n-\tary->wc_discrim = xdr_one;\n-\tary->wc_nchunks = cpu_to_be32(chunks);\n-}\n-\n-void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *ary,\n-\t\t\t\t     int chunk_no,\n-\t\t\t\t     __be32 rs_handle,\n-\t\t\t\t     __be64 rs_offset,\n-\t\t\t\t     u32 write_len)\n-{\n-\tstruct rpcrdma_segment *seg = &ary->wc_array[chunk_no].wc_target;\n-\tseg->rs_handle = rs_handle;\n-\tseg->rs_offset = rs_offset;\n-\tseg->rs_length = cpu_to_be32(write_len);\n-}"
        },
        {
          "filename": "net/sunrpc/xprtrdma/svc_rdma_recvfrom.c",
          "status": "modified",
          "additions": 66,
          "deletions": 13,
          "patch": "@@ -558,33 +558,85 @@ static void rdma_read_complete(struct svc_rqst *rqstp,\n \trqstp->rq_arg.buflen = head->arg.buflen;\n }\n \n+static void svc_rdma_send_error(struct svcxprt_rdma *xprt,\n+\t\t\t\t__be32 *rdma_argp, int status)\n+{\n+\tstruct svc_rdma_op_ctxt *ctxt;\n+\t__be32 *p, *err_msgp;\n+\tunsigned int length;\n+\tstruct page *page;\n+\tint ret;\n+\n+\tret = svc_rdma_repost_recv(xprt, GFP_KERNEL);\n+\tif (ret)\n+\t\treturn;\n+\n+\tpage = alloc_page(GFP_KERNEL);\n+\tif (!page)\n+\t\treturn;\n+\terr_msgp = page_address(page);\n+\n+\tp = err_msgp;\n+\t*p++ = *rdma_argp;\n+\t*p++ = *(rdma_argp + 1);\n+\t*p++ = xprt->sc_fc_credits;\n+\t*p++ = rdma_error;\n+\tif (status == -EPROTONOSUPPORT) {\n+\t\t*p++ = err_vers;\n+\t\t*p++ = rpcrdma_version;\n+\t\t*p++ = rpcrdma_version;\n+\t} else {\n+\t\t*p++ = err_chunk;\n+\t}\n+\tlength = (unsigned long)p - (unsigned long)err_msgp;\n+\n+\t/* Map transport header; no RPC message payload */\n+\tctxt = svc_rdma_get_context(xprt);\n+\tret = svc_rdma_map_reply_hdr(xprt, ctxt, err_msgp, length);\n+\tif (ret) {\n+\t\tdprintk(\"svcrdma: Error %d mapping send for protocol error\\n\",\n+\t\t\tret);\n+\t\treturn;\n+\t}\n+\n+\tret = svc_rdma_post_send_wr(xprt, ctxt, 1, 0);\n+\tif (ret) {\n+\t\tdprintk(\"svcrdma: Error %d posting send for protocol error\\n\",\n+\t\t\tret);\n+\t\tsvc_rdma_unmap_dma(ctxt);\n+\t\tsvc_rdma_put_context(ctxt, 1);\n+\t}\n+}\n+\n /* By convention, backchannel calls arrive via rdma_msg type\n  * messages, and never populate the chunk lists. This makes\n  * the RPC/RDMA header small and fixed in size, so it is\n  * straightforward to check the RPC header's direction field.\n  */\n-static bool\n-svc_rdma_is_backchannel_reply(struct svc_xprt *xprt, struct rpcrdma_msg *rmsgp)\n+static bool svc_rdma_is_backchannel_reply(struct svc_xprt *xprt,\n+\t\t\t\t\t  __be32 *rdma_resp)\n {\n-\t__be32 *p = (__be32 *)rmsgp;\n+\t__be32 *p;\n \n \tif (!xprt->xpt_bc_xprt)\n \t\treturn false;\n \n-\tif (rmsgp->rm_type != rdma_msg)\n+\tp = rdma_resp + 3;\n+\tif (*p++ != rdma_msg)\n \t\treturn false;\n-\tif (rmsgp->rm_body.rm_chunks[0] != xdr_zero)\n+\n+\tif (*p++ != xdr_zero)\n \t\treturn false;\n-\tif (rmsgp->rm_body.rm_chunks[1] != xdr_zero)\n+\tif (*p++ != xdr_zero)\n \t\treturn false;\n-\tif (rmsgp->rm_body.rm_chunks[2] != xdr_zero)\n+\tif (*p++ != xdr_zero)\n \t\treturn false;\n \n-\t/* sanity */\n-\tif (p[7] != rmsgp->rm_xid)\n+\t/* XID sanity */\n+\tif (*p++ != *rdma_resp)\n \t\treturn false;\n \t/* call direction */\n-\tif (p[8] == cpu_to_be32(RPC_CALL))\n+\tif (*p == cpu_to_be32(RPC_CALL))\n \t\treturn false;\n \n \treturn true;\n@@ -650,8 +702,9 @@ int svc_rdma_recvfrom(struct svc_rqst *rqstp)\n \t\tgoto out_drop;\n \trqstp->rq_xprt_hlen = ret;\n \n-\tif (svc_rdma_is_backchannel_reply(xprt, rmsgp)) {\n-\t\tret = svc_rdma_handle_bc_reply(xprt->xpt_bc_xprt, rmsgp,\n+\tif (svc_rdma_is_backchannel_reply(xprt, &rmsgp->rm_xid)) {\n+\t\tret = svc_rdma_handle_bc_reply(xprt->xpt_bc_xprt,\n+\t\t\t\t\t       &rmsgp->rm_xid,\n \t\t\t\t\t       &rqstp->rq_arg);\n \t\tsvc_rdma_put_context(ctxt, 0);\n \t\tif (ret)\n@@ -686,7 +739,7 @@ int svc_rdma_recvfrom(struct svc_rqst *rqstp)\n \treturn ret;\n \n out_err:\n-\tsvc_rdma_send_error(rdma_xprt, rmsgp, ret);\n+\tsvc_rdma_send_error(rdma_xprt, &rmsgp->rm_xid, ret);\n \tsvc_rdma_put_context(ctxt, 0);\n \treturn 0;\n "
        },
        {
          "filename": "net/sunrpc/xprtrdma/svc_rdma_rw.c",
          "status": "added",
          "additions": 512,
          "deletions": 0,
          "patch": "@@ -0,0 +1,512 @@\n+/*\n+ * Copyright (c) 2016 Oracle.  All rights reserved.\n+ *\n+ * Use the core R/W API to move RPC-over-RDMA Read and Write chunks.\n+ */\n+\n+#include <linux/sunrpc/rpc_rdma.h>\n+#include <linux/sunrpc/svc_rdma.h>\n+#include <linux/sunrpc/debug.h>\n+\n+#include <rdma/rw.h>\n+\n+#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n+\n+/* Each R/W context contains state for one chain of RDMA Read or\n+ * Write Work Requests.\n+ *\n+ * Each WR chain handles a single contiguous server-side buffer,\n+ * because scatterlist entries after the first have to start on\n+ * page alignment. xdr_buf iovecs cannot guarantee alignment.\n+ *\n+ * Each WR chain handles only one R_key. Each RPC-over-RDMA segment\n+ * from a client may contain a unique R_key, so each WR chain moves\n+ * up to one segment at a time.\n+ *\n+ * The scatterlist makes this data structure over 4KB in size. To\n+ * make it less likely to fail, and to handle the allocation for\n+ * smaller I/O requests without disabling bottom-halves, these\n+ * contexts are created on demand, but cached and reused until the\n+ * controlling svcxprt_rdma is destroyed.\n+ */\n+struct svc_rdma_rw_ctxt {\n+\tstruct list_head\trw_list;\n+\tstruct rdma_rw_ctx\trw_ctx;\n+\tint\t\t\trw_nents;\n+\tstruct sg_table\t\trw_sg_table;\n+\tstruct scatterlist\trw_first_sgl[0];\n+};\n+\n+static inline struct svc_rdma_rw_ctxt *\n+svc_rdma_next_ctxt(struct list_head *list)\n+{\n+\treturn list_first_entry_or_null(list, struct svc_rdma_rw_ctxt,\n+\t\t\t\t\trw_list);\n+}\n+\n+static struct svc_rdma_rw_ctxt *\n+svc_rdma_get_rw_ctxt(struct svcxprt_rdma *rdma, unsigned int sges)\n+{\n+\tstruct svc_rdma_rw_ctxt *ctxt;\n+\n+\tspin_lock(&rdma->sc_rw_ctxt_lock);\n+\n+\tctxt = svc_rdma_next_ctxt(&rdma->sc_rw_ctxts);\n+\tif (ctxt) {\n+\t\tlist_del(&ctxt->rw_list);\n+\t\tspin_unlock(&rdma->sc_rw_ctxt_lock);\n+\t} else {\n+\t\tspin_unlock(&rdma->sc_rw_ctxt_lock);\n+\t\tctxt = kmalloc(sizeof(*ctxt) +\n+\t\t\t       SG_CHUNK_SIZE * sizeof(struct scatterlist),\n+\t\t\t       GFP_KERNEL);\n+\t\tif (!ctxt)\n+\t\t\tgoto out;\n+\t\tINIT_LIST_HEAD(&ctxt->rw_list);\n+\t}\n+\n+\tctxt->rw_sg_table.sgl = ctxt->rw_first_sgl;\n+\tif (sg_alloc_table_chained(&ctxt->rw_sg_table, sges,\n+\t\t\t\t   ctxt->rw_sg_table.sgl)) {\n+\t\tkfree(ctxt);\n+\t\tctxt = NULL;\n+\t}\n+out:\n+\treturn ctxt;\n+}\n+\n+static void svc_rdma_put_rw_ctxt(struct svcxprt_rdma *rdma,\n+\t\t\t\t struct svc_rdma_rw_ctxt *ctxt)\n+{\n+\tsg_free_table_chained(&ctxt->rw_sg_table, true);\n+\n+\tspin_lock(&rdma->sc_rw_ctxt_lock);\n+\tlist_add(&ctxt->rw_list, &rdma->sc_rw_ctxts);\n+\tspin_unlock(&rdma->sc_rw_ctxt_lock);\n+}\n+\n+/**\n+ * svc_rdma_destroy_rw_ctxts - Free accumulated R/W contexts\n+ * @rdma: transport about to be destroyed\n+ *\n+ */\n+void svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma)\n+{\n+\tstruct svc_rdma_rw_ctxt *ctxt;\n+\n+\twhile ((ctxt = svc_rdma_next_ctxt(&rdma->sc_rw_ctxts)) != NULL) {\n+\t\tlist_del(&ctxt->rw_list);\n+\t\tkfree(ctxt);\n+\t}\n+}\n+\n+/* A chunk context tracks all I/O for moving one Read or Write\n+ * chunk. This is a a set of rdma_rw's that handle data movement\n+ * for all segments of one chunk.\n+ *\n+ * These are small, acquired with a single allocator call, and\n+ * no more than one is needed per chunk. They are allocated on\n+ * demand, and not cached.\n+ */\n+struct svc_rdma_chunk_ctxt {\n+\tstruct ib_cqe\t\tcc_cqe;\n+\tstruct svcxprt_rdma\t*cc_rdma;\n+\tstruct list_head\tcc_rwctxts;\n+\tint\t\t\tcc_sqecount;\n+\tenum dma_data_direction cc_dir;\n+};\n+\n+static void svc_rdma_cc_init(struct svcxprt_rdma *rdma,\n+\t\t\t     struct svc_rdma_chunk_ctxt *cc,\n+\t\t\t     enum dma_data_direction dir)\n+{\n+\tcc->cc_rdma = rdma;\n+\tsvc_xprt_get(&rdma->sc_xprt);\n+\n+\tINIT_LIST_HEAD(&cc->cc_rwctxts);\n+\tcc->cc_sqecount = 0;\n+\tcc->cc_dir = dir;\n+}\n+\n+static void svc_rdma_cc_release(struct svc_rdma_chunk_ctxt *cc)\n+{\n+\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n+\tstruct svc_rdma_rw_ctxt *ctxt;\n+\n+\twhile ((ctxt = svc_rdma_next_ctxt(&cc->cc_rwctxts)) != NULL) {\n+\t\tlist_del(&ctxt->rw_list);\n+\n+\t\trdma_rw_ctx_destroy(&ctxt->rw_ctx, rdma->sc_qp,\n+\t\t\t\t    rdma->sc_port_num, ctxt->rw_sg_table.sgl,\n+\t\t\t\t    ctxt->rw_nents, cc->cc_dir);\n+\t\tsvc_rdma_put_rw_ctxt(rdma, ctxt);\n+\t}\n+\tsvc_xprt_put(&rdma->sc_xprt);\n+}\n+\n+/* State for sending a Write or Reply chunk.\n+ *  - Tracks progress of writing one chunk over all its segments\n+ *  - Stores arguments for the SGL constructor functions\n+ */\n+struct svc_rdma_write_info {\n+\t/* write state of this chunk */\n+\tunsigned int\t\twi_seg_off;\n+\tunsigned int\t\twi_seg_no;\n+\tunsigned int\t\twi_nsegs;\n+\t__be32\t\t\t*wi_segs;\n+\n+\t/* SGL constructor arguments */\n+\tstruct xdr_buf\t\t*wi_xdr;\n+\tunsigned char\t\t*wi_base;\n+\tunsigned int\t\twi_next_off;\n+\n+\tstruct svc_rdma_chunk_ctxt\twi_cc;\n+};\n+\n+static struct svc_rdma_write_info *\n+svc_rdma_write_info_alloc(struct svcxprt_rdma *rdma, __be32 *chunk)\n+{\n+\tstruct svc_rdma_write_info *info;\n+\n+\tinfo = kmalloc(sizeof(*info), GFP_KERNEL);\n+\tif (!info)\n+\t\treturn info;\n+\n+\tinfo->wi_seg_off = 0;\n+\tinfo->wi_seg_no = 0;\n+\tinfo->wi_nsegs = be32_to_cpup(++chunk);\n+\tinfo->wi_segs = ++chunk;\n+\tsvc_rdma_cc_init(rdma, &info->wi_cc, DMA_TO_DEVICE);\n+\treturn info;\n+}\n+\n+static void svc_rdma_write_info_free(struct svc_rdma_write_info *info)\n+{\n+\tsvc_rdma_cc_release(&info->wi_cc);\n+\tkfree(info);\n+}\n+\n+/**\n+ * svc_rdma_write_done - Write chunk completion\n+ * @cq: controlling Completion Queue\n+ * @wc: Work Completion\n+ *\n+ * Pages under I/O are freed by a subsequent Send completion.\n+ */\n+static void svc_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc)\n+{\n+\tstruct ib_cqe *cqe = wc->wr_cqe;\n+\tstruct svc_rdma_chunk_ctxt *cc =\n+\t\t\tcontainer_of(cqe, struct svc_rdma_chunk_ctxt, cc_cqe);\n+\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n+\tstruct svc_rdma_write_info *info =\n+\t\t\tcontainer_of(cc, struct svc_rdma_write_info, wi_cc);\n+\n+\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n+\twake_up(&rdma->sc_send_wait);\n+\n+\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n+\t\tset_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);\n+\t\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n+\t\t\tpr_err(\"svcrdma: write ctx: %s (%u/0x%x)\\n\",\n+\t\t\t       ib_wc_status_msg(wc->status),\n+\t\t\t       wc->status, wc->vendor_err);\n+\t}\n+\n+\tsvc_rdma_write_info_free(info);\n+}\n+\n+/* This function sleeps when the transport's Send Queue is congested.\n+ *\n+ * Assumptions:\n+ * - If ib_post_send() succeeds, only one completion is expected,\n+ *   even if one or more WRs are flushed. This is true when posting\n+ *   an rdma_rw_ctx or when posting a single signaled WR.\n+ */\n+static int svc_rdma_post_chunk_ctxt(struct svc_rdma_chunk_ctxt *cc)\n+{\n+\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n+\tstruct svc_xprt *xprt = &rdma->sc_xprt;\n+\tstruct ib_send_wr *first_wr, *bad_wr;\n+\tstruct list_head *tmp;\n+\tstruct ib_cqe *cqe;\n+\tint ret;\n+\n+\tfirst_wr = NULL;\n+\tcqe = &cc->cc_cqe;\n+\tlist_for_each(tmp, &cc->cc_rwctxts) {\n+\t\tstruct svc_rdma_rw_ctxt *ctxt;\n+\n+\t\tctxt = list_entry(tmp, struct svc_rdma_rw_ctxt, rw_list);\n+\t\tfirst_wr = rdma_rw_ctx_wrs(&ctxt->rw_ctx, rdma->sc_qp,\n+\t\t\t\t\t   rdma->sc_port_num, cqe, first_wr);\n+\t\tcqe = NULL;\n+\t}\n+\n+\tdo {\n+\t\tif (atomic_sub_return(cc->cc_sqecount,\n+\t\t\t\t      &rdma->sc_sq_avail) > 0) {\n+\t\t\tret = ib_post_send(rdma->sc_qp, first_wr, &bad_wr);\n+\t\t\tif (ret)\n+\t\t\t\tbreak;\n+\t\t\treturn 0;\n+\t\t}\n+\n+\t\tatomic_inc(&rdma_stat_sq_starve);\n+\t\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n+\t\twait_event(rdma->sc_send_wait,\n+\t\t\t   atomic_read(&rdma->sc_sq_avail) > cc->cc_sqecount);\n+\t} while (1);\n+\n+\tpr_err(\"svcrdma: ib_post_send failed (%d)\\n\", ret);\n+\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n+\n+\t/* If even one was posted, there will be a completion. */\n+\tif (bad_wr != first_wr)\n+\t\treturn 0;\n+\n+\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n+\twake_up(&rdma->sc_send_wait);\n+\treturn -ENOTCONN;\n+}\n+\n+/* Build and DMA-map an SGL that covers one kvec in an xdr_buf\n+ */\n+static void svc_rdma_vec_to_sg(struct svc_rdma_write_info *info,\n+\t\t\t       unsigned int len,\n+\t\t\t       struct svc_rdma_rw_ctxt *ctxt)\n+{\n+\tstruct scatterlist *sg = ctxt->rw_sg_table.sgl;\n+\n+\tsg_set_buf(&sg[0], info->wi_base, len);\n+\tinfo->wi_base += len;\n+\n+\tctxt->rw_nents = 1;\n+}\n+\n+/* Build and DMA-map an SGL that covers part of an xdr_buf's pagelist.\n+ */\n+static void svc_rdma_pagelist_to_sg(struct svc_rdma_write_info *info,\n+\t\t\t\t    unsigned int remaining,\n+\t\t\t\t    struct svc_rdma_rw_ctxt *ctxt)\n+{\n+\tunsigned int sge_no, sge_bytes, page_off, page_no;\n+\tstruct xdr_buf *xdr = info->wi_xdr;\n+\tstruct scatterlist *sg;\n+\tstruct page **page;\n+\n+\tpage_off = (info->wi_next_off + xdr->page_base) & ~PAGE_MASK;\n+\tpage_no = (info->wi_next_off + xdr->page_base) >> PAGE_SHIFT;\n+\tpage = xdr->pages + page_no;\n+\tinfo->wi_next_off += remaining;\n+\tsg = ctxt->rw_sg_table.sgl;\n+\tsge_no = 0;\n+\tdo {\n+\t\tsge_bytes = min_t(unsigned int, remaining,\n+\t\t\t\t  PAGE_SIZE - page_off);\n+\t\tsg_set_page(sg, *page, sge_bytes, page_off);\n+\n+\t\tremaining -= sge_bytes;\n+\t\tsg = sg_next(sg);\n+\t\tpage_off = 0;\n+\t\tsge_no++;\n+\t\tpage++;\n+\t} while (remaining);\n+\n+\tctxt->rw_nents = sge_no;\n+}\n+\n+/* Construct RDMA Write WRs to send a portion of an xdr_buf containing\n+ * an RPC Reply.\n+ */\n+static int\n+svc_rdma_build_writes(struct svc_rdma_write_info *info,\n+\t\t      void (*constructor)(struct svc_rdma_write_info *info,\n+\t\t\t\t\t  unsigned int len,\n+\t\t\t\t\t  struct svc_rdma_rw_ctxt *ctxt),\n+\t\t      unsigned int remaining)\n+{\n+\tstruct svc_rdma_chunk_ctxt *cc = &info->wi_cc;\n+\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n+\tstruct svc_rdma_rw_ctxt *ctxt;\n+\t__be32 *seg;\n+\tint ret;\n+\n+\tcc->cc_cqe.done = svc_rdma_write_done;\n+\tseg = info->wi_segs + info->wi_seg_no * rpcrdma_segment_maxsz;\n+\tdo {\n+\t\tunsigned int write_len;\n+\t\tu32 seg_length, seg_handle;\n+\t\tu64 seg_offset;\n+\n+\t\tif (info->wi_seg_no >= info->wi_nsegs)\n+\t\t\tgoto out_overflow;\n+\n+\t\tseg_handle = be32_to_cpup(seg);\n+\t\tseg_length = be32_to_cpup(seg + 1);\n+\t\txdr_decode_hyper(seg + 2, &seg_offset);\n+\t\tseg_offset += info->wi_seg_off;\n+\n+\t\twrite_len = min(remaining, seg_length - info->wi_seg_off);\n+\t\tctxt = svc_rdma_get_rw_ctxt(rdma,\n+\t\t\t\t\t    (write_len >> PAGE_SHIFT) + 2);\n+\t\tif (!ctxt)\n+\t\t\tgoto out_noctx;\n+\n+\t\tconstructor(info, write_len, ctxt);\n+\t\tret = rdma_rw_ctx_init(&ctxt->rw_ctx, rdma->sc_qp,\n+\t\t\t\t       rdma->sc_port_num, ctxt->rw_sg_table.sgl,\n+\t\t\t\t       ctxt->rw_nents, 0, seg_offset,\n+\t\t\t\t       seg_handle, DMA_TO_DEVICE);\n+\t\tif (ret < 0)\n+\t\t\tgoto out_initerr;\n+\n+\t\tlist_add(&ctxt->rw_list, &cc->cc_rwctxts);\n+\t\tcc->cc_sqecount += ret;\n+\t\tif (write_len == seg_length - info->wi_seg_off) {\n+\t\t\tseg += 4;\n+\t\t\tinfo->wi_seg_no++;\n+\t\t\tinfo->wi_seg_off = 0;\n+\t\t} else {\n+\t\t\tinfo->wi_seg_off += write_len;\n+\t\t}\n+\t\tremaining -= write_len;\n+\t} while (remaining);\n+\n+\treturn 0;\n+\n+out_overflow:\n+\tdprintk(\"svcrdma: inadequate space in Write chunk (%u)\\n\",\n+\t\tinfo->wi_nsegs);\n+\treturn -E2BIG;\n+\n+out_noctx:\n+\tdprintk(\"svcrdma: no R/W ctxs available\\n\");\n+\treturn -ENOMEM;\n+\n+out_initerr:\n+\tsvc_rdma_put_rw_ctxt(rdma, ctxt);\n+\tpr_err(\"svcrdma: failed to map pagelist (%d)\\n\", ret);\n+\treturn -EIO;\n+}\n+\n+/* Send one of an xdr_buf's kvecs by itself. To send a Reply\n+ * chunk, the whole RPC Reply is written back to the client.\n+ * This function writes either the head or tail of the xdr_buf\n+ * containing the Reply.\n+ */\n+static int svc_rdma_send_xdr_kvec(struct svc_rdma_write_info *info,\n+\t\t\t\t  struct kvec *vec)\n+{\n+\tinfo->wi_base = vec->iov_base;\n+\treturn svc_rdma_build_writes(info, svc_rdma_vec_to_sg,\n+\t\t\t\t     vec->iov_len);\n+}\n+\n+/* Send an xdr_buf's page list by itself. A Write chunk is\n+ * just the page list. a Reply chunk is the head, page list,\n+ * and tail. This function is shared between the two types\n+ * of chunk.\n+ */\n+static int svc_rdma_send_xdr_pagelist(struct svc_rdma_write_info *info,\n+\t\t\t\t      struct xdr_buf *xdr)\n+{\n+\tinfo->wi_xdr = xdr;\n+\tinfo->wi_next_off = 0;\n+\treturn svc_rdma_build_writes(info, svc_rdma_pagelist_to_sg,\n+\t\t\t\t     xdr->page_len);\n+}\n+\n+/**\n+ * svc_rdma_send_write_chunk - Write all segments in a Write chunk\n+ * @rdma: controlling RDMA transport\n+ * @wr_ch: Write chunk provided by client\n+ * @xdr: xdr_buf containing the data payload\n+ *\n+ * Returns a non-negative number of bytes the chunk consumed, or\n+ *\t%-E2BIG if the payload was larger than the Write chunk,\n+ *\t%-ENOMEM if rdma_rw context pool was exhausted,\n+ *\t%-ENOTCONN if posting failed (connection is lost),\n+ *\t%-EIO if rdma_rw initialization failed (DMA mapping, etc).\n+ */\n+int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma, __be32 *wr_ch,\n+\t\t\t      struct xdr_buf *xdr)\n+{\n+\tstruct svc_rdma_write_info *info;\n+\tint ret;\n+\n+\tif (!xdr->page_len)\n+\t\treturn 0;\n+\n+\tinfo = svc_rdma_write_info_alloc(rdma, wr_ch);\n+\tif (!info)\n+\t\treturn -ENOMEM;\n+\n+\tret = svc_rdma_send_xdr_pagelist(info, xdr);\n+\tif (ret < 0)\n+\t\tgoto out_err;\n+\n+\tret = svc_rdma_post_chunk_ctxt(&info->wi_cc);\n+\tif (ret < 0)\n+\t\tgoto out_err;\n+\treturn xdr->page_len;\n+\n+out_err:\n+\tsvc_rdma_write_info_free(info);\n+\treturn ret;\n+}\n+\n+/**\n+ * svc_rdma_send_reply_chunk - Write all segments in the Reply chunk\n+ * @rdma: controlling RDMA transport\n+ * @rp_ch: Reply chunk provided by client\n+ * @writelist: true if client provided a Write list\n+ * @xdr: xdr_buf containing an RPC Reply\n+ *\n+ * Returns a non-negative number of bytes the chunk consumed, or\n+ *\t%-E2BIG if the payload was larger than the Reply chunk,\n+ *\t%-ENOMEM if rdma_rw context pool was exhausted,\n+ *\t%-ENOTCONN if posting failed (connection is lost),\n+ *\t%-EIO if rdma_rw initialization failed (DMA mapping, etc).\n+ */\n+int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma, __be32 *rp_ch,\n+\t\t\t      bool writelist, struct xdr_buf *xdr)\n+{\n+\tstruct svc_rdma_write_info *info;\n+\tint consumed, ret;\n+\n+\tinfo = svc_rdma_write_info_alloc(rdma, rp_ch);\n+\tif (!info)\n+\t\treturn -ENOMEM;\n+\n+\tret = svc_rdma_send_xdr_kvec(info, &xdr->head[0]);\n+\tif (ret < 0)\n+\t\tgoto out_err;\n+\tconsumed = xdr->head[0].iov_len;\n+\n+\t/* Send the page list in the Reply chunk only if the\n+\t * client did not provide Write chunks.\n+\t */\n+\tif (!writelist && xdr->page_len) {\n+\t\tret = svc_rdma_send_xdr_pagelist(info, xdr);\n+\t\tif (ret < 0)\n+\t\t\tgoto out_err;\n+\t\tconsumed += xdr->page_len;\n+\t}\n+\n+\tif (xdr->tail[0].iov_len) {\n+\t\tret = svc_rdma_send_xdr_kvec(info, &xdr->tail[0]);\n+\t\tif (ret < 0)\n+\t\t\tgoto out_err;\n+\t\tconsumed += xdr->tail[0].iov_len;\n+\t}\n+\n+\tret = svc_rdma_post_chunk_ctxt(&info->wi_cc);\n+\tif (ret < 0)\n+\t\tgoto out_err;\n+\treturn consumed;\n+\n+out_err:\n+\tsvc_rdma_write_info_free(info);\n+\treturn ret;\n+}"
        },
        {
          "filename": "net/sunrpc/xprtrdma/svc_rdma_sendto.c",
          "status": "modified",
          "additions": 482,
          "deletions": 496,
          "patch": "@@ -1,4 +1,5 @@\n /*\n+ * Copyright (c) 2016 Oracle. All rights reserved.\n  * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.\n  * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n  *\n@@ -40,6 +41,63 @@\n  * Author: Tom Tucker <tom@opengridcomputing.com>\n  */\n \n+/* Operation\n+ *\n+ * The main entry point is svc_rdma_sendto. This is called by the\n+ * RPC server when an RPC Reply is ready to be transmitted to a client.\n+ *\n+ * The passed-in svc_rqst contains a struct xdr_buf which holds an\n+ * XDR-encoded RPC Reply message. sendto must construct the RPC-over-RDMA\n+ * transport header, post all Write WRs needed for this Reply, then post\n+ * a Send WR conveying the transport header and the RPC message itself to\n+ * the client.\n+ *\n+ * svc_rdma_sendto must fully transmit the Reply before returning, as\n+ * the svc_rqst will be recycled as soon as sendto returns. Remaining\n+ * resources referred to by the svc_rqst are also recycled at that time.\n+ * Therefore any resources that must remain longer must be detached\n+ * from the svc_rqst and released later.\n+ *\n+ * Page Management\n+ *\n+ * The I/O that performs Reply transmission is asynchronous, and may\n+ * complete well after sendto returns. Thus pages under I/O must be\n+ * removed from the svc_rqst before sendto returns.\n+ *\n+ * The logic here depends on Send Queue and completion ordering. Since\n+ * the Send WR is always posted last, it will always complete last. Thus\n+ * when it completes, it is guaranteed that all previous Write WRs have\n+ * also completed.\n+ *\n+ * Write WRs are constructed and posted. Each Write segment gets its own\n+ * svc_rdma_rw_ctxt, allowing the Write completion handler to find and\n+ * DMA-unmap the pages under I/O for that Write segment. The Write\n+ * completion handler does not release any pages.\n+ *\n+ * When the Send WR is constructed, it also gets its own svc_rdma_op_ctxt.\n+ * The ownership of all of the Reply's pages are transferred into that\n+ * ctxt, the Send WR is posted, and sendto returns.\n+ *\n+ * The svc_rdma_op_ctxt is presented when the Send WR completes. The\n+ * Send completion handler finally releases the Reply's pages.\n+ *\n+ * This mechanism also assumes that completions on the transport's Send\n+ * Completion Queue do not run in parallel. Otherwise a Write completion\n+ * and Send completion running at the same time could release pages that\n+ * are still DMA-mapped.\n+ *\n+ * Error Handling\n+ *\n+ * - If the Send WR is posted successfully, it will either complete\n+ *   successfully, or get flushed. Either way, the Send completion\n+ *   handler releases the Reply's pages.\n+ * - If the Send WR cannot be not posted, the forward path releases\n+ *   the Reply's pages.\n+ *\n+ * This handles the case, without the use of page reference counting,\n+ * where two different Write segments send portions of the same page.\n+ */\n+\n #include <linux/sunrpc/debug.h>\n #include <linux/sunrpc/rpc_rdma.h>\n #include <linux/spinlock.h>\n@@ -55,121 +113,149 @@ static u32 xdr_padsize(u32 len)\n \treturn (len & 3) ? (4 - (len & 3)) : 0;\n }\n \n-int svc_rdma_map_xdr(struct svcxprt_rdma *xprt,\n-\t\t     struct xdr_buf *xdr,\n-\t\t     struct svc_rdma_req_map *vec,\n-\t\t     bool write_chunk_present)\n+/* Returns length of transport header, in bytes.\n+ */\n+static unsigned int svc_rdma_reply_hdr_len(__be32 *rdma_resp)\n {\n-\tint sge_no;\n-\tu32 sge_bytes;\n-\tu32 page_bytes;\n-\tu32 page_off;\n-\tint page_no;\n-\n-\tif (xdr->len !=\n-\t    (xdr->head[0].iov_len + xdr->page_len + xdr->tail[0].iov_len)) {\n-\t\tpr_err(\"svcrdma: %s: XDR buffer length error\\n\", __func__);\n-\t\treturn -EIO;\n-\t}\n+\tunsigned int nsegs;\n+\t__be32 *p;\n \n-\t/* Skip the first sge, this is for the RPCRDMA header */\n-\tsge_no = 1;\n+\tp = rdma_resp;\n+\n+\t/* RPC-over-RDMA V1 replies never have a Read list. */\n+\tp += rpcrdma_fixed_maxsz + 1;\n \n-\t/* Head SGE */\n-\tvec->sge[sge_no].iov_base = xdr->head[0].iov_base;\n-\tvec->sge[sge_no].iov_len = xdr->head[0].iov_len;\n-\tsge_no++;\n-\n-\t/* pages SGE */\n-\tpage_no = 0;\n-\tpage_bytes = xdr->page_len;\n-\tpage_off = xdr->page_base;\n-\twhile (page_bytes) {\n-\t\tvec->sge[sge_no].iov_base =\n-\t\t\tpage_address(xdr->pages[page_no]) + page_off;\n-\t\tsge_bytes = min_t(u32, page_bytes, (PAGE_SIZE - page_off));\n-\t\tpage_bytes -= sge_bytes;\n-\t\tvec->sge[sge_no].iov_len = sge_bytes;\n-\n-\t\tsge_no++;\n-\t\tpage_no++;\n-\t\tpage_off = 0; /* reset for next time through loop */\n+\t/* Skip Write list. */\n+\twhile (*p++ != xdr_zero) {\n+\t\tnsegs = be32_to_cpup(p++);\n+\t\tp += nsegs * rpcrdma_segment_maxsz;\n \t}\n \n-\t/* Tail SGE */\n-\tif (xdr->tail[0].iov_len) {\n-\t\tunsigned char *base = xdr->tail[0].iov_base;\n-\t\tsize_t len = xdr->tail[0].iov_len;\n-\t\tu32 xdr_pad = xdr_padsize(xdr->page_len);\n+\t/* Skip Reply chunk. */\n+\tif (*p++ != xdr_zero) {\n+\t\tnsegs = be32_to_cpup(p++);\n+\t\tp += nsegs * rpcrdma_segment_maxsz;\n+\t}\n \n-\t\tif (write_chunk_present && xdr_pad) {\n-\t\t\tbase += xdr_pad;\n-\t\t\tlen -= xdr_pad;\n-\t\t}\n+\treturn (unsigned long)p - (unsigned long)rdma_resp;\n+}\n \n-\t\tif (len) {\n-\t\t\tvec->sge[sge_no].iov_base = base;\n-\t\t\tvec->sge[sge_no].iov_len = len;\n-\t\t\tsge_no++;\n+/* One Write chunk is copied from Call transport header to Reply\n+ * transport header. Each segment's length field is updated to\n+ * reflect number of bytes consumed in the segment.\n+ *\n+ * Returns number of segments in this chunk.\n+ */\n+static unsigned int xdr_encode_write_chunk(__be32 *dst, __be32 *src,\n+\t\t\t\t\t   unsigned int remaining)\n+{\n+\tunsigned int i, nsegs;\n+\tu32 seg_len;\n+\n+\t/* Write list discriminator */\n+\t*dst++ = *src++;\n+\n+\t/* number of segments in this chunk */\n+\tnsegs = be32_to_cpup(src);\n+\t*dst++ = *src++;\n+\n+\tfor (i = nsegs; i; i--) {\n+\t\t/* segment's RDMA handle */\n+\t\t*dst++ = *src++;\n+\n+\t\t/* bytes returned in this segment */\n+\t\tseg_len = be32_to_cpu(*src);\n+\t\tif (remaining >= seg_len) {\n+\t\t\t/* entire segment was consumed */\n+\t\t\t*dst = *src;\n+\t\t\tremaining -= seg_len;\n+\t\t} else {\n+\t\t\t/* segment only partly filled */\n+\t\t\t*dst = cpu_to_be32(remaining);\n+\t\t\tremaining = 0;\n \t\t}\n-\t}\n+\t\tdst++; src++;\n \n-\tdprintk(\"svcrdma: %s: sge_no %d page_no %d \"\n-\t\t\"page_base %u page_len %u head_len %zu tail_len %zu\\n\",\n-\t\t__func__, sge_no, page_no, xdr->page_base, xdr->page_len,\n-\t\txdr->head[0].iov_len, xdr->tail[0].iov_len);\n+\t\t/* segment's RDMA offset */\n+\t\t*dst++ = *src++;\n+\t\t*dst++ = *src++;\n+\t}\n \n-\tvec->count = sge_no;\n-\treturn 0;\n+\treturn nsegs;\n }\n \n-static dma_addr_t dma_map_xdr(struct svcxprt_rdma *xprt,\n-\t\t\t      struct xdr_buf *xdr,\n-\t\t\t      u32 xdr_off, size_t len, int dir)\n+/* The client provided a Write list in the Call message. Fill in\n+ * the segments in the first Write chunk in the Reply's transport\n+ * header with the number of bytes consumed in each segment.\n+ * Remaining chunks are returned unused.\n+ *\n+ * Assumptions:\n+ *  - Client has provided only one Write chunk\n+ */\n+static void svc_rdma_xdr_encode_write_list(__be32 *rdma_resp, __be32 *wr_ch,\n+\t\t\t\t\t   unsigned int consumed)\n {\n-\tstruct page *page;\n-\tdma_addr_t dma_addr;\n-\tif (xdr_off < xdr->head[0].iov_len) {\n-\t\t/* This offset is in the head */\n-\t\txdr_off += (unsigned long)xdr->head[0].iov_base & ~PAGE_MASK;\n-\t\tpage = virt_to_page(xdr->head[0].iov_base);\n-\t} else {\n-\t\txdr_off -= xdr->head[0].iov_len;\n-\t\tif (xdr_off < xdr->page_len) {\n-\t\t\t/* This offset is in the page list */\n-\t\t\txdr_off += xdr->page_base;\n-\t\t\tpage = xdr->pages[xdr_off >> PAGE_SHIFT];\n-\t\t\txdr_off &= ~PAGE_MASK;\n-\t\t} else {\n-\t\t\t/* This offset is in the tail */\n-\t\t\txdr_off -= xdr->page_len;\n-\t\t\txdr_off += (unsigned long)\n-\t\t\t\txdr->tail[0].iov_base & ~PAGE_MASK;\n-\t\t\tpage = virt_to_page(xdr->tail[0].iov_base);\n-\t\t}\n+\tunsigned int nsegs;\n+\t__be32 *p, *q;\n+\n+\t/* RPC-over-RDMA V1 replies never have a Read list. */\n+\tp = rdma_resp + rpcrdma_fixed_maxsz + 1;\n+\n+\tq = wr_ch;\n+\twhile (*q != xdr_zero) {\n+\t\tnsegs = xdr_encode_write_chunk(p, q, consumed);\n+\t\tq += 2 + nsegs * rpcrdma_segment_maxsz;\n+\t\tp += 2 + nsegs * rpcrdma_segment_maxsz;\n+\t\tconsumed = 0;\n \t}\n-\tdma_addr = ib_dma_map_page(xprt->sc_cm_id->device, page, xdr_off,\n-\t\t\t\t   min_t(size_t, PAGE_SIZE, len), dir);\n-\treturn dma_addr;\n+\n+\t/* Terminate Write list */\n+\t*p++ = xdr_zero;\n+\n+\t/* Reply chunk discriminator; may be replaced later */\n+\t*p = xdr_zero;\n+}\n+\n+/* The client provided a Reply chunk in the Call message. Fill in\n+ * the segments in the Reply chunk in the Reply message with the\n+ * number of bytes consumed in each segment.\n+ *\n+ * Assumptions:\n+ * - Reply can always fit in the provided Reply chunk\n+ */\n+static void svc_rdma_xdr_encode_reply_chunk(__be32 *rdma_resp, __be32 *rp_ch,\n+\t\t\t\t\t    unsigned int consumed)\n+{\n+\t__be32 *p;\n+\n+\t/* Find the Reply chunk in the Reply's xprt header.\n+\t * RPC-over-RDMA V1 replies never have a Read list.\n+\t */\n+\tp = rdma_resp + rpcrdma_fixed_maxsz + 1;\n+\n+\t/* Skip past Write list */\n+\twhile (*p++ != xdr_zero)\n+\t\tp += 1 + be32_to_cpup(p) * rpcrdma_segment_maxsz;\n+\n+\txdr_encode_write_chunk(p, rp_ch, consumed);\n }\n \n /* Parse the RPC Call's transport header.\n  */\n-static void svc_rdma_get_write_arrays(struct rpcrdma_msg *rmsgp,\n-\t\t\t\t      struct rpcrdma_write_array **write,\n-\t\t\t\t      struct rpcrdma_write_array **reply)\n+static void svc_rdma_get_write_arrays(__be32 *rdma_argp,\n+\t\t\t\t      __be32 **write, __be32 **reply)\n {\n \t__be32 *p;\n \n-\tp = (__be32 *)&rmsgp->rm_body.rm_chunks[0];\n+\tp = rdma_argp + rpcrdma_fixed_maxsz;\n \n \t/* Read list */\n \twhile (*p++ != xdr_zero)\n \t\tp += 5;\n \n \t/* Write list */\n \tif (*p != xdr_zero) {\n-\t\t*write = (struct rpcrdma_write_array *)p;\n+\t\t*write = p;\n \t\twhile (*p++ != xdr_zero)\n \t\t\tp += 1 + be32_to_cpu(*p) * 4;\n \t} else {\n@@ -179,7 +265,7 @@ static void svc_rdma_get_write_arrays(struct rpcrdma_msg *rmsgp,\n \n \t/* Reply chunk */\n \tif (*p != xdr_zero)\n-\t\t*reply = (struct rpcrdma_write_array *)p;\n+\t\t*reply = p;\n \telse\n \t\t*reply = NULL;\n }\n@@ -189,360 +275,321 @@ static void svc_rdma_get_write_arrays(struct rpcrdma_msg *rmsgp,\n  * Invalidate, and responder chooses one rkey to invalidate.\n  *\n  * Find a candidate rkey to invalidate when sending a reply.  Picks the\n- * first rkey it finds in the chunks lists.\n+ * first R_key it finds in the chunk lists.\n  *\n  * Returns zero if RPC's chunk lists are empty.\n  */\n-static u32 svc_rdma_get_inv_rkey(struct rpcrdma_msg *rdma_argp,\n-\t\t\t\t struct rpcrdma_write_array *wr_ary,\n-\t\t\t\t struct rpcrdma_write_array *rp_ary)\n+static u32 svc_rdma_get_inv_rkey(__be32 *rdma_argp,\n+\t\t\t\t __be32 *wr_lst, __be32 *rp_ch)\n {\n-\tstruct rpcrdma_read_chunk *rd_ary;\n-\tstruct rpcrdma_segment *arg_ch;\n+\t__be32 *p;\n \n-\trd_ary = (struct rpcrdma_read_chunk *)&rdma_argp->rm_body.rm_chunks[0];\n-\tif (rd_ary->rc_discrim != xdr_zero)\n-\t\treturn be32_to_cpu(rd_ary->rc_target.rs_handle);\n+\tp = rdma_argp + rpcrdma_fixed_maxsz;\n+\tif (*p != xdr_zero)\n+\t\tp += 2;\n+\telse if (wr_lst && be32_to_cpup(wr_lst + 1))\n+\t\tp = wr_lst + 2;\n+\telse if (rp_ch && be32_to_cpup(rp_ch + 1))\n+\t\tp = rp_ch + 2;\n+\telse\n+\t\treturn 0;\n+\treturn be32_to_cpup(p);\n+}\n \n-\tif (wr_ary && be32_to_cpu(wr_ary->wc_nchunks)) {\n-\t\targ_ch = &wr_ary->wc_array[0].wc_target;\n-\t\treturn be32_to_cpu(arg_ch->rs_handle);\n-\t}\n+/* ib_dma_map_page() is used here because svc_rdma_dma_unmap()\n+ * is used during completion to DMA-unmap this memory, and\n+ * it uses ib_dma_unmap_page() exclusively.\n+ */\n+static int svc_rdma_dma_map_buf(struct svcxprt_rdma *rdma,\n+\t\t\t\tstruct svc_rdma_op_ctxt *ctxt,\n+\t\t\t\tunsigned int sge_no,\n+\t\t\t\tunsigned char *base,\n+\t\t\t\tunsigned int len)\n+{\n+\tunsigned long offset = (unsigned long)base & ~PAGE_MASK;\n+\tstruct ib_device *dev = rdma->sc_cm_id->device;\n+\tdma_addr_t dma_addr;\n \n-\tif (rp_ary && be32_to_cpu(rp_ary->wc_nchunks)) {\n-\t\targ_ch = &rp_ary->wc_array[0].wc_target;\n-\t\treturn be32_to_cpu(arg_ch->rs_handle);\n-\t}\n+\tdma_addr = ib_dma_map_page(dev, virt_to_page(base),\n+\t\t\t\t   offset, len, DMA_TO_DEVICE);\n+\tif (ib_dma_mapping_error(dev, dma_addr))\n+\t\treturn -EIO;\n \n+\tctxt->sge[sge_no].addr = dma_addr;\n+\tctxt->sge[sge_no].length = len;\n+\tctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;\n+\tsvc_rdma_count_mappings(rdma, ctxt);\n \treturn 0;\n }\n \n-/* Assumptions:\n- * - The specified write_len can be represented in sc_max_sge * PAGE_SIZE\n- */\n-static int send_write(struct svcxprt_rdma *xprt, struct svc_rqst *rqstp,\n-\t\t      u32 rmr, u64 to,\n-\t\t      u32 xdr_off, int write_len,\n-\t\t      struct svc_rdma_req_map *vec)\n+static int svc_rdma_dma_map_page(struct svcxprt_rdma *rdma,\n+\t\t\t\t struct svc_rdma_op_ctxt *ctxt,\n+\t\t\t\t unsigned int sge_no,\n+\t\t\t\t struct page *page,\n+\t\t\t\t unsigned int offset,\n+\t\t\t\t unsigned int len)\n {\n-\tstruct ib_rdma_wr write_wr;\n-\tstruct ib_sge *sge;\n-\tint xdr_sge_no;\n-\tint sge_no;\n-\tint sge_bytes;\n-\tint sge_off;\n-\tint bc;\n-\tstruct svc_rdma_op_ctxt *ctxt;\n+\tstruct ib_device *dev = rdma->sc_cm_id->device;\n+\tdma_addr_t dma_addr;\n \n-\tif (vec->count > RPCSVC_MAXPAGES) {\n-\t\tpr_err(\"svcrdma: Too many pages (%lu)\\n\", vec->count);\n+\tdma_addr = ib_dma_map_page(dev, page, offset, len, DMA_TO_DEVICE);\n+\tif (ib_dma_mapping_error(dev, dma_addr))\n \t\treturn -EIO;\n-\t}\n \n-\tdprintk(\"svcrdma: RDMA_WRITE rmr=%x, to=%llx, xdr_off=%d, \"\n-\t\t\"write_len=%d, vec->sge=%p, vec->count=%lu\\n\",\n-\t\trmr, (unsigned long long)to, xdr_off,\n-\t\twrite_len, vec->sge, vec->count);\n+\tctxt->sge[sge_no].addr = dma_addr;\n+\tctxt->sge[sge_no].length = len;\n+\tctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;\n+\tsvc_rdma_count_mappings(rdma, ctxt);\n+\treturn 0;\n+}\n \n-\tctxt = svc_rdma_get_context(xprt);\n+/**\n+ * svc_rdma_map_reply_hdr - DMA map the transport header buffer\n+ * @rdma: controlling transport\n+ * @ctxt: op_ctxt for the Send WR\n+ * @rdma_resp: buffer containing transport header\n+ * @len: length of transport header\n+ *\n+ * Returns:\n+ *\t%0 if the header is DMA mapped,\n+ *\t%-EIO if DMA mapping failed.\n+ */\n+int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,\n+\t\t\t   struct svc_rdma_op_ctxt *ctxt,\n+\t\t\t   __be32 *rdma_resp,\n+\t\t\t   unsigned int len)\n+{\n \tctxt->direction = DMA_TO_DEVICE;\n-\tsge = ctxt->sge;\n-\n-\t/* Find the SGE associated with xdr_off */\n-\tfor (bc = xdr_off, xdr_sge_no = 1; bc && xdr_sge_no < vec->count;\n-\t     xdr_sge_no++) {\n-\t\tif (vec->sge[xdr_sge_no].iov_len > bc)\n-\t\t\tbreak;\n-\t\tbc -= vec->sge[xdr_sge_no].iov_len;\n-\t}\n-\n-\tsge_off = bc;\n-\tbc = write_len;\n-\tsge_no = 0;\n-\n-\t/* Copy the remaining SGE */\n-\twhile (bc != 0) {\n-\t\tsge_bytes = min_t(size_t,\n-\t\t\t  bc, vec->sge[xdr_sge_no].iov_len-sge_off);\n-\t\tsge[sge_no].length = sge_bytes;\n-\t\tsge[sge_no].addr =\n-\t\t\tdma_map_xdr(xprt, &rqstp->rq_res, xdr_off,\n-\t\t\t\t    sge_bytes, DMA_TO_DEVICE);\n-\t\txdr_off += sge_bytes;\n-\t\tif (ib_dma_mapping_error(xprt->sc_cm_id->device,\n-\t\t\t\t\t sge[sge_no].addr))\n-\t\t\tgoto err;\n-\t\tsvc_rdma_count_mappings(xprt, ctxt);\n-\t\tsge[sge_no].lkey = xprt->sc_pd->local_dma_lkey;\n-\t\tctxt->count++;\n-\t\tsge_off = 0;\n-\t\tsge_no++;\n-\t\txdr_sge_no++;\n-\t\tif (xdr_sge_no > vec->count) {\n-\t\t\tpr_err(\"svcrdma: Too many sges (%d)\\n\", xdr_sge_no);\n-\t\t\tgoto err;\n-\t\t}\n-\t\tbc -= sge_bytes;\n-\t\tif (sge_no == xprt->sc_max_sge)\n-\t\t\tbreak;\n-\t}\n-\n-\t/* Prepare WRITE WR */\n-\tmemset(&write_wr, 0, sizeof write_wr);\n-\tctxt->cqe.done = svc_rdma_wc_write;\n-\twrite_wr.wr.wr_cqe = &ctxt->cqe;\n-\twrite_wr.wr.sg_list = &sge[0];\n-\twrite_wr.wr.num_sge = sge_no;\n-\twrite_wr.wr.opcode = IB_WR_RDMA_WRITE;\n-\twrite_wr.wr.send_flags = IB_SEND_SIGNALED;\n-\twrite_wr.rkey = rmr;\n-\twrite_wr.remote_addr = to;\n-\n-\t/* Post It */\n-\tatomic_inc(&rdma_stat_write);\n-\tif (svc_rdma_send(xprt, &write_wr.wr))\n-\t\tgoto err;\n-\treturn write_len - bc;\n- err:\n-\tsvc_rdma_unmap_dma(ctxt);\n-\tsvc_rdma_put_context(ctxt, 0);\n-\treturn -EIO;\n+\tctxt->pages[0] = virt_to_page(rdma_resp);\n+\tctxt->count = 1;\n+\treturn svc_rdma_dma_map_page(rdma, ctxt, 0, ctxt->pages[0], 0, len);\n }\n \n-noinline\n-static int send_write_chunks(struct svcxprt_rdma *xprt,\n-\t\t\t     struct rpcrdma_write_array *wr_ary,\n-\t\t\t     struct rpcrdma_msg *rdma_resp,\n-\t\t\t     struct svc_rqst *rqstp,\n-\t\t\t     struct svc_rdma_req_map *vec)\n+/* Load the xdr_buf into the ctxt's sge array, and DMA map each\n+ * element as it is added.\n+ *\n+ * Returns the number of sge elements loaded on success, or\n+ * a negative errno on failure.\n+ */\n+static int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,\n+\t\t\t\t  struct svc_rdma_op_ctxt *ctxt,\n+\t\t\t\t  struct xdr_buf *xdr, __be32 *wr_lst)\n {\n-\tu32 xfer_len = rqstp->rq_res.page_len;\n-\tint write_len;\n-\tu32 xdr_off;\n-\tint chunk_off;\n-\tint chunk_no;\n-\tint nchunks;\n-\tstruct rpcrdma_write_array *res_ary;\n+\tunsigned int len, sge_no, remaining, page_off;\n+\tstruct page **ppages;\n+\tunsigned char *base;\n+\tu32 xdr_pad;\n \tint ret;\n \n-\tres_ary = (struct rpcrdma_write_array *)\n-\t\t&rdma_resp->rm_body.rm_chunks[1];\n-\n-\t/* Write chunks start at the pagelist */\n-\tnchunks = be32_to_cpu(wr_ary->wc_nchunks);\n-\tfor (xdr_off = rqstp->rq_res.head[0].iov_len, chunk_no = 0;\n-\t     xfer_len && chunk_no < nchunks;\n-\t     chunk_no++) {\n-\t\tstruct rpcrdma_segment *arg_ch;\n-\t\tu64 rs_offset;\n-\n-\t\targ_ch = &wr_ary->wc_array[chunk_no].wc_target;\n-\t\twrite_len = min(xfer_len, be32_to_cpu(arg_ch->rs_length));\n-\n-\t\t/* Prepare the response chunk given the length actually\n-\t\t * written */\n-\t\txdr_decode_hyper((__be32 *)&arg_ch->rs_offset, &rs_offset);\n-\t\tsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\n-\t\t\t\t\t\targ_ch->rs_handle,\n-\t\t\t\t\t\targ_ch->rs_offset,\n-\t\t\t\t\t\twrite_len);\n-\t\tchunk_off = 0;\n-\t\twhile (write_len) {\n-\t\t\tret = send_write(xprt, rqstp,\n-\t\t\t\t\t be32_to_cpu(arg_ch->rs_handle),\n-\t\t\t\t\t rs_offset + chunk_off,\n-\t\t\t\t\t xdr_off,\n-\t\t\t\t\t write_len,\n-\t\t\t\t\t vec);\n-\t\t\tif (ret <= 0)\n-\t\t\t\tgoto out_err;\n-\t\t\tchunk_off += ret;\n-\t\t\txdr_off += ret;\n-\t\t\txfer_len -= ret;\n-\t\t\twrite_len -= ret;\n+\tsge_no = 1;\n+\n+\tret = svc_rdma_dma_map_buf(rdma, ctxt, sge_no++,\n+\t\t\t\t   xdr->head[0].iov_base,\n+\t\t\t\t   xdr->head[0].iov_len);\n+\tif (ret < 0)\n+\t\treturn ret;\n+\n+\t/* If a Write chunk is present, the xdr_buf's page list\n+\t * is not included inline. However the Upper Layer may\n+\t * have added XDR padding in the tail buffer, and that\n+\t * should not be included inline.\n+\t */\n+\tif (wr_lst) {\n+\t\tbase = xdr->tail[0].iov_base;\n+\t\tlen = xdr->tail[0].iov_len;\n+\t\txdr_pad = xdr_padsize(xdr->page_len);\n+\n+\t\tif (len && xdr_pad) {\n+\t\t\tbase += xdr_pad;\n+\t\t\tlen -= xdr_pad;\n \t\t}\n+\n+\t\tgoto tail;\n+\t}\n+\n+\tppages = xdr->pages + (xdr->page_base >> PAGE_SHIFT);\n+\tpage_off = xdr->page_base & ~PAGE_MASK;\n+\tremaining = xdr->page_len;\n+\twhile (remaining) {\n+\t\tlen = min_t(u32, PAGE_SIZE - page_off, remaining);\n+\n+\t\tret = svc_rdma_dma_map_page(rdma, ctxt, sge_no++,\n+\t\t\t\t\t    *ppages++, page_off, len);\n+\t\tif (ret < 0)\n+\t\t\treturn ret;\n+\n+\t\tremaining -= len;\n+\t\tpage_off = 0;\n \t}\n-\t/* Update the req with the number of chunks actually used */\n-\tsvc_rdma_xdr_encode_write_list(rdma_resp, chunk_no);\n \n-\treturn rqstp->rq_res.page_len;\n+\tbase = xdr->tail[0].iov_base;\n+\tlen = xdr->tail[0].iov_len;\n+tail:\n+\tif (len) {\n+\t\tret = svc_rdma_dma_map_buf(rdma, ctxt, sge_no++, base, len);\n+\t\tif (ret < 0)\n+\t\t\treturn ret;\n+\t}\n \n-out_err:\n-\tpr_err(\"svcrdma: failed to send write chunks, rc=%d\\n\", ret);\n-\treturn -EIO;\n+\treturn sge_no - 1;\n }\n \n-noinline\n-static int send_reply_chunks(struct svcxprt_rdma *xprt,\n-\t\t\t     struct rpcrdma_write_array *rp_ary,\n-\t\t\t     struct rpcrdma_msg *rdma_resp,\n-\t\t\t     struct svc_rqst *rqstp,\n-\t\t\t     struct svc_rdma_req_map *vec)\n+/* The svc_rqst and all resources it owns are released as soon as\n+ * svc_rdma_sendto returns. Transfer pages under I/O to the ctxt\n+ * so they are released by the Send completion handler.\n+ */\n+static void svc_rdma_save_io_pages(struct svc_rqst *rqstp,\n+\t\t\t\t   struct svc_rdma_op_ctxt *ctxt)\n {\n-\tu32 xfer_len = rqstp->rq_res.len;\n-\tint write_len;\n-\tu32 xdr_off;\n-\tint chunk_no;\n-\tint chunk_off;\n-\tint nchunks;\n-\tstruct rpcrdma_segment *ch;\n-\tstruct rpcrdma_write_array *res_ary;\n-\tint ret;\n+\tint i, pages = rqstp->rq_next_page - rqstp->rq_respages;\n \n-\t/* XXX: need to fix when reply lists occur with read-list and or\n-\t * write-list */\n-\tres_ary = (struct rpcrdma_write_array *)\n-\t\t&rdma_resp->rm_body.rm_chunks[2];\n-\n-\t/* xdr offset starts at RPC message */\n-\tnchunks = be32_to_cpu(rp_ary->wc_nchunks);\n-\tfor (xdr_off = 0, chunk_no = 0;\n-\t     xfer_len && chunk_no < nchunks;\n-\t     chunk_no++) {\n-\t\tu64 rs_offset;\n-\t\tch = &rp_ary->wc_array[chunk_no].wc_target;\n-\t\twrite_len = min(xfer_len, be32_to_cpu(ch->rs_length));\n-\n-\t\t/* Prepare the reply chunk given the length actually\n-\t\t * written */\n-\t\txdr_decode_hyper((__be32 *)&ch->rs_offset, &rs_offset);\n-\t\tsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\n-\t\t\t\t\t\tch->rs_handle, ch->rs_offset,\n-\t\t\t\t\t\twrite_len);\n-\t\tchunk_off = 0;\n-\t\twhile (write_len) {\n-\t\t\tret = send_write(xprt, rqstp,\n-\t\t\t\t\t be32_to_cpu(ch->rs_handle),\n-\t\t\t\t\t rs_offset + chunk_off,\n-\t\t\t\t\t xdr_off,\n-\t\t\t\t\t write_len,\n-\t\t\t\t\t vec);\n-\t\t\tif (ret <= 0)\n-\t\t\t\tgoto out_err;\n-\t\t\tchunk_off += ret;\n-\t\t\txdr_off += ret;\n-\t\t\txfer_len -= ret;\n-\t\t\twrite_len -= ret;\n-\t\t}\n+\tctxt->count += pages;\n+\tfor (i = 0; i < pages; i++) {\n+\t\tctxt->pages[i + 1] = rqstp->rq_respages[i];\n+\t\trqstp->rq_respages[i] = NULL;\n \t}\n-\t/* Update the req with the number of chunks actually used */\n-\tsvc_rdma_xdr_encode_reply_array(res_ary, chunk_no);\n+\trqstp->rq_next_page = rqstp->rq_respages + 1;\n+}\n \n-\treturn rqstp->rq_res.len;\n+/**\n+ * svc_rdma_post_send_wr - Set up and post one Send Work Request\n+ * @rdma: controlling transport\n+ * @ctxt: op_ctxt for transmitting the Send WR\n+ * @num_sge: number of SGEs to send\n+ * @inv_rkey: R_key argument to Send With Invalidate, or zero\n+ *\n+ * Returns:\n+ *\t%0 if the Send* was posted successfully,\n+ *\t%-ENOTCONN if the connection was lost or dropped,\n+ *\t%-EINVAL if there was a problem with the Send we built,\n+ *\t%-ENOMEM if ib_post_send failed.\n+ */\n+int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,\n+\t\t\t  struct svc_rdma_op_ctxt *ctxt, int num_sge,\n+\t\t\t  u32 inv_rkey)\n+{\n+\tstruct ib_send_wr *send_wr = &ctxt->send_wr;\n \n-out_err:\n-\tpr_err(\"svcrdma: failed to send reply chunks, rc=%d\\n\", ret);\n-\treturn -EIO;\n+\tdprintk(\"svcrdma: posting Send WR with %u sge(s)\\n\", num_sge);\n+\n+\tsend_wr->next = NULL;\n+\tctxt->cqe.done = svc_rdma_wc_send;\n+\tsend_wr->wr_cqe = &ctxt->cqe;\n+\tsend_wr->sg_list = ctxt->sge;\n+\tsend_wr->num_sge = num_sge;\n+\tsend_wr->send_flags = IB_SEND_SIGNALED;\n+\tif (inv_rkey) {\n+\t\tsend_wr->opcode = IB_WR_SEND_WITH_INV;\n+\t\tsend_wr->ex.invalidate_rkey = inv_rkey;\n+\t} else {\n+\t\tsend_wr->opcode = IB_WR_SEND;\n+\t}\n+\n+\treturn svc_rdma_send(rdma, send_wr);\n }\n \n-/* This function prepares the portion of the RPCRDMA message to be\n- * sent in the RDMA_SEND. This function is called after data sent via\n- * RDMA has already been transmitted. There are three cases:\n- * - The RPCRDMA header, RPC header, and payload are all sent in a\n- *   single RDMA_SEND. This is the \"inline\" case.\n- * - The RPCRDMA header and some portion of the RPC header and data\n- *   are sent via this RDMA_SEND and another portion of the data is\n- *   sent via RDMA.\n- * - The RPCRDMA header [NOMSG] is sent in this RDMA_SEND and the RPC\n- *   header and data are all transmitted via RDMA.\n- * In all three cases, this function prepares the RPCRDMA header in\n- * sge[0], the 'type' parameter indicates the type to place in the\n- * RPCRDMA header, and the 'byte_count' field indicates how much of\n- * the XDR to include in this RDMA_SEND. NB: The offset of the payload\n- * to send is zero in the XDR.\n+/* Prepare the portion of the RPC Reply that will be transmitted\n+ * via RDMA Send. The RPC-over-RDMA transport header is prepared\n+ * in sge[0], and the RPC xdr_buf is prepared in following sges.\n+ *\n+ * Depending on whether a Write list or Reply chunk is present,\n+ * the server may send all, a portion of, or none of the xdr_buf.\n+ * In the latter case, only the transport header (sge[0]) is\n+ * transmitted.\n+ *\n+ * RDMA Send is the last step of transmitting an RPC reply. Pages\n+ * involved in the earlier RDMA Writes are here transferred out\n+ * of the rqstp and into the ctxt's page array. These pages are\n+ * DMA unmapped by each Write completion, but the subsequent Send\n+ * completion finally releases these pages.\n+ *\n+ * Assumptions:\n+ * - The Reply's transport header will never be larger than a page.\n  */\n-static int send_reply(struct svcxprt_rdma *rdma,\n-\t\t      struct svc_rqst *rqstp,\n-\t\t      struct page *page,\n-\t\t      struct rpcrdma_msg *rdma_resp,\n-\t\t      struct svc_rdma_req_map *vec,\n-\t\t      int byte_count,\n-\t\t      u32 inv_rkey)\n+static int svc_rdma_send_reply_msg(struct svcxprt_rdma *rdma,\n+\t\t\t\t   __be32 *rdma_argp, __be32 *rdma_resp,\n+\t\t\t\t   struct svc_rqst *rqstp,\n+\t\t\t\t   __be32 *wr_lst, __be32 *rp_ch)\n {\n \tstruct svc_rdma_op_ctxt *ctxt;\n-\tstruct ib_send_wr send_wr;\n-\tu32 xdr_off;\n-\tint sge_no;\n-\tint sge_bytes;\n-\tint page_no;\n-\tint pages;\n-\tint ret = -EIO;\n-\n-\t/* Prepare the context */\n+\tu32 inv_rkey;\n+\tint ret;\n+\n+\tdprintk(\"svcrdma: sending %s reply: head=%zu, pagelen=%u, tail=%zu\\n\",\n+\t\t(rp_ch ? \"RDMA_NOMSG\" : \"RDMA_MSG\"),\n+\t\trqstp->rq_res.head[0].iov_len,\n+\t\trqstp->rq_res.page_len,\n+\t\trqstp->rq_res.tail[0].iov_len);\n+\n \tctxt = svc_rdma_get_context(rdma);\n-\tctxt->direction = DMA_TO_DEVICE;\n-\tctxt->pages[0] = page;\n-\tctxt->count = 1;\n \n-\t/* Prepare the SGE for the RPCRDMA Header */\n-\tctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;\n-\tctxt->sge[0].length =\n-\t    svc_rdma_xdr_get_reply_hdr_len((__be32 *)rdma_resp);\n-\tctxt->sge[0].addr =\n-\t    ib_dma_map_page(rdma->sc_cm_id->device, page, 0,\n-\t\t\t    ctxt->sge[0].length, DMA_TO_DEVICE);\n-\tif (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr))\n+\tret = svc_rdma_map_reply_hdr(rdma, ctxt, rdma_resp,\n+\t\t\t\t     svc_rdma_reply_hdr_len(rdma_resp));\n+\tif (ret < 0)\n \t\tgoto err;\n-\tsvc_rdma_count_mappings(rdma, ctxt);\n-\n-\tctxt->direction = DMA_TO_DEVICE;\n \n-\t/* Map the payload indicated by 'byte_count' */\n-\txdr_off = 0;\n-\tfor (sge_no = 1; byte_count && sge_no < vec->count; sge_no++) {\n-\t\tsge_bytes = min_t(size_t, vec->sge[sge_no].iov_len, byte_count);\n-\t\tbyte_count -= sge_bytes;\n-\t\tctxt->sge[sge_no].addr =\n-\t\t\tdma_map_xdr(rdma, &rqstp->rq_res, xdr_off,\n-\t\t\t\t    sge_bytes, DMA_TO_DEVICE);\n-\t\txdr_off += sge_bytes;\n-\t\tif (ib_dma_mapping_error(rdma->sc_cm_id->device,\n-\t\t\t\t\t ctxt->sge[sge_no].addr))\n+\tif (!rp_ch) {\n+\t\tret = svc_rdma_map_reply_msg(rdma, ctxt,\n+\t\t\t\t\t     &rqstp->rq_res, wr_lst);\n+\t\tif (ret < 0)\n \t\t\tgoto err;\n-\t\tsvc_rdma_count_mappings(rdma, ctxt);\n-\t\tctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;\n-\t\tctxt->sge[sge_no].length = sge_bytes;\n \t}\n-\tif (byte_count != 0) {\n-\t\tpr_err(\"svcrdma: Could not map %d bytes\\n\", byte_count);\n+\n+\tsvc_rdma_save_io_pages(rqstp, ctxt);\n+\n+\tinv_rkey = 0;\n+\tif (rdma->sc_snd_w_inv)\n+\t\tinv_rkey = svc_rdma_get_inv_rkey(rdma_argp, wr_lst, rp_ch);\n+\tret = svc_rdma_post_send_wr(rdma, ctxt, 1 + ret, inv_rkey);\n+\tif (ret)\n \t\tgoto err;\n-\t}\n \n-\t/* Save all respages in the ctxt and remove them from the\n-\t * respages array. They are our pages until the I/O\n-\t * completes.\n+\treturn 0;\n+\n+err:\n+\tpr_err(\"svcrdma: failed to post Send WR (%d)\\n\", ret);\n+\tsvc_rdma_unmap_dma(ctxt);\n+\tsvc_rdma_put_context(ctxt, 1);\n+\treturn ret;\n+}\n+\n+/* Given the client-provided Write and Reply chunks, the server was not\n+ * able to form a complete reply. Return an RDMA_ERROR message so the\n+ * client can retire this RPC transaction. As above, the Send completion\n+ * routine releases payload pages that were part of a previous RDMA Write.\n+ *\n+ * Remote Invalidation is skipped for simplicity.\n+ */\n+static int svc_rdma_send_error_msg(struct svcxprt_rdma *rdma,\n+\t\t\t\t   __be32 *rdma_resp, struct svc_rqst *rqstp)\n+{\n+\tstruct svc_rdma_op_ctxt *ctxt;\n+\t__be32 *p;\n+\tint ret;\n+\n+\tctxt = svc_rdma_get_context(rdma);\n+\n+\t/* Replace the original transport header with an\n+\t * RDMA_ERROR response. XID etc are preserved.\n \t */\n-\tpages = rqstp->rq_next_page - rqstp->rq_respages;\n-\tfor (page_no = 0; page_no < pages; page_no++) {\n-\t\tctxt->pages[page_no+1] = rqstp->rq_respages[page_no];\n-\t\tctxt->count++;\n-\t\trqstp->rq_respages[page_no] = NULL;\n-\t}\n-\trqstp->rq_next_page = rqstp->rq_respages + 1;\n+\tp = rdma_resp + 3;\n+\t*p++ = rdma_error;\n+\t*p   = err_chunk;\n \n-\tif (sge_no > rdma->sc_max_sge) {\n-\t\tpr_err(\"svcrdma: Too many sges (%d)\\n\", sge_no);\n+\tret = svc_rdma_map_reply_hdr(rdma, ctxt, rdma_resp, 20);\n+\tif (ret < 0)\n \t\tgoto err;\n-\t}\n-\tmemset(&send_wr, 0, sizeof send_wr);\n-\tctxt->cqe.done = svc_rdma_wc_send;\n-\tsend_wr.wr_cqe = &ctxt->cqe;\n-\tsend_wr.sg_list = ctxt->sge;\n-\tsend_wr.num_sge = sge_no;\n-\tif (inv_rkey) {\n-\t\tsend_wr.opcode = IB_WR_SEND_WITH_INV;\n-\t\tsend_wr.ex.invalidate_rkey = inv_rkey;\n-\t} else\n-\t\tsend_wr.opcode = IB_WR_SEND;\n-\tsend_wr.send_flags =  IB_SEND_SIGNALED;\n \n-\tret = svc_rdma_send(rdma, &send_wr);\n+\tsvc_rdma_save_io_pages(rqstp, ctxt);\n+\n+\tret = svc_rdma_post_send_wr(rdma, ctxt, 1 + ret, 0);\n \tif (ret)\n \t\tgoto err;\n \n \treturn 0;\n \n- err:\n+err:\n+\tpr_err(\"svcrdma: failed to post Send WR (%d)\\n\", ret);\n \tsvc_rdma_unmap_dma(ctxt);\n \tsvc_rdma_put_context(ctxt, 1);\n \treturn ret;\n@@ -552,39 +599,36 @@ void svc_rdma_prep_reply_hdr(struct svc_rqst *rqstp)\n {\n }\n \n+/**\n+ * svc_rdma_sendto - Transmit an RPC reply\n+ * @rqstp: processed RPC request, reply XDR already in ::rq_res\n+ *\n+ * Any resources still associated with @rqstp are released upon return.\n+ * If no reply message was possible, the connection is closed.\n+ *\n+ * Returns:\n+ *\t%0 if an RPC reply has been successfully posted,\n+ *\t%-ENOMEM if a resource shortage occurred (connection is lost),\n+ *\t%-ENOTCONN if posting failed (connection is lost).\n+ */\n int svc_rdma_sendto(struct svc_rqst *rqstp)\n {\n \tstruct svc_xprt *xprt = rqstp->rq_xprt;\n \tstruct svcxprt_rdma *rdma =\n \t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n-\tstruct rpcrdma_msg *rdma_argp;\n-\tstruct rpcrdma_msg *rdma_resp;\n-\tstruct rpcrdma_write_array *wr_ary, *rp_ary;\n-\tint ret;\n-\tint inline_bytes;\n+\t__be32 *p, *rdma_argp, *rdma_resp, *wr_lst, *rp_ch;\n+\tstruct xdr_buf *xdr = &rqstp->rq_res;\n \tstruct page *res_page;\n-\tstruct svc_rdma_req_map *vec;\n-\tu32 inv_rkey;\n-\t__be32 *p;\n-\n-\tdprintk(\"svcrdma: sending response for rqstp=%p\\n\", rqstp);\n+\tint ret;\n \n-\t/* Get the RDMA request header. The receive logic always\n-\t * places this at the start of page 0.\n+\t/* Find the call's chunk lists to decide how to send the reply.\n+\t * Receive places the Call's xprt header at the start of page 0.\n \t */\n \trdma_argp = page_address(rqstp->rq_pages[0]);\n-\tsvc_rdma_get_write_arrays(rdma_argp, &wr_ary, &rp_ary);\n-\n-\tinv_rkey = 0;\n-\tif (rdma->sc_snd_w_inv)\n-\t\tinv_rkey = svc_rdma_get_inv_rkey(rdma_argp, wr_ary, rp_ary);\n+\tsvc_rdma_get_write_arrays(rdma_argp, &wr_lst, &rp_ch);\n \n-\t/* Build an req vec for the XDR */\n-\tvec = svc_rdma_get_req_map(rdma);\n-\tret = svc_rdma_map_xdr(rdma, &rqstp->rq_res, vec, wr_ary != NULL);\n-\tif (ret)\n-\t\tgoto err0;\n-\tinline_bytes = rqstp->rq_res.len;\n+\tdprintk(\"svcrdma: preparing response for XID 0x%08x\\n\",\n+\t\tbe32_to_cpup(rdma_argp));\n \n \t/* Create the RDMA response header. xprt->xpt_mutex,\n \t * acquired in svc_send(), serializes RPC replies. The\n@@ -598,115 +642,57 @@ int svc_rdma_sendto(struct svc_rqst *rqstp)\n \t\tgoto err0;\n \trdma_resp = page_address(res_page);\n \n-\tp = &rdma_resp->rm_xid;\n-\t*p++ = rdma_argp->rm_xid;\n-\t*p++ = rdma_argp->rm_vers;\n+\tp = rdma_resp;\n+\t*p++ = *rdma_argp;\n+\t*p++ = *(rdma_argp + 1);\n \t*p++ = rdma->sc_fc_credits;\n-\t*p++ = rp_ary ? rdma_nomsg : rdma_msg;\n+\t*p++ = rp_ch ? rdma_nomsg : rdma_msg;\n \n \t/* Start with empty chunks */\n \t*p++ = xdr_zero;\n \t*p++ = xdr_zero;\n \t*p   = xdr_zero;\n \n-\t/* Send any write-chunk data and build resp write-list */\n-\tif (wr_ary) {\n-\t\tret = send_write_chunks(rdma, wr_ary, rdma_resp, rqstp, vec);\n+\tif (wr_lst) {\n+\t\t/* XXX: Presume the client sent only one Write chunk */\n+\t\tret = svc_rdma_send_write_chunk(rdma, wr_lst, xdr);\n \t\tif (ret < 0)\n-\t\t\tgoto err1;\n-\t\tinline_bytes -= ret + xdr_padsize(ret);\n+\t\t\tgoto err2;\n+\t\tsvc_rdma_xdr_encode_write_list(rdma_resp, wr_lst, ret);\n \t}\n-\n-\t/* Send any reply-list data and update resp reply-list */\n-\tif (rp_ary) {\n-\t\tret = send_reply_chunks(rdma, rp_ary, rdma_resp, rqstp, vec);\n+\tif (rp_ch) {\n+\t\tret = svc_rdma_send_reply_chunk(rdma, rp_ch, wr_lst, xdr);\n \t\tif (ret < 0)\n-\t\t\tgoto err1;\n-\t\tinline_bytes -= ret;\n+\t\t\tgoto err2;\n+\t\tsvc_rdma_xdr_encode_reply_chunk(rdma_resp, rp_ch, ret);\n \t}\n \n-\t/* Post a fresh Receive buffer _before_ sending the reply */\n \tret = svc_rdma_post_recv(rdma, GFP_KERNEL);\n \tif (ret)\n \t\tgoto err1;\n-\n-\tret = send_reply(rdma, rqstp, res_page, rdma_resp, vec,\n-\t\t\t inline_bytes, inv_rkey);\n+\tret = svc_rdma_send_reply_msg(rdma, rdma_argp, rdma_resp, rqstp,\n+\t\t\t\t      wr_lst, rp_ch);\n \tif (ret < 0)\n \t\tgoto err0;\n+\treturn 0;\n \n-\tsvc_rdma_put_req_map(rdma, vec);\n-\tdprintk(\"svcrdma: send_reply returns %d\\n\", ret);\n-\treturn ret;\n+ err2:\n+\tif (ret != -E2BIG)\n+\t\tgoto err1;\n+\n+\tret = svc_rdma_post_recv(rdma, GFP_KERNEL);\n+\tif (ret)\n+\t\tgoto err1;\n+\tret = svc_rdma_send_error_msg(rdma, rdma_resp, rqstp);\n+\tif (ret < 0)\n+\t\tgoto err0;\n+\treturn 0;\n \n  err1:\n \tput_page(res_page);\n  err0:\n-\tsvc_rdma_put_req_map(rdma, vec);\n \tpr_err(\"svcrdma: Could not send reply, err=%d. Closing transport.\\n\",\n \t       ret);\n-\tset_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);\n+\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n \treturn -ENOTCONN;\n }\n-\n-void svc_rdma_send_error(struct svcxprt_rdma *xprt, struct rpcrdma_msg *rmsgp,\n-\t\t\t int status)\n-{\n-\tstruct ib_send_wr err_wr;\n-\tstruct page *p;\n-\tstruct svc_rdma_op_ctxt *ctxt;\n-\tenum rpcrdma_errcode err;\n-\t__be32 *va;\n-\tint length;\n-\tint ret;\n-\n-\tret = svc_rdma_repost_recv(xprt, GFP_KERNEL);\n-\tif (ret)\n-\t\treturn;\n-\n-\tp = alloc_page(GFP_KERNEL);\n-\tif (!p)\n-\t\treturn;\n-\tva = page_address(p);\n-\n-\t/* XDR encode an error reply */\n-\terr = ERR_CHUNK;\n-\tif (status == -EPROTONOSUPPORT)\n-\t\terr = ERR_VERS;\n-\tlength = svc_rdma_xdr_encode_error(xprt, rmsgp, err, va);\n-\n-\tctxt = svc_rdma_get_context(xprt);\n-\tctxt->direction = DMA_TO_DEVICE;\n-\tctxt->count = 1;\n-\tctxt->pages[0] = p;\n-\n-\t/* Prepare SGE for local address */\n-\tctxt->sge[0].lkey = xprt->sc_pd->local_dma_lkey;\n-\tctxt->sge[0].length = length;\n-\tctxt->sge[0].addr = ib_dma_map_page(xprt->sc_cm_id->device,\n-\t\t\t\t\t    p, 0, length, DMA_TO_DEVICE);\n-\tif (ib_dma_mapping_error(xprt->sc_cm_id->device, ctxt->sge[0].addr)) {\n-\t\tdprintk(\"svcrdma: Error mapping buffer for protocol error\\n\");\n-\t\tsvc_rdma_put_context(ctxt, 1);\n-\t\treturn;\n-\t}\n-\tsvc_rdma_count_mappings(xprt, ctxt);\n-\n-\t/* Prepare SEND WR */\n-\tmemset(&err_wr, 0, sizeof(err_wr));\n-\tctxt->cqe.done = svc_rdma_wc_send;\n-\terr_wr.wr_cqe = &ctxt->cqe;\n-\terr_wr.sg_list = ctxt->sge;\n-\terr_wr.num_sge = 1;\n-\terr_wr.opcode = IB_WR_SEND;\n-\terr_wr.send_flags = IB_SEND_SIGNALED;\n-\n-\t/* Post It */\n-\tret = svc_rdma_send(xprt, &err_wr);\n-\tif (ret) {\n-\t\tdprintk(\"svcrdma: Error %d posting send for protocol error\\n\",\n-\t\t\tret);\n-\t\tsvc_rdma_unmap_dma(ctxt);\n-\t\tsvc_rdma_put_context(ctxt, 1);\n-\t}\n-}"
        },
        {
          "filename": "net/sunrpc/xprtrdma/svc_rdma_transport.c",
          "status": "modified",
          "additions": 7,
          "deletions": 103,
          "patch": "@@ -272,85 +272,6 @@ static void svc_rdma_destroy_ctxts(struct svcxprt_rdma *xprt)\n \t}\n }\n \n-static struct svc_rdma_req_map *alloc_req_map(gfp_t flags)\n-{\n-\tstruct svc_rdma_req_map *map;\n-\n-\tmap = kmalloc(sizeof(*map), flags);\n-\tif (map)\n-\t\tINIT_LIST_HEAD(&map->free);\n-\treturn map;\n-}\n-\n-static bool svc_rdma_prealloc_maps(struct svcxprt_rdma *xprt)\n-{\n-\tunsigned int i;\n-\n-\t/* One for each receive buffer on this connection. */\n-\ti = xprt->sc_max_requests;\n-\n-\twhile (i--) {\n-\t\tstruct svc_rdma_req_map *map;\n-\n-\t\tmap = alloc_req_map(GFP_KERNEL);\n-\t\tif (!map) {\n-\t\t\tdprintk(\"svcrdma: No memory for request map\\n\");\n-\t\t\treturn false;\n-\t\t}\n-\t\tlist_add(&map->free, &xprt->sc_maps);\n-\t}\n-\treturn true;\n-}\n-\n-struct svc_rdma_req_map *svc_rdma_get_req_map(struct svcxprt_rdma *xprt)\n-{\n-\tstruct svc_rdma_req_map *map = NULL;\n-\n-\tspin_lock(&xprt->sc_map_lock);\n-\tif (list_empty(&xprt->sc_maps))\n-\t\tgoto out_empty;\n-\n-\tmap = list_first_entry(&xprt->sc_maps,\n-\t\t\t       struct svc_rdma_req_map, free);\n-\tlist_del_init(&map->free);\n-\tspin_unlock(&xprt->sc_map_lock);\n-\n-out:\n-\tmap->count = 0;\n-\treturn map;\n-\n-out_empty:\n-\tspin_unlock(&xprt->sc_map_lock);\n-\n-\t/* Pre-allocation amount was incorrect */\n-\tmap = alloc_req_map(GFP_NOIO);\n-\tif (map)\n-\t\tgoto out;\n-\n-\tWARN_ONCE(1, \"svcrdma: empty request map list?\\n\");\n-\treturn NULL;\n-}\n-\n-void svc_rdma_put_req_map(struct svcxprt_rdma *xprt,\n-\t\t\t  struct svc_rdma_req_map *map)\n-{\n-\tspin_lock(&xprt->sc_map_lock);\n-\tlist_add(&map->free, &xprt->sc_maps);\n-\tspin_unlock(&xprt->sc_map_lock);\n-}\n-\n-static void svc_rdma_destroy_maps(struct svcxprt_rdma *xprt)\n-{\n-\twhile (!list_empty(&xprt->sc_maps)) {\n-\t\tstruct svc_rdma_req_map *map;\n-\n-\t\tmap = list_first_entry(&xprt->sc_maps,\n-\t\t\t\t       struct svc_rdma_req_map, free);\n-\t\tlist_del(&map->free);\n-\t\tkfree(map);\n-\t}\n-}\n-\n /* QP event handler */\n static void qp_event_handler(struct ib_event *event, void *context)\n {\n@@ -473,24 +394,6 @@ void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)\n \tsvc_rdma_put_context(ctxt, 1);\n }\n \n-/**\n- * svc_rdma_wc_write - Invoked by RDMA provider for each polled Write WC\n- * @cq:        completion queue\n- * @wc:        completed WR\n- *\n- */\n-void svc_rdma_wc_write(struct ib_cq *cq, struct ib_wc *wc)\n-{\n-\tstruct ib_cqe *cqe = wc->wr_cqe;\n-\tstruct svc_rdma_op_ctxt *ctxt;\n-\n-\tsvc_rdma_send_wc_common_put(cq, wc, \"write\");\n-\n-\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n-\tsvc_rdma_unmap_dma(ctxt);\n-\tsvc_rdma_put_context(ctxt, 0);\n-}\n-\n /**\n  * svc_rdma_wc_reg - Invoked by RDMA provider for each polled FASTREG WC\n  * @cq:        completion queue\n@@ -561,14 +464,14 @@ static struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *serv,\n \tINIT_LIST_HEAD(&cma_xprt->sc_read_complete_q);\n \tINIT_LIST_HEAD(&cma_xprt->sc_frmr_q);\n \tINIT_LIST_HEAD(&cma_xprt->sc_ctxts);\n-\tINIT_LIST_HEAD(&cma_xprt->sc_maps);\n+\tINIT_LIST_HEAD(&cma_xprt->sc_rw_ctxts);\n \tinit_waitqueue_head(&cma_xprt->sc_send_wait);\n \n \tspin_lock_init(&cma_xprt->sc_lock);\n \tspin_lock_init(&cma_xprt->sc_rq_dto_lock);\n \tspin_lock_init(&cma_xprt->sc_frmr_q_lock);\n \tspin_lock_init(&cma_xprt->sc_ctxt_lock);\n-\tspin_lock_init(&cma_xprt->sc_map_lock);\n+\tspin_lock_init(&cma_xprt->sc_rw_ctxt_lock);\n \n \t/*\n \t * Note that this implies that the underlying transport support\n@@ -999,6 +902,7 @@ static struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)\n \t\tnewxprt, newxprt->sc_cm_id);\n \n \tdev = newxprt->sc_cm_id->device;\n+\tnewxprt->sc_port_num = newxprt->sc_cm_id->port_num;\n \n \t/* Qualify the transport resource defaults with the\n \t * capabilities of this particular device */\n@@ -1014,13 +918,11 @@ static struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)\n \t\t\t\t\t    svcrdma_max_bc_requests);\n \tnewxprt->sc_rq_depth = newxprt->sc_max_requests +\n \t\t\t       newxprt->sc_max_bc_requests;\n-\tnewxprt->sc_sq_depth = RPCRDMA_SQ_DEPTH_MULT * newxprt->sc_rq_depth;\n+\tnewxprt->sc_sq_depth = newxprt->sc_rq_depth;\n \tatomic_set(&newxprt->sc_sq_avail, newxprt->sc_sq_depth);\n \n \tif (!svc_rdma_prealloc_ctxts(newxprt))\n \t\tgoto errout;\n-\tif (!svc_rdma_prealloc_maps(newxprt))\n-\t\tgoto errout;\n \n \t/*\n \t * Limit ORD based on client limit, local device limit, and\n@@ -1050,6 +952,8 @@ static struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)\n \tmemset(&qp_attr, 0, sizeof qp_attr);\n \tqp_attr.event_handler = qp_event_handler;\n \tqp_attr.qp_context = &newxprt->sc_xprt;\n+\tqp_attr.port_num = newxprt->sc_cm_id->port_num;\n+\tqp_attr.cap.max_rdma_ctxs = newxprt->sc_max_requests;\n \tqp_attr.cap.max_send_wr = newxprt->sc_sq_depth;\n \tqp_attr.cap.max_recv_wr = newxprt->sc_rq_depth;\n \tqp_attr.cap.max_send_sge = newxprt->sc_max_sge;\n@@ -1248,8 +1152,8 @@ static void __svc_rdma_free(struct work_struct *work)\n \t}\n \n \trdma_dealloc_frmr_q(rdma);\n+\tsvc_rdma_destroy_rw_ctxts(rdma);\n \tsvc_rdma_destroy_ctxts(rdma);\n-\tsvc_rdma_destroy_maps(rdma);\n \n \t/* Destroy the QP if present (not a listener) */\n \tif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 1,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 7,
        "max_directory_depth": 4
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "aa22f4da2a46b484a257d167c67a2adc1b7aaf68",
            "date": "2025-01-26T00:23:38Z",
            "author_login": "torvalds"
          },
          {
            "sha": "eda061cccd146fcbe71051bb4aa5a8672b71216e",
            "date": "2025-01-26T00:19:10Z",
            "author_login": "torvalds"
          },
          {
            "sha": "08de7f9d4d39fd9aa5e747a13acc891214fa2d5f",
            "date": "2025-01-26T00:12:07Z",
            "author_login": "torvalds"
          },
          {
            "sha": "647d69605c70368d54fc012fce8a43e8e5955b04",
            "date": "2025-01-26T00:03:40Z",
            "author_login": "torvalds"
          },
          {
            "sha": "184a0997fb77f4a9527fc867fcd16806776c27ce",
            "date": "2025-01-25T23:59:46Z",
            "author_login": "torvalds"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": null,
    "cvss_vector": null,
    "cwe_id": "CWE-404",
    "description": "The NFSv4 implementation in the Linux kernel through 4.11.1 allows local users to cause a denial of service (resource consumption) by leveraging improper channel callback shutdown when unmounting an NFSv4 filesystem, aka a \"module reference and kernel daemon\" leak.",
    "attack_vector": null,
    "attack_complexity": null
  },
  "temporal_data": {
    "published_date": "2017-05-18T06:29:00.700",
    "last_modified": "2024-11-21T03:35:14.353",
    "fix_date": "2017-05-10T20:29:23Z"
  },
  "references": [
    {
      "url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=c70422f760c120480fee4de6c38804c72aa26bc1",
      "source": "cve@mitre.org",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://www.securityfocus.com/bid/98551",
      "source": "cve@mitre.org",
      "tags": []
    },
    {
      "url": "https://bugzilla.redhat.com/show_bug.cgi?id=1451386",
      "source": "cve@mitre.org",
      "tags": [
        "Issue Tracking",
        "Patch"
      ]
    },
    {
      "url": "https://github.com/torvalds/linux/commit/c70422f760c120480fee4de6c38804c72aa26bc1",
      "source": "cve@mitre.org",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://www.spinics.net/lists/linux-nfs/msg63334.html",
      "source": "cve@mitre.org",
      "tags": [
        "Mailing List",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=c70422f760c120480fee4de6c38804c72aa26bc1",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://www.securityfocus.com/bid/98551",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://bugzilla.redhat.com/show_bug.cgi?id=1451386",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Issue Tracking",
        "Patch"
      ]
    },
    {
      "url": "https://github.com/torvalds/linux/commit/c70422f760c120480fee4de6c38804c72aa26bc1",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://www.spinics.net/lists/linux-nfs/msg63334.html",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Mailing List",
        "Patch",
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T22:58:58.935880",
    "processing_status": "enhanced"
  },
  "repository_context": {
    "name": "linux",
    "owner": "torvalds",
    "created_at": "2011-09-04T22:48:12Z",
    "updated_at": "2025-01-14T12:39:03Z",
    "pushed_at": "2025-01-13T17:27:04Z",
    "size": 5361369,
    "stars": 185823,
    "forks": 54743,
    "open_issues": 437,
    "watchers": 185823,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "C": 1326937103,
      "Assembly": 9568292,
      "Shell": 5072004,
      "Python": 2974128,
      "Makefile": 2713905,
      "Perl": 1253637,
      "Rust": 807711,
      "Roff": 202277,
      "C++": 173382,
      "SmPL": 165946,
      "Yacc": 127472,
      "Lex": 71321,
      "Awk": 69539,
      "Jinja": 30138,
      "UnrealScript": 16848,
      "Gherkin": 10172,
      "M4": 3329,
      "MATLAB": 2482,
      "sed": 2433,
      "Clojure": 2411,
      "XS": 1239,
      "RPC": 962
    },
    "commit_activity": {
      "total_commits_last_year": 46007,
      "avg_commits_per_week": 884.75,
      "days_active_last_year": 359
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": false,
      "has_issues": false,
      "allow_forking": true,
      "is_template": false,
      "license": "other"
    },
    "collected_at": "2025-01-14T12:53:59.486675"
  }
}