{
  "cve_id": "CVE-2018-10861",
  "github_data": {
    "repository": "ceph/ceph",
    "fix_commit": "975528f632f73fbffa3f1fee304e3bbe3296cffc",
    "related_commits": [
      "975528f632f73fbffa3f1fee304e3bbe3296cffc",
      "975528f632f73fbffa3f1fee304e3bbe3296cffc"
    ],
    "patch_url": "https://github.com/ceph/ceph/commit/975528f632f73fbffa3f1fee304e3bbe3296cffc.patch",
    "fix_commit_details": {
      "sha": "975528f632f73fbffa3f1fee304e3bbe3296cffc",
      "commit_date": "2018-07-09T13:29:32Z",
      "author": {
        "login": "liewegas",
        "type": "User",
        "stats": {
          "total_commits": 25876,
          "average_weekly_commits": 29.00896860986547,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 589
        }
      },
      "commit_message": {
        "title": "Merge remote-tracking branch 'private/wip-mon-snap-caps'",
        "length": 56,
        "has_description": false,
        "references_issue": false
      },
      "stats": {
        "total": 486,
        "additions": 464,
        "deletions": 22
      },
      "files": [
        {
          "filename": "qa/workunits/rados/test_pool_access.sh",
          "status": "modified",
          "additions": 92,
          "deletions": 7,
          "patch": "@@ -2,22 +2,107 @@\n \n set -ex\n \n-expect_1()\n+KEYRING=$(mktemp)\n+trap cleanup EXIT ERR HUP INT QUIT\n+\n+cleanup() {\n+    (ceph auth del client.mon_read || true) >/dev/null 2>&1\n+    (ceph auth del client.mon_write || true) >/dev/null 2>&1\n+\n+    rm -f $KEYRING\n+}\n+\n+expect_false()\n {\n-  set -x\n-  set +e\n-  \"$@\"\n-  if [ $? == 1 ]; then return 0; else return 1; fi\n+\tset -x\n+\tif \"$@\"; then return 1; else return 0; fi\n+}\n+\n+create_pool_op() {\n+  ID=$1\n+  POOL=$2\n+\n+  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\n+import rados\n+\n+cluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\n+cluster.connect()\n+cluster.create_pool(\"${POOL}\")\n+EOF\n }\n \n+delete_pool_op() {\n+  ID=$1\n+  POOL=$2\n+\n+  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\n+import rados\n+\n+cluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\n+cluster.connect()\n+cluster.delete_pool(\"${POOL}\")\n+EOF\n+}\n+\n+create_pool_snap_op() {\n+  ID=$1\n+  POOL=$2\n+  SNAP=$3\n+\n+  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\n+import rados\n+\n+cluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\n+cluster.connect()\n+ioctx = cluster.open_ioctx(\"${POOL}\")\n+\n+ioctx.create_snap(\"${SNAP}\")\n+EOF\n+}\n+\n+remove_pool_snap_op() {\n+  ID=$1\n+  POOL=$2\n+  SNAP=$3\n+\n+  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\n+import rados\n+\n+cluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\n+cluster.connect()\n+ioctx = cluster.open_ioctx(\"${POOL}\")\n+\n+ioctx.remove_snap(\"${SNAP}\")\n+EOF\n+}\n+\n+test_pool_op()\n+{\n+    ceph auth get-or-create client.mon_read mon 'allow r' >> $KEYRING\n+    ceph auth get-or-create client.mon_write mon 'allow *' >> $KEYRING\n+\n+    expect_false create_pool_op mon_read pool1\n+    create_pool_op mon_write pool1\n+\n+    expect_false create_pool_snap_op mon_read pool1 snap1\n+    create_pool_snap_op mon_write pool1 snap1\n+\n+    expect_false remove_pool_snap_op mon_read pool1 snap1\n+    remove_pool_snap_op mon_write pool1 snap1\n+\n+    expect_false delete_pool_op mon_read pool1\n+    delete_pool_op mon_write pool1\n+}\n \n key=`ceph auth get-or-create-key client.poolaccess1 mon 'allow r' osd 'allow *'`\n rados --id poolaccess1 --key $key -p rbd ls\n \n key=`ceph auth get-or-create-key client.poolaccess2 mon 'allow r' osd 'allow * pool=nopool'`\n-expect_1 rados --id poolaccess2 --key $key -p rbd ls\n+expect_false rados --id poolaccess2 --key $key -p rbd ls\n \n key=`ceph auth get-or-create-key client.poolaccess3 mon 'allow r' osd 'allow rw pool=nopool'`\n-expect_1 rados --id poolaccess3 --key $key -p rbd ls\n+expect_false rados --id poolaccess3 --key $key -p rbd ls\n+\n+test_pool_op\n \n echo OK"
        },
        {
          "filename": "qa/workunits/rbd/permissions.sh",
          "status": "modified",
          "additions": 92,
          "deletions": 0,
          "patch": "@@ -29,11 +29,27 @@ recreate_pools() {\n delete_users() {\n     (ceph auth del client.volumes || true) >/dev/null 2>&1\n     (ceph auth del client.images || true) >/dev/null 2>&1\n+\n+    (ceph auth del client.snap_none || true) >/dev/null 2>&1\n+    (ceph auth del client.snap_all || true) >/dev/null 2>&1\n+    (ceph auth del client.snap_pool || true) >/dev/null 2>&1\n+    (ceph auth del client.snap_profile_all || true) >/dev/null 2>&1\n+    (ceph auth del client.snap_profile_pool || true) >/dev/null 2>&1\n+\n+    (ceph auth del client.mon_write || true) >/dev/null 2>&1\n }\n \n create_users() {\n     ceph auth get-or-create client.volumes mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd-read-only pool=images' >> $KEYRING\n     ceph auth get-or-create client.images mon 'profile rbd' osd 'profile rbd pool=images' >> $KEYRING\n+\n+    ceph auth get-or-create client.snap_none mon 'allow r' >> $KEYRING\n+    ceph auth get-or-create client.snap_all mon 'allow r' osd 'allow w' >> $KEYRING\n+    ceph auth get-or-create client.snap_pool mon 'allow r' osd 'allow w pool=images' >> $KEYRING\n+    ceph auth get-or-create client.snap_profile_all mon 'allow r' osd 'profile rbd' >> $KEYRING\n+    ceph auth get-or-create client.snap_profile_pool mon 'allow r' osd 'profile rbd pool=images' >> $KEYRING\n+\n+    ceph auth get-or-create client.mon_write mon 'allow *' >> $KEYRING\n }\n \n expect() {\n@@ -142,9 +158,83 @@ test_volumes_access() {\n     rbd -k $KEYRING --id volumes rm volumes/child\n }\n \n+create_self_managed_snapshot() {\n+  ID=$1\n+  POOL=$2\n+\n+  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\n+import rados\n+\n+cluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\n+cluster.connect()\n+ioctx = cluster.open_ioctx(\"${POOL}\")\n+\n+snap_id = ioctx.create_self_managed_snap()\n+print (\"Created snap id {}\".format(snap_id))\n+EOF\n+}\n+\n+remove_self_managed_snapshot() {\n+  ID=$1\n+  POOL=$2\n+\n+  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\n+import rados\n+\n+cluster1 = rados.Rados(conffile=\"\", rados_id=\"mon_write\")\n+cluster1.connect()\n+ioctx1 = cluster1.open_ioctx(\"${POOL}\")\n+\n+snap_id = ioctx1.create_self_managed_snap()\n+print (\"Created snap id {}\".format(snap_id))\n+\n+cluster2 = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\n+cluster2.connect()\n+ioctx2 = cluster2.open_ioctx(\"${POOL}\")\n+\n+ioctx2.remove_self_managed_snap(snap_id)\n+print (\"Removed snap id {}\".format(snap_id))\n+EOF\n+}\n+\n+test_remove_self_managed_snapshots() {\n+    # Ensure users cannot create self-managed snapshots w/o permissions\n+    expect 1 create_self_managed_snapshot snap_none images\n+    expect 1 create_self_managed_snapshot snap_none volumes\n+\n+    create_self_managed_snapshot snap_all images\n+    create_self_managed_snapshot snap_all volumes\n+\n+    create_self_managed_snapshot snap_pool images\n+    expect 1 create_self_managed_snapshot snap_pool volumes\n+\n+    create_self_managed_snapshot snap_profile_all images\n+    create_self_managed_snapshot snap_profile_all volumes\n+\n+    create_self_managed_snapshot snap_profile_pool images\n+    expect 1 create_self_managed_snapshot snap_profile_pool volumes\n+\n+    # Ensure users cannot delete self-managed snapshots w/o permissions\n+    expect 1 remove_self_managed_snapshot snap_none images\n+    expect 1 remove_self_managed_snapshot snap_none volumes\n+\n+    remove_self_managed_snapshot snap_all images\n+    remove_self_managed_snapshot snap_all volumes\n+\n+    remove_self_managed_snapshot snap_pool images\n+    expect 1 remove_self_managed_snapshot snap_pool volumes\n+\n+    remove_self_managed_snapshot snap_profile_all images\n+    remove_self_managed_snapshot snap_profile_all volumes\n+\n+    remove_self_managed_snapshot snap_profile_pool images\n+    expect 1 remove_self_managed_snapshot snap_profile_pool volumes\n+}\n+\n cleanup() {\n     rm -f $KEYRING\n }\n+\n KEYRING=$(mktemp)\n trap cleanup EXIT ERR HUP INT QUIT\n \n@@ -157,6 +247,8 @@ test_images_access\n recreate_pools\n test_volumes_access\n \n+test_remove_self_managed_snapshots\n+\n delete_pools\n delete_users\n "
        },
        {
          "filename": "src/mon/OSDMonitor.cc",
          "status": "modified",
          "additions": 136,
          "deletions": 15,
          "patch": "@@ -76,6 +76,9 @@\n #include \"include/str_map.h\"\n #include \"include/scope_guard.h\"\n \n+#include \"auth/cephx/CephxKeyServer.h\"\n+#include \"osd/OSDCap.h\"\n+\n #include \"json_spirit/json_spirit_reader.h\"\n \n #include <boost/algorithm/string/predicate.hpp>\n@@ -91,6 +94,87 @@ const uint32_t MAX_POOL_APPLICATIONS = 4;\n const uint32_t MAX_POOL_APPLICATION_KEYS = 64;\n const uint32_t MAX_POOL_APPLICATION_LENGTH = 128;\n \n+bool is_osd_writable(const OSDCapGrant& grant, const std::string* pool_name) {\n+  // Note: this doesn't include support for the application tag match\n+  if ((grant.spec.allow & OSD_CAP_W) != 0) {\n+    auto& match = grant.match;\n+    if (match.is_match_all()) {\n+      return true;\n+    } else if (pool_name != nullptr && match.auid < 0 &&\n+               !match.pool_namespace.pool_name.empty() &&\n+               match.pool_namespace.pool_name == *pool_name) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool is_unmanaged_snap_op_permitted(CephContext* cct,\n+                                    const KeyServer& key_server,\n+                                    const EntityName& entity_name,\n+                                    const MonCap& mon_caps,\n+                                    const std::string* pool_name)\n+{\n+  typedef std::map<std::string, std::string> CommandArgs;\n+\n+  if (mon_caps.is_capable(cct, CEPH_ENTITY_TYPE_MON,\n+                               entity_name, \"osd\",\n+                               \"osd pool op unmanaged-snap\",\n+                               (pool_name == nullptr ?\n+                                  CommandArgs{} /* pool DNE, require unrestricted cap */ :\n+                                  CommandArgs{{\"poolname\", *pool_name}}),\n+                                false, true, false)) {\n+    return true;\n+  }\n+\n+  AuthCapsInfo caps_info;\n+  if (!key_server.get_service_caps(entity_name, CEPH_ENTITY_TYPE_OSD,\n+                                   caps_info)) {\n+    dout(10) << \"unable to locate OSD cap data for \" << entity_name\n+             << \" in auth db\" << dendl;\n+    return false;\n+  }\n+\n+  string caps_str;\n+  if (caps_info.caps.length() > 0) {\n+    auto p = caps_info.caps.cbegin();\n+    try {\n+      decode(caps_str, p);\n+    } catch (const buffer::error &err) {\n+      derr << \"corrupt OSD cap data for \" << entity_name << \" in auth db\"\n+           << dendl;\n+      return false;\n+    }\n+  }\n+\n+  OSDCap osd_cap;\n+  if (!osd_cap.parse(caps_str, nullptr)) {\n+    dout(10) << \"unable to parse OSD cap data for \" << entity_name\n+             << \" in auth db\" << dendl;\n+    return false;\n+  }\n+\n+  // if the entity has write permissions in one or all pools, permit\n+  // usage of unmanaged-snapshots\n+  if (osd_cap.allow_all()) {\n+    return true;\n+  }\n+\n+  for (auto& grant : osd_cap.grants) {\n+    if (grant.profile.is_valid()) {\n+      for (auto& profile_grant : grant.profile_grants) {\n+        if (is_osd_writable(profile_grant, pool_name)) {\n+          return true;\n+        }\n+      }\n+    } else if (is_osd_writable(grant, pool_name)) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\n+\n } // anonymous namespace\n \n void LastEpochClean::Lec::report(ps_t ps, epoch_t last_epoch_clean)\n@@ -11707,11 +11791,61 @@ bool OSDMonitor::prepare_command_impl(MonOpRequestRef op,\n   return true;\n }\n \n-bool OSDMonitor::preprocess_pool_op(MonOpRequestRef op) \n+bool OSDMonitor::enforce_pool_op_caps(MonOpRequestRef op)\n {\n   op->mark_osdmon_event(__func__);\n+\n   MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n-  \n+  MonSession *session = m->get_session();\n+  if (!session) {\n+    _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n+    return true;\n+  }\n+\n+  switch (m->op) {\n+  case POOL_OP_CREATE_UNMANAGED_SNAP:\n+  case POOL_OP_DELETE_UNMANAGED_SNAP:\n+    {\n+      const std::string* pool_name = nullptr;\n+      const pg_pool_t *pg_pool = osdmap.get_pg_pool(m->pool);\n+      if (pg_pool != nullptr) {\n+        pool_name = &osdmap.get_pool_name(m->pool);\n+      }\n+\n+      if (!is_unmanaged_snap_op_permitted(cct, mon->key_server,\n+                                          session->entity_name, session->caps,\n+                                          pool_name)) {\n+        dout(0) << \"got unmanaged-snap pool op from entity with insufficient \"\n+                << \"privileges. message: \" << *m  << std::endl\n+                << \"caps: \" << session->caps << dendl;\n+        _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n+        return true;\n+      }\n+    }\n+    break;\n+  default:\n+    if (!session->is_capable(\"osd\", MON_CAP_W)) {\n+      dout(0) << \"got pool op from entity with insufficient privileges. \"\n+              << \"message: \" << *m  << std::endl\n+              << \"caps: \" << session->caps << dendl;\n+      _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n+      return true;\n+    }\n+    break;\n+  }\n+\n+  return false;\n+}\n+\n+bool OSDMonitor::preprocess_pool_op(MonOpRequestRef op)\n+{\n+  op->mark_osdmon_event(__func__);\n+  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n+\n+  if (enforce_pool_op_caps(op)) {\n+    return true;\n+  }\n+\n   if (m->fsid != mon->monmap->fsid) {\n     dout(0) << __func__ << \" drop message on fsid \" << m->fsid\n             << \" != \" << mon->monmap->fsid << \" for \" << *m << dendl;\n@@ -11795,19 +11929,6 @@ bool OSDMonitor::preprocess_pool_op_create(MonOpRequestRef op)\n {\n   op->mark_osdmon_event(__func__);\n   MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n-  MonSession *session = m->get_session();\n-  if (!session) {\n-    _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n-    return true;\n-  }\n-  if (!session->is_capable(\"osd\", MON_CAP_W)) {\n-    dout(5) << \"attempt to create new pool without sufficient auid privileges!\"\n-\t    << \"message: \" << *m  << std::endl\n-\t    << \"caps: \" << session->caps << dendl;\n-    _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n-    return true;\n-  }\n-\n   int64_t pool = osdmap.lookup_pg_pool_name(m->name.c_str());\n   if (pool >= 0) {\n     _pool_op_reply(op, 0, osdmap.get_epoch());"
        },
        {
          "filename": "src/mon/OSDMonitor.h",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -400,6 +400,7 @@ class OSDMonitor : public PaxosService {\n   int _prepare_remove_pool(int64_t pool, ostream *ss, bool no_fake);\n   int _prepare_rename_pool(int64_t pool, string newname);\n \n+  bool enforce_pool_op_caps(MonOpRequestRef op);\n   bool preprocess_pool_op (MonOpRequestRef op);\n   bool preprocess_pool_op_create (MonOpRequestRef op);\n   bool prepare_pool_op (MonOpRequestRef op);"
        },
        {
          "filename": "src/pybind/rados/rados.pyx",
          "status": "modified",
          "additions": 106,
          "deletions": 0,
          "patch": "@@ -235,6 +235,17 @@ cdef extern from \"rados/librados.h\" nogil:\n     int rados_ioctx_snap_list(rados_ioctx_t io, rados_snap_t * snaps, int maxlen)\n     int rados_ioctx_snap_get_stamp(rados_ioctx_t io, rados_snap_t id, time_t * t)\n \n+    int rados_ioctx_selfmanaged_snap_create(rados_ioctx_t io,\n+                                            rados_snap_t *snapid)\n+    int rados_ioctx_selfmanaged_snap_remove(rados_ioctx_t io,\n+                                            rados_snap_t snapid)\n+    int rados_ioctx_selfmanaged_snap_set_write_ctx(rados_ioctx_t io,\n+                                                   rados_snap_t snap_seq,\n+                                                   rados_snap_t *snap,\n+                                                   int num_snaps)\n+    int rados_ioctx_selfmanaged_snap_rollback(rados_ioctx_t io, const char *oid,\n+                                              rados_snap_t snapid)\n+\n     int rados_lock_exclusive(rados_ioctx_t io, const char * oid, const char * name,\n                              const char * cookie, const char * desc,\n                              timeval * duration, uint8_t flags)\n@@ -3182,6 +3193,101 @@ returned %d, but should return zero on success.\" % (self.name, ret))\n         if ret != 0:\n             raise make_ex(ret, \"Failed to rollback %s\" % oid)\n \n+    def create_self_managed_snap(self):\n+        \"\"\"\n+        Creates a self-managed snapshot\n+\n+        :returns: snap id on success\n+\n+        :raises: :class:`Error`\n+        \"\"\"\n+        self.require_ioctx_open()\n+        cdef:\n+            rados_snap_t _snap_id\n+        with nogil:\n+            ret = rados_ioctx_selfmanaged_snap_create(self.io, &_snap_id)\n+        if ret != 0:\n+            raise make_ex(ret, \"Failed to create self-managed snapshot\")\n+        return int(_snap_id)\n+\n+    @requires(('snap_id', int))\n+    def remove_self_managed_snap(self, snap_id):\n+        \"\"\"\n+        Removes a self-managed snapshot\n+\n+        :param snap_id: the name of the snapshot\n+        :type snap_id: int\n+\n+        :raises: :class:`TypeError`\n+        :raises: :class:`Error`\n+        \"\"\"\n+        self.require_ioctx_open()\n+        cdef:\n+            rados_snap_t _snap_id = snap_id\n+        with nogil:\n+            ret = rados_ioctx_selfmanaged_snap_remove(self.io, _snap_id)\n+        if ret != 0:\n+            raise make_ex(ret, \"Failed to remove self-managed snapshot\")\n+\n+    def set_self_managed_snap_write(self, snaps):\n+        \"\"\"\n+        Updates the write context to the specified self-managed\n+        snapshot ids.\n+\n+        :param snaps: all associated self-managed snapshot ids\n+        :type snaps: list\n+\n+        :raises: :class:`TypeError`\n+        :raises: :class:`Error`\n+        \"\"\"\n+        self.require_ioctx_open()\n+        sorted_snaps = []\n+        snap_seq = 0\n+        if snaps:\n+            sorted_snaps = sorted([int(x) for x in snaps], reverse=True)\n+            snap_seq = sorted_snaps[0]\n+\n+        cdef:\n+            rados_snap_t _snap_seq = snap_seq\n+            rados_snap_t *_snaps = NULL\n+            int _num_snaps = len(sorted_snaps)\n+        try:\n+            _snaps = <rados_snap_t *>malloc(_num_snaps * sizeof(rados_snap_t))\n+            for i in range(len(sorted_snaps)):\n+                _snaps[i] = sorted_snaps[i]\n+            with nogil:\n+                ret = rados_ioctx_selfmanaged_snap_set_write_ctx(self.io,\n+                                                                 _snap_seq,\n+                                                                 _snaps,\n+                                                                 _num_snaps)\n+            if ret != 0:\n+                raise make_ex(ret, \"Failed to update snapshot write context\")\n+        finally:\n+            free(_snaps)\n+\n+    @requires(('oid', str_type), ('snap_id', int))\n+    def rollback_self_managed_snap(self, oid, snap_id):\n+        \"\"\"\n+        Rolls an specific object back to a self-managed snapshot revision\n+\n+        :param oid: the name of the object\n+        :type oid: str\n+        :param snap_id: the name of the snapshot\n+        :type snap_id: int\n+\n+        :raises: :class:`TypeError`\n+        :raises: :class:`Error`\n+        \"\"\"\n+        self.require_ioctx_open()\n+        oid = cstr(oid, 'oid')\n+        cdef:\n+            char *_oid = oid\n+            rados_snap_t _snap_id = snap_id\n+        with nogil:\n+            ret = rados_ioctx_selfmanaged_snap_rollback(self.io, _oid, _snap_id)\n+        if ret != 0:\n+            raise make_ex(ret, \"Failed to rollback %s\" % oid)\n+\n     def get_last_version(self):\n         \"\"\"\n         Return the version of the last object read or written to."
        },
        {
          "filename": "src/test/pybind/test_rados.py",
          "status": "modified",
          "additions": 37,
          "deletions": 0,
          "patch": "@@ -1011,6 +1011,43 @@ def test_write(self):\n         eq(self.object.read(3), b'bar')\n         eq(self.object.read(3), b'baz')\n \n+class TestIoCtxSelfManagedSnaps(object):\n+    def setUp(self):\n+        self.rados = Rados(conffile='')\n+        self.rados.connect()\n+        self.rados.create_pool('test_pool')\n+        assert self.rados.pool_exists('test_pool')\n+        self.ioctx = self.rados.open_ioctx('test_pool')\n+\n+    def tearDown(self):\n+        cmd = {\"prefix\":\"osd unset\", \"key\":\"noup\"}\n+        self.rados.mon_command(json.dumps(cmd), b'')\n+        self.ioctx.close()\n+        self.rados.delete_pool('test_pool')\n+        self.rados.shutdown()\n+\n+    def test(self):\n+        # cannot mix-and-match pool and self-managed snapshot mode\n+        self.ioctx.set_self_managed_snap_write([])\n+        self.ioctx.write('abc', b'abc')\n+        snap_id_1 = self.ioctx.create_self_managed_snap()\n+        self.ioctx.set_self_managed_snap_write([snap_id_1])\n+\n+        self.ioctx.write('abc', b'def')\n+        snap_id_2 = self.ioctx.create_self_managed_snap()\n+        self.ioctx.set_self_managed_snap_write([snap_id_1, snap_id_2])\n+\n+        self.ioctx.write('abc', b'ghi')\n+\n+        self.ioctx.rollback_self_managed_snap('abc', snap_id_1)\n+        eq(self.ioctx.read('abc'), b'abc')\n+\n+        self.ioctx.rollback_self_managed_snap('abc', snap_id_2)\n+        eq(self.ioctx.read('abc'), b'def')\n+\n+        self.ioctx.remove_self_managed_snap(snap_id_1)\n+        self.ioctx.remove_self_managed_snap(snap_id_2)\n+\n class TestCommand(object):\n \n     def setUp(self):"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 2,
        "unique_directories": 5,
        "max_directory_depth": 3
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "3fa2f0e9616b24dc8b5c61aaefdb99cded10fb56",
            "date": "2025-01-14T13:14:52Z",
            "author_login": "JonBailey1993"
          },
          {
            "sha": "a0f2a3b82fbcfe1f2593eeee35d93cfe0effa2f0",
            "date": "2025-01-14T09:24:50Z",
            "author_login": "zdover23"
          },
          {
            "sha": "f8ba6c61c2ede07dba2df62fc4801aa2101f378a",
            "date": "2025-01-14T08:25:26Z",
            "author_login": "aclamk"
          },
          {
            "sha": "262221d35fd97c0a5ae4ae44390cce1735c9bd4f",
            "date": "2025-01-14T01:28:46Z",
            "author_login": "cyx1231st"
          },
          {
            "sha": "4a03c75b011ae2021b2dd26981ba6223cacad3b1",
            "date": "2025-01-14T00:54:34Z",
            "author_login": "SrinivasaBharath"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": null,
    "cvss_vector": null,
    "cwe_id": "CWE-285",
    "description": "A flaw was found in the way ceph mon handles user requests. Any authenticated ceph user having read access to ceph can delete, create ceph storage pools and corrupt snapshot images. Ceph branches master, mimic, luminous and jewel are believed to be affected.",
    "attack_vector": null,
    "attack_complexity": null
  },
  "temporal_data": {
    "published_date": "2018-07-10T14:29:00.213",
    "last_modified": "2024-11-21T03:42:09.890",
    "fix_date": "2018-07-09T13:29:32Z"
  },
  "references": [
    {
      "url": "http://lists.opensuse.org/opensuse-security-announce/2019-04/msg00100.html",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://tracker.ceph.com/issues/24838",
      "source": "secalert@redhat.com",
      "tags": [
        "Issue Tracking",
        "Vendor Advisory"
      ]
    },
    {
      "url": "http://www.securityfocus.com/bid/104742",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory",
        "VDB Entry"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2177",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2179",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2261",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2274",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://bugzilla.redhat.com/show_bug.cgi?id=1593308",
      "source": "secalert@redhat.com",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/ceph/ceph/commit/975528f632f73fbffa3f1fee304e3bbe3296cffc",
      "source": "secalert@redhat.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://www.debian.org/security/2018/dsa-4339",
      "source": "secalert@redhat.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://lists.opensuse.org/opensuse-security-announce/2019-04/msg00100.html",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://tracker.ceph.com/issues/24838",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Issue Tracking",
        "Vendor Advisory"
      ]
    },
    {
      "url": "http://www.securityfocus.com/bid/104742",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory",
        "VDB Entry"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2177",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2179",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2261",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://access.redhat.com/errata/RHSA-2018:2274",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://bugzilla.redhat.com/show_bug.cgi?id=1593308",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/ceph/ceph/commit/975528f632f73fbffa3f1fee304e3bbe3296cffc",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://www.debian.org/security/2018/dsa-4339",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T22:59:28.631643",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "ceph",
    "owner": "ceph",
    "created_at": "2011-09-01T21:41:26Z",
    "updated_at": "2025-01-14T13:14:58Z",
    "pushed_at": "2025-01-14T13:14:52Z",
    "size": 812257,
    "stars": 14427,
    "forks": 6048,
    "open_issues": 848,
    "watchers": 14427,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [
      "argonaut",
      "bobtail",
      "cuttlefish"
    ],
    "languages": {
      "C++": 50285765,
      "Python": 12334734,
      "Raku": 3800803,
      "C": 3470858,
      "TypeScript": 3405230,
      "Shell": 2345539,
      "HTML": 881613,
      "CMake": 874020,
      "Terra": 773718,
      "Cython": 630103,
      "Jsonnet": 382417,
      "JavaScript": 141459,
      "Java": 109488,
      "Gherkin": 97105,
      "Perl": 68932,
      "Assembly": 53250,
      "SCSS": 50600,
      "Jinja": 37039,
      "Roff": 26436,
      "PowerShell": 10012,
      "Dockerfile": 9328,
      "Makefile": 4786,
      "Awk": 3196,
      "CSS": 2086,
      "DIGITAL Command Language": 2074,
      "Lua": 1304,
      "SWIG": 951,
      "Turing": 941,
      "Smarty": 173
    },
    "commit_activity": {
      "total_commits_last_year": 6520,
      "avg_commits_per_week": 125.38461538461539,
      "days_active_last_year": 355
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": true,
      "has_issues": false,
      "allow_forking": true,
      "is_template": false,
      "license": "other"
    },
    "collected_at": "2025-01-14T14:03:33.965039"
  }
}