{
  "cve_id": "CVE-2018-7753",
  "github_data": {
    "repository": "mozilla/bleach",
    "fix_commit": "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
    "related_commits": [
      "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
      "c5df5789ec3471a31311f42c2d19fc2cf21b35ef"
    ],
    "patch_url": "https://github.com/mozilla/bleach/commit/c5df5789ec3471a31311f42c2d19fc2cf21b35ef.patch",
    "fix_commit_details": {
      "sha": "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
      "commit_date": "2018-03-05T21:17:29Z",
      "author": {
        "login": "willkg",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Merge pull request #356 from willkg/fix-entities",
        "length": 68,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 232,
        "additions": 210,
        "deletions": 22
      },
      "files": [
        {
          "filename": "bleach/sanitizer.py",
          "status": "modified",
          "additions": 116,
          "deletions": 18,
          "patch": "@@ -4,6 +4,7 @@\n import string\n \n import six\n+from six.moves.urllib.parse import urlparse\n from xml.sax.saxutils import unescape\n \n import html5lib\n@@ -27,8 +28,11 @@\n from bleach.utils import alphabetize_attributes, force_unicode\n \n \n+#: Map of entity name to expanded entity\n+ENTITIES = entities\n+\n #: Trie of html entity string -> character representation\n-ENTITIES_TRIE = Trie(entities)\n+ENTITIES_TRIE = Trie(ENTITIES)\n \n #: List of allowed tags\n ALLOWED_TAGS = [\n@@ -79,13 +83,61 @@\n INVISIBLE_REPLACEMENT_CHAR = '?'\n \n \n+def convert_entity(value):\n+    \"\"\"Convert an entity (minus the & and ; part) into what it represents\n+\n+    This handles numeric, hex, and text entities.\n+\n+    :arg value: the string (minus the ``&`` and ``;`` part) to convert\n+\n+    :returns: unicode character\n+\n+    \"\"\"\n+    if value[0] == '#':\n+        if value[1] in ('x', 'X'):\n+            return six.unichr(int(value[2:], 16))\n+        return six.unichr(int(value[1:], 10))\n+\n+    return ENTITIES[value]\n+\n+\n+def convert_entities(text):\n+    \"\"\"Converts all found entities in the text\n+\n+    :arg text: the text to convert entities in\n+\n+    :returns: unicode text with converted entities\n+\n+    \"\"\"\n+    if '&' not in text:\n+        return text\n+\n+    new_text = []\n+    for part in next_possible_entity(text):\n+        if not part:\n+            continue\n+\n+        if part.startswith('&'):\n+            entity = match_entity(part)\n+            if entity is not None:\n+                new_text.append(convert_entity(entity))\n+                remainder = part[len(entity) + 2:]\n+                if part:\n+                    new_text.append(remainder)\n+                continue\n+\n+        new_text.append(part)\n+\n+    return u''.join(new_text)\n+\n+\n class BleachHTMLTokenizer(HTMLTokenizer):\n     def consumeEntity(self, allowedChar=None, fromAttribute=False):\n         # We don't want to consume and convert entities, so this overrides the\n         # html5lib tokenizer's consumeEntity so that it's now a no-op.\n         #\n         # However, when that gets called, it's consumed an &, so we put that in\n-        # the steam.\n+        # the stream.\n         if fromAttribute:\n             self.currentToken['data'][-1][1] += '&'\n \n@@ -479,15 +531,69 @@ def sanitize_characters(self, token):\n                     new_tokens.append({'type': 'Entity', 'name': entity})\n                     # Length of the entity plus 2--one for & at the beginning\n                     # and and one for ; at the end\n-                    part = part[len(entity) + 2:]\n-                    if part:\n-                        new_tokens.append({'type': 'Characters', 'data': part})\n+                    remainder = part[len(entity) + 2:]\n+                    if remainder:\n+                        new_tokens.append({'type': 'Characters', 'data': remainder})\n                     continue\n \n             new_tokens.append({'type': 'Characters', 'data': part})\n \n         return new_tokens\n \n+    def sanitize_uri_value(self, value, allowed_protocols):\n+        \"\"\"Checks a uri value to see if it's allowed\n+\n+        :arg value: the uri value to sanitize\n+        :arg allowed_protocols: list of allowed protocols\n+\n+        :returns: allowed value or None\n+\n+        \"\"\"\n+        # NOTE(willkg): This transforms the value into one that's easier to\n+        # match and verify, but shouldn't get returned since it's vastly\n+        # different than the original value.\n+\n+        # Convert all character entities in the value\n+        new_value = convert_entities(value)\n+\n+        # Nix backtick, space characters, and control characters\n+        new_value = re.sub(\n+            \"[`\\000-\\040\\177-\\240\\s]+\",\n+            '',\n+            new_value\n+        )\n+\n+        # Remove REPLACEMENT characters\n+        new_value = new_value.replace('\\ufffd', '')\n+\n+        # Lowercase it--this breaks the value, but makes it easier to match\n+        # against\n+        new_value = new_value.lower()\n+\n+        # Drop attributes with uri values that have protocols that aren't\n+        # allowed\n+        parsed = urlparse(new_value)\n+        if parsed.scheme:\n+            # If urlparse found a scheme, check that\n+            if parsed.scheme in allowed_protocols:\n+                return value\n+\n+        else:\n+            # Allow uris that are just an anchor\n+            if new_value.startswith('#'):\n+                return value\n+\n+            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"\n+            if ':' in new_value and new_value.split(':')[0] in allowed_protocols:\n+                return value\n+\n+            # If there's no protocol/scheme specified, then assume it's \"http\"\n+            # and see if that's allowed\n+            if 'http' in allowed_protocols:\n+                return value\n+\n+        return None\n+\n     def allow_token(self, token):\n         \"\"\"Handles the case where we're allowing the tag\"\"\"\n         if 'data' in token:\n@@ -508,21 +614,13 @@ def allow_token(self, token):\n                 if not self.attr_filter(token['name'], name, val):\n                     continue\n \n-                # Look at attributes that have uri values\n+                # Drop attributes with uri values that use a disallowed protocol\n+                # Sanitize attributes with uri values\n                 if namespaced_name in self.attr_val_is_uri:\n-                    val_unescaped = re.sub(\n-                        \"[`\\000-\\040\\177-\\240\\s]+\",\n-                        '',\n-                        unescape(val)).lower()\n-\n-                    # Remove replacement characters from unescaped characters.\n-                    val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")\n-\n-                    # Drop attributes with uri values that have protocols that\n-                    # aren't allowed\n-                    if (re.match(r'^[a-z0-9][-+.a-z0-9]*:', val_unescaped) and\n-                            (val_unescaped.split(':')[0] not in self.allowed_protocols)):\n+                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)\n+                    if new_value is None:\n                         continue\n+                    val = new_value\n \n                 # Drop values in svg attrs with non-local IRIs\n                 if namespaced_name in self.svg_attr_val_allows_ref:"
        },
        {
          "filename": "tests/test_clean.py",
          "status": "modified",
          "additions": 94,
          "deletions": 4,
          "patch": "@@ -213,7 +213,7 @@ def test_nested_script_tag():\n     ('an < entity', 'an &lt; entity'),\n     ('tag < <em>and</em> entity', 'tag &lt; <em>and</em> entity'),\n ])\n-def test_bare_entities(text, expected):\n+def test_bare_entities_get_escaped_correctly(text, expected):\n     assert clean(text) == expected\n \n \n@@ -277,7 +277,7 @@ def test_bare_entities(text, expected):\n     # Verify that clean() doesn't unescape entities.\n     ('&#39;&#34;', '&#39;&#34;'),\n ])\n-def test_character_entities(text, expected):\n+def test_character_entities_handling(text, expected):\n     assert clean(text) == expected\n \n \n@@ -534,10 +534,100 @@ def test_attributes_list():\n \n     # Unspecified protocols are not allowed\n     (\n-        '<a href=\"http://xx.com\">invalid href</a>',\n+        '<a href=\"http://example.com\">invalid href</a>',\n         {'protocols': ['myprotocol']},\n         '<a>invalid href</a>'\n-    )\n+    ),\n+\n+    # Anchors are ok\n+    (\n+        '<a href=\"#example.com\">foo</a>',\n+        {'protocols': []},\n+        '<a href=\"#example.com\">foo</a>'\n+    ),\n+\n+    # Allow implicit http if allowed\n+    (\n+        '<a href=\"example.com\">valid</a>',\n+        {'protocols': ['http']},\n+        '<a href=\"example.com\">valid</a>'\n+    ),\n+    (\n+        '<a href=\"example.com:8000\">valid</a>',\n+        {'protocols': ['http']},\n+        '<a href=\"example.com:8000\">valid</a>'\n+    ),\n+    (\n+        '<a href=\"localhost\">valid</a>',\n+        {'protocols': ['http']},\n+        '<a href=\"localhost\">valid</a>'\n+    ),\n+    (\n+        '<a href=\"localhost:8000\">valid</a>',\n+        {'protocols': ['http']},\n+        '<a href=\"localhost:8000\">valid</a>'\n+    ),\n+    (\n+        '<a href=\"192.168.100.100\">valid</a>',\n+        {'protocols': ['http']},\n+        '<a href=\"192.168.100.100\">valid</a>'\n+    ),\n+    (\n+        '<a href=\"192.168.100.100:8000\">valid</a>',\n+        {'protocols': ['http']},\n+        '<a href=\"192.168.100.100:8000\">valid</a>'\n+    ),\n+\n+    # Disallow implicit http if disallowed\n+    (\n+        '<a href=\"example.com\">foo</a>',\n+        {'protocols': []},\n+        '<a>foo</a>'\n+    ),\n+    (\n+        '<a href=\"example.com:8000\">foo</a>',\n+        {'protocols': []},\n+        '<a>foo</a>'\n+    ),\n+    (\n+        '<a href=\"localhost\">foo</a>',\n+        {'protocols': []},\n+        '<a>foo</a>'\n+    ),\n+    (\n+        '<a href=\"localhost:8000\">foo</a>',\n+        {'protocols': []},\n+        '<a>foo</a>'\n+    ),\n+    (\n+        '<a href=\"192.168.100.100\">foo</a>',\n+        {'protocols': []},\n+        '<a>foo</a>'\n+    ),\n+    (\n+        '<a href=\"192.168.100.100:8000\">foo</a>',\n+        {'protocols': []},\n+        '<a>foo</a>'\n+    ),\n+\n+    # Disallowed protocols with sneaky character entities\n+    (\n+        '<a href=\"javas&#x09;cript:alert(1)\">alert</a>',\n+        {},\n+        '<a>alert</a>'\n+    ),\n+    (\n+        '<a href=\"&#14;javascript:alert(1)\">alert</a>',\n+        {},\n+        '<a>alert</a>'\n+    ),\n+\n+    # Checking the uri should change it at all\n+    (\n+        '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>',\n+        {},\n+        '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>'\n+    ),\n ])\n def test_uri_value_allowed_protocols(data, kwargs, expected):\n     assert clean(data, **kwargs) == expected"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 1,
        "unique_directories": 2,
        "max_directory_depth": 1
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "73871d766de1e33a296eeb4f9faf2451f28bee39",
            "date": "2024-10-29T18:24:12Z",
            "author_login": "willkg"
          },
          {
            "sha": "156c5898b3b20e4a582b4a366c18355ecad477cf",
            "date": "2024-10-28T17:34:17Z",
            "author_login": "willkg"
          },
          {
            "sha": "5a4790716e8b976a838327593baf6c3b9f95612f",
            "date": "2024-10-28T16:52:55Z",
            "author_login": "willkg"
          },
          {
            "sha": "6a2ec5ca371cb666fbd843f14ecf1e93e8f7ea87",
            "date": "2024-10-25T18:17:42Z",
            "author_login": "willkg"
          },
          {
            "sha": "32efc2656bbfb825637fa6a435af11181664fcdf",
            "date": "2024-10-25T18:05:48Z",
            "author_login": "willkg"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": null,
    "cvss_vector": null,
    "cwe_id": "CWE-20",
    "description": "An issue was discovered in Bleach 2.1.x before 2.1.3. Attributes that have URI values weren't properly sanitized if the values contained character entities. Using character entities, it was possible to construct a URI value with a scheme that was not allowed that would slide through unsanitized.",
    "attack_vector": null,
    "attack_complexity": null
  },
  "temporal_data": {
    "published_date": "2018-03-07T23:29:00.273",
    "last_modified": "2024-11-21T04:12:40.100",
    "fix_date": "2018-03-05T21:17:29Z"
  },
  "references": [
    {
      "url": "https://bugs.debian.org/892252",
      "source": "cve@mitre.org",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/mozilla/bleach/commit/c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
      "source": "cve@mitre.org",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/mozilla/bleach/releases/tag/v2.1.3",
      "source": "cve@mitre.org",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://bugs.debian.org/892252",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/mozilla/bleach/commit/c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/mozilla/bleach/releases/tag/v2.1.3",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T22:59:17.546959",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "bleach",
    "owner": "mozilla",
    "created_at": "2010-02-19T01:12:41Z",
    "updated_at": "2025-01-14T02:22:26Z",
    "pushed_at": "2024-10-29T18:31:13Z",
    "size": 1236,
    "stars": 2667,
    "forks": 249,
    "open_issues": 4,
    "watchers": 2667,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [
      "main"
    ],
    "languages": {
      "Python": 691186,
      "HTML": 6908,
      "Shell": 1895,
      "Makefile": 751
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": false,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "other"
    },
    "collected_at": "2025-01-14T16:26:08.130686"
  }
}