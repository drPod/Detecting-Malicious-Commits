{
  "cve_id": "CVE-2021-3842",
  "github_data": {
    "repository": "nltk/nltk",
    "fix_commit": "2a50a3edc9d35f57ae42a921c621edc160877f4d",
    "related_commits": [
      "2a50a3edc9d35f57ae42a921c621edc160877f4d",
      "2a50a3edc9d35f57ae42a921c621edc160877f4d"
    ],
    "patch_url": "https://github.com/nltk/nltk/commit/2a50a3edc9d35f57ae42a921c621edc160877f4d.patch",
    "fix_commit_details": {
      "sha": "2a50a3edc9d35f57ae42a921c621edc160877f4d",
      "commit_date": "2021-12-08T14:19:56Z",
      "author": {
        "login": "tomaarsen",
        "type": "User",
        "stats": {
          "total_commits": 209,
          "average_weekly_commits": 0.1707516339869281,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 56
        }
      },
      "commit_message": {
        "title": "Resolve ReDoS opportunity by fixing incorrectly specified regex (#2906)",
        "length": 71,
        "has_description": false,
        "references_issue": true
      },
      "stats": {
        "total": 40,
        "additions": 20,
        "deletions": 20
      },
      "files": [
        {
          "filename": "nltk/parse/malt.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -32,7 +32,7 @@ def malt_regex_tagger():\n             (r\"\\)$\", \")\"),  # round brackets\n             (r\"\\[$\", \"[\"),\n             (r\"\\]$\", \"]\"),  # square brackets\n-            (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers\n+            (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers\n             (r\"(The|the|A|a|An|an)$\", \"DT\"),  # articles\n             (r\"(He|he|She|she|It|it|I|me|Me|You|you)$\", \"PRP\"),  # pronouns\n             (r\"(His|his|Her|her|Its|its)$\", \"PRP$\"),  # possessive"
        },
        {
          "filename": "nltk/sem/glue.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -703,7 +703,7 @@ def get_pos_tagger(self):\n \n         regexp_tagger = RegexpTagger(\n             [\n-                (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers\n+                (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers\n                 (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles\n                 (r\".*able$\", \"JJ\"),  # adjectives\n                 (r\".*ness$\", \"NN\"),  # nouns formed from adjectives"
        },
        {
          "filename": "nltk/tag/brill.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -329,7 +329,7 @@ def print_train_stats():\n             )\n             print(\n                 \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"\n-                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats)\n+                \"final: {finalerrors:5d} {finalacc:.4f}\".format(**train_stats)\n             )\n             head = \"#ID | Score (train) |  #Rules     | Template\"\n             print(head, \"\\n\", \"-\" * len(head), sep=\"\")"
        },
        {
          "filename": "nltk/tag/brill_trainer.py",
          "status": "modified",
          "additions": 11,
          "deletions": 11,
          "patch": "@@ -91,7 +91,7 @@ def __init__(\n     # Training\n \n     def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n-        \"\"\"\n+        r\"\"\"\n         Trains the Brill tagger on the corpus *train_sents*,\n         producing at most *max_rules* transformations, each of which\n         reduces the net number of errors in the corpus by at least\n@@ -111,7 +111,7 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n         >>> testing_data = [untag(s) for s in gold_data]\n \n         >>> backoff = RegexpTagger([\n-        ... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n+        ... (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n         ... (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n         ... (r'.*able$', 'JJ'),                # adjectives\n         ... (r'.*ness$', 'NN'),                # nouns formed from adjectives\n@@ -125,7 +125,7 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n         >>> baseline = backoff #see NOTE1\n \n         >>> baseline.evaluate(gold_data) #doctest: +ELLIPSIS\n-        0.2450142...\n+        0.2433862...\n \n         >>> # Set up templates\n         >>> Template._cleartemplates() #clear any templates created in earlier tests\n@@ -137,7 +137,7 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n         >>> tagger1 = tt.train(training_data, max_rules=10)\n         TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)\n         Finding initial useful rules...\n-            Found 845 useful rules.\n+            Found 847 useful rules.\n         <BLANKLINE>\n                    B      |\n            S   F   r   O  |        Score = Fixed - Broken\n@@ -150,7 +150,7 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n           85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]\n           69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]\n           51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]\n-          47  63  16 161  | NN->IN if Pos:NNS@[-1]\n+          47  63  16 162  | NN->IN if Pos:NNS@[-1]\n           33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]\n           26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]\n           24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]\n@@ -162,11 +162,11 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n \n         >>> train_stats = tagger1.train_stats()\n         >>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]\n-        [1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]\n+        [1776, 1270, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]\n \n         >>> tagger1.print_template_statistics(printunused=False)\n         TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)\n-        TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750\n+        TRAIN (   2417 tokens) initial  1776 0.2652 final:  1270 0.4746\n         #ID | Score (train) |  #Rules     | Template\n         --------------------------------------------\n         001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))\n@@ -175,7 +175,7 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n         <BLANKLINE>\n \n         >>> tagger1.evaluate(gold_data) # doctest: +ELLIPSIS\n-        0.43996...\n+        0.43833...\n \n         >>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)\n \n@@ -185,13 +185,13 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n         True\n \n         >>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]\n-        [1855, 1376, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]\n+        [1859, 1380, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]\n \n         >>> # A high-accuracy tagger\n         >>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)\n         TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99)\n         Finding initial useful rules...\n-            Found 845 useful rules.\n+            Found 847 useful rules.\n         <BLANKLINE>\n                    B      |\n            S   F   r   O  |        Score = Fixed - Broken\n@@ -212,7 +212,7 @@ def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n           18  18   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]\n \n         >>> tagger2.evaluate(gold_data)  # doctest: +ELLIPSIS\n-        0.44159544...\n+        0.43996743...\n         >>> tagger2.rules()[2:4]\n         (Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))\n "
        },
        {
          "filename": "nltk/tag/sequential.py",
          "status": "modified",
          "additions": 4,
          "deletions": 4,
          "patch": "@@ -337,7 +337,7 @@ class UnigramTagger(NgramTagger):\n         >>> test_sent = brown.sents(categories='news')[0]\n         >>> unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])\n         >>> for tok, tag in unigram_tagger.tag(test_sent):\n-        ...     print(\"({}, {}), \".format(tok, tag))\n+        ...     print(\"({}, {}), \".format(tok, tag)) # doctest: +NORMALIZE_WHITESPACE\n         (The, AT), (Fulton, NP-TL), (County, NN-TL), (Grand, JJ-TL),\n         (Jury, NN-TL), (said, VBD), (Friday, NR), (an, AT),\n         (investigation, NN), (of, IN), (Atlanta's, NP$), (recent, JJ),\n@@ -491,7 +491,7 @@ def context(self, tokens, index, history):\n \n @jsontags.register_tag\n class RegexpTagger(SequentialBackoffTagger):\n-    \"\"\"\n+    r\"\"\"\n     Regular Expression Tagger\n \n     The RegexpTagger assigns tags to tokens by comparing their\n@@ -503,7 +503,7 @@ class RegexpTagger(SequentialBackoffTagger):\n         >>> from nltk.tag import RegexpTagger\n         >>> test_sent = brown.sents(categories='news')[0]\n         >>> regexp_tagger = RegexpTagger(\n-        ...     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n+        ...     [(r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n         ...      (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n         ...      (r'.*able$', 'JJ'),                # adjectives\n         ...      (r'.*ness$', 'NN'),                # nouns formed from adjectives\n@@ -515,7 +515,7 @@ class RegexpTagger(SequentialBackoffTagger):\n         ... ])\n         >>> regexp_tagger\n         <Regexp Tagger: size=9>\n-        >>> regexp_tagger.tag(test_sent)\n+        >>> regexp_tagger.tag(test_sent) # doctest: +NORMALIZE_WHITESPACE\n         [('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'),\n         ('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'),\n         (\"Atlanta's\", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'),"
        },
        {
          "filename": "nltk/tbl/demo.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -393,11 +393,11 @@ def _demo_plot(learning_curve_output, teststats, trainstats=None, take=None):\n     plt.savefig(learning_curve_output)\n \n \n-NN_CD_TAGGER = RegexpTagger([(r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"), (r\".*\", \"NN\")])\n+NN_CD_TAGGER = RegexpTagger([(r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"), (r\".*\", \"NN\")])\n \n REGEXP_TAGGER = RegexpTagger(\n     [\n-        (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers\n+        (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"CD\"),  # cardinal numbers\n         (r\"(The|the|A|a|An|an)$\", \"AT\"),  # articles\n         (r\".*able$\", \"JJ\"),  # adjectives\n         (r\".*ness$\", \"NN\"),  # nouns formed from adjectives"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 4,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "7397ccfed06e7c836d3acb0b9197f6e6b26c6741",
            "date": "2024-11-11T06:24:40Z",
            "author_login": "ekaf"
          },
          {
            "sha": "e7f6724af9fbfee9b8d3bdd636cbd75c57dc9323",
            "date": "2024-10-21T08:33:09Z",
            "author_login": "drewvid"
          },
          {
            "sha": "9a5622f8a5b228df9499cd03181d9f8491e39f17",
            "date": "2024-09-25T08:35:04Z",
            "author_login": "stevenbird"
          },
          {
            "sha": "d1dabecd1fe57035e836d1942e897398c49c42db",
            "date": "2024-09-24T00:50:33Z",
            "author_login": "stevenbird"
          },
          {
            "sha": "1502e55d491d54bcbf5a3e3040ac2415fcda0f6f",
            "date": "2024-09-10T10:47:29Z",
            "author_login": "ekaf"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.5,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H",
    "cwe_id": "CWE-1333",
    "description": "nltk is vulnerable to Inefficient Regular Expression Complexity",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2022-01-04T15:15:07.833",
    "last_modified": "2024-11-21T06:22:36.753",
    "fix_date": "2021-12-08T14:19:56Z"
  },
  "references": [
    {
      "url": "https://github.com/nltk/nltk/commit/2a50a3edc9d35f57ae42a921c621edc160877f4d",
      "source": "security@huntr.dev",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://huntr.dev/bounties/761a761e-2be2-430a-8d92-6f74ffe9866a",
      "source": "security@huntr.dev",
      "tags": [
        "Exploit",
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/commit/2a50a3edc9d35f57ae42a921c621edc160877f4d",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://huntr.dev/bounties/761a761e-2be2-430a-8d92-6f74ffe9866a",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Exploit",
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:02:37.044651",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "nltk",
    "owner": "nltk",
    "created_at": "2009-09-07T10:53:58Z",
    "updated_at": "2025-01-14T09:59:07Z",
    "pushed_at": "2024-11-11T06:24:40Z",
    "size": 353364,
    "stars": 13765,
    "forks": 2907,
    "open_issues": 283,
    "watchers": 13765,
    "has_security_policy": false,
    "default_branch": "develop",
    "protected_branches": [],
    "languages": {
      "Python": 4882494,
      "Jupyter Notebook": 56591,
      "HTML": 24786,
      "Makefile": 7734,
      "Shell": 4707,
      "CSS": 705
    },
    "commit_activity": {
      "total_commits_last_year": 161,
      "avg_commits_per_week": 3.0961538461538463,
      "days_active_last_year": 41
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-14T17:48:59.248251"
  }
}