{
  "cve_id": "CVE-2021-41124",
  "github_data": {
    "repository": "scrapy-plugins/scrapy-splash",
    "fix_commit": "2b253e57fe64ec575079c8cdc99fe2013502ea31",
    "related_commits": [
      "2b253e57fe64ec575079c8cdc99fe2013502ea31",
      "2b253e57fe64ec575079c8cdc99fe2013502ea31"
    ],
    "patch_url": "https://github.com/scrapy-plugins/scrapy-splash/commit/2b253e57fe64ec575079c8cdc99fe2013502ea31.patch",
    "fix_commit_details": {
      "sha": "2b253e57fe64ec575079c8cdc99fe2013502ea31",
      "commit_date": "2021-10-04T14:00:56Z",
      "author": {
        "login": "Gallaecio",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Implement SPLASH_USER and SPLASH_PASS",
        "length": 37,
        "has_description": false,
        "references_issue": false
      },
      "stats": {
        "total": 641,
        "additions": 547,
        "deletions": 94
      },
      "files": [
        {
          "filename": "CHANGES.rst",
          "status": "modified",
          "additions": 13,
          "deletions": 0,
          "patch": "@@ -4,6 +4,19 @@ Changes\n 0.8.0 (2021-10-04)\n ------------------\n \n+*   **Security bug fix:**\n+\n+    If you use :ref:`HttpAuthMiddleware` (i.e. the ``http_user`` and\n+    ``http_pass`` spider attributes) for Splash authentication, any non-Splash\n+    request will expose your credentials to the request target. This includes\n+    ``robots.txt`` requests sent by Scrapy when the ``ROBOTSTXT_OBEY`` setting\n+    is set to ``True``.\n+\n+    Use the new ``SPLASH_USER`` and ``SPLASH_PASS`` settings instead to set\n+    your Splash authentication credentials safely.\n+\n+    .. _HttpAuthMiddleware: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\n+\n *   Responses now expose the HTTP status code and headers from Splash as\n     ``response.splash_response_status`` and\n     ``response.splash_response_headers`` (#158)"
        },
        {
          "filename": "README.rst",
          "status": "modified",
          "additions": 22,
          "deletions": 3,
          "patch": "@@ -582,12 +582,31 @@ on Splash server and is not sent with each request (it requires Splash 2.1+)::\n HTTP Basic Auth\n ===============\n \n-If you need HTTP Basic Authentication to access Splash, use\n-Scrapy's HttpAuthMiddleware_.\n+If you need to use HTTP Basic Authentication to access Splash, use the\n+``SPLASH_USER`` and ``SPLASH_PASS`` optional settings::\n+\n+    SPLASH_USER = 'user'\n+    SPLASH_PASS = 'userpass'\n \n Another option is ``meta['splash']['splash_headers']``: it allows to set\n custom headers which are sent to Splash server; add Authorization header\n-to ``splash_headers`` if HttpAuthMiddleware doesn't fit for some reason.\n+to ``splash_headers`` if you want to change credentials per-request::\n+\n+    import scrapy\n+    from w3lib.http import basic_auth_header\n+\n+    class MySpider(scrapy.Spider):\n+        # ...\n+        def start_requests(self):\n+            auth = basic_auth_header('user', 'userpass')\n+            yield SplashRequest(url, self.parse,\n+                                splash_headers={'Authorization': auth})\n+\n+**WARNING:** Don't use :ref:`HttpAuthMiddleware`\n+(i.e. ``http_user`` / ``http_pass`` spider attributes) for Splash\n+authentication: if you occasionally send a non-Splash request from your spider,\n+you may expose Splash credentials to a remote website, as HttpAuthMiddleware\n+sets credentials for all requests unconditionally.\n \n .. _HttpAuthMiddleware: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\n "
        },
        {
          "filename": "example/scrashtest/settings.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -20,3 +20,4 @@\n # SPLASH_URL = 'http://192.168.59.103:8050/'\n DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\n HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\n+ROBOTSTXT_OBEY = True\n\\ No newline at end of file"
        },
        {
          "filename": "scrapy_splash/middleware.py",
          "status": "modified",
          "additions": 68,
          "deletions": 12,
          "patch": "@@ -10,11 +10,13 @@\n from six.moves.urllib.parse import urljoin\n from six.moves.http_cookiejar import CookieJar\n \n+from w3lib.http import basic_auth_header\n import scrapy\n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http.headers import Headers\n from scrapy.http.response.text import TextResponse\n from scrapy import signals\n+from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware\n \n from scrapy_splash.responsetypes import responsetypes\n from scrapy_splash.cookies import jar_to_har, har_to_jar\n@@ -222,26 +224,34 @@ class SplashMiddleware(object):\n     retry_498_priority_adjust = +50\n     remote_keys_key = '_splash_remote_keys'\n \n-    def __init__(self, crawler, splash_base_url, slot_policy, log_400):\n+    def __init__(self, crawler, splash_base_url, slot_policy, log_400, auth):\n         self.crawler = crawler\n         self.splash_base_url = splash_base_url\n         self.slot_policy = slot_policy\n         self.log_400 = log_400\n         self.crawler.signals.connect(self.spider_opened, signals.spider_opened)\n+        self.auth = auth\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        splash_base_url = crawler.settings.get('SPLASH_URL',\n-                                               cls.default_splash_url)\n-        log_400 = crawler.settings.getbool('SPLASH_LOG_400', True)\n-        slot_policy = crawler.settings.get('SPLASH_SLOT_POLICY',\n-                                           cls.default_policy)\n+        s = crawler.settings\n+        splash_base_url = s.get('SPLASH_URL', cls.default_splash_url)\n+        log_400 = s.getbool('SPLASH_LOG_400', True)\n+        slot_policy = s.get('SPLASH_SLOT_POLICY', cls.default_policy)\n         if slot_policy not in SlotPolicy._known:\n             raise NotConfigured(\"Incorrect slot policy: %r\" % slot_policy)\n \n-        return cls(crawler, splash_base_url, slot_policy, log_400)\n+        splash_user = s.get('SPLASH_USER', '')\n+        splash_pass = s.get('SPLASH_PASS', '')\n+        auth = None\n+        if splash_user or splash_pass:\n+            auth = basic_auth_header(splash_user, splash_pass)\n+        return cls(crawler, splash_base_url, slot_policy, log_400, auth)\n \n     def spider_opened(self, spider):\n+        if _http_auth_enabled(spider):\n+            replace_downloader_middleware(self.crawler, RobotsTxtMiddleware,\n+                                          SafeRobotsTxtMiddleware)\n         if not hasattr(spider, 'state'):\n             spider.state = {}\n \n@@ -260,21 +270,24 @@ def _remote_keys(self):\n     def process_request(self, request, spider):\n         if 'splash' not in request.meta:\n             return\n+        splash_options = request.meta['splash']\n \n         if request.method not in {'GET', 'POST'}:\n-            logger.warning(\n+            logger.error(\n                 \"Currently only GET and POST requests are supported by \"\n-                \"SplashMiddleware; %(request)s will be handled without Splash\",\n+                \"SplashMiddleware; %(request)s is dropped\",\n                 {'request': request},\n                 extra={'spider': spider}\n             )\n-            return request\n+            self.crawler.stats.inc_value('splash/dropped/method/{}'.format(\n+                request.method))\n+            raise IgnoreRequest(\"SplashRequest doesn't support \"\n+                                \"HTTP {} method\".format(request.method))\n \n         if request.meta.get(\"_splash_processed\"):\n             # don't process the same request more than once\n             return\n \n-        splash_options = request.meta['splash']\n         request.meta['_splash_processed'] = True\n \n         slot_policy = splash_options.get('slot_policy', self.slot_policy)\n@@ -319,6 +332,10 @@ def process_request(self, request, spider):\n         if not splash_options.get('dont_send_headers'):\n             headers = scrapy_headers_to_unicode_dict(request.headers)\n             if headers:\n+                # Headers set by HttpAuthMiddleware should be used for Splash,\n+                # not for the remote website (backwards compatibility).\n+                if _http_auth_enabled(spider):\n+                    headers.pop('Authorization', None)\n                 args.setdefault('headers', headers)\n \n         body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)\n@@ -353,6 +370,8 @@ def process_request(self, request, spider):\n         splash_url = urljoin(splash_base_url, endpoint)\n \n         headers = Headers({'Content-Type': 'application/json'})\n+        if self.auth is not None:\n+            headers['Authorization'] = self.auth\n         headers.update(splash_options.get('splash_headers', {}))\n         new_request = request.replace(\n             url=splash_url,\n@@ -361,6 +380,7 @@ def process_request(self, request, spider):\n             headers=headers,\n             priority=request.priority + self.rescheduling_priority_adjust\n         )\n+        new_request.meta['dont_obey_robotstxt'] = True\n         self.crawler.stats.inc_value('splash/%s/request_count' % endpoint)\n         return new_request\n \n@@ -478,3 +498,39 @@ def _get_slot_key(self, request_or_response):\n         return self.crawler.engine.downloader._get_slot_key(\n             request_or_response, None\n         )\n+\n+\n+class SafeRobotsTxtMiddleware(RobotsTxtMiddleware):\n+    def process_request(self, request, spider):\n+        # disable robots.txt for Splash requests\n+        if _http_auth_enabled(spider) and 'splash' in request.meta:\n+            return\n+        return super(SafeRobotsTxtMiddleware, self).process_request(\n+            request, spider)\n+\n+\n+def _http_auth_enabled(spider):\n+    # FIXME: this function should always return False if HttpAuthMiddleware is\n+    # not in a middleware list.\n+    return getattr(spider, 'http_user', '') or getattr(spider, 'http_pass', '')\n+\n+\n+def replace_downloader_middleware(crawler, old_cls, new_cls):\n+    \"\"\" Replace downloader middleware with another one \"\"\"\n+    try:\n+        new_mw = new_cls.from_crawler(crawler)\n+    except NotConfigured:\n+        return\n+\n+    mw_manager = crawler.engine.downloader.middleware\n+    mw_manager.middlewares = tuple([\n+        mw if mw.__class__ is not old_cls else new_mw\n+        for mw in mw_manager.middlewares\n+    ])\n+    for method_name, callbacks in mw_manager.methods.items():\n+        for idx, meth in enumerate(callbacks):\n+            method_cls = meth.__self__.__class__\n+            if method_cls is old_cls:\n+                new_meth = getattr(new_mw, method_name)\n+                # logger.debug(\"{} is replaced with {}\".format(meth, new_meth))\n+                callbacks[idx] = new_meth"
        },
        {
          "filename": "tests/conftest.py",
          "status": "modified",
          "additions": 10,
          "deletions": 3,
          "patch": "@@ -1,11 +1,12 @@\n import os\n \n import pytest\n-from scrapy.settings import Settings\n+from .mockserver import MockServer\n+from .resources import SplashProtected\n \n \n @pytest.fixture()\n-def settings(request):\n+def settings():\n     \"\"\" Default scrapy-splash settings \"\"\"\n     s = dict(\n         # collect scraped items to .collected_items attribute\n@@ -28,6 +29,12 @@ def settings(request):\n         DUPEFILTER_CLASS='scrapy_splash.SplashAwareDupeFilter',\n         HTTPCACHE_STORAGE='scrapy_splash.SplashAwareFSCacheStorage',\n     )\n-    return Settings(s)\n+    return s\n \n \n+@pytest.fixture()\n+def settings_auth(settings):\n+    with MockServer(SplashProtected) as s:\n+        print(\"splash url:\", s.root_url)\n+        settings['SPLASH_URL'] = s.root_url\n+        yield settings"
        },
        {
          "filename": "tests/resources.py",
          "status": "added",
          "additions": 108,
          "deletions": 0,
          "patch": "@@ -0,0 +1,108 @@\n+# -*- coding: utf-8 -*-\n+import os\n+from six.moves.urllib.parse import urlparse\n+\n+from twisted.web.resource import Resource\n+from zope.interface import implementer\n+from twisted.web import resource, guard, proxy\n+from twisted.cred.portal import IRealm, Portal\n+from twisted.cred.checkers import InMemoryUsernamePasswordDatabaseDontUse\n+\n+from scrapy_splash.utils import to_bytes\n+\n+\n+class HtmlResource(Resource):\n+    isLeaf = True\n+    content_type = 'text/html'\n+    html = ''\n+    extra_headers = {}\n+    status_code = 200\n+\n+    def render_GET(self, request):\n+        request.setHeader(b'content-type', to_bytes(self.content_type))\n+        for name, value in self.extra_headers.items():\n+            request.setHeader(to_bytes(name), to_bytes(value))\n+        request.setResponseCode(self.status_code)\n+        return to_bytes(self.html)\n+\n+\n+class HelloWorld(HtmlResource):\n+    html = \"\"\"\n+    <html><body><script>document.write('hello world!');</script></body></html>\n+    \"\"\"\n+    extra_headers = {'X-MyHeader': 'my value', 'Set-Cookie': 'sessionid=ABCD'}\n+\n+\n+class HelloWorldDisallowByRobots(HelloWorld):\n+    \"\"\" Disallow itself via robots.txt \"\"\"\n+    isLeaf = False\n+\n+    def getChild(self, name, request):\n+        if name == b\"robots.txt\":\n+            return self.RobotsTxt()\n+        return self\n+\n+    class RobotsTxt(Resource):\n+        isLeaf = True\n+        def render_GET(self, request):\n+            return b'User-Agent: *\\nDisallow: /\\n'\n+\n+\n+class HelloWorldDisallowAuth(HelloWorldDisallowByRobots):\n+    \"\"\" Disallow itself via robots.txt if a request to robots.txt\n+    contains basic auth header. \"\"\"\n+    class RobotsTxt(HelloWorldDisallowByRobots.RobotsTxt):\n+        def render_GET(self, request):\n+            if request.requestHeaders.hasHeader('Authorization'):\n+                return super(HelloWorldDisallowAuth.RobotsTxt, self).render_GET(request)\n+            request.setResponseCode(404)\n+            return b''\n+\n+\n+class Http400Resource(HtmlResource):\n+    status_code = 400\n+    html = \"Website returns HTTP 400 error\"\n+\n+\n+class ManyCookies(Resource, object):\n+    class SetMyCookie(HtmlResource):\n+        html = \"hello!\"\n+        extra_headers = {'Set-Cookie': 'login=1'}\n+\n+    def __init__(self):\n+        super(ManyCookies, self).__init__()\n+        self.putChild(b'', HelloWorld())\n+        self.putChild(b'login', self.SetMyCookie())\n+\n+\n+def splash_proxy():\n+    splash_url = os.environ.get('SPLASH_URL')\n+    p = urlparse(splash_url)\n+    return lambda: proxy.ReverseProxyResource(p.hostname, int(p.port), b'')\n+\n+\n+def password_protected(resource_cls, username, password):\n+    # Sorry, but this is nuts. A zillion of classes, arbitrary\n+    # unicode / bytes requirements at random places. Is there a simpler\n+    # way to get HTTP Basic Auth working in Twisted?\n+    @implementer(IRealm)\n+    class SimpleRealm(object):\n+        def requestAvatar(self, avatarId, mind, *interfaces):\n+            if resource.IResource in interfaces:\n+                return resource.IResource, resource_cls(), lambda: None\n+            raise NotImplementedError()\n+\n+    creds = {username: password}\n+    checkers = [InMemoryUsernamePasswordDatabaseDontUse(**creds)]\n+    return lambda: guard.HTTPAuthSessionWrapper(\n+        Portal(SimpleRealm(), checkers),\n+        [guard.BasicCredentialFactory(b'example.com')])\n+\n+\n+HelloWorldProtected = password_protected(HelloWorld, 'user', b'userpass')\n+HelloWorldProtected.__name__ = 'HelloWorldProtected'\n+HelloWorldProtected.__module__ = __name__\n+\n+SplashProtected = password_protected(splash_proxy(), 'user', b'userpass')\n+SplashProtected.__name__ = 'SplashProtected'\n+SplashProtected.__module__ = __name__"
        },
        {
          "filename": "tests/test_integration.py",
          "status": "modified",
          "additions": 271,
          "deletions": 42,
          "patch": "@@ -1,11 +1,22 @@\n # -*- coding: utf-8 -*-\n+import pytest\n import scrapy\n+from pkg_resources import parse_version\n from pytest_twisted import inlineCallbacks\n-from twisted.web.resource import Resource\n from w3lib.url import canonicalize_url\n+from w3lib.http import basic_auth_header\n \n from scrapy_splash import SplashRequest\n-from .utils import crawl_items, requires_splash, HtmlResource\n+from .utils import crawl_items, requires_splash\n+from .resources import (\n+    HelloWorld,\n+    Http400Resource,\n+    ManyCookies,\n+    HelloWorldProtected,\n+    HelloWorldDisallowByRobots,\n+    HelloWorldDisallowAuth,\n+)\n+\n \n DEFAULT_SCRIPT = \"\"\"\n function main(splash)\n@@ -16,7 +27,10 @@\n     http_method=splash.args.http_method,\n     body=splash.args.body,\n   }\n-  local wait = tonumber(splash.args.wait or 0.5)  \n+  local wait = 0.01\n+  if splash.args.wait ~= nil then\n+    wait = splash.args.wait\n+  end\n   assert(splash:wait(wait))\n \n   local entries = splash:history()\n@@ -34,49 +48,60 @@\n \"\"\"\n \n \n-class HelloWorld(HtmlResource):\n-    html = \"\"\"\n-    <html><body><script>document.write('hello world!');</script></body></html>\n-    \"\"\"\n-    extra_headers = {'X-MyHeader': 'my value', 'Set-Cookie': 'sessionid=ABCD'}\n+class ResponseSpider(scrapy.Spider):\n+    \"\"\" Make a request to URL, return Scrapy response \"\"\"\n+    custom_settings = {\n+        'HTTPERROR_ALLOW_ALL': True,\n+        'ROBOTSTXT_OBEY': True,\n+    }\n+    url = None\n \n+    def start_requests(self):\n+        yield SplashRequest(self.url)\n \n-class Http400Resource(HtmlResource):\n-    status_code = 400\n-    html = \"Website returns HTTP 400 error\"\n+    def parse(self, response):\n+        yield {'response': response}\n \n \n+class LuaSpider(ResponseSpider):\n+    \"\"\" Make a request to URL using default Lua script \"\"\"\n+    headers = None\n+    splash_headers = None\n \n-class ManyCookies(Resource, object):\n-    class SetMyCookie(HtmlResource):\n-        html = \"hello!\"\n-        extra_headers = {'Set-Cookie': 'login=1'}\n+    def start_requests(self):\n+        yield SplashRequest(self.url,\n+                            endpoint='execute',\n+                            args={'lua_source': DEFAULT_SCRIPT},\n+                            headers=self.headers,\n+                            splash_headers=self.splash_headers)\n \n-    def __init__(self):\n-        super(ManyCookies, self).__init__()\n-        self.putChild(b'', HelloWorld())\n-        self.putChild(b'login', self.SetMyCookie())\n \n+class ScrapyAuthSpider(LuaSpider):\n+    \"\"\" Spider with incorrect (old, insecure) auth method \"\"\"\n+    http_user = 'user'\n+    http_pass = 'userpass'\n \n \n-class ResponseSpider(scrapy.Spider):\n-    \"\"\" Make a request to URL, return Scrapy response \"\"\"\n-    url = None\n+class NonSplashSpider(ResponseSpider):\n+    \"\"\" Spider which uses HTTP auth and doesn't use Splash \"\"\"\n+    http_user = 'user'\n+    http_pass = 'userpass'\n \n     def start_requests(self):\n-        yield SplashRequest(self.url)\n+        yield scrapy.Request(self.url)\n \n-    def parse(self, response):\n-        yield {'response': response}\n+\n+def assert_single_response(items):\n+    assert len(items) == 1\n+    return items[0]['response']\n \n \n @requires_splash\n @inlineCallbacks\n def test_basic(settings):\n     items, url, crawler = yield crawl_items(ResponseSpider, HelloWorld,\n                                             settings)\n-    assert len(items) == 1\n-    resp = items[0]['response']\n+    resp = assert_single_response(items)\n     assert resp.url == url\n     assert resp.css('body::text').extract_first().strip() == \"hello world!\"\n \n@@ -124,8 +149,7 @@ def start_requests(self):\n \n     items, url, crawler = yield crawl_items(LuaScriptSpider, HelloWorld,\n                                             settings)\n-    assert len(items) == 1\n-    resp = items[0]['response']\n+    resp = assert_single_response(items)\n     assert resp.url == url + \"/#foo\"\n     assert resp.status == resp.splash_response_status == 200\n     assert resp.css('body::text').extract_first().strip() == \"hello world!\"\n@@ -140,29 +164,19 @@ def start_requests(self):\n @inlineCallbacks\n def test_bad_request(settings):\n     class BadRequestSpider(ResponseSpider):\n-        custom_settings = {'HTTPERROR_ALLOW_ALL': True}\n-\n         def start_requests(self):\n             yield SplashRequest(self.url, endpoint='execute',\n                                 args={'lua_source': DEFAULT_SCRIPT, 'wait': 'bar'})\n \n-    class GoodRequestSpider(ResponseSpider):\n-        custom_settings = {'HTTPERROR_ALLOW_ALL': True}\n-\n-        def start_requests(self):\n-            yield SplashRequest(self.url, endpoint='execute',\n-                                args={'lua_source': DEFAULT_SCRIPT})\n-\n-\n     items, url, crawler = yield crawl_items(BadRequestSpider, HelloWorld,\n                                             settings)\n-    resp = items[0]['response']\n+    resp = assert_single_response(items)\n     assert resp.status == 400\n     assert resp.splash_response_status == 400\n \n-    items, url, crawler = yield crawl_items(GoodRequestSpider, Http400Resource,\n+    items, url, crawler = yield crawl_items(LuaSpider, Http400Resource,\n                                             settings)\n-    resp = items[0]['response']\n+    resp = assert_single_response(items)\n     assert resp.status == 400\n     assert resp.splash_response_status == 200\n \n@@ -306,3 +320,218 @@ def _cookie_dict(har_cookies):\n         'bomb': BOMB,\n     }\n     assert splash_request_headers.get(b'Cookie') is None\n+\n+\n+@requires_splash\n+@inlineCallbacks\n+def test_access_http_auth(settings):\n+    # website is protected\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorldProtected,\n+                                            settings)\n+    response = assert_single_response(items)\n+    assert response.status == 401\n+    assert response.splash_response_status == 200\n+\n+    # header can be used to access it\n+    AUTH_HEADERS = {'Authorization': basic_auth_header('user', 'userpass')}\n+    kwargs = {'headers': AUTH_HEADERS}\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorldProtected,\n+                                            settings, kwargs)\n+    response = assert_single_response(items)\n+    assert 'hello' in response.body_as_unicode()\n+    assert response.status == 200\n+    assert response.splash_response_status == 200\n+\n+\n+@requires_splash\n+@inlineCallbacks\n+def test_protected_splash_no_auth(settings_auth):\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorld,\n+                                            settings_auth)\n+    response = assert_single_response(items)\n+    assert 'Unauthorized' in response.body_as_unicode()\n+    assert 'hello' not in response.body_as_unicode()\n+    assert response.status == 401\n+    assert response.splash_response_status == 401\n+\n+\n+@requires_splash\n+@inlineCallbacks\n+def test_protected_splash_manual_headers_auth(settings_auth):\n+    AUTH_HEADERS = {'Authorization': basic_auth_header('user', 'userpass')}\n+    kwargs = {'splash_headers': AUTH_HEADERS}\n+\n+    # auth via splash_headers should work\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorld,\n+                                            settings_auth, kwargs)\n+    response = assert_single_response(items)\n+    assert 'hello' in response.body_as_unicode()\n+    assert response.status == 200\n+    assert response.splash_response_status == 200\n+\n+    # but only for Splash, not for a remote website\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorldProtected,\n+                                            settings_auth, kwargs)\n+    response = assert_single_response(items)\n+    assert 'hello' not in response.body_as_unicode()\n+    assert response.status == 401\n+    assert response.splash_response_status == 200\n+\n+\n+@requires_splash\n+@inlineCallbacks\n+def test_protected_splash_settings_auth(settings_auth):\n+    settings_auth['SPLASH_USER'] = 'user'\n+    settings_auth['SPLASH_PASS'] = 'userpass'\n+\n+    # settings works\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorld,\n+                                            settings_auth)\n+    response = assert_single_response(items)\n+    assert 'Unauthorized' not in response.body_as_unicode()\n+    assert 'hello' in response.body_as_unicode()\n+    assert response.status == 200\n+    assert response.splash_response_status == 200\n+\n+    # they can be overridden via splash_headers\n+    bad_auth = {'splash_headers': {'Authorization': 'foo'}}\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorld,\n+                                            settings_auth, bad_auth)\n+    response = assert_single_response(items)\n+    assert response.status == 401\n+    assert response.splash_response_status == 401\n+\n+    # auth error on remote website\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorldProtected,\n+                                            settings_auth)\n+    response = assert_single_response(items)\n+    assert response.status == 401\n+    assert response.splash_response_status == 200\n+\n+    # auth both for Splash and for the remote website\n+    REMOTE_AUTH = {'Authorization': basic_auth_header('user', 'userpass')}\n+    remote_auth_kwargs = {'headers': REMOTE_AUTH}\n+    items, url, crawler = yield crawl_items(LuaSpider, HelloWorldProtected,\n+                                            settings_auth, remote_auth_kwargs)\n+    response = assert_single_response(items)\n+    assert response.status == 200\n+    assert response.splash_response_status == 200\n+    assert 'hello' in response.body_as_unicode()\n+\n+    # enable remote auth, but not splash auth - request should fail\n+    del settings_auth['SPLASH_USER']\n+    del settings_auth['SPLASH_PASS']\n+    items, url, crawler = yield crawl_items(LuaSpider,\n+                                            HelloWorldProtected,\n+                                            settings_auth, remote_auth_kwargs)\n+    response = assert_single_response(items)\n+    assert response.status == 401\n+    assert response.splash_response_status == 401\n+\n+\n+@requires_splash\n+@inlineCallbacks\n+def test_protected_splash_httpauth_middleware(settings_auth):\n+    # httpauth middleware should enable auth for Splash, for backwards\n+    # compatibility reasons\n+    items, url, crawler = yield crawl_items(ScrapyAuthSpider, HelloWorld,\n+                                            settings_auth)\n+    response = assert_single_response(items)\n+    assert 'Unauthorized' not in response.body_as_unicode()\n+    assert 'hello' in response.body_as_unicode()\n+    assert response.status == 200\n+    assert response.splash_response_status == 200\n+\n+    # but not for a remote website\n+    items, url, crawler = yield crawl_items(ScrapyAuthSpider,\n+                                            HelloWorldProtected,\n+                                            settings_auth)\n+    response = assert_single_response(items)\n+    assert 'hello' not in response.body_as_unicode()\n+    assert response.status == 401\n+    assert response.splash_response_status == 200\n+\n+    # headers shouldn't be sent to robots.txt file\n+    items, url, crawler = yield crawl_items(ScrapyAuthSpider,\n+                                            HelloWorldDisallowAuth,\n+                                            settings_auth)\n+    response = assert_single_response(items)\n+    assert 'hello' in response.body_as_unicode()\n+    assert response.status == 200\n+    assert response.splash_response_status == 200\n+\n+    # httpauth shouldn't be disabled for non-Splash requests\n+    items, url, crawler = yield crawl_items(NonSplashSpider,\n+                                            HelloWorldProtected,\n+                                            settings_auth)\n+    response = assert_single_response(items)\n+    assert 'hello' in response.body_as_unicode()\n+    assert response.status == 200\n+    assert not hasattr(response, 'splash_response_status')\n+\n+\n+@pytest.mark.xfail(\n+    parse_version(scrapy.__version__) < parse_version(\"1.1\"),\n+    reason=\"https://github.com/scrapy/scrapy/issues/1471\",\n+    strict=True,\n+    run=True,\n+)\n+@requires_splash\n+@inlineCallbacks\n+def test_robotstxt_can_work(settings_auth):\n+\n+    def assert_robots_disabled(items):\n+        response = assert_single_response(items)\n+        assert response.status == response.splash_response_status == 200\n+        assert b'hello' in response.body\n+\n+    def assert_robots_enabled(items, crawler):\n+        assert len(items) == 0\n+        assert crawler.stats.get_value('downloader/exception_type_count/scrapy.exceptions.IgnoreRequest') == 1\n+\n+    def _crawl_items(spider, resource):\n+        return crawl_items(\n+            spider,\n+            resource,\n+            settings_auth,\n+            url_path='/',  # https://github.com/scrapy/protego/issues/17\n+        )\n+\n+    # when old auth method is used, robots.txt should be disabled\n+    items, url, crawler = yield _crawl_items(ScrapyAuthSpider,\n+                                             HelloWorldDisallowByRobots)\n+    assert_robots_disabled(items)\n+\n+    # but robots.txt should still work for non-Splash requests\n+    items, url, crawler = yield _crawl_items(NonSplashSpider,\n+                                             HelloWorldDisallowByRobots)\n+    assert_robots_enabled(items, crawler)\n+\n+    # robots.txt should work when a proper auth method is used\n+    settings_auth['SPLASH_USER'] = 'user'\n+    settings_auth['SPLASH_PASS'] = 'userpass'\n+    items, url, crawler = yield _crawl_items(LuaSpider,\n+                                             HelloWorldDisallowByRobots)\n+    assert_robots_enabled(items, crawler)\n+\n+    # disable robotstxt middleware - robots middleware shouldn't work\n+    class DontObeyRobotsSpider(LuaSpider):\n+        custom_settings = {\n+            'HTTPERROR_ALLOW_ALL': True,\n+            'ROBOTSTXT_OBEY': False,\n+        }\n+    items, url, crawler = yield _crawl_items(DontObeyRobotsSpider,\n+                                             HelloWorldDisallowByRobots)\n+    assert_robots_disabled(items)\n+\n+    # disable robotstxt middleware via request meta\n+    class MetaDontObeyRobotsSpider(ResponseSpider):\n+        def start_requests(self):\n+            yield SplashRequest(self.url,\n+                                endpoint='execute',\n+                                meta={'dont_obey_robotstxt': True},\n+                                args={'lua_source': DEFAULT_SCRIPT})\n+\n+    items, url, crawler = yield _crawl_items(MetaDontObeyRobotsSpider,\n+                                             HelloWorldDisallowByRobots)\n+    assert_robots_disabled(items)"
        },
        {
          "filename": "tests/test_middleware.py",
          "status": "modified",
          "additions": 42,
          "deletions": 13,
          "patch": "@@ -32,8 +32,8 @@ def _get_crawler(settings_dict):\n     return crawler\n \n \n-def _get_mw():\n-    crawler = _get_crawler({})\n+def _get_mw(settings_dict=None):\n+    crawler = _get_crawler(settings_dict or {})\n     return SplashMiddleware.from_crawler(crawler)\n \n \n@@ -70,6 +70,7 @@ def test_splash_request():\n     # check request preprocessing\n     req2 = cookie_mw.process_request(req, None) or req\n     req2 = mw.process_request(req2, None) or req2\n+\n     assert req2 is not None\n     assert req2 is not req\n     assert req2.url == \"http://127.0.0.1:8050/render.html\"\n@@ -139,7 +140,9 @@ def cb():\n         headers={'X-My-Header': 'value'}\n     )\n     req2 = cookie_mw.process_request(req, None) or req\n-    req2 = mw.process_request(req2, None)\n+    req2 = mw.process_request(req2, None) or req2\n+\n+    assert req2.meta['ajax_crawlable'] is True\n     assert req2.meta['splash'] == {\n         'endpoint': 'execute',\n         'splash_url': \"http://mysplash.example.com\",\n@@ -348,7 +351,7 @@ def test_magic_response2():\n     mw = _get_mw()\n     req = SplashRequest('http://example.com/', magic_response=True,\n                         headers={'foo': 'bar'}, dont_send_headers=True)\n-    req = mw.process_request(req, None)\n+    req = mw.process_request(req, None) or req\n     assert 'headers' not in req.meta['splash']['args']\n \n     resp_data = {\n@@ -372,7 +375,7 @@ def test_unicode_url():\n     req = SplashRequest(\n         # note unicode URL\n         u\"http://example.com/\", endpoint='execute')\n-    req2 = mw.process_request(req, None)\n+    req2 = mw.process_request(req, None) or req\n     res = {'html': '<html><body>Hello</body></html>'}\n     res_body = json.dumps(res)\n     response = TextResponse(\"http://mysplash.example.com/execute\",\n@@ -387,7 +390,7 @@ def test_unicode_url():\n def test_magic_response_http_error():\n     mw = _get_mw()\n     req = SplashRequest('http://example.com/foo')\n-    req = mw.process_request(req, None)\n+    req = mw.process_request(req, None) or req\n \n     resp_data = {\n         \"info\": {\n@@ -414,7 +417,7 @@ def test_magic_response_http_error():\n def test_change_response_class_to_text():\n     mw = _get_mw()\n     req = SplashRequest('http://example.com/', magic_response=True)\n-    req = mw.process_request(req, None)\n+    req = mw.process_request(req, None) or req\n     # Such response can come when downloading a file,\n     # or returning splash:html(): the headers say it's binary,\n     # but it can be decoded so it becomes a TextResponse.\n@@ -437,7 +440,7 @@ def test_change_response_class_to_json_binary():\n     # but this is ok because magic_response presumes we are expecting\n     # a valid splash json response.\n     req = SplashRequest('http://example.com/', magic_response=False)\n-    req = mw.process_request(req, None)\n+    req = mw.process_request(req, None) or req\n     resp = Response('http://mysplash.example.com/execute',\n                     headers={b'Content-Type': b'application/json'},\n                     body=b'non-decodable data: \\x98\\x11\\xe7\\x17\\x8f',\n@@ -474,7 +477,7 @@ def _get_req():\n     # first call\n     req = _get_req()\n     req = cookie_mw.process_request(req, spider) or req\n-    req = mw.process_request(req, spider)\n+    req = mw.process_request(req, spider) or req\n     req = cache_mw.process_request(req, spider) or req\n     assert isinstance(req, scrapy.Request)  # first call; the cache is empty\n \n@@ -498,7 +501,7 @@ def _get_req():\n     # second call\n     req = _get_req()\n     req = cookie_mw.process_request(req, spider) or req\n-    req = mw.process_request(req, spider)\n+    req = mw.process_request(req, spider) or req\n     cached_resp = cache_mw.process_request(req, spider) or req\n \n     # response should be from cache:\n@@ -666,6 +669,7 @@ def test_override_splash_url():\n         }\n     })\n     req = mw.process_request(req1, None)\n+    req = mw.process_request(req, None) or req\n     assert req.url == 'http://splash.example.com/render.png'\n     assert json.loads(to_native_str(req.body)) == {'url': req1.url}\n \n@@ -677,6 +681,7 @@ def test_url_with_fragment():\n         'splash': {'args': {'url': url}}\n     })\n     req = mw.process_request(req, None)\n+    req = mw.process_request(req, None) or req\n     assert json.loads(to_native_str(req.body)) == {'url': url}\n \n \n@@ -685,6 +690,7 @@ def test_splash_request_url_with_fragment():\n     url = \"http://example.com#id1\"\n     req = SplashRequest(url)\n     req = mw.process_request(req, None)\n+    req = mw.process_request(req, None) or req\n     assert json.loads(to_native_str(req.body)) == {'url': url}\n \n \n@@ -740,7 +746,7 @@ def test_slot_policy_per_domain():\n \n def test_slot_policy_scrapy_default():\n     mw = _get_mw()\n-    req = scrapy.Request(\"http://example.com\", meta = {'splash': {\n+    req = scrapy.Request(\"http://example.com\", meta={'splash': {\n         'slot_policy': scrapy_splash.SlotPolicy.SCRAPY_DEFAULT\n     }})\n     req = mw.process_request(req, None)\n@@ -749,7 +755,7 @@ def test_slot_policy_scrapy_default():\n \n def test_adjust_timeout():\n     mw = _get_mw()\n-    req1 = scrapy.Request(\"http://example.com\", meta = {\n+    req1 = scrapy.Request(\"http://example.com\", meta={\n         'splash': {'args': {'timeout': 60, 'html': 1}},\n \n         # download_timeout is always present,\n@@ -759,9 +765,32 @@ def test_adjust_timeout():\n     req1 = mw.process_request(req1, None)\n     assert req1.meta['download_timeout'] > 60\n \n-    req2 = scrapy.Request(\"http://example.com\", meta = {\n+    req2 = scrapy.Request(\"http://example.com\", meta={\n         'splash': {'args': {'html': 1}},\n         'download_timeout': 30,\n     })\n     req2 = mw.process_request(req2, None)\n     assert req2.meta['download_timeout'] == 30\n+\n+\n+def test_auth():\n+    def assert_auth_header(user, pwd, header):\n+        mw = _get_mw({'SPLASH_USER': user, 'SPLASH_PASS': pwd})\n+        req = mw.process_request(SplashRequest(\"http://example.com\"), None)\n+        assert 'Authorization' in req.headers\n+        assert req.headers['Authorization'] == header\n+\n+    def assert_no_auth_header(user, pwd):\n+        if user is not None or pwd is not None:\n+            mw = _get_mw({'SPLASH_USER': user, 'SPLASH_PASS': pwd})\n+        else:\n+            mw = _get_mw()\n+        req = mw.process_request(SplashRequest(\"http://example.com\"), None)\n+        assert 'Authorization' not in req.headers\n+\n+    assert_auth_header('root', '', b'Basic cm9vdDo=')\n+    assert_auth_header('root', 'pwd', b'Basic cm9vdDpwd2Q=')\n+    assert_auth_header('', 'pwd', b'Basic OnB3ZA==')\n+\n+    assert_no_auth_header('', '')\n+    assert_no_auth_header(None, None)\n\\ No newline at end of file"
        },
        {
          "filename": "tests/utils.py",
          "status": "modified",
          "additions": 12,
          "deletions": 21,
          "patch": "@@ -3,11 +3,9 @@\n import pytest\n from pytest_twisted import inlineCallbacks\n from twisted.internet.defer import returnValue\n-from twisted.web.resource import Resource\n from scrapy.crawler import Crawler\n \n-from scrapy_splash.utils import to_bytes\n-from tests.mockserver import MockServer\n+from .mockserver import MockServer\n \n \n requires_splash = pytest.mark.skipif(\n@@ -16,33 +14,26 @@\n )\n \n \n-class HtmlResource(Resource):\n-    isLeaf = True\n-    content_type = 'text/html'\n-    html = ''\n-    extra_headers = {}\n-    status_code = 200\n-\n-    def render_GET(self, request):\n-        request.setHeader(b'content-type', to_bytes(self.content_type))\n-        for name, value in self.extra_headers.items():\n-            request.setHeader(to_bytes(name), to_bytes(value))\n-        request.setResponseCode(self.status_code)\n-        return to_bytes(self.html)\n-\n-\n @inlineCallbacks\n-def crawl_items(spider_cls, resource_cls, settings, spider_kwargs=None):\n+def crawl_items(\n+    spider_cls,\n+    resource_cls,\n+    settings,\n+    spider_kwargs=None,\n+    url_path=\"\",\n+):\n     \"\"\" Use spider_cls to crawl resource_cls. URL of the resource is passed\n     to the spider as ``url`` argument.\n     Return ``(items, resource_url, crawler)`` tuple.\n     \"\"\"\n     spider_kwargs = {} if spider_kwargs is None else spider_kwargs\n     crawler = make_crawler(spider_cls, settings)\n     with MockServer(resource_cls) as s:\n-        root_url = s.root_url\n+        print(\"mock server\", s.root_url)\n+        root_url = s.root_url + url_path\n         yield crawler.crawl(url=root_url, **spider_kwargs)\n-    result = crawler.spider.collected_items, s.root_url, crawler\n+    items = getattr(crawler.spider, 'collected_items', [])\n+    result = items, root_url, crawler\n     returnValue(result)\n \n "
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 1,
        "dependency_files": 0,
        "test_files": 6,
        "unique_directories": 4,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "6a9eb9cd35bc81bebec9719860da5c8a5d2b35bf",
            "date": "2023-02-03T12:06:46Z",
            "author_login": "Gallaecio"
          },
          {
            "sha": "3c6a39a4e4dd5df7a98889f025729350effa3380",
            "date": "2023-02-03T11:51:06Z",
            "author_login": "kmike"
          },
          {
            "sha": "cfb1ded738f8bd0f2eb40ed50ca99e51608c6415",
            "date": "2023-02-03T11:45:26Z",
            "author_login": "Gallaecio"
          },
          {
            "sha": "8e8c6d80a7bb85e87185a2ed4359cac43a946775",
            "date": "2023-02-03T10:18:13Z",
            "author_login": "Gallaecio"
          },
          {
            "sha": "14d976b9e3e0b81d34490f41d4fd10b47774a3a9",
            "date": "2023-02-03T10:14:22Z",
            "author_login": "Gallaecio"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.4,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:N/A:N",
    "cwe_id": "CWE-200",
    "description": "Scrapy-splash is a library which provides Scrapy and JavaScript integration. In affected versions users who use [`HttpAuthMiddleware`](http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth) (i.e. the `http_user` and `http_pass` spider attributes) for Splash authentication will have any non-Splash request expose your credentials to the request target. This includes `robots.txt` requests sent by Scrapy when the `ROBOTSTXT_OBEY` setting is set to `True`. Upgrade to scrapy-splash 0.8.0 and use the new `SPLASH_USER` and `SPLASH_PASS` settings instead to set your Splash authentication credentials safely. If you cannot upgrade, set your Splash request credentials on a per-request basis, [using the `splash_headers` request parameter](https://github.com/scrapy-plugins/scrapy-splash/tree/0.8.x#http-basic-auth), instead of defining them globally using the [`HttpAuthMiddleware`](http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth). Alternatively, make sure all your requests go through Splash. That includes disabling the [robots.txt middleware](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#topics-dlmw-robots).",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2021-10-05T21:15:09.590",
    "last_modified": "2024-11-21T06:25:31.447",
    "fix_date": "2021-10-04T14:00:56Z"
  },
  "references": [
    {
      "url": "https://github.com/scrapy-plugins/scrapy-splash/commit/2b253e57fe64ec575079c8cdc99fe2013502ea31",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy-plugins/scrapy-splash/security/advisories/GHSA-823f-cwm9-4g74",
      "source": "security-advisories@github.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy-plugins/scrapy-splash/commit/2b253e57fe64ec575079c8cdc99fe2013502ea31",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy-plugins/scrapy-splash/security/advisories/GHSA-823f-cwm9-4g74",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:02:07.813556",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "scrapy-splash",
    "owner": "scrapy-plugins",
    "created_at": "2013-05-13T20:40:11Z",
    "updated_at": "2025-01-11T20:11:57Z",
    "pushed_at": "2023-02-06T17:14:19Z",
    "size": 222,
    "stars": 3171,
    "forks": 454,
    "open_issues": 81,
    "watchers": 3171,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "Python": 113662
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "bsd-3-clause"
    },
    "collected_at": "2025-01-14T16:43:40.203985"
  }
}