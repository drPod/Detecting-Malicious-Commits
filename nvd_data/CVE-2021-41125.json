{
  "cve_id": "CVE-2021-41125",
  "github_data": {
    "repository": "scrapy/scrapy",
    "fix_commit": "b01d69a1bf48060daec8f751368622352d8b85a6",
    "related_commits": [
      "b01d69a1bf48060daec8f751368622352d8b85a6",
      "b01d69a1bf48060daec8f751368622352d8b85a6"
    ],
    "patch_url": "https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6.patch",
    "fix_commit_details": {
      "sha": "b01d69a1bf48060daec8f751368622352d8b85a6",
      "commit_date": "2019-08-16T09:53:42Z",
      "author": {
        "login": "wRAR",
        "type": "User",
        "stats": {
          "total_commits": 1018,
          "average_weekly_commits": 1.176878612716763,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 196
        }
      },
      "commit_message": {
        "title": "Add http_auth_domain to HttpAuthMiddleware.",
        "length": 43,
        "has_description": false,
        "references_issue": false
      },
      "stats": {
        "total": 124,
        "additions": 118,
        "deletions": 6
      },
      "files": [
        {
          "filename": "docs/topics/downloader-middleware.rst",
          "status": "modified",
          "additions": 16,
          "deletions": 2,
          "patch": "@@ -315,8 +315,21 @@ HttpAuthMiddleware\n     This middleware authenticates all requests generated from certain spiders\n     using `Basic access authentication`_ (aka. HTTP auth).\n \n-    To enable HTTP authentication from certain spiders, set the ``http_user``\n-    and ``http_pass`` attributes of those spiders.\n+    To enable HTTP authentication for a spider, set the ``http_user`` and\n+    ``http_pass`` spider attributes to the authentication data and the\n+    ``http_auth_domain`` spider attribute to the domain which requires this\n+    authentication (its subdomains will be also handled in the same way).\n+    You can set ``http_auth_domain`` to ``None`` to enable the\n+    authentication for all requests but usually this is not needed.\n+\n+    .. warning::\n+        In the previous Scrapy versions HttpAuthMiddleware sent the\n+        authentication data with all requests, which is a security problem if\n+        the spider makes requests to several different domains. Currently if\n+        the ``http_auth_domain`` attribute is not set, the middleware will use\n+        the domain of the first request, which will work for some spider but\n+        not for others. In the future the middleware will produce an error\n+        instead.\n \n     Example::\n \n@@ -326,6 +339,7 @@ HttpAuthMiddleware\n \n             http_user = 'someuser'\n             http_pass = 'somepass'\n+            http_auth_domain = 'intranet.example.com'\n             name = 'intranet.example.com'\n \n             # .. rest of the spider code omitted ..."
        },
        {
          "filename": "scrapy/downloadermiddlewares/httpauth.py",
          "status": "modified",
          "additions": 20,
          "deletions": 1,
          "patch": "@@ -3,10 +3,14 @@\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n+import warnings\n \n from w3lib.http import basic_auth_header\n \n from scrapy import signals\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.url import url_is_from_any_domain\n \n \n class HttpAuthMiddleware(object):\n@@ -24,8 +28,23 @@ def spider_opened(self, spider):\n         pwd = getattr(spider, 'http_pass', '')\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n+            if not hasattr(spider, 'http_auth_domain'):\n+                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '\n+                              'problems if the spider makes requests to several different domains. http_auth_domain '\n+                              'will be set to the domain of the first request, please set it to the correct value '\n+                              'explicitly.',\n+                              category=ScrapyDeprecationWarning)\n+                self.domain_unset = True\n+            else:\n+                self.domain = spider.http_auth_domain\n+                self.domain_unset = False\n \n     def process_request(self, request, spider):\n         auth = getattr(self, 'auth', None)\n         if auth and b'Authorization' not in request.headers:\n-            request.headers[b'Authorization'] = auth\n+            domain = urlparse_cached(request).hostname\n+            if self.domain_unset:\n+                self.domain = domain\n+                self.domain_unset = False\n+            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n+                request.headers[b'Authorization'] = auth"
        },
        {
          "filename": "tests/test_downloadermiddleware_httpauth.py",
          "status": "modified",
          "additions": 82,
          "deletions": 3,
          "patch": "@@ -1,13 +1,60 @@\n import unittest\n \n+from w3lib.http import basic_auth_header\n+\n from scrapy.http import Request\n from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n from scrapy.spiders import Spider\n \n \n+class TestSpiderLegacy(Spider):\n+    http_user = 'foo'\n+    http_pass = 'bar'\n+\n+\n class TestSpider(Spider):\n     http_user = 'foo'\n     http_pass = 'bar'\n+    http_auth_domain = 'example.com'\n+\n+\n+class TestSpiderAny(Spider):\n+    http_user = 'foo'\n+    http_pass = 'bar'\n+    http_auth_domain = None\n+\n+\n+class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.spider = TestSpiderLegacy('foo')\n+\n+    def test_auth(self):\n+        mw = HttpAuthMiddleware()\n+        mw.spider_opened(self.spider)\n+\n+        # initial request, sets the domain and sends the header\n+        req = Request('http://example.com/')\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+        # subsequent request to the same domain, should send the header\n+        req = Request('http://example.com/')\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+        # subsequent request to a different domain, shouldn't send the header\n+        req = Request('http://example-noauth.com/')\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertNotIn('Authorization', req.headers)\n+\n+    def test_auth_already_set(self):\n+        mw = HttpAuthMiddleware()\n+        mw.spider_opened(self.spider)\n+        req = Request('http://example.com/',\n+                      headers=dict(Authorization='Digest 123'))\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n \n \n class HttpAuthMiddlewareTest(unittest.TestCase):\n@@ -20,13 +67,45 @@ def setUp(self):\n     def tearDown(self):\n         del self.mw\n \n+    def test_no_auth(self):\n+        req = Request('http://example-noauth.com/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertNotIn('Authorization', req.headers)\n+\n+    def test_auth_domain(self):\n+        req = Request('http://example.com/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+    def test_auth_subdomain(self):\n+        req = Request('http://foo.example.com/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+    def test_auth_already_set(self):\n+        req = Request('http://example.com/',\n+                      headers=dict(Authorization='Digest 123'))\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n+\n+\n+class HttpAuthAnyMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.mw = HttpAuthMiddleware()\n+        self.spider = TestSpiderAny('foo')\n+        self.mw.spider_opened(self.spider)\n+\n+    def tearDown(self):\n+        del self.mw\n+\n     def test_auth(self):\n-        req = Request('http://scrapytest.org/')\n+        req = Request('http://example.com/')\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n \n     def test_auth_already_set(self):\n-        req = Request('http://scrapytest.org/',\n+        req = Request('http://example.com/',\n                       headers=dict(Authorization='Digest 123'))\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers['Authorization'], b'Digest 123')"
        }
      ],
      "file_patterns": {
        "security_files": 2,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 1,
        "unique_directories": 3,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "1c1e83895c15dc491c6c133982cde22d778dcae6",
            "date": "2025-01-14T15:40:24Z",
            "author_login": "protokoul"
          },
          {
            "sha": "98ba61256deceba7b04b938a97005258f4ef5c66",
            "date": "2025-01-14T14:36:56Z",
            "author_login": "Gallaecio"
          },
          {
            "sha": "402500b164efc01257679247d3dd1628a5f90f5e",
            "date": "2025-01-10T18:08:27Z",
            "author_login": "ionut-ciubotariu"
          },
          {
            "sha": "1fc91bb46262118c9ff7aa2b4719d880f727699f",
            "date": "2025-01-08T16:28:51Z",
            "author_login": "BurnzZ"
          },
          {
            "sha": "b6d69e389576c137a33d14c9e9319891dce68442",
            "date": "2025-01-07T17:29:31Z",
            "author_login": "Gallaecio"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 5.7,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:L/UI:R/S:U/C:H/I:N/A:N",
    "cwe_id": "CWE-200",
    "description": "Scrapy is a high-level web crawling and scraping framework for Python. If you use `HttpAuthMiddleware` (i.e. the `http_user` and `http_pass` spider attributes) for HTTP authentication, all requests will expose your credentials to the request target. This includes requests generated by Scrapy components, such as `robots.txt` requests sent by Scrapy when the `ROBOTSTXT_OBEY` setting is set to `True`, or as requests reached through redirects. Upgrade to Scrapy 2.5.1 and use the new `http_auth_domain` spider attribute to control which domains are allowed to receive the configured HTTP authentication credentials. If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.5.1 is not an option, you may upgrade to Scrapy 1.8.1 instead. If you cannot upgrade, set your HTTP authentication credentials on a per-request basis, using for example the `w3lib.http.basic_auth_header` function to convert your credentials into a value that you can assign to the `Authorization` header of your request, instead of defining your credentials globally using `HttpAuthMiddleware`.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2021-10-06T18:15:10.953",
    "last_modified": "2024-11-21T06:25:31.600",
    "fix_date": "2019-08-16T09:53:42Z"
  },
  "references": [
    {
      "url": "http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth",
      "source": "security-advisories@github.com",
      "tags": [
        "Vendor Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498",
      "source": "security-advisories@github.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html",
      "source": "security-advisories@github.com",
      "tags": [
        "Mailing List",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header",
      "source": "security-advisories@github.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Vendor Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Mailing List",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:02:07.814862",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "scrapy",
    "owner": "scrapy",
    "created_at": "2010-02-22T02:01:14Z",
    "updated_at": "2025-01-14T12:03:42Z",
    "pushed_at": "2025-01-10T18:08:27Z",
    "size": 26971,
    "stars": 53789,
    "forks": 10611,
    "open_issues": 611,
    "watchers": 53789,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "Python": 2367474,
      "HTML": 3290,
      "Roff": 2010,
      "Shell": 252
    },
    "commit_activity": {
      "total_commits_last_year": 389,
      "avg_commits_per_week": 7.480769230769231,
      "days_active_last_year": 122
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "bsd-3-clause"
    },
    "collected_at": "2025-01-14T13:18:23.407520"
  }
}