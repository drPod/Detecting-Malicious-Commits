{
  "cve_id": "CVE-2021-41131",
  "github_data": {
    "repository": "theupdateframework/python-tuf",
    "fix_commit": "4ad7ae48fda594b640139c3b7eae21ed5155a102",
    "related_commits": [
      "4ad7ae48fda594b640139c3b7eae21ed5155a102",
      "4ad7ae48fda594b640139c3b7eae21ed5155a102"
    ],
    "patch_url": "https://github.com/theupdateframework/python-tuf/commit/4ad7ae48fda594b640139c3b7eae21ed5155a102.patch",
    "fix_commit_details": {
      "sha": "4ad7ae48fda594b640139c3b7eae21ed5155a102",
      "commit_date": "2021-10-19T13:21:29Z",
      "author": {
        "login": "joshuagl",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Merge pull request from GHSA-wjw6-2cqr-j4qr",
        "length": 89,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 660,
        "additions": 424,
        "deletions": 236
      },
      "files": [
        {
          "filename": "tests/repository_data/fishy_rolenames/1.a.json",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": "@@ -0,0 +1,15 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\",\n+   \"sig\": \"6550a087bd0f01648f57e02a275f20c8e38974271d73739c446f53a028c4118e070b1d37224bc022ab6e0500c8051494f276365868ed6039ec49c7ecd8b9f602\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "tests/repository_data/fishy_rolenames/metadata/1...json",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": "@@ -0,0 +1,15 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\",\n+   \"sig\": \"c0266de0724c2ab9c14e679b258033fe3aff8ce3c99419479456170975bb43de9e8539caed437cccc8e6c6068252a921f7badc5384149dab18261a7f157ae406\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "tests/repository_data/fishy_rolenames/metadata/1.root.json",
          "status": "added",
          "additions": 71,
          "deletions": 0,
          "patch": "@@ -0,0 +1,71 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\",\n+   \"sig\": \"1d3b9cebfdab388db500d01cb2cd499f016320029df17bf2f1196d8f83f12d041832dc165f23667e537d8a8aa66c716d19835bd2bcd55d4c18bbbd0c6eaf4b06\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"root\",\n+  \"consistent_snapshot\": true,\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"keys\": {\n+   \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"d98dace51d795525971342b9f7317cea0d743710dca932543fedb92bb083c2c0\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   },\n+   \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"46d386175220afd55ad9b09b6b18fa96cd69e25bc29c97ed7024a522e7e7938c\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   },\n+   \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"a7beb72fb686a645f5ffd52e246a55d2914411853c70a5b47d837ed7b4c40734\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   },\n+   \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\": {\n+    \"keytype\": \"ed25519\",\n+    \"keyval\": {\n+     \"public\": \"efdf10805063c1b7356f40ede43d2c5c6d2d11d79e350887ce96fe5d1e44901a\"\n+    },\n+    \"scheme\": \"ed25519\"\n+   }\n+  },\n+  \"roles\": {\n+   \"root\": {\n+    \"keyids\": [\n+     \"b24fc41c37a5e3c7b504516351633494e462137338182d8f701dc889acbd2eb6\"\n+    ],\n+    \"threshold\": 1\n+   },\n+   \"snapshot\": {\n+    \"keyids\": [\n+     \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\"\n+    ],\n+    \"threshold\": 1\n+   },\n+   \"targets\": {\n+    \"keyids\": [\n+     \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\"\n+    ],\n+    \"threshold\": 1\n+   },\n+   \"timestamp\": {\n+    \"keyids\": [\n+     \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\"\n+    ],\n+    \"threshold\": 1\n+   }\n+  },\n+  \"spec_version\": \"1.0.19\",\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "tests/repository_data/fishy_rolenames/metadata/1.targets.json",
          "status": "added",
          "additions": 75,
          "deletions": 0,
          "patch": "@@ -0,0 +1,75 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"c808865e701882b89c075941ca158034d8c47bde97f1dcdb2afd854334a3ffef\",\n+   \"sig\": \"ffa055ab5108f9d22f309fecd0160b02971d7a454c8d48db4f99cdaf114b329a401b756a11e42630bff6667ad897fb05f501e3299d25fe786d12651cb0db6c06\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"delegations\": {\n+   \"keys\": {\n+    \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\": {\n+     \"keytype\": \"ed25519\",\n+     \"keyval\": {\n+      \"public\": \"45d4d9ee28ef61506695130fe600d637e5f2de0de72473c280b02b89467d7aab\"\n+     },\n+     \"scheme\": \"ed25519\"\n+    },\n+    \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\": {\n+     \"keytype\": \"ed25519\",\n+     \"keyval\": {\n+      \"public\": \"abe021d7594f04467627c2be390c665b311dceb83cceb685edc9b90a6e229d08\"\n+     },\n+     \"scheme\": \"ed25519\"\n+    },\n+    \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\": {\n+     \"keytype\": \"ed25519\",\n+     \"keyval\": {\n+      \"public\": \"52b790190bccf730fad4b769e7073c1551938101483ff8612534eb9105426dce\"\n+     },\n+     \"scheme\": \"ed25519\"\n+    }\n+   },\n+   \"roles\": [\n+    {\n+     \"keyids\": [\n+      \"056a036ef6f15c1dbff1f3d61dfadfc9e92699f6b66a2e21513698b576cc498d\"\n+     ],\n+     \"name\": \"../a\",\n+     \"paths\": [\n+      \"*\"\n+     ],\n+     \"terminating\": false,\n+     \"threshold\": 1\n+    },\n+    {\n+     \"keyids\": [\n+      \"c4f5b1013293e01cedb1680fc3aa670278fd46277c62d0bfa24ffff5f0ad0602\"\n+     ],\n+     \"name\": \".\",\n+     \"paths\": [\n+      \"*\"\n+     ],\n+     \"terminating\": false,\n+     \"threshold\": 1\n+    },\n+    {\n+     \"keyids\": [\n+      \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\"\n+     ],\n+     \"name\": \"\\u00f6\",\n+     \"paths\": [\n+      \"*\"\n+     ],\n+     \"terminating\": false,\n+     \"threshold\": 1\n+    }\n+   ]\n+  },\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "tests/repository_data/fishy_rolenames/metadata/1.\u00f6.json",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": "@@ -0,0 +1,15 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"e38fb1b3a2dea12551541bbb205f09609d9386e147207182c8b900bc0a25e2b8\",\n+   \"sig\": \"854fdccea623c33bf968c7ef5abea6e5e5f7c390a691ae0ae5ad87a7580fc00910b566d5dbdbfcaa948f2d8fe4348eecd5a12710d05f576aecf83fbec32c580b\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"targets\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"spec_version\": \"1.0.19\",\n+  \"targets\": {},\n+  \"version\": 1\n+ }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "tests/repository_data/fishy_rolenames/metadata/2.snapshot.json",
          "status": "added",
          "additions": 28,
          "deletions": 0,
          "patch": "@@ -0,0 +1,28 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"e1f4f87b77838c39ec348fc6e74a10e28272fb6bf3f45bff09cd694148150095\",\n+   \"sig\": \"f00f4b0040dc6879e7ad69867ba611d52bd5e9993cbfd27e6d8073449356c716b4277093c67ae70eba90ab0367a070e69be750284e70e1135615832efda54008\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"snapshot\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"meta\": {\n+   \"../a.json\": {\n+    \"version\": 1\n+   },\n+   \"..json\": {\n+    \"version\": 1\n+   },\n+   \"targets.json\": {\n+    \"version\": 1\n+   },\n+   \"\\u00f6.json\": {\n+    \"version\": 1\n+   }\n+  },\n+  \"spec_version\": \"1.0.19\",\n+  \"version\": 2\n+ }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "tests/repository_data/fishy_rolenames/metadata/timestamp.json",
          "status": "added",
          "additions": 19,
          "deletions": 0,
          "patch": "@@ -0,0 +1,19 @@\n+{\n+ \"signatures\": [\n+  {\n+   \"keyid\": \"965e45aad2af966bafe3719a99152fa34576a07b61742e6501c0b235fd3b8f9c\",\n+   \"sig\": \"5a0040f56454f2f338acb8a81b4c2e170e0bc61219a7cd823f635dfc9faeefcf30dfe9c792f148a25949cc9594f8ac1bfffe436b737eff140d236eba57fe9e08\"\n+  }\n+ ],\n+ \"signed\": {\n+  \"_type\": \"timestamp\",\n+  \"expires\": \"2021-10-22T11:21:56Z\",\n+  \"meta\": {\n+   \"snapshot.json\": {\n+    \"version\": 2\n+   }\n+  },\n+  \"spec_version\": \"1.0.19\",\n+  \"version\": 2\n+ }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "tests/repository_simulator.py",
          "status": "modified",
          "additions": 59,
          "deletions": 22,
          "patch": "@@ -59,6 +59,8 @@\n from tuf.api.serialization.json import JSONSerializer\n from tuf.exceptions import FetcherHTTPError\n from tuf.api.metadata import (\n+    DelegatedRole,\n+    Delegations,\n     Key,\n     Metadata,\n     MetaFile,\n@@ -106,6 +108,9 @@ def __init__(self):\n         self.dump_dir = None\n         self.dump_version = 0\n \n+        now = datetime.utcnow()\n+        self.safe_expiry = now.replace(microsecond=0) + timedelta(days=30)\n+\n         self._initialize()\n \n     @property\n@@ -135,20 +140,19 @@ def create_key(self) -> Tuple[Key, SSlibSigner]:\n \n     def _initialize(self):\n         \"\"\"Setup a minimal valid repository\"\"\"\n-        expiry = datetime.utcnow().replace(microsecond=0) + timedelta(days=30)\n \n-        targets = Targets(1, SPEC_VER, expiry, {}, None)\n+        targets = Targets(1, SPEC_VER, self.safe_expiry, {}, None)\n         self.md_targets = Metadata(targets, OrderedDict())\n \n         meta = {\"targets.json\": MetaFile(targets.version)}\n-        snapshot = Snapshot(1, SPEC_VER, expiry, meta)\n+        snapshot = Snapshot(1, SPEC_VER, self.safe_expiry, meta)\n         self.md_snapshot = Metadata(snapshot, OrderedDict())\n \n         snapshot_meta = MetaFile(snapshot.version)\n-        timestamp = Timestamp(1, SPEC_VER, expiry, snapshot_meta)\n+        timestamp = Timestamp(1, SPEC_VER, self.safe_expiry, snapshot_meta)\n         self.md_timestamp = Metadata(timestamp, OrderedDict())\n \n-        root = Root(1, SPEC_VER, expiry, {}, {}, True)\n+        root = Root(1, SPEC_VER, self.safe_expiry, {}, {}, True)\n         for role in [\"root\", \"timestamp\", \"snapshot\", \"targets\"]:\n             key, signer = self.create_key()\n             root.roles[role] = Role([], 1)\n@@ -172,27 +176,27 @@ def publish_root(self):\n     def fetch(self, url: str) -> Iterator[bytes]:\n         if not self.root.consistent_snapshot:\n             raise NotImplementedError(\"non-consistent snapshot not supported\")\n-\n-        spliturl = parse.urlparse(url)\n-        if spliturl.path.startswith(\"/metadata/\"):\n-            parts = spliturl.path[len(\"/metadata/\") :].split(\".\")\n-            if len(parts) == 3:\n-                version: Optional[int] = int(parts[0])\n-                role = parts[1]\n-            else:\n+        path = parse.urlparse(url).path\n+        if path.startswith(\"/metadata/\") and path.endswith(\".json\"):\n+            ver_and_name = path[len(\"/metadata/\") :][: -len(\".json\")]\n+            # only consistent_snapshot supported ATM: timestamp is special case\n+            if ver_and_name == \"timestamp\":\n                 version = None\n-                role = parts[0]\n+                role = \"timestamp\"\n+            else:\n+                version, _, role = ver_and_name.partition(\".\")\n+                version = int(version)\n             yield self._fetch_metadata(role, version)\n-        elif spliturl.path.startswith(\"/targets/\"):\n+        elif path.startswith(\"/targets/\"):\n             # figure out target path and hash prefix\n-            path = spliturl.path[len(\"/targets/\") :]\n-            dir_parts, sep , prefixed_filename = path.rpartition(\"/\")\n+            target_path = path[len(\"/targets/\") :]\n+            dir_parts, sep , prefixed_filename = target_path.rpartition(\"/\")\n             prefix, _, filename = prefixed_filename.partition(\".\")\n             target_path = f\"{dir_parts}{sep}{filename}\"\n \n             yield self._fetch_target(target_path, prefix)\n         else:\n-            raise FetcherHTTPError(f\"Unknown path '{spliturl.path}'\", 404)\n+            raise FetcherHTTPError(f\"Unknown path '{path}'\", 404)\n \n     def _fetch_target(self, target_path: str, hash: Optional[str]) -> bytes:\n         \"\"\"Return data for 'target_path', checking 'hash' if it is given.\n@@ -268,12 +272,14 @@ def update_timestamp(self):\n \n     def update_snapshot(self):\n         for role, delegate in self.all_targets():\n-            self.snapshot.meta[f\"{role}.json\"].version = delegate.version\n-\n+            hashes = None\n+            length = None\n             if self.compute_metafile_hashes_length:\n                 hashes, length = self._compute_hashes_and_length(role)\n-                self.snapshot.meta[f\"{role}.json\"].hashes = hashes\n-                self.snapshot.meta[f\"{role}.json\"].length = length\n+\n+            self.snapshot.meta[f\"{role}.json\"] = MetaFile(\n+                delegate.version, length, hashes\n+            )\n \n         self.snapshot.version += 1\n         self.update_timestamp()\n@@ -288,6 +294,37 @@ def add_target(self, role: str, data: bytes, path: str):\n         targets.targets[path] = target\n         self.target_files[path] = RepositoryTarget(data, target)\n \n+    def add_delegation(\n+        self,\n+        delegator_name: str,\n+        name: str,\n+        targets: Targets,\n+        terminating: bool,\n+        paths: Optional[List[str]],\n+        hash_prefixes: Optional[List[str]],\n+    ):\n+        if delegator_name == \"targets\":\n+            delegator = self.targets\n+        else:\n+            delegator = self.md_delegates[delegator_name].signed\n+\n+        # Create delegation\n+        role = DelegatedRole(name, [], 1, terminating, paths, hash_prefixes)\n+        if delegator.delegations is None:\n+            delegator.delegations = Delegations({}, {})\n+        # put delegation last by default\n+        delegator.delegations.roles[role.name] = role\n+\n+        # By default add one new key for the role\n+        key, signer = self.create_key()\n+        delegator.add_key(role.name, key)\n+        if role.name not in self.signers:\n+            self.signers[role.name] = []\n+        self.signers[role.name].append(signer)\n+\n+        # Add metadata for the role\n+        self.md_delegates[role.name] = Metadata(targets, OrderedDict())\n+\n     def write(self):\n         \"\"\"Dump current repository metadata to self.dump_dir\n "
        },
        {
          "filename": "tests/test_updater.py",
          "status": "modified",
          "additions": 72,
          "deletions": 68,
          "patch": "@@ -454,74 +454,6 @@ def test_1__refresh_must_not_count_duplicate_keyids_towards_threshold(self):\n           \"Expected a NoWorkingMirrorError composed of one BadSignatureError\")\n \n \n-  def test_1__update_fileinfo(self):\n-      # Tests\n-      # Verify that the 'self.fileinfo' dictionary is empty (its starts off empty\n-      # and is only populated if _update_fileinfo() is called.\n-      fileinfo_dict = self.repository_updater.fileinfo\n-      self.assertEqual(len(fileinfo_dict), 0)\n-\n-      # Load the fileinfo of the top-level root role.  This populates the\n-      # 'self.fileinfo' dictionary.\n-      self.repository_updater._update_fileinfo('root.json')\n-      self.assertEqual(len(fileinfo_dict), 1)\n-      self.assertTrue(tuf.formats.FILEDICT_SCHEMA.matches(fileinfo_dict))\n-      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n-      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n-      root_fileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n-      self.assertTrue('root.json' in fileinfo_dict)\n-      self.assertEqual(fileinfo_dict['root.json'], root_fileinfo)\n-\n-      # Verify that 'self.fileinfo' is incremented if another role is updated.\n-      self.repository_updater._update_fileinfo('targets.json')\n-      self.assertEqual(len(fileinfo_dict), 2)\n-\n-      # Verify that 'self.fileinfo' is inremented if a non-existent role is\n-      # requested, and has its fileinfo entry set to 'None'.\n-      self.repository_updater._update_fileinfo('bad_role.json')\n-      self.assertEqual(len(fileinfo_dict), 3)\n-      self.assertEqual(fileinfo_dict['bad_role.json'], None)\n-\n-\n-\n-\n-  def test_2__fileinfo_has_changed(self):\n-      #  Verify that the method returns 'False' if file info was not changed.\n-      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n-      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n-      root_fileinfo = tuf.formats.make_targets_fileinfo(length, hashes)\n-      self.assertFalse(self.repository_updater._fileinfo_has_changed('root.json',\n-                                                             root_fileinfo))\n-\n-      # Verify that the method returns 'True' if length or hashes were changed.\n-      new_length = 8\n-      new_root_fileinfo = tuf.formats.make_targets_fileinfo(new_length, hashes)\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n-                                                             new_root_fileinfo))\n-      # Hashes were changed.\n-      new_hashes = {'sha256': self.random_string()}\n-      new_root_fileinfo = tuf.formats.make_targets_fileinfo(length, new_hashes)\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n-                                                             new_root_fileinfo))\n-\n-      # Verify that _fileinfo_has_changed() returns True if no fileinfo (or set\n-      # to None) exists for some role.\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('bad.json',\n-          new_root_fileinfo))\n-\n-      saved_fileinfo = self.repository_updater.fileinfo['root.json']\n-      self.repository_updater.fileinfo['root.json'] = None\n-      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n-          new_root_fileinfo))\n-\n-\n-      self.repository_updater.fileinfo['root.json'] = saved_fileinfo\n-      new_root_fileinfo['hashes']['sha666'] = '666'\n-      self.repository_updater._fileinfo_has_changed('root.json',\n-          new_root_fileinfo)\n-\n-\n-\n   def test_2__import_delegations(self):\n     # Setup.\n     # In order to test '_import_delegations' the parent of the delegation\n@@ -639,6 +571,20 @@ def test_2__move_current_to_previous(self):\n     self.repository_updater._move_current_to_previous('snapshot')\n     self.assertTrue(os.path.exists(previous_snapshot_filepath))\n \n+    # assert that non-ascii alphanumeric role name \"../\u00e4\" (that is url encoded\n+    # in local filename) works\n+    encoded_current = os.path.join(\n+      self.client_metadata_current, '..%2F%C3%A4.json'\n+    )\n+    encoded_previous = os.path.join(\n+      self.client_metadata_previous, '..%2F%C3%A4.json'\n+    )\n+\n+    with open(encoded_current, \"w\"):\n+      pass\n+    self.repository_updater._move_current_to_previous('../\u00e4')\n+    self.assertTrue(os.path.exists(encoded_previous))\n+\n \n \n \n@@ -2073,6 +2019,64 @@ def test_get_updater(self):\n     self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n \n \n+class TestUpdaterRolenames(unittest_toolbox.Modified_TestCase):\n+  def setUp(self):\n+    unittest_toolbox.Modified_TestCase.setUp(self)\n+\n+    repo_dir = os.path.join(os.getcwd(), 'repository_data', 'fishy_rolenames')\n+\n+    self.client_dir = self.make_temp_directory()\n+    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"))\n+    os.makedirs(os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"previous\"))\n+    shutil.copy(\n+      os.path.join(repo_dir, 'metadata', '1.root.json'),\n+      os.path.join(self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\", \"root.json\")\n+    )\n+\n+    simple_server_path = os.path.join(os.getcwd(), 'simple_server.py')\n+    self.server_process_handler = utils.TestServerProcess(log=logger,\n+        server=simple_server_path)\n+\n+    url_prefix = 'http://' + utils.TEST_HOST_ADDRESS + ':' \\\n+        + str(self.server_process_handler.port) + \"/repository_data/fishy_rolenames\"\n+\n+    tuf.settings.repositories_directory = self.client_dir\n+    mirrors = {'mirror1': {\n+      'url_prefix': url_prefix,\n+      'metadata_path': 'metadata/',\n+      'targets_path': ''\n+    }}\n+    self.updater = updater.Updater(\"fishy_rolenames\", mirrors)\n+\n+  def tearDown(self):\n+    tuf.roledb.clear_roledb(clear_all=True)\n+    tuf.keydb.clear_keydb(clear_all=True)\n+    self.server_process_handler.flush_log()\n+    self.server_process_handler.clean()\n+    unittest_toolbox.Modified_TestCase.tearDown(self)\n+\n+  def test_unusual_rolenames(self):\n+    \"\"\"Test rolenames that may be tricky to handle as filenames\n+\n+    The test data in repository_data/fishy_rolenames has been produced\n+    semi-manually using RepositorySimulator: using the RepositorySimulator\n+    in these tests directly (like test_updater_with_simulator.py does for\n+    ngclient) might make more sense... but would require some integration work\n+    \"\"\"\n+\n+    # Make a target search that fetches the delegated targets\n+    self.updater.refresh()\n+    with self.assertRaises(tuf.exceptions.UnknownTargetError):\n+      self.updater.get_one_valid_targetinfo(\"anything\")\n+\n+    # Assert that the metadata files are in the client metadata directory\n+    metadata_dir = os.path.join(\n+      self.client_dir, \"fishy_rolenames\", \"metadata\", \"current\"\n+    )\n+    local_metadata = os.listdir(metadata_dir)\n+    for fname in ['%C3%B6.json', '..%2Fa.json', '..json']:\n+      self.assertTrue(fname in local_metadata)\n+\n \n def _load_role_keys(keystore_directory):\n "
        },
        {
          "filename": "tests/test_updater_with_simulator.py",
          "status": "modified",
          "additions": 25,
          "deletions": 0,
          "patch": "@@ -9,6 +9,7 @@\n import os\n import sys\n import tempfile\n+from tuf.api.metadata import SPECIFICATION_VERSION, Targets\n from typing import Optional, Tuple\n from tuf.exceptions import UnsignedMetadataError, BadVersionNumberError\n import unittest\n@@ -125,6 +126,30 @@ def test_targets(self, test_case_data: Tuple[str, bytes, str]):\n \n \n \n+    def test_fishy_rolenames(self):\n+        roles_to_filenames = {\n+            \"../a\": \"..%2Fa.json\",\n+            \"\": \".json\",\n+            \".\": \"..json\",\n+            \"/\": \"%2F.json\",\n+            \"\u00f6\": \"%C3%B6.json\"\n+        }\n+\n+        # Add new delegated targets, update the snapshot\n+        spec_version = \".\".join(SPECIFICATION_VERSION)\n+        targets = Targets(1, spec_version, self.sim.safe_expiry, {}, None)\n+        for role in roles_to_filenames.keys():\n+            self.sim.add_delegation(\"targets\", role, targets, False, [\"*\"], None)\n+        self.sim.update_snapshot()\n+\n+        updater = self._run_refresh()\n+\n+        # trigger updater to fetch the delegated metadata, check filenames\n+        updater.get_one_valid_targetinfo(\"anything\")\n+        local_metadata = os.listdir(self.metadata_dir)\n+        for fname in roles_to_filenames.values():\n+            self.assertTrue(fname in local_metadata)\n+\n     def test_keys_and_signatures(self):\n         \"\"\"Example of the two trickiest test areas: keys and root updates\"\"\"\n "
        },
        {
          "filename": "tuf/client/updater.py",
          "status": "modified",
          "additions": 21,
          "deletions": 136,
          "patch": "@@ -122,6 +122,7 @@\n import copy\n import warnings\n import io\n+from urllib import parse\n \n from securesystemslib import exceptions as sslib_exceptions\n from securesystemslib import formats as sslib_formats\n@@ -781,7 +782,13 @@ def __str__(self):\n     return self.repository_name\n \n \n+  @staticmethod\n+  def _get_local_filename(rolename: str) -> str:\n+    \"\"\"Return safe local filename for roles metadata\n \n+    Use URL encoding to prevent issues with path separators and\n+    with forbidden characters in Windows filesystems\"\"\"\n+    return parse.quote(rolename, '') + '.json'\n \n \n   def _load_metadata_from_file(self, metadata_set, metadata_role):\n@@ -827,7 +834,7 @@ def _load_metadata_from_file(self, metadata_set, metadata_role):\n \n     # Save and construct the full metadata path.\n     metadata_directory = self.metadata_directory[metadata_set]\n-    metadata_filename = metadata_role + '.json'\n+    metadata_filename = self._get_local_filename(metadata_role)\n     metadata_filepath = os.path.join(metadata_directory, metadata_filename)\n \n     # Ensure the metadata path is valid/exists, else ignore the call.\n@@ -1656,10 +1663,6 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n       None.\n     \"\"\"\n \n-    # Construct the metadata filename as expected by the download/mirror\n-    # modules.\n-    metadata_filename = metadata_role + '.json'\n-\n     # Attempt a file download from each mirror until the file is downloaded and\n     # verified.  If the signature of the downloaded file is valid, proceed,\n     # otherwise log a warning and try the next mirror.  'metadata_file_object'\n@@ -1676,7 +1679,11 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n     # best length we can get for it, not request a specific version, but\n     # perform the rest of the checks (e.g., signature verification).\n \n-    remote_filename = metadata_filename\n+    # Construct the metadata filename as expected by the download/mirror\n+    # modules. Local filename is quoted to protect against names like\"../file\".\n+\n+    remote_filename = metadata_role + '.json'\n+    local_filename = self._get_local_filename(metadata_role)\n     filename_version = ''\n \n     if self.consistent_snapshot and version:\n@@ -1693,12 +1700,12 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n     # First, move the 'current' metadata file to the 'previous' directory\n     # if it exists.\n     current_filepath = os.path.join(self.metadata_directory['current'],\n-                metadata_filename)\n+                local_filename)\n     current_filepath = os.path.abspath(current_filepath)\n     sslib_util.ensure_parent_dir(current_filepath)\n \n     previous_filepath = os.path.join(self.metadata_directory['previous'],\n-        metadata_filename)\n+        local_filename)\n     previous_filepath = os.path.abspath(previous_filepath)\n \n     if os.path.exists(current_filepath):\n@@ -1726,7 +1733,7 @@ def _update_metadata(self, metadata_role, upperbound_filelength, version=None):\n     logger.debug('Updated ' + repr(current_filepath) + '.')\n     self.metadata['previous'][metadata_role] = current_metadata_object\n     self.metadata['current'][metadata_role] = updated_metadata_object\n-    self._update_versioninfo(metadata_filename)\n+    self._update_versioninfo(remote_filename)\n \n \n \n@@ -1973,9 +1980,11 @@ def _update_versioninfo(self, metadata_filename):\n     # __init__ (such as with delegated metadata), then get the version\n     # info now.\n \n-    # Save the path to the current metadata file for 'metadata_filename'.\n+    # 'metadata_filename' is the key from meta dictionary: build the\n+    # corresponding local filepath like _get_local_filename()\n+    local_filename = parse.quote(metadata_filename, \"\")\n     current_filepath = os.path.join(self.metadata_directory['current'],\n-        metadata_filename)\n+        local_filename)\n \n     # If the path is invalid, simply return and leave versioninfo unset.\n     if not os.path.exists(current_filepath):\n@@ -2028,130 +2037,6 @@ def _update_versioninfo(self, metadata_filename):\n \n \n \n-\n-  def _fileinfo_has_changed(self, metadata_filename, new_fileinfo):\n-    \"\"\"\n-    <Purpose>\n-      Non-public method that determines whether the current fileinfo of\n-      'metadata_filename' differs from 'new_fileinfo'.  The 'new_fileinfo'\n-      argument should be extracted from the latest copy of the metadata that\n-      references 'metadata_filename'.  Example: 'root.json' would be referenced\n-      by 'snapshot.json'.\n-\n-      'new_fileinfo' should only be 'None' if this is for updating 'root.json'\n-      without having 'snapshot.json' available.\n-\n-    <Arguments>\n-      metadadata_filename:\n-        The metadata filename for the role.  For the 'root' role,\n-        'metadata_filename' would be 'root.json'.\n-\n-      new_fileinfo:\n-        A dict object representing the new file information for\n-        'metadata_filename'.  'new_fileinfo' may be 'None' when\n-        updating 'root' without having 'snapshot' available.  This\n-        dict conforms to 'tuf.formats.TARGETS_FILEINFO_SCHEMA' and has\n-        the form:\n-\n-        {'length': 23423\n-         'hashes': {'sha256': adfbc32343..}}\n-\n-    <Exceptions>\n-      None.\n-\n-    <Side Effects>\n-      If there is no fileinfo currently loaded for 'metada_filename',\n-      try to load it.\n-\n-    <Returns>\n-      Boolean.  True if the fileinfo has changed, false otherwise.\n-    \"\"\"\n-\n-    # If there is no fileinfo currently stored for 'metadata_filename',\n-    # try to load the file, calculate the fileinfo, and store it.\n-    if metadata_filename not in self.fileinfo:\n-      self._update_fileinfo(metadata_filename)\n-\n-    # Return true if there is no fileinfo for 'metadata_filename'.\n-    # 'metadata_filename' is not in the 'self.fileinfo' store\n-    # and it doesn't exist in the 'current' metadata location.\n-    if self.fileinfo[metadata_filename] is None:\n-      return True\n-\n-    current_fileinfo = self.fileinfo[metadata_filename]\n-\n-    if current_fileinfo['length'] != new_fileinfo['length']:\n-      return True\n-\n-    # Now compare hashes. Note that the reason we can't just do a simple\n-    # equality check on the fileinfo dicts is that we want to support the\n-    # case where the hash algorithms listed in the metadata have changed\n-    # without having that result in considering all files as needing to be\n-    # updated, or not all hash algorithms listed can be calculated on the\n-    # specific client.\n-    for algorithm, hash_value in new_fileinfo['hashes'].items():\n-      # We're only looking for a single match. This isn't a security\n-      # check, we just want to prevent unnecessary downloads.\n-      if algorithm in current_fileinfo['hashes']:\n-        if hash_value == current_fileinfo['hashes'][algorithm]:\n-          return False\n-\n-    return True\n-\n-\n-\n-\n-\n-  def _update_fileinfo(self, metadata_filename):\n-    \"\"\"\n-    <Purpose>\n-      Non-public method that updates the 'self.fileinfo' entry for the metadata\n-      belonging to 'metadata_filename'.  If the 'current' metadata for\n-      'metadata_filename' cannot be loaded, set its fileinfo' to 'None' to\n-      signal that it is not in the 'self.fileinfo' AND it also doesn't exist\n-      locally.\n-\n-    <Arguments>\n-      metadata_filename:\n-        The metadata filename for the role.  For the 'root' role,\n-        'metadata_filename' would be 'root.json'.\n-\n-    <Exceptions>\n-      None.\n-\n-    <Side Effects>\n-      The file details of 'metadata_filename' is calculated and\n-      stored in 'self.fileinfo'.\n-\n-    <Returns>\n-      None.\n-    \"\"\"\n-\n-    # In case we delayed loading the metadata and didn't do it in\n-    # __init__ (such as with delegated metadata), then get the file\n-    # info now.\n-\n-    # Save the path to the current metadata file for 'metadata_filename'.\n-    current_filepath = os.path.join(self.metadata_directory['current'],\n-        metadata_filename)\n-\n-    # If the path is invalid, simply return and leave fileinfo unset.\n-    if not os.path.exists(current_filepath):\n-      self.fileinfo[metadata_filename] = None\n-      return\n-\n-    # Extract the file information from the actual file and save it\n-    # to the fileinfo store.\n-    file_length, hashes = sslib_util.get_file_details(current_filepath)\n-    metadata_fileinfo = formats.make_targets_fileinfo(file_length, hashes)\n-    self.fileinfo[metadata_filename] = metadata_fileinfo\n-\n-\n-\n-\n-\n-\n-\n   def _move_current_to_previous(self, metadata_role):\n     \"\"\"\n     <Purpose>\n@@ -2175,7 +2060,7 @@ def _move_current_to_previous(self, metadata_role):\n     \"\"\"\n \n     # Get the 'current' and 'previous' full file paths for 'metadata_role'\n-    metadata_filepath = metadata_role + '.json'\n+    metadata_filepath = self._get_local_filename(metadata_role)\n     previous_filepath = os.path.join(self.metadata_directory['previous'],\n                                      metadata_filepath)\n     current_filepath = os.path.join(self.metadata_directory['current'],"
        },
        {
          "filename": "tuf/ngclient/updater.py",
          "status": "modified",
          "additions": 9,
          "deletions": 10,
          "patch": "@@ -278,28 +278,27 @@ def _download_metadata(\n     ) -> bytes:\n         \"\"\"Download a metadata file and return it as bytes\"\"\"\n         if version is None:\n-            filename = f\"{rolename}.json\"\n+            url = f\"{self._metadata_base_url}{rolename}.json\"\n         else:\n-            filename = f\"{version}.{rolename}.json\"\n-        url = parse.urljoin(self._metadata_base_url, filename)\n+            url = f\"{self._metadata_base_url}{version}.{rolename}.json\"\n         return self._fetcher.download_bytes(url, length)\n \n     def _load_local_metadata(self, rolename: str) -> bytes:\n-        with open(os.path.join(self._dir, f\"{rolename}.json\"), \"rb\") as f:\n+        encoded_name = parse.quote(rolename, \"\")\n+        with open(os.path.join(self._dir, f\"{encoded_name}.json\"), \"rb\") as f:\n             return f.read()\n \n     def _persist_metadata(self, rolename: str, data: bytes) -> None:\n-        \"\"\"Acts as an atomic write operation to make sure\n-        that if the process of writing is halted, at least the\n-        original data is left intact.\n-        \"\"\"\n+        \"\"\"Write metadata to disk atomically to avoid data loss.\"\"\"\n \n-        original_filename = os.path.join(self._dir, f\"{rolename}.json\")\n+        # encode the rolename to avoid issues with e.g. path separators\n+        encoded_name = parse.quote(rolename, \"\")\n+        filename = os.path.join(self._dir, f\"{encoded_name}.json\")\n         with tempfile.NamedTemporaryFile(\n             dir=self._dir, delete=False\n         ) as temp_file:\n             temp_file.write(data)\n-        os.replace(temp_file.name, original_filename)\n+        os.replace(temp_file.name, filename)\n \n     def _load_root(self) -> None:\n         \"\"\"Load remote root metadata."
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 7,
        "dependency_files": 0,
        "test_files": 10,
        "unique_directories": 5,
        "max_directory_depth": 4
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "e49b613cf8d8d47040fdd7161d4896f2d654850e",
            "date": "2025-01-14T08:27:24Z",
            "author_login": "dependabot[bot]"
          },
          {
            "sha": "43221a931a47d85dda8e4c92c764371453f1536b",
            "date": "2025-01-07T08:35:55Z",
            "author_login": "dependabot[bot]"
          },
          {
            "sha": "467e806614ca2b8dd10a52ce46ff547d16219489",
            "date": "2025-01-06T14:25:55Z",
            "author_login": "kairoaraujo"
          },
          {
            "sha": "3c4fcde38a7acb3f55cfb07bfe824edb398562f3",
            "date": "2025-01-06T14:25:31Z",
            "author_login": "kairoaraujo"
          },
          {
            "sha": "83ec7be7cf3b08991c02b85872f3799e9c6342b3",
            "date": "2024-12-11T08:36:00Z",
            "author_login": "jku"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.5,
    "cvss_vector": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:C/C:L/I:H/A:N",
    "cwe_id": "CWE-22",
    "description": "python-tuf is a Python reference implementation of The Update Framework (TUF). In both clients (`tuf/client` and `tuf/ngclient`), there is a path traversal vulnerability that in the worst case can overwrite files ending in `.json` anywhere on the client system on a call to `get_one_valid_targetinfo()`. It occurs because the rolename is used to form the filename, and may contain path traversal characters (ie `../../name.json`). The impact is mitigated by a few facts: It only affects implementations that allow arbitrary rolename selection for delegated targets metadata, The attack requires the ability to A) insert new metadata for the path-traversing role and B) get the role delegated by an existing targets metadata, The written file content is heavily restricted since it needs to be a valid, signed targets file. The file extension is always .json. A fix is available in version 0.19 or newer. There are no workarounds that do not require code changes. Clients can restrict the allowed character set for rolenames, or they can store metadata in files named in a way that is not vulnerable: neither of these approaches is possible without modifying python-tuf.",
    "attack_vector": "NETWORK",
    "attack_complexity": "HIGH"
  },
  "temporal_data": {
    "published_date": "2021-10-19T18:15:07.967",
    "last_modified": "2024-11-21T06:25:32.677",
    "fix_date": "2021-10-19T13:21:29Z"
  },
  "references": [
    {
      "url": "https://github.com/theupdateframework/python-tuf/commit/4ad7ae48fda594b640139c3b7eae21ed5155a102",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/theupdateframework/python-tuf/issues/1527",
      "source": "security-advisories@github.com",
      "tags": [
        "Issue Tracking",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/theupdateframework/python-tuf/security/advisories/GHSA-wjw6-2cqr-j4qr",
      "source": "security-advisories@github.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/theupdateframework/python-tuf/commit/4ad7ae48fda594b640139c3b7eae21ed5155a102",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/theupdateframework/python-tuf/issues/1527",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Issue Tracking",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/theupdateframework/python-tuf/security/advisories/GHSA-wjw6-2cqr-j4qr",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:02:31.838546",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "python-tuf",
    "owner": "theupdateframework",
    "created_at": "2013-01-31T18:36:34Z",
    "updated_at": "2025-01-14T12:27:57Z",
    "pushed_at": "2025-01-14T08:40:58Z",
    "size": 18682,
    "stars": 1636,
    "forks": 275,
    "open_issues": 90,
    "watchers": 1636,
    "has_security_policy": false,
    "default_branch": "develop",
    "protected_branches": [
      "develop",
      "series/3.1"
    ],
    "languages": {
      "Python": 433939
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-14T13:52:51.004287"
  }
}