{
  "cve_id": "CVE-2021-41149",
  "github_data": {
    "repository": "awslabs/tough",
    "fix_commit": "1809b9bd1106d78a51fbea3071aa97a3530bac9a",
    "related_commits": [
      "1809b9bd1106d78a51fbea3071aa97a3530bac9a",
      "1809b9bd1106d78a51fbea3071aa97a3530bac9a"
    ],
    "patch_url": null,
    "fix_commit_details": {
      "sha": "1809b9bd1106d78a51fbea3071aa97a3530bac9a",
      "commit_date": "2021-10-19T14:34:15Z",
      "author": {
        "login": "webern",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Merge pull request from GHSA-x3r5-q6mj-m485",
        "length": 90,
        "has_description": true,
        "references_issue": false
      },
      "stats": {
        "total": 2764,
        "additions": 2418,
        "deletions": 346
      },
      "files": [
        {
          "filename": "Cargo.lock",
          "status": "modified",
          "additions": 25,
          "deletions": 4,
          "patch": "@@ -944,6 +944,24 @@ dependencies = [\n  \"vcpkg\",\n ]\n \n+[[package]]\n+name = \"path-absolutize\"\n+version = \"3.0.11\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"b288298a7a3a7b42539e3181ba590d32f2d91237b0691ed5f103875c754b3bf5\"\n+dependencies = [\n+ \"path-dedot\",\n+]\n+\n+[[package]]\n+name = \"path-dedot\"\n+version = \"3.0.14\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"4bfa72956f6be8524f7f7e2b07972dda393cb0008a6df4451f658b7e1bd1af80\"\n+dependencies = [\n+ \"once_cell\",\n+]\n+\n [[package]]\n name = \"pem\"\n version = \"1.0.0\"\n@@ -1760,7 +1778,7 @@ dependencies = [\n \n [[package]]\n name = \"tough\"\n-version = \"0.11.3\"\n+version = \"0.12.0\"\n dependencies = [\n  \"chrono\",\n  \"dyn-clone\",\n@@ -1769,8 +1787,11 @@ dependencies = [\n  \"hex-literal\",\n  \"httptest\",\n  \"log\",\n+ \"maplit\",\n  \"olpc-cjson\",\n+ \"path-absolutize\",\n  \"pem\",\n+ \"percent-encoding\",\n  \"reqwest\",\n  \"ring\",\n  \"serde\",\n@@ -1785,7 +1806,7 @@ dependencies = [\n \n [[package]]\n name = \"tough-kms\"\n-version = \"0.3.3\"\n+version = \"0.3.4\"\n dependencies = [\n  \"base64\",\n  \"bytes\",\n@@ -1804,7 +1825,7 @@ dependencies = [\n \n [[package]]\n name = \"tough-ssm\"\n-version = \"0.6.3\"\n+version = \"0.6.4\"\n dependencies = [\n  \"rusoto_core\",\n  \"rusoto_credential\",\n@@ -1856,7 +1877,7 @@ checksum = \"59547bce71d9c38b83d9c0e92b6066c4253371f15005def0c30d9657f50c7642\"\n \n [[package]]\n name = \"tuftool\"\n-version = \"0.6.4\"\n+version = \"0.7.0\"\n dependencies = [\n  \"assert_cmd\",\n  \"chrono\","
        },
        {
          "filename": "tough-kms/CHANGELOG.md",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "patch": "@@ -4,6 +4,10 @@ All notable changes to this project will be documented in this file.\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n+## [0.3.4] - 2021-10-19\n+### Changes\n+- Update dependencies.\n+\n ## [0.3.3] - 2021-09-15\n ### Changes\n - Update dependencies.\n@@ -60,6 +64,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n ### Added\n - Everything!\n \n+[0.3.4]: https://github.com/awslabs/tough/compare/tough-kms-v0.3.3...tough-kms-v0.3.4\n [0.3.3]: https://github.com/awslabs/tough/compare/tough-kms-v0.3.2...tough-kms-v0.3.3\n [0.3.2]: https://github.com/awslabs/tough/compare/tough-kms-v0.3.1...tough-kms-v0.3.2\n [0.3.1]: https://github.com/awslabs/tough/compare/tough-kms-v0.3.0...tough-kms-v0.3.1"
        },
        {
          "filename": "tough-kms/Cargo.toml",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -1,6 +1,6 @@\n [package]\n name = \"tough-kms\"\n-version = \"0.3.3\"\n+version = \"0.3.4\"\n description = \"Implements AWS KMS as a key source for TUF signing keys\"\n authors = [\"Shailesh Gothi <gothisg@amazon.com>\"]\n license = \"MIT OR Apache-2.0\"\n@@ -15,7 +15,7 @@ rusoto-native-tls = [\"rusoto_core/native-tls\", \"rusoto_credential\", \"rusoto_kms/\n rusoto-rustls = [\"rusoto_core/rustls\", \"rusoto_credential\", \"rusoto_kms/rustls\"]\n \n [dependencies]\n-tough = { version = \"0.11.3\", path = \"../tough\", features = [\"http\"] }\n+tough = { version = \"0.12.0\", path = \"../tough\", features = [\"http\"] }\n ring = { version = \"0.16.16\", features = [\"std\"] }\n rusoto_core = { version = \"0.47\", optional = true, default-features = false }\n rusoto_credential = { version = \"0.47\", optional = true }"
        },
        {
          "filename": "tough-ssm/CHANGELOG.md",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "patch": "@@ -4,6 +4,10 @@ All notable changes to this project will be documented in this file.\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n+## [0.6.4] - 2021-10-19\n+### Changes\n+- Update dependencies.\n+\n ## [0.6.3] - 2021-09-15\n ### Changes\n - Update dependencies.\n@@ -69,6 +73,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n ### Added\n - Everything!\n \n+[0.6.4]: https://github.com/awslabs/tough/compare/tough-ssm-v0.6.3...tough-ssm-v0.6.4\n [0.6.3]: https://github.com/awslabs/tough/compare/tough-ssm-v0.6.2...tough-ssm-v0.6.3\n [0.6.2]: https://github.com/awslabs/tough/compare/tough-ssm-v0.6.1...tough-ssm-v0.6.2\n [0.6.1]: https://github.com/awslabs/tough/compare/tough-ssm-v0.6.0...tough-ssm-v0.6.1"
        },
        {
          "filename": "tough-ssm/Cargo.toml",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -1,6 +1,6 @@\n [package]\n name = \"tough-ssm\"\n-version = \"0.6.3\"\n+version = \"0.6.4\"\n description = \"Implements AWS SSM as a key source for TUF signing keys\"\n authors = [\"Zac Mrowicki <mrowicki@amazon.com>\"]\n license = \"MIT OR Apache-2.0\"\n@@ -15,7 +15,7 @@ rusoto-native-tls = [\"rusoto_core/native-tls\", \"rusoto_credential\", \"rusoto_ssm/\n rusoto-rustls = [\"rusoto_core/rustls\", \"rusoto_credential\", \"rusoto_ssm/rustls\"]\n \n [dependencies]\n-tough = { version = \"0.11.3\", path = \"../tough\", features = [\"http\"] }\n+tough = { version = \"0.12.0\", path = \"../tough\", features = [\"http\"] }\n rusoto_core = { version = \"0.47\", optional = true, default-features = false }\n rusoto_credential = { version = \"0.47\", optional = true }\n rusoto_ssm = { version = \"0.47\", optional = true, default-features = false }"
        },
        {
          "filename": "tough/CHANGELOG.md",
          "status": "modified",
          "additions": 11,
          "deletions": 1,
          "patch": "@@ -4,6 +4,15 @@ All notable changes to this project will be documented in this file.\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n+## [0.12.0] - 2021-10-19\n+### Breaking Changes\n+- Target names are now specified with a struct, `TargetName`, instead of `String`.\n+\n+### Changes\n+- Update dependencies.\n+- Fix an issue where delegated role names with path traversal constructs could cause files to be written in unexpected locations.\n+- Fix a similar issue with path traversal constructs in target names.\n+\n ## [0.11.3] - 2021-09-15\n ### Changes\n - Update dependencies.\n@@ -149,7 +158,8 @@ For changes that require modification of calling code see #120 and #121.\n ### Added\n - Everything!\n \n-[Unreleased]: https://github.com/awslabs/tough/compare/tough-v0.11.3...HEAD\n+[Unreleased]: https://github.com/awslabs/tough/compare/tough-v0.12.0...HEAD\n+[0.12.0]: https://github.com/awslabs/tough/compare/tough-v0.11.3...tough-v0.12.0\n [0.11.3]: https://github.com/awslabs/tough/compare/tough-v0.11.2...tough-v0.11.3\n [0.11.2]: https://github.com/awslabs/tough/compare/tough-v0.11.1...tough-v0.11.2\n [0.11.1]: https://github.com/awslabs/tough/compare/tough-v0.11.0...tough-v0.11.1"
        },
        {
          "filename": "tough/Cargo.toml",
          "status": "modified",
          "additions": 4,
          "deletions": 1,
          "patch": "@@ -1,6 +1,6 @@\n [package]\n name = \"tough\"\n-version = \"0.11.3\"\n+version = \"0.12.0\"\n description = \"The Update Framework (TUF) repository client\"\n authors = [\"iliana destroyer of worlds <iweller@amazon.com>\"]\n license = \"MIT OR Apache-2.0\"\n@@ -15,7 +15,9 @@ globset = { version = \"0.4.8\" }\n hex = \"0.4.2\"\n log = \"0.4.8\"\n olpc-cjson = { version = \"0.1.0\", path = \"../olpc-cjson\" }\n+path-absolutize = \"3\"\n pem = \"1.0.0\"\n+percent-encoding = \"2\"\n reqwest = { version = \"0.11.1\", optional = true, default-features = false, features = [\"blocking\"] }\n ring = { version = \"0.16.16\", features = [\"std\"] }\n serde = { version = \"1.0.125\", features = [\"derive\"] }\n@@ -30,6 +32,7 @@ walkdir = \"2.3.2\"\n [dev-dependencies]\n hex-literal = \"0.3.3\"\n httptest = \"0.15\"\n+maplit = \"1.0.1\"\n \n [features]\n http = [\"reqwest\"]"
        },
        {
          "filename": "tough/src/cache.rs",
          "status": "modified",
          "additions": 22,
          "deletions": 27,
          "patch": "@@ -1,9 +1,8 @@\n use crate::error::{self, Result};\n use crate::fetch::{fetch_max_size, fetch_sha256};\n use crate::schema::{RoleType, Target};\n-use crate::Repository;\n+use crate::{encode_filename, Prefix, Repository, TargetName};\n use snafu::{OptionExt, ResultExt};\n-use std::fs::OpenOptions;\n use std::io::{Read, Write};\n use std::path::Path;\n \n@@ -38,8 +37,9 @@ impl Repository {\n \n         // Fetch targets and save them to the outdir\n         if let Some(target_list) = targets_subset {\n-            for target_name in target_list.iter() {\n-                self.cache_target(&targets_outdir, target_name.as_ref())?;\n+            for raw_name in target_list.iter() {\n+                let target_name = TargetName::new(raw_name.as_ref())?;\n+                self.cache_target(&targets_outdir, &target_name)?;\n             }\n         } else {\n             let targets = &self.targets.signed.targets_map();\n@@ -162,10 +162,10 @@ impl Repository {\n                     .meta\n                     .get(&format!(\"{}.json\", name))?\n                     .version,\n-                name\n+                encode_filename(name)\n             ))\n         } else {\n-            Some(format!(\"{}.json\", name))\n+            Some(format!(\"{}.json\", encode_filename(name)))\n         }\n     }\n \n@@ -203,24 +203,16 @@ impl Repository {\n \n     /// Saves a signed target to the specified `outdir`. Retains the digest-prepended filename if\n     /// consistent snapshots are used.\n-    fn cache_target<P: AsRef<Path>>(&self, outdir: P, name: &str) -> Result<()> {\n-        let t = self\n-            .targets\n-            .signed\n-            .find_target(name)\n-            .context(error::CacheTargetMissing {\n-                target_name: name.to_owned(),\n-            })?;\n-        let (sha, filename) = self.target_digest_and_filename(t, name);\n-        let mut reader = self.fetch_target(t, &sha, filename.as_str())?;\n-        let path = outdir.as_ref().join(filename);\n-        let mut f = OpenOptions::new()\n-            .write(true)\n-            .create(true)\n-            .open(&path)\n-            .context(error::CacheTargetWrite { path: path.clone() })?;\n-        let _ = std::io::copy(&mut reader, &mut f).context(error::CacheTargetWrite { path })?;\n-        Ok(())\n+    fn cache_target<P: AsRef<Path>>(&self, outdir: P, name: &TargetName) -> Result<()> {\n+        self.save_target(\n+            name,\n+            outdir,\n+            if self.consistent_snapshot {\n+                Prefix::Digest\n+            } else {\n+                Prefix::None\n+            },\n+        )\n     }\n \n     /// Gets the max size of the snapshot.json file as specified by the timestamp file.\n@@ -242,13 +234,16 @@ impl Repository {\n     pub(crate) fn target_digest_and_filename(\n         &self,\n         target: &Target,\n-        name: &str,\n+        name: &TargetName,\n     ) -> (Vec<u8>, String) {\n         let sha256 = &target.hashes.sha256.clone().into_vec();\n         if self.consistent_snapshot {\n-            (sha256.clone(), format!(\"{}.{}\", hex::encode(sha256), name))\n+            (\n+                sha256.clone(),\n+                format!(\"{}.{}\", hex::encode(sha256), name.resolved()),\n+            )\n         } else {\n-            (sha256.clone(), name.to_owned())\n+            (sha256.clone(), name.resolved().to_owned())\n         }\n     }\n "
        },
        {
          "filename": "tough/src/editor/mod.rs",
          "status": "modified",
          "additions": 48,
          "deletions": 26,
          "patch": "@@ -21,15 +21,17 @@ use crate::schema::{\n     Targets, Timestamp, TimestampMeta,\n };\n use crate::transport::Transport;\n-use crate::Limits;\n-use crate::Repository;\n+use crate::{encode_filename, Limits};\n+use crate::{Repository, TargetName};\n use chrono::{DateTime, Utc};\n use ring::digest::{SHA256, SHA256_OUTPUT_LEN};\n use ring::rand::SystemRandom;\n use serde_json::Value;\n use snafu::{ensure, OptionExt, ResultExt};\n use std::borrow::Cow;\n use std::collections::HashMap;\n+use std::convert::TryInto;\n+use std::fmt::Display;\n use std::num::NonZeroU64;\n use std::path::Path;\n use url::Url;\n@@ -193,6 +195,14 @@ impl RepositoryEditor {\n             .build_timestamp(&signed_snapshot)\n             .and_then(|timestamp| SignedRole::new(timestamp, &root, keys, &rng))?;\n \n+        // This validation can only be done from the top level targets.json role. This check verifies\n+        // that each target's delegate hierarchy is a match (i.e. its delegate ownership is valid).\n+        signed_targets\n+            .signed\n+            .signed\n+            .validate()\n+            .context(error::InvalidPath)?;\n+\n         Ok(SignedRepository {\n             root: self.signed_root,\n             targets: signed_targets,\n@@ -256,13 +266,17 @@ impl RepositoryEditor {\n     }\n \n     /// Add a `Target` to the repository\n-    pub fn add_target(&mut self, name: &str, target: Target) -> Result<&mut Self> {\n-        self.targets_editor_mut()?.add_target(name, target);\n+    pub fn add_target<T, E>(&mut self, name: T, target: Target) -> Result<&mut Self>\n+    where\n+        T: TryInto<TargetName, Error = E>,\n+        E: Display,\n+    {\n+        self.targets_editor_mut()?.add_target(name, target)?;\n         Ok(self)\n     }\n \n     /// Remove a `Target` from the repository\n-    pub fn remove_target(&mut self, name: &str) -> Result<&mut Self> {\n+    pub fn remove_target(&mut self, name: &TargetName) -> Result<&mut Self> {\n         self.targets_editor_mut()?.remove_target(name);\n \n         Ok(self)\n@@ -279,7 +293,7 @@ impl RepositoryEditor {\n         P: AsRef<Path>,\n     {\n         let (target_name, target) = RepositoryEditor::build_target(target_path)?;\n-        self.add_target(&target_name, target)?;\n+        self.add_target(target_name, target)?;\n         Ok(self)\n     }\n \n@@ -292,31 +306,32 @@ impl RepositoryEditor {\n     {\n         for target in targets {\n             let (target_name, target) = RepositoryEditor::build_target(target)?;\n-            self.add_target(&target_name, target)?;\n+            self.add_target(target_name, target)?;\n         }\n \n         Ok(self)\n     }\n \n     /// Builds a target struct for the given path\n-    pub fn build_target<P>(target_path: P) -> Result<(String, Target)>\n+    pub fn build_target<P>(target_path: P) -> Result<(TargetName, Target)>\n     where\n         P: AsRef<Path>,\n     {\n         let target_path = target_path.as_ref();\n \n+        // Get the file name as a string\n+        let target_name = TargetName::new(\n+            target_path\n+                .file_name()\n+                .context(error::NoFileName { path: target_path })?\n+                .to_str()\n+                .context(error::PathUtf8 { path: target_path })?,\n+        )?;\n+\n         // Build a Target from the path given. If it is not a file, this will fail\n         let target =\n             Target::from_path(target_path).context(error::TargetFromPath { path: target_path })?;\n \n-        // Get the file name as a string\n-        let target_name = target_path\n-            .file_name()\n-            .context(error::NoFileName { path: target_path })?\n-            .to_str()\n-            .context(error::PathUtf8 { path: target_path })?\n-            .to_owned();\n-\n         Ok((target_name, target))\n     }\n \n@@ -487,11 +502,15 @@ impl RepositoryEditor {\n             .signed;\n         let metadata_base_url = parse_url(metadata_url)?;\n         // path to updated metadata\n+        let encoded_name = encode_filename(name);\n+        let encoded_filename = format!(\"{}.json\", encoded_name);\n         let role_url =\n             metadata_base_url\n-                .join(&format!(\"{}.json\", name))\n-                .context(error::JoinUrl {\n-                    path: name.to_string(),\n+                .join(&encoded_filename)\n+                .with_context(|| error::JoinUrlEncoded {\n+                    original: name,\n+                    encoded: encoded_name,\n+                    filename: encoded_filename,\n                     url: metadata_base_url.clone(),\n                 })?;\n         let reader = Box::new(fetch_max_size(\n@@ -548,13 +567,16 @@ impl RepositoryEditor {\n         // load the new roles\n         for name in new_roles {\n             // path to new metadata\n-            let role_url =\n-                metadata_base_url\n-                    .join(&format!(\"{}.json\", name))\n-                    .context(error::JoinUrl {\n-                        path: name.to_string(),\n-                        url: metadata_base_url.clone(),\n-                    })?;\n+            let encoded_name = encode_filename(&name);\n+            let encoded_filename = format!(\"{}.json\", encoded_name);\n+            let role_url = metadata_base_url.join(&encoded_filename).with_context(|| {\n+                error::JoinUrlEncoded {\n+                    original: &name,\n+                    encoded: encoded_name,\n+                    filename: encoded_filename,\n+                    url: metadata_base_url.clone(),\n+                }\n+            })?;\n             let reader = Box::new(fetch_max_size(\n                 transport.as_ref(),\n                 role_url,"
        },
        {
          "filename": "tough/src/editor/signed.rs",
          "status": "modified",
          "additions": 23,
          "deletions": 19,
          "patch": "@@ -27,6 +27,8 @@ use std::os::unix::fs::symlink;\n #[cfg(target_os = \"windows\")]\n use std::os::windows::fs::symlink_file as symlink;\n \n+use crate::TargetName;\n+use std::borrow::Cow;\n use std::path::{Path, PathBuf};\n use url::Url;\n use walkdir::WalkDir;\n@@ -310,7 +312,7 @@ impl SignedRepository {\n         input_path: &Path,\n         outdir: &Path,\n         replace_behavior: PathExists,\n-        target_filename: Option<&str>,\n+        target_filename: Option<&TargetName>,\n     ) -> Result<()> {\n         ensure!(\n             input_path.is_file(),\n@@ -352,7 +354,7 @@ impl SignedRepository {\n         input_path: &Path,\n         outdir: &Path,\n         replace_behavior: PathExists,\n-        target_filename: Option<&str>,\n+        target_filename: Option<&TargetName>,\n     ) -> Result<()> {\n         ensure!(\n             input_path.is_file(),\n@@ -385,7 +387,7 @@ impl SignedRepository {\n }\n \n impl TargetsWalker for SignedRepository {\n-    fn targets(&self) -> HashMap<String, &Target> {\n+    fn targets(&self) -> HashMap<TargetName, &Target> {\n         // Since there is access to `targets.json` metadata, all targets\n         // can be found using `targets_map()`\n         self.targets.signed.signed.targets_map()\n@@ -484,7 +486,7 @@ impl SignedDelegatedTargets {\n         input_path: &Path,\n         outdir: &Path,\n         replace_behavior: PathExists,\n-        target_filename: Option<&str>,\n+        target_filename: Option<&TargetName>,\n     ) -> Result<()> {\n         ensure!(\n             input_path.is_file(),\n@@ -526,7 +528,7 @@ impl SignedDelegatedTargets {\n         input_path: &Path,\n         outdir: &Path,\n         replace_behavior: PathExists,\n-        target_filename: Option<&str>,\n+        target_filename: Option<&TargetName>,\n     ) -> Result<()> {\n         ensure!(\n             input_path.is_file(),\n@@ -559,7 +561,7 @@ impl SignedDelegatedTargets {\n }\n \n impl TargetsWalker for SignedDelegatedTargets {\n-    fn targets(&self) -> HashMap<String, &Target> {\n+    fn targets(&self) -> HashMap<TargetName, &Target> {\n         // There are multiple `Targets` roles here that may or may not be related,\n         // so find all of the `Target`s related to each role and combine them.\n         let mut targets_map = HashMap::new();\n@@ -580,7 +582,7 @@ impl TargetsWalker for SignedDelegatedTargets {\n /// also determine if a file prefix needs to be used.\n trait TargetsWalker {\n     /// Returns a map of all targets this manager is responsible for\n-    fn targets(&self) -> HashMap<String, &Target>;\n+    fn targets(&self) -> HashMap<TargetName, &Target>;\n     /// Determines whether or not consistent snapshot filenames should be used\n     fn consistent_snapshot(&self) -> bool;\n \n@@ -595,7 +597,7 @@ trait TargetsWalker {\n         replace_behavior: PathExists,\n     ) -> Result<()>\n     where\n-        F: Fn(&Self, &Path, &Path, PathExists, Option<&str>) -> Result<()>,\n+        F: Fn(&Self, &Path, &Path, PathExists, Option<&TargetName>) -> Result<()>,\n     {\n         std::fs::create_dir_all(outdir).context(error::DirCreate { path: outdir })?;\n \n@@ -636,20 +638,22 @@ trait TargetsWalker {\n         &self,\n         input: &Path,\n         outdir: &Path,\n-        target_filename: Option<&str>,\n+        target_filename: Option<&TargetName>,\n     ) -> Result<TargetPath> {\n         let outdir = std::fs::canonicalize(outdir).context(error::AbsolutePath { path: outdir })?;\n \n         // If the caller requested a specific target filename, use that, otherwise use the filename\n         // component of the input path.\n-        let file_name = if let Some(target_filename) = target_filename {\n-            target_filename\n+        let target_name = if let Some(target_filename) = target_filename {\n+            Cow::Borrowed(target_filename)\n         } else {\n-            input\n-                .file_name()\n-                .context(error::NoFileName { path: input })?\n-                .to_str()\n-                .context(error::PathUtf8 { path: input })?\n+            Cow::Owned(TargetName::new(\n+                input\n+                    .file_name()\n+                    .context(error::NoFileName { path: input })?\n+                    .to_str()\n+                    .context(error::PathUtf8 { path: input })?,\n+            )?)\n         };\n \n         // create a Target object using the input path.\n@@ -660,7 +664,7 @@ trait TargetsWalker {\n         // with that name. If so...\n         let repo_targets = &self.targets();\n         let repo_target = repo_targets\n-            .get(file_name)\n+            .get(&target_name)\n             .context(error::PathIsNotTarget { path: input })?;\n         // compare the hashes of the target from the repo and the target we just created.  They\n         // should match, or we alert the caller; if target replacement is intended, it should\n@@ -678,10 +682,10 @@ trait TargetsWalker {\n             outdir.join(format!(\n                 \"{}.{}\",\n                 hex::encode(&target_from_path.hashes.sha256),\n-                file_name\n+                target_name.resolved()\n             ))\n         } else {\n-            outdir.join(&file_name)\n+            outdir.join(target_name.resolved())\n         };\n \n         // Return the target path, using the `TargetPath` enum that represents the type of file"
        },
        {
          "filename": "tough/src/editor/targets.rs",
          "status": "modified",
          "additions": 38,
          "deletions": 21,
          "patch": "@@ -15,14 +15,16 @@ use crate::schema::{\n     Targets,\n };\n use crate::transport::Transport;\n-use crate::Limits;\n-use crate::Repository;\n+use crate::{encode_filename, Limits};\n+use crate::{Repository, TargetName};\n use chrono::{DateTime, Utc};\n use ring::rand::SystemRandom;\n use serde_json::Value;\n use snafu::{OptionExt, ResultExt};\n use std::borrow::Cow;\n use std::collections::HashMap;\n+use std::convert::TryInto;\n+use std::fmt::Display;\n use std::num::NonZeroU64;\n use std::path::Path;\n use url::Url;\n@@ -65,9 +67,9 @@ pub struct TargetsEditor {\n     /// for \"targets\" on a repository that doesn't use delegated targets\n     delegations: Option<Delegations>,\n     /// New targets that were added to `name`\n-    new_targets: Option<HashMap<String, Target>>,\n+    new_targets: Option<HashMap<TargetName, Target>>,\n     /// Targets that were previously in `name`\n-    existing_targets: Option<HashMap<String, Target>>,\n+    existing_targets: Option<HashMap<TargetName, Target>>,\n     /// Version of the `Targets`\n     version: Option<NonZeroU64>,\n     /// Expiration of the `Targets`\n@@ -175,11 +177,21 @@ impl TargetsEditor {\n     }\n \n     /// Add a `Target` to the `Targets` role\n-    pub fn add_target(&mut self, name: &str, target: Target) -> &mut Self {\n+    pub fn add_target<T, E>(&mut self, name: T, target: Target) -> Result<&mut Self>\n+    where\n+        T: TryInto<TargetName, Error = E>,\n+        E: Display,\n+    {\n+        let target_name = name.try_into().map_err(|e| {\n+            error::InvalidTargetName {\n+                inner: e.to_string(),\n+            }\n+            .build()\n+        })?;\n         self.new_targets\n             .get_or_insert_with(HashMap::new)\n-            .insert(name.to_string(), target);\n-        self\n+            .insert(target_name, target);\n+        Ok(self)\n     }\n \n     /// Add a target to the repository using its path\n@@ -194,19 +206,20 @@ impl TargetsEditor {\n     {\n         let target_path = target_path.as_ref();\n \n+        // Get the file name as a string\n+        let target_name = TargetName::new(\n+            target_path\n+                .file_name()\n+                .context(error::NoFileName { path: target_path })?\n+                .to_str()\n+                .context(error::PathUtf8 { path: target_path })?,\n+        )?;\n+\n         // Build a Target from the path given. If it is not a file, this will fail\n         let target =\n             Target::from_path(target_path).context(error::TargetFromPath { path: target_path })?;\n \n-        // Get the file name as a string\n-        let target_name = target_path\n-            .file_name()\n-            .context(error::NoFileName { path: target_path })?\n-            .to_str()\n-            .context(error::PathUtf8 { path: target_path })?\n-            .to_owned();\n-\n-        self.add_target(&target_name, target);\n+        self.add_target(target_name, target)?;\n         Ok(self)\n     }\n \n@@ -224,7 +237,7 @@ impl TargetsEditor {\n     }\n \n     /// Remove a `Target` from the targets if it exists\n-    pub fn remove_target(&mut self, name: &str) -> &mut Self {\n+    pub fn remove_target(&mut self, name: &TargetName) -> &mut Self {\n         if let Some(targets) = self.existing_targets.as_mut() {\n             targets.remove(name);\n         }\n@@ -377,11 +390,15 @@ impl TargetsEditor {\n \n         let metadata_base_url = parse_url(metadata_url)?;\n         // path to updated metadata\n+        let encoded_name = encode_filename(name);\n+        let encoded_filename = format!(\"{}.json\", encoded_name);\n         let role_url =\n             metadata_base_url\n-                .join(&format!(\"{}.json\", name))\n-                .context(error::JoinUrl {\n-                    path: name.to_string(),\n+                .join(&encoded_filename)\n+                .with_context(|| error::JoinUrlEncoded {\n+                    original: name,\n+                    encoded: encoded_name,\n+                    filename: encoded_filename,\n                     url: metadata_base_url,\n                 })?;\n         let reader = Box::new(fetch_max_size(\n@@ -430,7 +447,7 @@ impl TargetsEditor {\n         // the most common use case, it's possible this is what a user wants.\n         // If it's important to have a non-empty targets, the object can be\n         // inspected by the calling code.\n-        let mut targets: HashMap<String, Target> = HashMap::new();\n+        let mut targets: HashMap<TargetName, Target> = HashMap::new();\n         if let Some(ref existing_targets) = self.existing_targets {\n             targets.extend(existing_targets.clone());\n         }"
        },
        {
          "filename": "tough/src/editor/test.rs",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -6,6 +6,7 @@ mod tests {\n     use crate::editor::RepositoryEditor;\n     use crate::key_source::LocalKeySource;\n     use crate::schema::{Signed, Snapshot, Target, Targets, Timestamp};\n+    use crate::TargetName;\n     use chrono::{Duration, Utc};\n     use std::num::NonZeroU64;\n     use std::path::PathBuf;\n@@ -73,7 +74,7 @@ mod tests {\n         editor\n             .targets(targets)\n             .unwrap()\n-            .add_target(\"file4.txt\", target4)\n+            .add_target(TargetName::new(\"file4.txt\").unwrap(), target4)\n             .unwrap()\n             .add_target_path(target3_path)\n             .unwrap();"
        },
        {
          "filename": "tough/src/error.rs",
          "status": "modified",
          "additions": 116,
          "deletions": 6,
          "patch": "@@ -6,7 +6,7 @@\n #![allow(clippy::default_trait_access)]\n \n use crate::schema::RoleType;\n-use crate::{schema, TransportError};\n+use crate::{schema, TargetName, TransportError};\n use chrono::{DateTime, Utc};\n use snafu::{Backtrace, Snafu};\n use std::io;\n@@ -141,6 +141,9 @@ pub enum Error {\n     #[snafu(display(\"Source path for target must be file or symlink - '{}'\", path.display()))]\n     InvalidFileType { path: PathBuf, backtrace: Backtrace },\n \n+    #[snafu(display(\"Encountered an invalid target name: {}\", inner))]\n+    InvalidTargetName { inner: String, backtrace: Backtrace },\n+\n     /// The library failed to create a URL from a base URL and a path.\n     #[snafu(display(\"Failed to join \\\"{}\\\" to URL \\\"{}\\\": {}\", path, url, source))]\n     JoinUrl {\n@@ -150,6 +153,23 @@ pub enum Error {\n         backtrace: Backtrace,\n     },\n \n+    #[snafu(display(\n+        \"After encoding the name '{}' to '{}', failed to join '{}' to URL '{}': {}\",\n+        original,\n+        encoded,\n+        filename,\n+        url,\n+        source\n+    ))]\n+    JoinUrlEncoded {\n+        original: String,\n+        encoded: String,\n+        filename: String,\n+        url: url::Url,\n+        source: url::ParseError,\n+        backtrace: Backtrace,\n+    },\n+\n     #[snafu(display(\"Unable to parse keypair: {}\", source))]\n     KeyPairFromKeySource {\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n@@ -201,6 +221,20 @@ pub enum Error {\n     #[snafu(display(\"Missing '{}' when building repo from RepositoryEditor\", field))]\n     Missing { field: String, backtrace: Backtrace },\n \n+    #[snafu(display(\"Unable to create NamedTempFile in directory '{}': {}\", path.display(), source))]\n+    NamedTempFileCreate {\n+        path: PathBuf,\n+        source: std::io::Error,\n+        backtrace: Backtrace,\n+    },\n+\n+    #[snafu(display(\"Unable to persist NamedTempFile to '{}': {}\", path.display(), source))]\n+    NamedTempFilePersist {\n+        path: PathBuf,\n+        source: tempfile::PersistError,\n+        backtrace: Backtrace,\n+    },\n+\n     /// Unable to determine file name (path ends in '..' or is '/')\n     #[snafu(display(\"Unable to determine file name from path: '{}'\", path.display()))]\n     NoFileName { path: PathBuf, backtrace: Backtrace },\n@@ -276,6 +310,52 @@ pub enum Error {\n         backtrace: Backtrace,\n     },\n \n+    #[snafu(display(\"Unable to get info about the outdir '{}': {}\", path.display(), source))]\n+    SaveTargetDirInfo {\n+        path: PathBuf,\n+        source: std::io::Error,\n+        backtrace: Backtrace,\n+    },\n+\n+    #[snafu(display(\"The outdir '{}' either does not exist or is not a directory\", path.display()))]\n+    SaveTargetOutdir { path: PathBuf, backtrace: Backtrace },\n+\n+    #[snafu(display(\"Unable to canonicalize the outdir '{}': {}\", path.display(), source))]\n+    SaveTargetOutdirCanonicalize {\n+        path: PathBuf,\n+        source: std::io::Error,\n+        backtrace: Backtrace,\n+    },\n+\n+    #[snafu(display(\n+        \"The path '{}' to which we would save target '{}' has no parent\",\n+        path.display(),\n+        name.raw(),\n+    ))]\n+    SaveTargetNoParent {\n+        path: PathBuf,\n+        name: TargetName,\n+        backtrace: Backtrace,\n+    },\n+\n+    #[snafu(display(\"The target '{}' was not found\", name.raw()))]\n+    SaveTargetNotFound {\n+        name: TargetName,\n+        backtrace: Backtrace,\n+    },\n+\n+    #[snafu(display(\n+        \"The target '{}' had an unsafe name. Not writing to '{}' because it is not in the outdir '{}'\",\n+        name.raw(),\n+        filepath.display(),\n+        outdir.display()\n+    ))]\n+    SaveTargetUnsafePath {\n+        name: TargetName,\n+        outdir: PathBuf,\n+        filepath: PathBuf,\n+    },\n+\n     #[snafu(display(\"Failed to serialize role '{}' for signing: {}\", role, source))]\n     SerializeRole {\n         role: String,\n@@ -342,6 +422,21 @@ pub enum Error {\n         backtrace: Backtrace,\n     },\n \n+    #[snafu(display(\"Unable to resolve the target name '{}': {}\", name, source))]\n+    TargetNameResolve {\n+        name: String,\n+        source: std::io::Error,\n+    },\n+\n+    #[snafu(display(\n+        \"Unable to resolve target name '{}', a path with no components was produced\",\n+        name\n+    ))]\n+    TargetNameComponentsEmpty { name: String },\n+\n+    #[snafu(display(\"Unable to resolve target name '{}', expected a rooted path\", name))]\n+    TargetNameRootMissing { name: String },\n+\n     /// A transport error occurred while fetching a URL.\n     #[snafu(display(\"Failed to fetch {}: {}\", url, source))]\n     Transport {\n@@ -350,6 +445,24 @@ pub enum Error {\n         backtrace: Backtrace,\n     },\n \n+    #[snafu(display(\n+        \"The target name '..' is unsafe. Interpreting it as a path could escape from the intended \\\n+        directory\",\n+    ))]\n+    UnsafeTargetNameDotDot {},\n+\n+    #[snafu(display(\n+        \"The target name '{}' is unsafe. Interpreting it as a path would lead to an empty filename\",\n+        name\n+    ))]\n+    UnsafeTargetNameEmpty { name: String },\n+\n+    #[snafu(display(\n+        \"The target name '{}' is unsafe. Interpreting it as a path would lead to a filename of '/'\",\n+        name\n+    ))]\n+    UnsafeTargetNameSlash { name: String },\n+\n     /// A metadata file could not be verified.\n     #[snafu(display(\"Failed to verify {} metadata: {}\", role, source))]\n     VerifyMetadata {\n@@ -414,9 +527,9 @@ pub enum Error {\n         backtrace: Backtrace,\n     },\n \n-    #[snafu(display(\"The target '{}' was not found\", target_name))]\n+    #[snafu(display(\"The target '{}' was not found\", target_name.raw()))]\n     CacheTargetMissing {\n-        target_name: String,\n+        target_name: TargetName,\n         source: crate::schema::Error,\n         backtrace: Backtrace,\n     },\n@@ -428,9 +541,6 @@ pub enum Error {\n         backtrace: Backtrace,\n     },\n \n-    #[snafu(display(\"Target file not delegated: {}\", target_url))]\n-    TargetNotFound { target_url: String },\n-\n     #[snafu(display(\"Delegated role not found: {}\", name))]\n     DelegateNotFound { name: String },\n "
        },
        {
          "filename": "tough/src/lib.rs",
          "status": "modified",
          "additions": 234,
          "deletions": 11,
          "patch": "@@ -40,6 +40,7 @@ mod io;\n pub mod key_source;\n pub mod schema;\n pub mod sign;\n+mod target_name;\n mod transport;\n \n use crate::datastore::Datastore;\n@@ -48,16 +49,22 @@ use crate::fetch::{fetch_max_size, fetch_sha256};\n /// An HTTP transport that includes retries.\n #[cfg(feature = \"http\")]\n pub use crate::http::{HttpTransport, HttpTransportBuilder, RetryRead};\n-use crate::schema::{DelegatedRole, Delegations};\n-use crate::schema::{Role, RoleType, Root, Signed, Snapshot, Timestamp};\n+use crate::schema::{\n+    DelegatedRole, Delegations, Role, RoleType, Root, Signed, Snapshot, Timestamp,\n+};\n+pub use crate::target_name::TargetName;\n pub use crate::transport::{\n     DefaultTransport, FilesystemTransport, Transport, TransportError, TransportErrorKind,\n };\n use chrono::{DateTime, Utc};\n+use log::warn;\n+use percent_encoding::{utf8_percent_encode, AsciiSet, NON_ALPHANUMERIC};\n use snafu::{ensure, OptionExt, ResultExt};\n use std::collections::HashMap;\n+use std::fs::create_dir_all;\n use std::io::Read;\n-use std::path::PathBuf;\n+use std::path::{Path, PathBuf};\n+use tempfile::NamedTempFile;\n use url::Url;\n \n /// Represents whether a Repository should fail to load when metadata is expired (`Safe`) or whether\n@@ -268,6 +275,16 @@ impl Default for Limits {\n     }\n }\n \n+/// Use this enum to specify whether or not we should include a prefix in the target name when\n+/// saving a target.\n+#[derive(Debug, Copy, Clone, Eq, PartialEq)]\n+pub enum Prefix {\n+    /// Do not prepend the target name when saving the target file, e.g. `my-target.txt`.\n+    None,\n+    /// Prepend the sha digest when saving the target file, e.g. `0123456789abcdef.my-target.txt`.\n+    Digest,\n+}\n+\n /// A TUF repository.\n ///\n /// You can create a `Repository` using a [`RepositoryLoader`].\n@@ -404,7 +421,7 @@ impl Repository {\n     /// before its checksum is validated. If the maximum size is reached or there is a checksum\n     /// mismatch, the reader returns a [`std::io::Error`]. **Consumers of this library must not use\n     /// data from the reader if it returns an error.**\n-    pub fn read_target(&self, name: &str) -> Result<Option<impl Read + Send>> {\n+    pub fn read_target(&self, name: &TargetName) -> Result<Option<impl Read + Send>> {\n         // Check for repository metadata expiration.\n         if self.expiration_enforcement == ExpirationEnforcement::Safe {\n             ensure!(\n@@ -440,12 +457,127 @@ impl Repository {\n         })\n     }\n \n+    /// Fetches a target from the repository and saves it to `outdir`. Attempts to do this as safely\n+    /// as possible by using `path_clean` to eliminate `../` path traversals from the the target's\n+    /// name. Ensures that the resulting filepath is in `outdir` or a child of `outdir`.\n+    ///\n+    /// # Parameters\n+    ///\n+    /// - `name`: the target name.\n+    /// - `outdir`: the directory to save the target in.\n+    /// - `prepend`: Whether or not to prepend the sha digest when saving the target file.\n+    ///\n+    /// # Preconditions and Behavior\n+    ///\n+    /// - `outdir` must exist. For safety we want to canonicalize the path before we join to it.\n+    /// - intermediate directories will be created in `outdir` with `create_dir_all`\n+    /// - Will error if the result of path resolution results in a filepath outside of `outdir` or\n+    ///   outside of a delegated target's correct path of delegation.\n+    ///\n+    pub fn save_target<P>(&self, name: &TargetName, outdir: P, prepend: Prefix) -> Result<()>\n+    where\n+        P: AsRef<Path>,\n+    {\n+        // Ensure the outdir exists then canonicalize the path.\n+        let outdir = outdir.as_ref();\n+        let outdir = outdir\n+            .canonicalize()\n+            .context(error::SaveTargetOutdirCanonicalize { path: outdir })?;\n+        ensure!(outdir.is_dir(), error::SaveTargetOutdir { path: outdir });\n+\n+        if name.resolved() != name.raw() {\n+            // Since target names with resolvable path segments are unusual and potentially unsafe,\n+            // we warn the user that we have encountered them.\n+            warn!(\n+                \"The target named '{}' had path segments that were resolved to produce the \\\n+                following name: {}\",\n+                name.raw(),\n+                name.resolved()\n+            );\n+        }\n+\n+        let filename = match prepend {\n+            Prefix::Digest => {\n+                let target = self.targets.signed.find_target(name).with_context(|| {\n+                    error::CacheTargetMissing {\n+                        target_name: name.clone(),\n+                    }\n+                })?;\n+                let sha256 = target.hashes.sha256.clone().into_vec();\n+                format!(\"{}.{}\", hex::encode(sha256), name.resolved())\n+            }\n+            Prefix::None => name.resolved().to_owned(),\n+        };\n+\n+        let resolved_filepath = outdir.join(filename);\n+\n+        // Find out what directory we will be writing the target file to.\n+        let filepath_dir =\n+            resolved_filepath\n+                .parent()\n+                .with_context(|| error::SaveTargetNoParent {\n+                    path: &resolved_filepath,\n+                    name: name.clone(),\n+                })?;\n+\n+        // Make sure the filepath we are writing to is in or below outdir.\n+        ensure!(\n+            filepath_dir.starts_with(&outdir),\n+            error::SaveTargetUnsafePath {\n+                name: name.clone(),\n+                outdir,\n+                filepath: &resolved_filepath,\n+            }\n+        );\n+\n+        // Fetch and write the target using NamedTempFile for an atomic file creation.\n+        let mut reader = self\n+            .read_target(name)?\n+            .with_context(|| error::SaveTargetNotFound { name: name.clone() })?;\n+        create_dir_all(&filepath_dir).context(error::DirCreate {\n+            path: &filepath_dir,\n+        })?;\n+        let mut f = NamedTempFile::new_in(&filepath_dir).context(error::NamedTempFileCreate {\n+            path: &filepath_dir,\n+        })?;\n+        std::io::copy(&mut reader, &mut f).context(error::FileWrite { path: &f.path() })?;\n+        f.persist(&resolved_filepath)\n+            .context(error::NamedTempFilePersist {\n+                path: resolved_filepath,\n+            })?;\n+\n+        Ok(())\n+    }\n+\n     /// Return the named `DelegatedRole` if found.\n     pub fn delegated_role(&self, name: &str) -> Option<&DelegatedRole> {\n         self.targets.signed.delegated_role(name).ok()\n     }\n }\n \n+/// The set of characters that will be escaped when converting a delegated role name into a\n+/// filename. This needs to at least include path traversal characters to prevent tough from writing\n+/// outside of its datastore.\n+///\n+/// In order to match the Python TUF implementation, we mimic the Python function\n+/// [urllib.parse.quote] (given a 'safe' parameter value of `\"\"`) which follows RFC 3986 and states\n+///\n+/// > Replace special characters in string using the %xx escape. Letters, digits, and the characters\n+/// `_.-~` are never quoted.\n+///\n+/// [urllib.parse.quote]: https://docs.python.org/3/library/urllib.parse.html#url-quoting\n+const CHARACTERS_TO_ESCAPE: AsciiSet = NON_ALPHANUMERIC\n+    .remove(b'_')\n+    .remove(b'.')\n+    .remove(b'-')\n+    .remove(b'~');\n+\n+/// Percent encode a potential filename to ensure it is safe and does not have path traversal\n+/// characters.\n+pub(crate) fn encode_filename<S: AsRef<str>>(name: S) -> String {\n+    utf8_percent_encode(name.as_ref(), &CHARACTERS_TO_ESCAPE).to_string()\n+}\n+\n /// Ensures that system time has not stepped backward since it was last sampled\n fn system_time(datastore: &Datastore) -> Result<DateTime<Utc>> {\n     let file = \"latest_known_time.json\";\n@@ -997,6 +1129,9 @@ fn load_targets(\n         )?;\n     }\n \n+    // This validation can only be done from the top level targets.json role. This check verifies\n+    // that each target's delegate hierarchy is a match (i.e. it's delegate ownership is valid).\n+    targets.signed.validate().context(error::InvalidPath)?;\n     Ok(targets)\n }\n \n@@ -1023,9 +1158,13 @@ fn load_delegations(\n             })?;\n \n         let path = if consistent_snapshot {\n-            format!(\"{}.{}.json\", &role_meta.version, &delegated_role.name)\n+            format!(\n+                \"{}.{}.json\",\n+                &role_meta.version,\n+                encode_filename(&delegated_role.name)\n+            )\n         } else {\n-            format!(\"{}.json\", &delegated_role.name)\n+            format!(\"{}.json\", encode_filename(&delegated_role.name))\n         };\n         let role_url = metadata_base_url.join(&path).context(error::JoinUrl {\n             path: path.clone(),\n@@ -1058,11 +1197,6 @@ fn load_delegations(\n                 expected: role_meta.version\n             }\n         );\n-        {\n-            if let Some(delegations) = role.signed.delegations.as_ref() {\n-                delegations.verify_paths().context(error::InvalidPath {})?;\n-            }\n-        }\n \n         datastore.create(&path, &role)?;\n         delegated_roles.insert(delegated_role.name.clone(), Some(role));\n@@ -1124,4 +1258,93 @@ mod tests {\n         let default = ExpirationEnforcement::default();\n         assert_eq!(default, ExpirationEnforcement::Safe);\n     }\n+\n+    #[test]\n+    fn encode_filename_1() {\n+        let input = \"../a\";\n+        let expected = \"..%2Fa\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_2() {\n+        let input = \"\";\n+        let expected = \"\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_3() {\n+        let input = \".\";\n+        let expected = \".\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_4() {\n+        let input = \"/\";\n+        let expected = \"%2F\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_5() {\n+        let input = \"\u00f6\";\n+        let expected = \"%C3%B6\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_6() {\n+        let input = \"!@#$%^&*()[]|\\\\~`'\\\";:.,><?/-_\";\n+        let expected =\n+            \"%21%40%23%24%25%5E%26%2A%28%29%5B%5D%7C%5C~%60%27%22%3B%3A.%2C%3E%3C%3F%2F-_\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_7() {\n+        let input = \"../../strange/role/../name\";\n+        let expected = \"..%2F..%2Fstrange%2Frole%2F..%2Fname\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_8() {\n+        let input = \"../\ud83c\udf7a/( \u0361\u00b0 \u035c\u0296 \u0361\u00b0)\";\n+        let expected = \"..%2F%F0%9F%8D%BA%2F%28%20%CD%A1%C2%B0%20%CD%9C%CA%96%20%CD%A1%C2%B0%29\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_9() {\n+        let input = \"\u16a9 os, \u16b1 rad, \u16b3 cen, \u16b7 gyfu, \u16b9 \u01bfynn, \u16bb h\u00e6gl, ...\";\n+        let expected = \"%E1%9A%A9%20os%2C%20%E1%9A%B1%20rad%2C%20%E1%9A%B3%20cen%2C%20%E1%9A%B7%20gyfu%2C%20%E1%9A%B9%20%C6%BFynn%2C%20%E1%9A%BB%20h%C3%A6gl%2C%20...\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_10() {\n+        let input = \"../../path/like/dubious\";\n+        let expected = \"..%2F..%2Fpath%2Flike%2Fdubious\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n+\n+    #[test]\n+    fn encode_filename_11() {\n+        let input = \"\ud83c\udf7a/30\";\n+        let expected = \"%F0%9F%8D%BA%2F30\";\n+        let actual = encode_filename(input);\n+        assert_eq!(expected, actual);\n+    }\n }"
        },
        {
          "filename": "tough/src/schema/error.rs",
          "status": "modified",
          "additions": 10,
          "deletions": 2,
          "patch": "@@ -3,6 +3,7 @@\n #![allow(clippy::default_trait_access)]\n \n use crate::schema::RoleType;\n+use crate::TargetName;\n use snafu::{Backtrace, Snafu};\n use std::fmt::{self, Debug, Display};\n use std::path::PathBuf;\n@@ -40,6 +41,13 @@ pub enum Error {\n         backtrace: Backtrace,\n     },\n \n+    #[snafu(display(\"Failed to parse path pattern '{}' as a glob: {}\", pattern, source))]\n+    Glob {\n+        pattern: String,\n+        source: globset::Error,\n+        backtrace: Backtrace,\n+    },\n+\n     /// A downloaded target's checksum does not match the checksum listed in the repository\n     /// metadata.\n     #[snafu(display(\"Invalid key ID {}: calculated {}\", keyid, calculated))]\n@@ -105,8 +113,8 @@ pub enum Error {\n     UnmatchedPath { child: String },\n \n     /// No valid targets claims `target_file`\n-    #[snafu(display(\"Target file not delegated: {}\", target_file))]\n-    TargetNotFound { target_file: String },\n+    #[snafu(display(\"Target file not delegated: {}\", name.raw()))]\n+    TargetNotFound { name: TargetName },\n \n     #[snafu(display(\"Delegation doesn't contain targets field\"))]\n     NoTargets,"
        },
        {
          "filename": "tough/src/schema/mod.rs",
          "status": "modified",
          "additions": 272,
          "deletions": 91,
          "patch": "@@ -16,11 +16,14 @@ use crate::schema::iter::KeysIter;\n use crate::schema::key::Key;\n use crate::sign::Sign;\n pub use crate::transport::{FilesystemTransport, Transport};\n+use crate::{encode_filename, TargetName};\n use chrono::{DateTime, Utc};\n-use globset::Glob;\n+use globset::{Glob, GlobMatcher};\n+use hex::ToHex;\n use olpc_cjson::CanonicalFormatter;\n use ring::digest::{digest, Context, SHA256};\n-use serde::{Deserialize, Serialize};\n+use serde::de::Error as SerdeDeError;\n+use serde::{Deserialize, Deserializer, Serialize, Serializer};\n use serde_json::Value;\n use serde_plain::{derive_display_from_serialize, derive_fromstr_from_deserialize};\n use snafu::ResultExt;\n@@ -30,6 +33,7 @@ use std::io::Read;\n use std::num::NonZeroU64;\n use std::ops::{Deref, DerefMut};\n use std::path::Path;\n+use std::str::FromStr;\n \n /// The type of metadata role.\n #[derive(Debug, Clone, Copy, Deserialize, Serialize, PartialEq, Eq, Hash)]\n@@ -395,7 +399,7 @@ pub struct Targets {\n \n     /// Each key of the TARGETS object is a TARGETPATH. A TARGETPATH is a path to a file that is\n     /// relative to a mirror's base URL of targets.\n-    pub targets: HashMap<String, Target>,\n+    pub targets: HashMap<TargetName, Target>,\n \n     /// Delegations describes subsets of the targets for which responsibility is delegated to\n     /// another role.\n@@ -501,45 +505,54 @@ impl Targets {\n         }\n     }\n \n-    /// Given a target url, returns a reference to the Target struct or error if the target is unreachable\n-    pub fn find_target(&self, target_name: &str) -> Result<&Target> {\n+    /// Given a target url, returns a reference to the Target struct or error if the target is\n+    /// unreachable.\n+    ///\n+    /// **Caution**: does not imply that delegations in this struct or any child are valid.\n+    ///\n+    pub fn find_target(&self, target_name: &TargetName) -> Result<&Target> {\n         if let Some(target) = self.targets.get(target_name) {\n             return Ok(target);\n         }\n         if let Some(delegations) = &self.delegations {\n             for role in &delegations.roles {\n+                // If the target cannot match this DelegatedRole, then we do not want to recurse and\n+                // check any of its child roles either.\n+                if !role.paths.matches_target_name(target_name) {\n+                    continue;\n+                }\n                 if let Some(targets) = &role.targets {\n                     if let Ok(target) = targets.signed.find_target(target_name) {\n                         return Ok(target);\n                     }\n                 }\n             }\n         }\n-        Err(Error::TargetNotFound {\n-            target_file: target_name.to_string(),\n-        })\n+        error::TargetNotFound {\n+            name: target_name.clone(),\n+        }\n+        .fail()\n     }\n \n     /// Returns a hashmap of all targets and all delegated targets recursively\n-    pub fn targets_map(&self) -> HashMap<String, &Target> {\n-        let mut targets_map = HashMap::new();\n-        for target in &self.targets {\n-            targets_map.insert(target.0.clone(), target.1);\n-        }\n+    pub fn targets_map(&self) -> HashMap<TargetName, &Target> {\n+        self.targets_iter()\n+            .map(|(target_name, target)| (target_name.clone(), target))\n+            .collect()\n+    }\n+\n+    /// Returns an iterator of all targets and all delegated targets recursively\n+    pub fn targets_iter(&self) -> impl Iterator<Item = (&TargetName, &Target)> + '_ {\n+        let mut iter: Box<dyn Iterator<Item = (&TargetName, &Target)>> =\n+            Box::new(self.targets.iter());\n         if let Some(delegations) = &self.delegations {\n             for role in &delegations.roles {\n                 if let Some(targets) = &role.targets {\n-                    targets_map.extend(targets.signed.targets_map());\n+                    iter = Box::new(iter.chain(targets.signed.targets_iter()));\n                 }\n             }\n         }\n-\n-        targets_map\n-    }\n-\n-    /// Returns an iterator of all targets delegated\n-    pub fn targets_iter(&self) -> impl Iterator + '_ {\n-        self.targets_map().into_iter()\n+        iter\n     }\n \n     /// Recursively clears all targets\n@@ -555,12 +568,12 @@ impl Targets {\n     }\n \n     /// Add a target to targets\n-    pub fn add_target(&mut self, name: &str, target: Target) {\n-        self.targets.insert(name.to_string(), target);\n+    pub fn add_target(&mut self, name: TargetName, target: Target) {\n+        self.targets.insert(name, target);\n     }\n \n     /// Remove a target from targets\n-    pub fn remove_target(&mut self, name: &str) -> Option<Target> {\n+    pub fn remove_target(&mut self, name: &TargetName) -> Option<Target> {\n         self.targets.remove(name)\n     }\n \n@@ -698,6 +711,17 @@ impl Targets {\n \n         needed_roles\n     }\n+\n+    /// Calls `find_target` on each target (recursively provided by `targets_iter`). This\n+    /// proves that the target is either owned by us, or correctly matches through some hierarchy of\n+    /// [`PathSets`] below us. When called on the top level [`Targets`] of a repository, this proves\n+    /// that the ownership of each target is valid.\n+    pub(crate) fn validate(&self) -> Result<()> {\n+        for (target_name, _) in self.targets_iter() {\n+            self.find_target(target_name)?;\n+        }\n+        Ok(())\n+    }\n }\n \n impl Role for Targets {\n@@ -758,9 +782,9 @@ impl Role for DelegatedTargets {\n \n     fn filename(&self, consistent_snapshot: bool) -> String {\n         if consistent_snapshot {\n-            format!(\"{}.{}.json\", self.version(), self.name)\n+            format!(\"{}.{}.json\", self.version(), encode_filename(&self.name))\n         } else {\n-            format!(\"{}.json\", self.name)\n+            format!(\"{}.json\", encode_filename(&self.name))\n         }\n     }\n \n@@ -866,7 +890,7 @@ pub enum PathSet {\n     /// PATHPATTERN, it is RECOMMENDED that PATHPATTERN uses the forward slash (/) as directory\n     /// separator and does not start with a directory separator, akin to TARGETSPATH.\n     #[serde(rename = \"paths\")]\n-    Paths(Vec<String>),\n+    Paths(Vec<PathPattern>),\n \n     /// The \"path_hash_prefixes\" list is used to succinctly describe a set of target paths.\n     /// Specifically, each HEX_DIGEST in \"path_hash_prefixes\" describes a set of target paths;\n@@ -876,55 +900,145 @@ pub enum PathSet {\n     /// prefix as one of the prefixes in \"path_hash_prefixes\". This is useful to split a large\n     /// number of targets into separate bins identified by consistent hashing.\n     #[serde(rename = \"path_hash_prefixes\")]\n-    PathHashPrefixes(Vec<String>),\n+    PathHashPrefixes(Vec<PathHashPrefix>),\n+}\n+\n+/// A glob-like path pattern for matching delegated targets, e.g. `foo/bar/*`.\n+///\n+/// `PATHPATTERN` supports the Unix shell pattern matching convention for paths\n+/// ([glob](https://man7.org/linux/man-pages/man7/glob.7.html)bing pathnames). Its format may either\n+/// indicate a path to a single file, or to multiple files with the use of shell-style wildcards\n+/// (`*` or `?`). To avoid surprising behavior when matching targets with `PATHPATTERN` it is\n+/// RECOMMENDED that `PATHPATTERN` uses the forward slash (`/`) as directory separator and does\n+/// not start with a directory separator, as is also recommended for `TARGETPATH`. A path\n+/// separator in a path SHOULD NOT be matched by a wildcard in the `PATHPATTERN`.\n+///\n+/// Some example `PATHPATTERN`s and expected matches:\n+/// * a `PATHPATTERN` of `\"targets/*.tgz\"` would match file paths `\"targets/foo.tgz\"` and\n+///   `\"targets/bar.tgz\"`, but not `\"targets/foo.txt\"`.\n+/// * a `PATHPATTERN` of `\"foo-version-?.tgz\"` matches `\"foo-version-2.tgz\"` and\n+///     `\"foo-version-a.tgz\"`, but not `\"foo-version-alpha.tgz\"`.\n+/// * a `PATHPATTERN` of `\"*.tgz\"` would match `\"foo.tgz\"` and `\"bar.tgz\"`,\n+///   but not `\"targets/foo.tgz\"`\n+/// * a `PATHPATTERN` of `\"foo.tgz\"` would match only `\"foo.tgz\"`\n+#[derive(Clone, Debug)]\n+pub struct PathPattern {\n+    value: String,\n+    glob: GlobMatcher,\n+}\n+\n+impl PathPattern {\n+    /// Create a new, valid `PathPattern`. This will fail if we cannot parse the value as a glob. It is important that\n+    /// our implementation stop if it encounters a glob it cannot parse so that we do not load repositories where we\n+    /// cannot enforce delegate ownership.\n+    pub fn new<S: Into<String>>(value: S) -> Result<Self> {\n+        let value = value.into();\n+        let glob = Glob::new(&value)\n+            .context(error::Glob { pattern: &value })?\n+            .compile_matcher();\n+        Ok(Self { value, glob })\n+    }\n+\n+    /// Get the inner value of this `PathPattern` as a string.\n+    pub fn value(&self) -> &str {\n+        &self.value\n+    }\n+\n+    fn matches_target_name(&self, target_name: &TargetName) -> bool {\n+        self.glob.is_match(target_name.resolved())\n+    }\n+}\n+\n+impl FromStr for PathPattern {\n+    type Err = Error;\n+\n+    fn from_str(s: &str) -> Result<Self> {\n+        PathPattern::new(s)\n+    }\n+}\n+\n+impl PartialEq for PathPattern {\n+    fn eq(&self, other: &Self) -> bool {\n+        PartialEq::eq(&self.value, &other.value)\n+    }\n+}\n+\n+impl Serialize for PathPattern {\n+    fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        serializer.serialize_str(self.value().as_ref())\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for PathPattern {\n+    fn deserialize<D>(deserializer: D) -> std::result::Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        let s = <String>::deserialize(deserializer)?;\n+        PathPattern::new(s).map_err(|e| D::Error::custom(format!(\"{}\", e)))\n+    }\n+}\n+\n+/// The first characters found in the string representation of a sha256 digest. This can be used for\n+/// randomly sharding a repository. See [`PathSet::PathHashDigest`] for the description of how this\n+/// is used.\n+#[derive(Clone, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]\n+pub struct PathHashPrefix(String);\n+\n+impl PathHashPrefix {\n+    /// Create a new, valid `PathPattern`.\n+    pub fn new<S: Into<String>>(value: S) -> Result<Self> {\n+        // In case we choose to reject some of these in the future, we return a result. For now this\n+        // will always succeed.\n+        Ok(PathHashPrefix(value.into()))\n+    }\n+\n+    /// Get the inner value of this `PathPattern` as a string.\n+    pub fn value(&self) -> &str {\n+        &self.0\n+    }\n+\n+    fn matches_target_name(&self, target_name: &TargetName) -> bool {\n+        let target_name_digest =\n+            digest(&SHA256, target_name.resolved().as_bytes()).encode_hex::<String>();\n+        target_name_digest.starts_with(self.value())\n+    }\n+}\n+\n+impl FromStr for PathHashPrefix {\n+    type Err = Error;\n+\n+    fn from_str(s: &str) -> Result<Self> {\n+        PathHashPrefix::new(s)\n+    }\n }\n \n impl PathSet {\n-    /// Given a target string determines if paths match\n-    fn matched_target(&self, target: &str) -> bool {\n+    /// Given a `target_name`, returns whether or not this `PathSet` contains a pattern or hash\n+    /// prefix that matches.\n+    fn matches_target_name(&self, target_name: &TargetName) -> bool {\n         match self {\n             Self::Paths(paths) => {\n                 for path in paths {\n-                    if Self::matched_path(path, target) {\n+                    if path.matches_target_name(target_name) {\n                         return true;\n                     }\n                 }\n             }\n \n             Self::PathHashPrefixes(path_prefixes) => {\n-                for path in path_prefixes {\n-                    if Self::matched_prefix(path, target) {\n+                for prefix in path_prefixes {\n+                    if prefix.matches_target_name(target_name) {\n                         return true;\n                     }\n                 }\n             }\n         }\n         false\n     }\n-\n-    /// Given a path hash prefix and a target path determines if target is delegated by prefix\n-    fn matched_prefix(prefix: &str, target: &str) -> bool {\n-        let temp_target = target.to_string();\n-        let hash = digest(&SHA256, temp_target.as_bytes());\n-        hash.as_ref().starts_with(prefix.as_bytes())\n-    }\n-\n-    /// Given a shell style wildcard path determines if target matches the path\n-    fn matched_path(wildcardpath: &str, target: &str) -> bool {\n-        let glob = if let Ok(glob) = Glob::new(wildcardpath) {\n-            glob.compile_matcher()\n-        } else {\n-            return false;\n-        };\n-        glob.is_match(target)\n-    }\n-\n-    /// Returns a Vec representation of the `PathSet`\n-    pub fn vec(&self) -> &Vec<String> {\n-        match self {\n-            PathSet::Paths(x) | PathSet::PathHashPrefixes(x) => x,\n-        }\n-    }\n }\n \n impl Delegations {\n@@ -937,32 +1051,15 @@ impl Delegations {\n     }\n \n     /// Determines if target passes pathset specific matching\n-    pub fn target_is_delegated(&self, target: &str) -> bool {\n+    pub fn target_is_delegated(&self, target: &TargetName) -> bool {\n         for role in &self.roles {\n-            if role.paths.matched_target(target) {\n+            if role.paths.matches_target_name(target) {\n                 return true;\n             }\n         }\n         false\n     }\n \n-    /// Ensures that all delegated paths are allowed to be delegated\n-    pub fn verify_paths(&self) -> Result<()> {\n-        for sub_role in &self.roles {\n-            let pathset = match &sub_role.paths {\n-                PathSet::Paths(paths) | PathSet::PathHashPrefixes(paths) => paths,\n-            };\n-            for path in pathset {\n-                if !self.target_is_delegated(path) {\n-                    return Err(Error::UnmatchedPath {\n-                        child: path.to_string(),\n-                    });\n-                }\n-            }\n-        }\n-        Ok(())\n-    }\n-\n     /// Given an object/key that impls Sign, return the corresponding\n     /// key ID from Delegation\n     pub fn key_id(&self, key_pair: &dyn Sign) -> Option<Decoded<Hex>> {\n@@ -984,21 +1081,6 @@ impl DelegatedRole {\n             _extra: HashMap::new(),\n         }\n     }\n-\n-    /// Verify that paths can be delegated by this role\n-    pub fn verify_paths(&self, paths: &PathSet) -> Result<()> {\n-        let paths = match paths {\n-            PathSet::Paths(x) | PathSet::PathHashPrefixes(x) => x,\n-        };\n-        for path in paths {\n-            if !self.paths.matched_target(path) {\n-                return Err(Error::UnmatchedPath {\n-                    child: path.to_string(),\n-                });\n-            }\n-        }\n-        Ok(())\n-    }\n }\n \n // =^..^=   =^..^=   =^..^=   =^..^=   =^..^=   =^..^=   =^..^=   =^..^=   =^..^=   =^..^=   =^..^=\n@@ -1087,3 +1169,102 @@ impl Role for Timestamp {\n         \"timestamp.json\".to_string()\n     }\n }\n+\n+#[test]\n+fn targets_iter_and_map_test() {\n+    use maplit::hashmap;\n+\n+    // Create a dummy Target object.\n+    let nothing = Target {\n+        length: 0,\n+        hashes: Hashes {\n+            sha256: [0u8].to_vec().into(),\n+            _extra: Default::default(),\n+        },\n+        custom: Default::default(),\n+        _extra: Default::default(),\n+    };\n+\n+    // Create a hierarchy of targets/delegations: a -> b -> c\n+    let c_role = DelegatedRole {\n+        name: \"c-role\".to_string(),\n+        keyids: vec![],\n+        threshold: NonZeroU64::new(1).unwrap(),\n+        paths: PathSet::Paths(vec![PathPattern::new(\"*\").unwrap()]),\n+        terminating: false,\n+        targets: Some(Signed {\n+            signed: Targets {\n+                spec_version: \"\".to_string(),\n+                version: NonZeroU64::new(1).unwrap(),\n+                expires: Utc::now(),\n+                targets: hashmap! {\n+                    TargetName::new(\"c.txt\").unwrap() => nothing.clone(),\n+                },\n+                delegations: None,\n+                _extra: Default::default(),\n+            },\n+            signatures: vec![],\n+        }),\n+    };\n+    let b_delegations = Delegations {\n+        keys: Default::default(),\n+        roles: vec![c_role],\n+    };\n+    let b_role = DelegatedRole {\n+        name: \"b-role\".to_string(),\n+        keyids: vec![],\n+        threshold: NonZeroU64::new(1).unwrap(),\n+        paths: PathSet::Paths(vec![PathPattern::new(\"*\").unwrap()]),\n+        terminating: false,\n+        targets: Some(Signed {\n+            signed: Targets {\n+                spec_version: \"\".to_string(),\n+                version: NonZeroU64::new(1).unwrap(),\n+                expires: Utc::now(),\n+                targets: hashmap! {\n+                    TargetName::new(\"b.txt\").unwrap() => nothing.clone(),\n+                },\n+                delegations: Some(b_delegations),\n+                _extra: Default::default(),\n+            },\n+            signatures: vec![],\n+        }),\n+    };\n+    let a_delegations = Delegations {\n+        keys: Default::default(),\n+        roles: vec![b_role],\n+    };\n+    let a = Targets {\n+        spec_version: \"\".to_string(),\n+        version: NonZeroU64::new(1).unwrap(),\n+        expires: Utc::now(),\n+        targets: hashmap! {\n+            TargetName::new(\"a.txt\").unwrap() => nothing.clone(),\n+        },\n+        delegations: Some(a_delegations),\n+        _extra: Default::default(),\n+    };\n+\n+    // Assert that targets_iter is recursive and thus has a.txt, b.txt and c.txt\n+    assert!(a\n+        .targets_iter()\n+        .map(|(key, _)| key)\n+        .find(|&item| item.raw() == \"a.txt\")\n+        .is_some());\n+    assert!(a\n+        .targets_iter()\n+        .map(|(key, _)| key)\n+        .find(|&item| item.raw() == \"b.txt\")\n+        .is_some());\n+    assert!(a\n+        .targets_iter()\n+        .map(|(key, _)| key)\n+        .find(|&item| item.raw() == \"c.txt\")\n+        .is_some());\n+\n+    // Assert that targets_map is also recursive\n+    let map = a.targets_map();\n+    assert!(map.contains_key(&TargetName::new(\"a.txt\").unwrap()));\n+    assert!(map.contains_key(&TargetName::new(\"b.txt\").unwrap()));\n+    assert!(map.contains_key(&TargetName::new(\"c.txt\").unwrap()));\n+}"
        },
        {
          "filename": "tough/src/target_name.rs",
          "status": "added",
          "additions": 299,
          "deletions": 0,
          "patch": "@@ -0,0 +1,299 @@\n+use crate::error::{self, Result};\n+use path_absolutize::Absolutize;\n+use serde::de::Error;\n+use serde::{Deserialize, Deserializer, Serialize, Serializer};\n+use snafu::{ensure, OptionExt, ResultExt};\n+use std::convert::TryFrom;\n+use std::path::PathBuf;\n+use std::str::FromStr;\n+\n+/// Represents the name of a target in the repository. Path-like constructs are resolved (e.g.\n+/// `foo/../bar` becomes `bar`). Certain unsafe names are rejected when constructing a `TargetName`.\n+/// Unsafe names include:\n+/// - Anything that resolves to an empty string\n+/// - Anything that resolves to `/`\n+///\n+/// `TargetName` intentionally does not impl String-like traits so that we are forced to choose\n+/// between the resolved name and the raw/original name when we use it as a string.\n+///\n+/// Note that `Serialize` writes the `raw`, un-resolved name. You should not use the results of\n+/// serialization to form file paths.\n+///\n+#[derive(Debug, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]\n+pub struct TargetName {\n+    /// The name assigned to the target by the repository user.\n+    raw: String,\n+    /// If the `raw` name is path-like, and it resolves to a simpler path construct, then the\n+    /// resolved name is stored here. (As a CPU optimization).\n+    resolved: Option<String>,\n+}\n+\n+impl TargetName {\n+    /// Construct a new `TargetName`. Unsafe names will return an error.\n+    pub fn new<S: Into<String>>(raw: S) -> Result<Self> {\n+        let raw = raw.into();\n+        let resolved = clean_name(&raw)?;\n+        if raw == resolved {\n+            Ok(Self {\n+                raw,\n+                resolved: None,\n+            })\n+        } else {\n+            Ok(Self {\n+                raw,\n+                resolved: Some(resolved),\n+            })\n+        }\n+    }\n+\n+    /// Get the original, unchanged name (i.e. which might be something like `foo/../bar` instead of\n+    /// `bar`).\n+    pub fn raw(&self) -> &str {\n+        &self.raw\n+    }\n+\n+    /// Get the resolved name (i.e. which would be `bar` instead of `foo/../bar`).\n+    pub fn resolved(&self) -> &str {\n+        match &self.resolved {\n+            None => self.raw(),\n+            Some(resolved) => resolved,\n+        }\n+    }\n+}\n+\n+impl FromStr for TargetName {\n+    type Err = crate::error::Error;\n+\n+    fn from_str(s: &str) -> Result<Self> {\n+        Self::new(s)\n+    }\n+}\n+\n+impl TryFrom<String> for TargetName {\n+    type Error = crate::error::Error;\n+\n+    fn try_from(value: String) -> Result<Self> {\n+        TargetName::new(value)\n+    }\n+}\n+\n+impl TryFrom<&str> for TargetName {\n+    type Error = crate::error::Error;\n+\n+    fn try_from(value: &str) -> Result<Self> {\n+        TargetName::new(value)\n+    }\n+}\n+\n+impl Serialize for TargetName {\n+    fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        serializer.serialize_str(self.raw())\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for TargetName {\n+    fn deserialize<D>(deserializer: D) -> std::result::Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        let s = <String>::deserialize(deserializer)?;\n+        TargetName::new(s).map_err(|e| D::Error::custom(format!(\"{}\", e)))\n+    }\n+}\n+\n+// Resolves path-like constructs. e.g. `foo/../bar` becomes `bar`.\n+fn clean_name(name: &str) -> Result<String> {\n+    // This causes something to panic, so we check for it early.\n+    ensure!(name != \"..\", error::UnsafeTargetNameDotDot);\n+\n+    // Seems like bad things could happen if the target filename is the empty string.\n+    ensure!(!name.is_empty(), error::UnsafeTargetNameEmpty { name });\n+\n+    // If our name starts with absolute, then we need to remember this so we can restore it later.\n+    let name_path = PathBuf::from(name);\n+    let absolute = name_path.is_absolute();\n+\n+    let clean = {\n+        let proposed = name_path\n+            .absolutize_from(&PathBuf::from(\"/\"))\n+            .context(error::TargetNameResolve { name })?;\n+\n+        // `absolutize_from` will give us a path that starts with `/`, so we remove it if the\n+        // original name did not start with `/`\n+        if absolute {\n+            // If `name` started with `/`, then we have nothing left to do because absolutize_from\n+            // returns a rooted path.\n+            proposed.to_path_buf()\n+        } else {\n+            let mut components = proposed.components();\n+            // If the original name did not start with `/`, we need to remove the leading slash\n+            // here because absolutize_from will return a rooted path.\n+            let first_component = components\n+                .next()\n+                // If this error occurs then there is a bug or behavior change in absolutize_from.\n+                .context(error::TargetNameComponentsEmpty { name })?\n+                .as_os_str();\n+\n+            // If the first component isn't `/` then there is a bug or behavior change in\n+            // absolutize_from.\n+            ensure!(\n+                first_component == \"/\",\n+                error::TargetNameRootMissing { name }\n+            );\n+\n+            components.as_path().to_owned()\n+        }\n+    };\n+\n+    let final_name = clean\n+        .as_os_str()\n+        .to_str()\n+        .context(error::PathUtf8 { path: &clean })?\n+        .to_string();\n+\n+    // Check again to make sure we didn't end up with an empty string.\n+    ensure!(\n+        !final_name.is_empty(),\n+        error::UnsafeTargetNameEmpty { name }\n+    );\n+\n+    ensure!(final_name != \"/\", error::UnsafeTargetNameSlash { name });\n+\n+    Ok(final_name)\n+}\n+\n+#[test]\n+fn simple_1() {\n+    let name = \"/absolute/path/is/ok.txt\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn simple_2() {\n+    let name = \"relative/path/is/ok.txt\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn simple_3() {\n+    let name = \"not-path-like.txt\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_1() {\n+    let name = \"/this/../is/ok.txt\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = \"/is/ok.txt\";\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_2() {\n+    let name = \"../x\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = \"x\";\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_3() {\n+    let name = \"../../x\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = \"x\";\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_4() {\n+    let name = \"/../x\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = \"/x\";\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_5() {\n+    let name = \"/../../x\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = \"/x\";\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_6() {\n+    let name = \"/this/../../../../is/ok.txt\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = \"/is/ok.txt\";\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_7() {\n+    let name = \"foo\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn resolved_8() {\n+    let name = \"/foo\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn uncleaned_1() {\n+    let name = r#\"~/\\.\\.\"#;\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn uncleaned_2() {\n+    let name = r#\"funky\\/\\.\\.\\/name\"#;\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn uncleaned_3() {\n+    let name = \"/weird/\\\\..\\\\/path\";\n+    let actual = clean_name(name).unwrap();\n+    let expected = name;\n+    assert_eq!(expected, &actual);\n+}\n+\n+#[test]\n+fn bad_1() {\n+    let name = \"..\";\n+    let error = clean_name(name).err().unwrap();\n+    assert!(matches!(error, error::Error::UnsafeTargetNameDotDot { .. }));\n+}\n+\n+#[test]\n+fn bad_2() {\n+    let name = \"../\";\n+    let error = clean_name(name).err().unwrap();\n+    assert!(matches!(error, error::Error::UnsafeTargetNameEmpty { .. }));\n+}\n+\n+#[test]\n+fn bad_3() {\n+    let name = \"/..\";\n+    let error = clean_name(name).err().unwrap();\n+    assert!(matches!(error, error::Error::UnsafeTargetNameSlash { .. }));\n+}"
        },
        {
          "filename": "tough/src/transport.rs",
          "status": "modified",
          "additions": 5,
          "deletions": 8,
          "patch": "@@ -4,6 +4,7 @@ use dyn_clone::DynClone;\n use std::error::Error;\n use std::fmt::{Debug, Display, Formatter};\n use std::io::{ErrorKind, Read};\n+use std::path::PathBuf;\n use url::Url;\n \n /// A trait to abstract over the method/protocol by which files are obtained.\n@@ -148,14 +149,10 @@ impl Transport for FilesystemTransport {\n             ));\n         }\n \n-        // Convert the file URL into a file path\n-        let file_path = &url.to_file_path().map_err(|_e| {\n-            TransportError::new_with_cause(\n-                TransportErrorKind::Other,\n-                &url,\n-                \"unable to get filepath from URL\".to_string(),\n-            )\n-        })?;\n+        // Convert the file URL into a file path. We need to use url.path() and not\n+        // url.to_file_path() because to_file_path will decode the percent encoding which could\n+        // restore path traversal characters.\n+        let file_path = PathBuf::from(url.path());\n \n         // And open the file\n         let f = std::fs::File::open(file_path).map_err(|e| {"
        },
        {
          "filename": "tough/tests/data/consistent-snapshots/metadata/1.root.json",
          "status": "added",
          "additions": 50,
          "deletions": 0,
          "patch": "@@ -0,0 +1,50 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"root\",\n+    \"spec_version\": \"1.0.0\",\n+    \"consistent_snapshot\": true,\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"keys\": {\n+      \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\": {\n+        \"keytype\": \"rsa\",\n+        \"keyval\": {\n+          \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBojANBgkqhkiG9w0BAQEFAAOCAY8AMIIBigKCAYEAnL6u6Q9Q6pg1G5020a83\\nGlH/aFUO0PQ5leIpwWL8kWgpaWuUG7oRlOUG2/4cwN5FCvJJGXqU5AtSKq2fZ42J\\n5XR9QMip4Pg0Q6mE8XCvAXAoMnkWSchdzgT2GoEntaOeRRTCUGb/DsVoxsVXjV6m\\nFaRMx7nh8ggshMWgTYgTUDK+CSIBCcBWapCFq1BrM60XZmGTqeAuHSHaUUuF9G3b\\ngOflH5L9IpQkaHWbJtGvyKLr53mhWO2r8BPR3+CtNZojAnkwmu4lA94k8C7TLMdc\\nutzU4OzODe9UPERc33lRv8DBgsH3F077ZQwv/ikZXWSlACTDWZwenncCEwqdeDd4\\n+q2AHyqxRN7bUAh57mUN+kFd3SS/4T44sfBrJw6N4JV/mE+/YfRLWtpIKIsXnBCb\\nrC+dt96Vqz6g6eVVvqPwhOCSKcYsmp/iS6qwVn0Dq2SCrGG1FTmBjeA9ZkcjZhUG\\nQEMyMNhoS+U2Nx5oIEIq2kREpuu+KsBSTUaOgR07WNUxAgMBAAE=\\n-----END PUBLIC KEY-----\"\n+        },\n+        \"scheme\": \"rsassa-pss-sha256\"\n+      }\n+    },\n+    \"roles\": {\n+      \"targets\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"snapshot\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"timestamp\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"root\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"3550ab1882e31e18748c85d4f259bf2f84961affb7662988f862bf20a97785564937342c6e64caeb3855bf51bc01c2235cd442a5c501d2d002aa4147a80116a6c8bd26f6cf5c668500651f175dfacf71a33614abd0ab9d4fa226b14a301a4b5466e208bfd910162a133aa42716bfe8eac0a5a89da8b06faa2efc8e0c8003ba57828df23eda75b60db2c2e665da7639f9005fa6c320627aabcbd0ee13acf7e4247c0a4cc55dcb2a1a6277cad7a4a43addd5aa9c5a403cc80dfd0a2c09370bfd124bceda0e576d5ec3a62784109f9bd636c4f7e419aa742497a65fcfaeb81c75fe186094d4602205ae50f516fe351bb9698959fcc32f459887c8f40b40bcaddca0327d20223d32caa5f23f642177d0f76374fb9c2244187b8cf27ee050dec84d77400df8b957faadb9a93f7729f651832ef103603d12a50daa1606928ceb75e51cd918c6075ec1d7a761a45923a358e20932ba8c8313778643c91ac357d8f9eb0459a4ba305709b8edd5830956bf65204b9abfbca5f78474ba044efebe32d7137a\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/consistent-snapshots/metadata/1.snapshot.json",
          "status": "added",
          "additions": 23,
          "deletions": 0,
          "patch": "@@ -0,0 +1,23 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"snapshot\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"meta\": {\n+      \"targets.json\": {\n+        \"length\": 1458,\n+        \"hashes\": {\n+          \"sha256\": \"99b6979f3593b8fd0a9afd3ee3583e0565f3ecf399823e9c90557771c5a58b66\"\n+        },\n+        \"version\": 1\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"971b8c5faa0edcc1de2e649452f3d8657dacc5d96746ac17f2b7c6ce901be2049ba6abe758f79a2cd1b09f88d55163271c2586c9adb7f96e3b82ac1173bd406190f14090e2270274b8c7b0ec91de595ba2853254d6a6322d862e0f47f77a679ffcd56ed4f54da0cb472088e7c8eb39ad0b7538f02544112e5cf5a2c5fb8e9e9b2f3c53ae9f0238437b62efc4f97b3d23b6ba7fd2b001bd29ef0b668ac66cf0e30697d7dd608f9d97d4317d2ea9c2c766978429ff7c958b05ae841fe0a7ffac9c60c7e48b3cea9eca1f845c6633b5c7aab006e78431817efbce9725cf3674e2591276c9e77b70b5787be180e9a204fd21ae7be30ec51b2d80a2f6c7b2daeb41c869c71b4a8662f1ad3e131de2cc6f18755fd1623be834e5735871c37151c305e8a136a8676f0b514dd396a329784109a0b4dbc8f97f73dd9aa7fbc3962af516088e172afacc13efa35b3de13d059cff8afcc9accda7508a8c3542ec0e918f8a546dc60257abc6389b50746a1383007b82ac09c0ddb833cf38f88ec3fc287c1ab7\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/consistent-snapshots/metadata/1.targets.json",
          "status": "added",
          "additions": 32,
          "deletions": 0,
          "patch": "@@ -0,0 +1,32 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"targets\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"targets\": {\n+      \"data1.txt\": {\n+        \"length\": 14,\n+        \"hashes\": {\n+          \"sha256\": \"5aa1d2b3bea034a0f9d0b27a1bc72919b3145a2b092b72ac0415a05e07e2bdd1\"\n+        }\n+      },\n+      \"data2.txt\": {\n+        \"length\": 14,\n+        \"hashes\": {\n+          \"sha256\": \"732b0c04a45c1296a7adf26814d2622c288e5ae1ce0cd791da84aea5a745081c\"\n+        }\n+      }\n+    },\n+    \"delegations\": {\n+      \"keys\": {},\n+      \"roles\": []\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"585197fad043d64952af6139de776e3cc7eb95c4b1078a21b5da1d4badec3f36564ff7da2281355342e8e8dc7238170c7155130fdd54d2176447ca1646f21decbdbb513e4104d491001739a7a068b402360b1d6cf82b432b2441911891fcba817b698ec6a1127f48a3c940abe44bbb747942c93af75d08d527d3da0354935fa0e6b2576c177f5cc746e01ce8ade005558f406ba3b5f79e03d51ac7417ff416b0f9c7ddce2663aa388de8451480a97373b4cd653b25e026ec9d9f1a8a9923bb6bf43aa39765ea01296b3bf3f9879d478165dcea5706f94aff1e8ef6505342128cd6eea78fe51e030e135d3fc322dfd59674dee76abc8881cc0933489e686c37add6bc85cbbcf549f61ea4d531a0c24497951a85dff50a59b810b1ea2511bad84202cb5ee4bc67fc7716595192192a55c97651a6e5f1e855df587a4e0c7d31c2e530b8fe32cee627e638ec7e76069c0174562418007479448f7d05c0074d125f77c0fc440df08f37f2a12e5e16174580e3af8ee0252c30b82c7eedda7db137ac96\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/consistent-snapshots/metadata/timestamp.json",
          "status": "added",
          "additions": 23,
          "deletions": 0,
          "patch": "@@ -0,0 +1,23 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"timestamp\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"meta\": {\n+      \"snapshot.json\": {\n+        \"length\": 1250,\n+        \"hashes\": {\n+          \"sha256\": \"9717070e2c5a2d1757e11ca830863897eb2efd8b0b3483c72d5d2010bcc0d12e\"\n+        },\n+        \"version\": 1\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"31e6d6455a866b27a79108e52af4800533e0f6fc34f1667caf6a624823d02bb884226c5e8403b87b3af64622c63214789a8de9098db7df1ffc34fc89cdac8cbcb99885d0ca4a8b37d214a6d9ebbe3a68d11faeb28316372573ab1e2e4bce7f7f8d52d397a8d3c7e4813864f33eca3b726c66f4fa3c48acbd5e6b870ac03b07d5a27b7ea6762a6672eb21fdccf13cef9f899fbcf7b821b071f75f1bc6f963d94545bb6979bfdd1482daab2454fa1c9d8880f53ae096c8e3e0797a1152218738a8eee3615c11328a14be3f4c81c55f40cf6a4217797feb85f42d1e2e007460982c7d423d83d7d4129733aa2fabe9756f415f03bf2d31d49d7fde58b0865dd4386b75235c315335710ed159cd9407b1633da8406bd3c5b3c8bfec9f9a3636ceb69d066448ddf19910f3844cb9a7f63c8d84abecd188f8c354e6cf6347c0e37129ff969b4467a5a77eb4366f8f33e30aaaf190d21c99f6a277e069b02838f6e5203012fe4f88bf6e17c1bd3e3d8efe8690bfd341b883e6e41c9351d0d9b8a3630b0a\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/consistent-snapshots/targets/5aa1d2b3bea034a0f9d0b27a1bc72919b3145a2b092b72ac0415a05e07e2bdd1.data1.txt",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+123\n+456\n+789\n+0"
        },
        {
          "filename": "tough/tests/data/consistent-snapshots/targets/732b0c04a45c1296a7adf26814d2622c288e5ae1ce0cd791da84aea5a745081c.data2.txt",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+abc\n+def\n+hij\n+k"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/metadata/%E1%9A%A9%20os%2C%20%E1%9A%B1%20rad%2C%20%E1%9A%B3%20cen%2C%20%E1%9A%B7%20gyfu%2C%20%E1%9A%B9%20%C6%BFynn%2C%20%E1%9A%BB%20h%C3%A6gl%2C%20....json",
          "status": "added",
          "additions": 27,
          "deletions": 0,
          "patch": "@@ -0,0 +1,27 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"targets\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 3,\n+    \"expires\": \"3021-01-27T00:56:48.450414Z\",\n+    \"targets\": {},\n+    \"delegations\": {\n+      \"keys\": {\n+        \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\": {\n+          \"keytype\": \"rsa\",\n+          \"keyval\": {\n+            \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAq0Q5xtn047yiCQlsZELR\\nnUbk5tnIXEresMBpxu9NC5t2ywjhA/EQ22aX6Sy85PAvSUlE+DBIbLmm5o0EPbF8\\n7e6EkZxj5Nz3O/UYMCgqpLE1bNIFSZzQNXOAfYqWsTE9rIQpJnZfpmPSruZ95xdN\\nZsXh6rCdM3HfSpID+hE3Mq97dehdoW18DGxnorkuzTTLD9oA+Wz+Ctq1wpmBKraH\\npkI8Q4QQ0ej74dEgEXxdLlAjFnEWmU/yTwUoa5hXtYcwq7MB/haT9DkUmeI4Wyk7\\n5gamMun1tFgnEXso+YePUQg2ySMam0/nWVCbVMqVBveWk+TawT5Z8SytNKXQTCNS\\nswIDAQAB\\n-----END PUBLIC KEY-----\"\n+          },\n+          \"scheme\": \"rsassa-pss-sha256\"\n+        }\n+      },\n+      \"roles\": []\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\",\n+      \"sig\": \"4aba04bd0217a205b1d799a1b7b7163b46328e2192cc63a992b04858f5f6b4709dbe585f916141ba7ffe15f07915ae9d853a79c106dadb0782fd368842226c88b27de15f3a12e42a83c5e1332cb889dba5b833976c42f4007cc3606ef3a84851c92318f5fde2b1e8a6c99e8dc22f8651703f16dc7d220013cb9da3226a9836e1143670208a45e91b6b405d24667d355637b2b0f8ada410f1e819443102a6b270e23c41222c001399142a8abb3bfe63dc6d313b7b345dbb918f71c5c3ad6a02295fe224387175350c87c9e356740aafbad09fceae6354acbcc214cd504607a6867848f5460cea87e1ba73945dba2b155f08b61b58d6ee55bdebc2327ac5e854c3\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/metadata/%F0%9F%8D%BA%2F30.json",
          "status": "added",
          "additions": 27,
          "deletions": 0,
          "patch": "@@ -0,0 +1,27 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"targets\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 3,\n+    \"expires\": \"3021-01-27T00:56:42.752354Z\",\n+    \"targets\": {},\n+    \"delegations\": {\n+      \"keys\": {\n+        \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\": {\n+          \"keytype\": \"rsa\",\n+          \"keyval\": {\n+            \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAq0Q5xtn047yiCQlsZELR\\nnUbk5tnIXEresMBpxu9NC5t2ywjhA/EQ22aX6Sy85PAvSUlE+DBIbLmm5o0EPbF8\\n7e6EkZxj5Nz3O/UYMCgqpLE1bNIFSZzQNXOAfYqWsTE9rIQpJnZfpmPSruZ95xdN\\nZsXh6rCdM3HfSpID+hE3Mq97dehdoW18DGxnorkuzTTLD9oA+Wz+Ctq1wpmBKraH\\npkI8Q4QQ0ej74dEgEXxdLlAjFnEWmU/yTwUoa5hXtYcwq7MB/haT9DkUmeI4Wyk7\\n5gamMun1tFgnEXso+YePUQg2ySMam0/nWVCbVMqVBveWk+TawT5Z8SytNKXQTCNS\\nswIDAQAB\\n-----END PUBLIC KEY-----\"\n+          },\n+          \"scheme\": \"rsassa-pss-sha256\"\n+        }\n+      },\n+      \"roles\": []\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\",\n+      \"sig\": \"2fb047a4d9263ca4e15b7767010d00ef1b5f71cac307b4299f3520f15ec715f1a7bd62dd2da4d4f3e8cf3e8b961ed022c937f5276db1f2769b6d0ac04879ecf2c3450bb9e4bb2905b9a199f1378493d6e14837fb911b3de998c5c5a4d0b5aaa16fe3005e44833ad19777af105ceb7840b6f1e63736c50a1e89cb8a88bb0bce800dffd3f5d00d01aec53c4057cd8de03be11a212b3680646e819f6d3ef9c4fd5de00437e61ee32fe78a1d331d5e6211a315fc8f622f2e7716738ba06e5a0fac1c932f5805ae60cca4f1651b2b11318c223c3205e55a69ff92d0027144843ca77f7cdf0cbf8a820f6e3f88f0896cf7ec2b896d63dcc69466b21f62c76c2ff0a39f\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/metadata/..%2F..%2Fpath%2Flike%2Fdubious.json",
          "status": "added",
          "additions": 27,
          "deletions": 0,
          "patch": "@@ -0,0 +1,27 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"targets\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 3,\n+    \"expires\": \"3021-01-27T00:56:35.229228Z\",\n+    \"targets\": {},\n+    \"delegations\": {\n+      \"keys\": {\n+        \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\": {\n+          \"keytype\": \"rsa\",\n+          \"keyval\": {\n+            \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAq0Q5xtn047yiCQlsZELR\\nnUbk5tnIXEresMBpxu9NC5t2ywjhA/EQ22aX6Sy85PAvSUlE+DBIbLmm5o0EPbF8\\n7e6EkZxj5Nz3O/UYMCgqpLE1bNIFSZzQNXOAfYqWsTE9rIQpJnZfpmPSruZ95xdN\\nZsXh6rCdM3HfSpID+hE3Mq97dehdoW18DGxnorkuzTTLD9oA+Wz+Ctq1wpmBKraH\\npkI8Q4QQ0ej74dEgEXxdLlAjFnEWmU/yTwUoa5hXtYcwq7MB/haT9DkUmeI4Wyk7\\n5gamMun1tFgnEXso+YePUQg2ySMam0/nWVCbVMqVBveWk+TawT5Z8SytNKXQTCNS\\nswIDAQAB\\n-----END PUBLIC KEY-----\"\n+          },\n+          \"scheme\": \"rsassa-pss-sha256\"\n+        }\n+      },\n+      \"roles\": []\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\",\n+      \"sig\": \"60dfadc88922c7301b8a83cdacfe7ac490fdf2b9d6049ad931507f078b9c5a31ce41130c440798ed3755809702ea9d5243d41de9edbcf7ce8501feb5b9ad607f00814d5ee0d94e216d74cdbbbc9383dc2639e602393c8b4adf73b3e18423ea5e6da713c59b97fd0c719e863a406cff0f1cf7938d44bf292a1dc20b0c6a85e17ae77060a8fac84d34d821e0ed0fc4a77143d7ec39afc59b305ca4633e012485d38aef69fb780937a1f7dfbccbee988d2370758e8e8765796c9d85db6fc794a8d792a184d49e60a0c543abb6fc792cc85ba4c4c59a38f3199c91778d2d8110a1435de864d45d778a2809479367317951ce257c7dc245fa61300487633363566067\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/metadata/1.root.json",
          "status": "added",
          "additions": 50,
          "deletions": 0,
          "patch": "@@ -0,0 +1,50 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"root\",\n+    \"spec_version\": \"1.0.0\",\n+    \"consistent_snapshot\": false,\n+    \"version\": 1,\n+    \"expires\": \"3021-01-27T00:56:15Z\",\n+    \"keys\": {\n+      \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\": {\n+        \"keytype\": \"rsa\",\n+        \"keyval\": {\n+          \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAq0Q5xtn047yiCQlsZELR\\nnUbk5tnIXEresMBpxu9NC5t2ywjhA/EQ22aX6Sy85PAvSUlE+DBIbLmm5o0EPbF8\\n7e6EkZxj5Nz3O/UYMCgqpLE1bNIFSZzQNXOAfYqWsTE9rIQpJnZfpmPSruZ95xdN\\nZsXh6rCdM3HfSpID+hE3Mq97dehdoW18DGxnorkuzTTLD9oA+Wz+Ctq1wpmBKraH\\npkI8Q4QQ0ej74dEgEXxdLlAjFnEWmU/yTwUoa5hXtYcwq7MB/haT9DkUmeI4Wyk7\\n5gamMun1tFgnEXso+YePUQg2ySMam0/nWVCbVMqVBveWk+TawT5Z8SytNKXQTCNS\\nswIDAQAB\\n-----END PUBLIC KEY-----\"\n+        },\n+        \"scheme\": \"rsassa-pss-sha256\"\n+      }\n+    },\n+    \"roles\": {\n+      \"timestamp\": {\n+        \"keyids\": [\n+          \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"targets\": {\n+        \"keyids\": [\n+          \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"root\": {\n+        \"keyids\": [\n+          \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"snapshot\": {\n+        \"keyids\": [\n+          \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\"\n+        ],\n+        \"threshold\": 1\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\",\n+      \"sig\": \"a24efe8b7c35f6539cbc726030b295e01cb4a045e8b1d3b57af212aa51346ff5523e5a4282f704ecdcb2d5dc28ea710323d675b406bf85eb71b9b17678c43530357baac87e4bac45cc425fa2a06df8fdb29b42f73b627e2ea8131d0a2ac27d4acb27195fc4633596cd20c494ddc63fdbd6d7edd7fc14060018acf24d22afc5d105310ec4a8e620daee142985791859501f7885d98705354ee55d176449c2594815a5251872b1a95209dcea0056f982c26722183e36817f040bc6981491524bff711efe9ad403d7645b47e408585b6ac3b9778231a02c940d183d22668eb54d4e74daa741774c57450c790f823407c8695ad7ded0bd75e035f93f25e9710a5725\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/metadata/snapshot.json",
          "status": "added",
          "additions": 44,
          "deletions": 0,
          "patch": "@@ -0,0 +1,44 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"snapshot\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 2,\n+    \"expires\": \"3021-01-27T00:57:00.468905Z\",\n+    \"meta\": {\n+      \"\u16a9 os, \u16b1 rad, \u16b3 cen, \u16b7 gyfu, \u16b9 \u01bfynn, \u16bb h\u00e6gl, ....json\": {\n+        \"length\": 1548,\n+        \"hashes\": {\n+          \"sha256\": \"1f60219a41e50038468ab84e27a66e9c08674c6e064210acffe434a76de19dfb\"\n+        },\n+        \"version\": 3\n+      },\n+      \"targets.json\": {\n+        \"length\": 2775,\n+        \"hashes\": {\n+          \"sha256\": \"e94a1efb2c7fb59b80d4d88a4b2a1bd8955d0472105befb279f3dbf80c632255\"\n+        },\n+        \"version\": 2\n+      },\n+      \"../../path/like/dubious.json\": {\n+        \"length\": 1548,\n+        \"hashes\": {\n+          \"sha256\": \"67050f56e6d3d79a8393097b2f668fe1231299f97a015c81947d5caa06577a94\"\n+        },\n+        \"version\": 3\n+      },\n+      \"\ud83c\udf7a/30.json\": {\n+        \"length\": 1548,\n+        \"hashes\": {\n+          \"sha256\": \"1909ba25ca102c175566b93e6b1b6d3fb55f2b5a1ef88d50bcabd3064eb7fcc6\"\n+        },\n+        \"version\": 3\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\",\n+      \"sig\": \"0d0a5ff0a6a80cf574657dcaac159675734587493cf91fb54302bb33a99ef1841a4fbf7dfdbb10a7c104d5b9830a15f742898a2e656dcab1956ddeaa11cc1eae7a8413681a7de24a51342a18d77c843536c6c40aa2f7ce935a769ec853da70589432a9dcd95d5a130324bfb88d8be951a706d99ef76cf1f2677a2dde5d13ea75067d80aecafa91d6f2509a5e6a93c1b4649d101117c24000f4909ff30836409da975b6222b5e117692d96bec5c189371915b555d5f07588acd6c73eb0a94d7cff54080e547f1e4003b36f8cf37f388f88473011ea034ced32d998c1e9210a919c36003991c1d5284e4a6225a3dc4e969a3f172990c94fecf19b19a8feaad7fd5\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/metadata/targets.json",
          "status": "added",
          "additions": 74,
          "deletions": 0,
          "patch": "@@ -0,0 +1,74 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"targets\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 2,\n+    \"expires\": \"3021-01-27T00:57:00.468918Z\",\n+    \"targets\": {\n+      \"2.txt\": {\n+        \"length\": 2,\n+        \"hashes\": {\n+          \"sha256\": \"53c234e5e8472b6ac51c1ae1cab3fe06fad053beb8ebfd8977b010655bfdd3c3\"\n+        }\n+      },\n+      \"1.txt\": {\n+        \"length\": 2,\n+        \"hashes\": {\n+          \"sha256\": \"4355a46b19d348dc2f57c046f8ef63d4538ebb936000f3c9ee954a27460dd865\"\n+        }\n+      }\n+    },\n+    \"delegations\": {\n+      \"keys\": {\n+        \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\": {\n+          \"keytype\": \"rsa\",\n+          \"keyval\": {\n+            \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAq0Q5xtn047yiCQlsZELR\\nnUbk5tnIXEresMBpxu9NC5t2ywjhA/EQ22aX6Sy85PAvSUlE+DBIbLmm5o0EPbF8\\n7e6EkZxj5Nz3O/UYMCgqpLE1bNIFSZzQNXOAfYqWsTE9rIQpJnZfpmPSruZ95xdN\\nZsXh6rCdM3HfSpID+hE3Mq97dehdoW18DGxnorkuzTTLD9oA+Wz+Ctq1wpmBKraH\\npkI8Q4QQ0ej74dEgEXxdLlAjFnEWmU/yTwUoa5hXtYcwq7MB/haT9DkUmeI4Wyk7\\n5gamMun1tFgnEXso+YePUQg2ySMam0/nWVCbVMqVBveWk+TawT5Z8SytNKXQTCNS\\nswIDAQAB\\n-----END PUBLIC KEY-----\"\n+          },\n+          \"scheme\": \"rsassa-pss-sha256\"\n+        }\n+      },\n+      \"roles\": [\n+        {\n+          \"name\": \"../../path/like/dubious\",\n+          \"keyids\": [\n+            \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\"\n+          ],\n+          \"threshold\": 1,\n+          \"paths\": [\n+            \"foo\"\n+          ],\n+          \"terminating\": false\n+        },\n+        {\n+          \"name\": \"\ud83c\udf7a/30\",\n+          \"keyids\": [\n+            \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\"\n+          ],\n+          \"threshold\": 1,\n+          \"paths\": [\n+            \"foo\"\n+          ],\n+          \"terminating\": false\n+        },\n+        {\n+          \"name\": \"\u16a9 os, \u16b1 rad, \u16b3 cen, \u16b7 gyfu, \u16b9 \u01bfynn, \u16bb h\u00e6gl, ...\",\n+          \"keyids\": [\n+            \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\"\n+          ],\n+          \"threshold\": 1,\n+          \"paths\": [\n+            \"foo\"\n+          ],\n+          \"terminating\": false\n+        }\n+      ]\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\",\n+      \"sig\": \"3bd8272207c4733e070795dce7e479eaae40c39443837f2bca17824a80ea62d8be74b23f2f7336bb310beb09199cc2120f934a1b5e34c3a296604825e7145d723b083aade5114c2ec1a4b10c070e386b6b9b95ae828e2a5072ce4b21f3f06129837965be17661c4295a4086c917da1bff443b29e877c3bcb8fc72a3c688c546718deb11b5e354a5de8ff46f568d75e26af870c5b606eda946903a1abc05022b9e12c2b0c2943daf04fa0ad98543ee8326fabb86720becce985e924cb308a84e4734562080ecf09e336c423b2e38701d4bdd0df56ffae8d7055897a73b2287b6dc9eec8cb1e3726ee563ac8de1de3b13ad2cf7206cc7f5e3e1d02d662a2bb71eb\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/metadata/timestamp.json",
          "status": "added",
          "additions": 23,
          "deletions": 0,
          "patch": "@@ -0,0 +1,23 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"timestamp\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 2,\n+    \"expires\": \"3021-01-27T00:57:00.468924Z\",\n+    \"meta\": {\n+      \"snapshot.json\": {\n+        \"length\": 1659,\n+        \"hashes\": {\n+          \"sha256\": \"53cfa83b9ed82fee5da8db1d05efcd5555f3cbfc2624a63af1e2f93f4813bd2a\"\n+        },\n+        \"version\": 2\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"0f432a9dfff85a943dd7a4fb7ff3221ad6495ef81d0342cb74094c208aa8934b\",\n+      \"sig\": \"925d2691a908b3a0bee425e04ba35e6616abab4976166b1c0459c138fef933abf3fa0f7fd52e778ff8fc67c2c9503804ee03bc02bdf2109ee25ca60e4fa5395938748dfa00b8f60abaca3fb51ed47f566e44dbc20a380246f9fbcc32612d239b159370a7d9c9b387deaa0c0cf18bfd3e5de510df7214c19b2f032eefadc9b0e64e106ad147a41d76aad0e27310771b86f3b3d50853dde91128c8b9429f380f8703ce624c616633fbfb8991618c5607f8fcd12e22909c3d3fccbad26981635decf1a64d1c6fad1cffc3c697b442ddd051a72a0b0065302bbcdedb9f43ab49371d22cfaa3c7fd323c7266c08049c60eed17690b5c833f99acd48071b16d69e8d5d\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/targets/1.txt",
          "status": "added",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -0,0 +1 @@\n+1"
        },
        {
          "filename": "tough/tests/data/dubious-role-names/targets/2.txt",
          "status": "added",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -0,0 +1 @@\n+2"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/metadata/1.root.json",
          "status": "added",
          "additions": 50,
          "deletions": 0,
          "patch": "@@ -0,0 +1,50 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"root\",\n+    \"spec_version\": \"1.0.0\",\n+    \"consistent_snapshot\": false,\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"keys\": {\n+      \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\": {\n+        \"keytype\": \"rsa\",\n+        \"keyval\": {\n+          \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBojANBgkqhkiG9w0BAQEFAAOCAY8AMIIBigKCAYEAnL6u6Q9Q6pg1G5020a83\\nGlH/aFUO0PQ5leIpwWL8kWgpaWuUG7oRlOUG2/4cwN5FCvJJGXqU5AtSKq2fZ42J\\n5XR9QMip4Pg0Q6mE8XCvAXAoMnkWSchdzgT2GoEntaOeRRTCUGb/DsVoxsVXjV6m\\nFaRMx7nh8ggshMWgTYgTUDK+CSIBCcBWapCFq1BrM60XZmGTqeAuHSHaUUuF9G3b\\ngOflH5L9IpQkaHWbJtGvyKLr53mhWO2r8BPR3+CtNZojAnkwmu4lA94k8C7TLMdc\\nutzU4OzODe9UPERc33lRv8DBgsH3F077ZQwv/ikZXWSlACTDWZwenncCEwqdeDd4\\n+q2AHyqxRN7bUAh57mUN+kFd3SS/4T44sfBrJw6N4JV/mE+/YfRLWtpIKIsXnBCb\\nrC+dt96Vqz6g6eVVvqPwhOCSKcYsmp/iS6qwVn0Dq2SCrGG1FTmBjeA9ZkcjZhUG\\nQEMyMNhoS+U2Nx5oIEIq2kREpuu+KsBSTUaOgR07WNUxAgMBAAE=\\n-----END PUBLIC KEY-----\"\n+        },\n+        \"scheme\": \"rsassa-pss-sha256\"\n+      }\n+    },\n+    \"roles\": {\n+      \"snapshot\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"targets\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"timestamp\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      },\n+      \"root\": {\n+        \"keyids\": [\n+          \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+        ],\n+        \"threshold\": 1\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"9c835856f47dca75c63833581bd688ba397a2cade3df3a9356ee185e87db2831da21ac0fcd7f4530fb7cc8fb607f0a68be365e286cbcb04eb58acd0489e090531606b93b6397a0f91a59a58fd7089cb70e80f91f48819237cc464ad40914590c3ebcdcd9e9f57de917c014387eee1db06b7e3b2b1320eb1470ecafa347c3eeb213d7ca3586c492c3789e0e9a9b343838f4acc8141837b72af5f594bb5c845161e2ce8011ee561e6e467fbad6f2b53ebb2118b84132e58b6b777cf7f1674f8d403ab51616189c2e11c705c43abba88de7129d810a7c4d996d4ba995ac035a16f59ff958360c45608078e408796a815d65ee5906523074d5cfdda59a4ede34dce35336bd72a5a970c4a68436f3ae0c6f0685b519e7f66ac1970aa337efeb64db05f223c9ab02dda58cf85853c69932cac693a86358ec9ee93000aa0591e224f83d5b581dd6bbca0e7b639b0b773a688f9776aca91fc8434444aeb4717f8a8ab7901be048b1f1997720dc0b0e34ea4a8fb5317b14709cca7ab1683a6722785b94a3\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/metadata/delegated.json",
          "status": "added",
          "additions": 26,
          "deletions": 0,
          "patch": "@@ -0,0 +1,26 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"targets\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"targets\": {\n+      \"../delegated/foo/../subdir/data3.txt\": {\n+        \"length\": 14,\n+        \"hashes\": {\n+          \"sha256\": \"ca4fb8d92326eccc3bf37bb1b5cb2c57bb558c884db3180e6e3ab65631ed9cd2\"\n+        }\n+      }\n+    },\n+    \"delegations\": {\n+      \"keys\": {},\n+      \"roles\": []\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"2adaaef0b3e49a7bb88573e9fc6888543938c2e6784a6c464f59ba69804300da3a01fec06ddad9539f34722895bd799539c0e7645651abff6c1e8f7e4e4509f271989e92646b62ab14d39c7c2afba1dea350a9f019248b6972fca03992a1313f8a2b760c8c5c91a0216f5215a06123bfa39a195b6d3eb066c8796b68169679fbc21079f842c6dbf985eb3b1b54612f3c98a9300c291a7eec2dd16373fbabdedf6bf091a9edab5d9d36591a200aa1437f8cf238268798562d59edb6db1b1c75460a9f2a402d55e3e5d45286d51f6362b2daefa95cf7a3f99cec3fb086543cd15ab2ecd63469bb5bddaef50f9140b0af84fe37ed9fc30d6f019e94d0068006780258d9e8a643e17d34fe8f20ad19c7365f2b00c640c9c884a617d0b06c54203ca659799f33c0f4786e1dc0e8b6ab3650242e1d48ea5a0dae49ca0fda06b0ed597e2cdf9c141901d67997984e8816f744f54baaf29c2ff96e7afde644f364c9d175496f45fdb9a8584cd8289cf88337d4f7d968ecf9313725f362a7e05cd9873f55\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/metadata/snapshot.json",
          "status": "added",
          "additions": 30,
          "deletions": 0,
          "patch": "@@ -0,0 +1,30 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"snapshot\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"meta\": {\n+      \"delegated.json\": {\n+        \"length\": 1316,\n+        \"hashes\": {\n+          \"sha256\": \"b6eba7d8126d2082cce72d23b150694b2ccf57ccb31bae18c3af44d152fbb46e\"\n+        },\n+        \"version\": 1\n+      },\n+      \"targets.json\": {\n+        \"length\": 2640,\n+        \"hashes\": {\n+          \"sha256\": \"9db65e955c42616481ff9439ed984a6da4b6cf0d7c19a2b176d7eaa7a5b38243\"\n+        },\n+        \"version\": 1\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"9259ce071cd1bfc1c3823636632c61d1df94d96d2d69bdea83343b82ba25ff44cabdd64dfb072b47ba8f8acc3c7ec81f44794febab9b1b20158a8298d7b7e426fe47ef7efa38d9112c3433a597545d76b80cc409a45098cbf25f2fade0bc685fe5ceace937a9bd6e44a63be0c462e88ffae7ae083862d14aca60861d85c461bece6d80d440cc37fc1b99f213f576e0b7ac18d2aae76456f89608e4a16aa004ff8b30e6a48119471507af29136a27c652a5df645edb5c5934eca5937a4334992c65e5ec2139740332c47632825af24ebac9495f4d30273775832942ed1d3a220025c7907d8d237bf1bdda3a4eb37a7fcef59d464deb352d179d593d8a49d9d69f022d7feaf80b68aa1b62ce0d5cb4cc633cc6f7c9671742d1b439bc5e6bf0adecadb4109a29f37a582a12ac7334032571796e78364c0bdec873592f3bb20110498886b0dd24190c556920bd063789dd6f12f11481422d3a6ae03a53b7cf7ba0dd679135c087b5f4912e8356eaebfe930420f254c31289b5521f74f1d120f2e4cb\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/metadata/targets.json",
          "status": "added",
          "additions": 52,
          "deletions": 0,
          "patch": "@@ -0,0 +1,52 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"targets\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"targets\": {\n+      \"foo/../bar/../baz/../data1.txt\": {\n+        \"length\": 14,\n+        \"hashes\": {\n+          \"sha256\": \"5aa1d2b3bea034a0f9d0b27a1bc72919b3145a2b092b72ac0415a05e07e2bdd1\"\n+        }\n+      },\n+      \"foo/bar/baz/../data2.txt\": {\n+        \"length\": 14,\n+        \"hashes\": {\n+          \"sha256\": \"732b0c04a45c1296a7adf26814d2622c288e5ae1ce0cd791da84aea5a745081c\"\n+        }\n+      }\n+    },\n+    \"delegations\": {\n+      \"keys\": {\n+        \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\": {\n+          \"keytype\": \"rsa\",\n+          \"keyval\": {\n+            \"public\": \"-----BEGIN PUBLIC KEY-----\\nMIIBojANBgkqhkiG9w0BAQEFAAOCAY8AMIIBigKCAYEAnL6u6Q9Q6pg1G5020a83\\nGlH/aFUO0PQ5leIpwWL8kWgpaWuUG7oRlOUG2/4cwN5FCvJJGXqU5AtSKq2fZ42J\\n5XR9QMip4Pg0Q6mE8XCvAXAoMnkWSchdzgT2GoEntaOeRRTCUGb/DsVoxsVXjV6m\\nFaRMx7nh8ggshMWgTYgTUDK+CSIBCcBWapCFq1BrM60XZmGTqeAuHSHaUUuF9G3b\\ngOflH5L9IpQkaHWbJtGvyKLr53mhWO2r8BPR3+CtNZojAnkwmu4lA94k8C7TLMdc\\nutzU4OzODe9UPERc33lRv8DBgsH3F077ZQwv/ikZXWSlACTDWZwenncCEwqdeDd4\\n+q2AHyqxRN7bUAh57mUN+kFd3SS/4T44sfBrJw6N4JV/mE+/YfRLWtpIKIsXnBCb\\nrC+dt96Vqz6g6eVVvqPwhOCSKcYsmp/iS6qwVn0Dq2SCrGG1FTmBjeA9ZkcjZhUG\\nQEMyMNhoS+U2Nx5oIEIq2kREpuu+KsBSTUaOgR07WNUxAgMBAAE=\\n-----END PUBLIC KEY-----\"\n+          },\n+          \"scheme\": \"rsassa-pss-sha256\"\n+        }\n+      },\n+      \"roles\": [\n+        {\n+          \"name\": \"delegated\",\n+          \"keyids\": [\n+            \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\"\n+          ],\n+          \"threshold\": 1,\n+          \"paths\": [\n+            \"delegated/*\"\n+          ],\n+          \"terminating\": false\n+        }\n+      ]\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"037a918944ce4f8659a8a3f508140960488232b9414290741bec6afbb00ce9ebd2e3bf302365ee9a8271ad2884c19e460485bc7ab4a2a13997b4dc0185feabd379df998b217ab0b3a5422c929b7dbe98b227dddb38f4601cf805d7f7594df603d762a8d40cf1b940e13e7b40ca79448d2b629b555a300e1af61550eaff93a6f2d21d15db4ac782d5cd1e2423a8d8b716173923b683e08345ab1d924407fb4b843c9ab86925707aae92de4339d371861baf370202233b8a1a96185ecb219fb1500b6087d2744d20eaa26301c1bc2ae175107c9f77c5b786dd14c3efea5724c039abda455d71cd57d59d50a925b0107776f8480ba3b56b6543f707e7b56942748458165f0d6ecb4fe525812252dc13618b740f7eecc478f94c660fb05d1cbafa2740367637496d9ab9f59efda9abf3cb2de4d4b41e44fd59b4fe557b6ca40c6825f10740f3a51cf358ec6f7423c136541cd316a071c70a3eab0416143fea04e032d53b5e0f72f695b9048ff7042406b4890caa8b80e64110cd2bc6bbcf896b3f57\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/metadata/timestamp.json",
          "status": "added",
          "additions": 23,
          "deletions": 0,
          "patch": "@@ -0,0 +1,23 @@\n+{\n+  \"signed\": {\n+    \"_type\": \"timestamp\",\n+    \"spec_version\": \"1.0.0\",\n+    \"version\": 1,\n+    \"expires\": \"2999-01-01T00:00:00Z\",\n+    \"meta\": {\n+      \"snapshot.json\": {\n+        \"length\": 1448,\n+        \"hashes\": {\n+          \"sha256\": \"e6207dd2cafe95a9f3380b752a453749df016f88b431f79f4f07425b60f8ab4c\"\n+        },\n+        \"version\": 1\n+      }\n+    }\n+  },\n+  \"signatures\": [\n+    {\n+      \"keyid\": \"69f069cf595e3f09cbe99a9f0f82127f7c7d2fde859e30fd5c0f2b4fc9c4a507\",\n+      \"sig\": \"1368df2b0bdd0241cb4b2c45f5106ae6aed4e6c6a47bd1eaa8eadcd0f600fd635a171f56904436e84062892b8afc804a3071afaea6f4c1ac30e455e60f829c2907523686093707561c49ca4a1d1b0bfeb875bfaa6d83af57f74c8d15429e43f34b77e2f09bc5f1eddc655d33e271adf88fa15aa5de5aa2d60de2d387e07c8c8a94484f4fdeda4032e504648cd0a3fcdf51f1fecb0412d85e2575ff0f3dd9aaa0f2b020acad2182a8622307328d9cc7e29b6cf9507d3155916f459d516e7df2e89a7f39aab78a911216ceab189271dcc4191ef06e48b7ec85cc30fc2cc2590b734ac03527765f03deafeb7f718f6a6cc23675f759bab06840d4ae40ddce8968c7b9d681e4c62b49552b136e10ea79d7e966a605404b295ac4fd604608d817c6c68b9fabe0c7828a2cab812f2c1af3a8b21b11055871dc8e605a502c52d51441846009c7979cd14885723839b02a4f785baa7259b80cf0ed2d0bbdfcfd7081a21001d6cc2cd974e3ec92ab9d738a7570303e0b814ef77f23f4180cb92dd6881b0b\"\n+    }\n+  ]\n+}"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/targets/data1.txt",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+123\n+456\n+789\n+0"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/targets/delegated/subdir/data3.txt",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+!@#\n+$%^\n+&*(\n+)"
        },
        {
          "filename": "tough/tests/data/safe-target-paths/targets/foo/bar/data2.txt",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+abc\n+def\n+hij\n+k"
        },
        {
          "filename": "tough/tests/http.rs",
          "status": "modified",
          "additions": 6,
          "deletions": 4,
          "patch": "@@ -7,7 +7,7 @@ mod http_happy {\n     use httptest::{matchers::*, responders::*, Expectation, Server};\n     use std::fs::File;\n     use std::str::FromStr;\n-    use tough::{DefaultTransport, HttpTransport, RepositoryLoader, Transport};\n+    use tough::{DefaultTransport, HttpTransport, RepositoryLoader, TargetName, Transport};\n     use url::Url;\n \n     /// Set an expectation in a test HTTP server which serves a file from `tuf-reference-impl`.\n@@ -67,19 +67,21 @@ mod http_happy {\n         .load()\n         .unwrap();\n \n+        let file1 = TargetName::new(\"file1.txt\").unwrap();\n         assert_eq!(\n-            read_to_end(repo.read_target(\"file1.txt\").unwrap().unwrap()),\n+            read_to_end(repo.read_target(&file1).unwrap().unwrap()),\n             &b\"This is an example target file.\"[..]\n         );\n+        let file2 = TargetName::new(\"file2.txt\").unwrap();\n         assert_eq!(\n-            read_to_end(repo.read_target(\"file2.txt\").unwrap().unwrap()),\n+            read_to_end(repo.read_target(&file2).unwrap().unwrap()),\n             &b\"This is an another example target file.\"[..]\n         );\n         assert_eq!(\n             repo.targets()\n                 .signed\n                 .targets\n-                .get(\"file1.txt\")\n+                .get(&file1)\n                 .unwrap()\n                 .custom\n                 .get(\"file_permissions\")"
        },
        {
          "filename": "tough/tests/interop.rs",
          "status": "modified",
          "additions": 37,
          "deletions": 5,
          "patch": "@@ -4,7 +4,7 @@\n use std::fs::File;\n use tempfile::TempDir;\n use test_utils::{dir_url, read_to_end, test_data};\n-use tough::{FilesystemTransport, Limits, Repository, RepositoryLoader};\n+use tough::{FilesystemTransport, Limits, Repository, RepositoryLoader, TargetName};\n \n mod test_utils;\n \n@@ -27,19 +27,22 @@ fn test_tuf_reference_impl() {\n }\n \n fn assert_tuf_reference_impl(repo: &Repository) {\n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n+    let file2 = TargetName::new(\"file2.txt\").unwrap();\n+    let file3 = TargetName::new(\"file3.txt\").unwrap();\n     assert_eq!(\n-        read_to_end(repo.read_target(\"file1.txt\").unwrap().unwrap()),\n+        read_to_end(repo.read_target(&file1).unwrap().unwrap()),\n         &b\"This is an example target file.\"[..]\n     );\n     assert_eq!(\n-        read_to_end(repo.read_target(\"file2.txt\").unwrap().unwrap()),\n+        read_to_end(repo.read_target(&file2).unwrap().unwrap()),\n         &b\"This is an another example target file.\"[..]\n     );\n     assert_eq!(\n         repo.targets()\n             .signed\n             .targets\n-            .get(\"file1.txt\")\n+            .get(&file1)\n             .unwrap()\n             .custom\n             .get(\"file_permissions\")\n@@ -53,7 +56,7 @@ fn assert_tuf_reference_impl(repo: &Repository) {\n         .delegations\n         .as_ref()\n         .unwrap()\n-        .target_is_delegated(&\"file3.txt\".to_string()));\n+        .target_is_delegated(&file3));\n }\n \n /// Test that `tough` can process repositories generated by [`tuf`], the reference Python\n@@ -80,3 +83,32 @@ fn test_tuf_reference_impl_default_transport() {\n     .unwrap();\n     assert_tuf_reference_impl(&repo);\n }\n+\n+/// Test that `tough` can load a repository that has some unusual delegate role names. This ensures\n+/// that percent encoded role names are handled correctly and that path traversal characters in a\n+/// role name do not cause `tough` to write outside of its datastore.\n+#[test]\n+fn test_dubious_role_name() {\n+    let base = test_data().join(\"dubious-role-names\");\n+    let datastore = TempDir::new().unwrap();\n+\n+    let repo = RepositoryLoader::new(\n+        File::open(base.join(\"metadata\").join(\"1.root.json\")).unwrap(),\n+        dir_url(base.join(\"metadata\")),\n+        dir_url(base.join(\"targets\")),\n+    )\n+    .datastore(datastore.path())\n+    .load()\n+    .unwrap();\n+\n+    // Prove that the role name has path traversal characters.\n+    let expected_rolename = \"../../path/like/dubious\";\n+    assert_eq!(\n+        repo.delegated_role(expected_rolename).unwrap().name,\n+        expected_rolename\n+    );\n+\n+    // Prove that the the role's metadata filename has not been written outside of the datastore.\n+    let expected_filename = \"..%2F..%2Fpath%2Flike%2Fdubious.json\";\n+    assert!(datastore.path().join(expected_filename).is_file())\n+}"
        },
        {
          "filename": "tough/tests/repo_cache.rs",
          "status": "modified",
          "additions": 76,
          "deletions": 8,
          "patch": "@@ -5,8 +5,8 @@ use std::fs::File;\n use std::io::Read;\n use std::path::PathBuf;\n use tempfile::TempDir;\n-use test_utils::{dir_url, test_data};\n-use tough::{Repository, RepositoryLoader};\n+use test_utils::{dir_url, read_to_end, test_data, DATA_1, DATA_2};\n+use tough::{Repository, RepositoryLoader, TargetName};\n use url::Url;\n \n mod test_utils;\n@@ -72,17 +72,19 @@ fn test_repo_cache_all_targets() {\n \n     // the copied repo should have file1 and file2 (i.e. all of targets).\n     let mut file_data = Vec::new();\n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n     let file_size = copied_repo\n-        .read_target(\"file1.txt\")\n+        .read_target(&file1)\n         .unwrap()\n         .unwrap()\n         .read_to_end(&mut file_data)\n         .unwrap();\n     assert_eq!(31, file_size);\n \n     let mut file_data = Vec::new();\n+    let file2 = TargetName::new(\"file2.txt\").unwrap();\n     let file_size = copied_repo\n-        .read_target(\"file2.txt\")\n+        .read_target(&file2)\n         .unwrap()\n         .unwrap()\n         .read_to_end(&mut file_data)\n@@ -121,17 +123,19 @@ fn test_repo_cache_list_of_two_targets() {\n \n     // the copied repo should have file1 and file2 (i.e. all of the listed targets).\n     let mut file_data = Vec::new();\n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n     let file_size = copied_repo\n-        .read_target(\"file1.txt\")\n+        .read_target(&file1)\n         .unwrap()\n         .unwrap()\n         .read_to_end(&mut file_data)\n         .unwrap();\n     assert_eq!(31, file_size);\n \n     let mut file_data = Vec::new();\n+    let file2 = TargetName::new(\"file2.txt\").unwrap();\n     let file_size = copied_repo\n-        .read_target(\"file2.txt\")\n+        .read_target(&file2)\n         .unwrap()\n         .unwrap()\n         .read_to_end(&mut file_data)\n@@ -169,12 +173,14 @@ fn test_repo_cache_some() {\n     .unwrap();\n \n     // the copied repo should have file2 but not file1 (i.e. only the listed targets).\n-    let read_target_result = copied_repo.read_target(\"file1.txt\");\n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n+    let read_target_result = copied_repo.read_target(&file1);\n     assert!(read_target_result.is_err());\n \n     let mut file_data = Vec::new();\n+    let file2 = TargetName::new(\"file2.txt\").unwrap();\n     let file_size = copied_repo\n-        .read_target(\"file2.txt\")\n+        .read_target(&file2)\n         .unwrap()\n         .unwrap()\n         .read_to_end(&mut file_data)\n@@ -231,3 +237,65 @@ fn test_repo_cache_metadata_no_root_chain() {\n     // Verify we did not cache the root.json\n     assert!(!metadata_destination.join(\"1.root.json\").exists());\n }\n+\n+/// Test that the repo.cache() function prepends target names with sha digest.\n+#[test]\n+fn test_repo_cache_consistent_snapshots() {\n+    let repo_name = \"consistent-snapshots\";\n+    let metadata_dir = test_data().join(repo_name).join(\"metadata\");\n+    let targets_dir = test_data().join(repo_name).join(\"targets\");\n+    let root = metadata_dir.join(\"1.root.json\");\n+    let repo = RepositoryLoader::new(\n+        File::open(&root).unwrap(),\n+        dir_url(metadata_dir),\n+        dir_url(targets_dir),\n+    )\n+    .load()\n+    .unwrap();\n+\n+    // cache the repo for future use\n+    let destination = TempDir::new().unwrap();\n+    let metadata_destination = destination.as_ref().join(\"metadata\");\n+    let targets_destination = destination.as_ref().join(\"targets\");\n+    // let targets_subset = vec![\"file2.txt\".to_string()];\n+    repo.cache(\n+        &metadata_destination,\n+        &targets_destination,\n+        Option::<&[&str]>::None,\n+        true,\n+    )\n+    .unwrap();\n+\n+    // check that we can load the copied repo.\n+    let copied_repo = RepositoryLoader::new(\n+        File::open(&root).unwrap(),\n+        dir_url(&metadata_destination),\n+        dir_url(&targets_destination),\n+    )\n+    .load()\n+    .unwrap();\n+\n+    // the copied repo should have file2 but not file1 (i.e. only the listed targets).\n+    let data1 = String::from_utf8(read_to_end(\n+        copied_repo\n+            .read_target(&TargetName::new(\"data1.txt\").unwrap())\n+            .unwrap()\n+            .unwrap(),\n+    ))\n+    .unwrap();\n+    assert_eq!(data1, DATA_1);\n+\n+    let data2 = String::from_utf8(read_to_end(\n+        copied_repo\n+            .read_target(&TargetName::new(\"data2.txt\").unwrap())\n+            .unwrap()\n+            .unwrap(),\n+    ))\n+    .unwrap();\n+    assert_eq!(data2, DATA_2);\n+\n+    // assert that the target has its digest prepended\n+    let expected_filepath = targets_destination\n+        .join(\"5aa1d2b3bea034a0f9d0b27a1bc72919b3145a2b092b72ac0415a05e07e2bdd1.data1.txt\");\n+    assert!(expected_filepath.is_file())\n+}"
        },
        {
          "filename": "tough/tests/repo_editor.rs",
          "status": "modified",
          "additions": 14,
          "deletions": 12,
          "patch": "@@ -16,8 +16,8 @@ use tough::key_source::LocalKeySource;\n use tough::schema::decoded::Decoded;\n use tough::schema::decoded::Hex;\n use tough::schema::key::Key;\n-use tough::schema::PathSet;\n-use tough::{Repository, RepositoryLoader};\n+use tough::schema::{PathPattern, PathSet};\n+use tough::{Repository, RepositoryLoader, TargetName};\n use url::Url;\n \n mod test_utils;\n@@ -165,7 +165,7 @@ fn create_sign_write_reload_repo() {\n         .delegate_role(\n             \"role1\",\n             role1_key,\n-            PathSet::Paths([\"file?.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"file?.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Utc::now().checked_add_signed(Duration::days(21)).unwrap(),\n             NonZeroU64::new(1).unwrap(),\n@@ -182,7 +182,7 @@ fn create_sign_write_reload_repo() {\n         .delegate_role(\n             \"role2\",\n             role2_key,\n-            PathSet::Paths([\"file1.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"file1.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Utc::now().checked_add_signed(Duration::days(21)).unwrap(),\n             NonZeroU64::new(1).unwrap(),\n@@ -191,7 +191,7 @@ fn create_sign_write_reload_repo() {\n         .delegate_role(\n             \"role3\",\n             role1_key,\n-            PathSet::Paths([\"file1.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"file1.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Utc::now().checked_add_signed(Duration::days(21)).unwrap(),\n             NonZeroU64::new(1).unwrap(),\n@@ -209,7 +209,7 @@ fn create_sign_write_reload_repo() {\n         .delegate_role(\n             \"role4\",\n             role2_key,\n-            PathSet::Paths([\"file1.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"file1.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Utc::now().checked_add_signed(Duration::days(21)).unwrap(),\n             NonZeroU64::new(1).unwrap(),\n@@ -292,7 +292,7 @@ fn create_role_flow() {\n         .add_role(\n             \"A\",\n             metadata_base_url_out.as_str(),\n-            PathSet::Paths([\"*.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"*.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Some(key_hash_map(role1_key)),\n         )\n@@ -369,7 +369,7 @@ fn create_role_flow() {\n         .add_role(\n             \"B\",\n             metadata_base_url_out.as_str(),\n-            PathSet::Paths([\"file?.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"file?.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Some(key_hash_map(role2_key)),\n         )\n@@ -492,7 +492,7 @@ fn update_targets_flow() {\n         .add_role(\n             \"A\",\n             metadata_base_url_out.as_str(),\n-            PathSet::Paths([\"*.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"*.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Some(key_hash_map(role1_key)),\n         )\n@@ -569,7 +569,7 @@ fn update_targets_flow() {\n         .add_role(\n             \"B\",\n             metadata_base_url_out.as_str(),\n-            PathSet::Paths([\"file?.txt\".to_string()].to_vec()),\n+            PathSet::Paths(vec![PathPattern::new(\"file?.txt\").unwrap()]),\n             NonZeroU64::new(1).unwrap(),\n             Some(key_hash_map(role2_key)),\n         )\n@@ -713,8 +713,9 @@ fn update_targets_flow() {\n     .load()\n     .unwrap();\n \n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n     assert_eq!(\n-        read_to_end(new_repo.read_target(\"file1.txt\").unwrap().unwrap()),\n+        read_to_end(new_repo.read_target(&file1).unwrap().unwrap()),\n         &b\"This is an example target file.\"[..]\n     );\n \n@@ -799,8 +800,9 @@ fn update_targets_flow() {\n     .load()\n     .unwrap();\n \n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n     assert_eq!(\n-        read_to_end(new_repo.read_target(\"file1.txt\").unwrap().unwrap()),\n+        read_to_end(new_repo.read_target(&file1).unwrap().unwrap()),\n         &b\"Updated file1.txt\"[..]\n     );\n }"
        },
        {
          "filename": "tough/tests/target_path_safety.rs",
          "status": "added",
          "additions": 177,
          "deletions": 0,
          "patch": "@@ -0,0 +1,177 @@\n+mod test_utils;\n+\n+use chrono::{DateTime, TimeZone, Utc};\n+use maplit::hashmap;\n+use ring::rand::SystemRandom;\n+use std::collections::HashMap;\n+use std::fs::{self, create_dir_all, File};\n+use std::num::NonZeroU64;\n+use std::path::Path;\n+use tempfile::TempDir;\n+use test_utils::{dir_url, test_data, DATA_1, DATA_2, DATA_3};\n+use tough::editor::signed::SignedRole;\n+use tough::editor::RepositoryEditor;\n+use tough::key_source::{KeySource, LocalKeySource};\n+use tough::schema::{KeyHolder, PathPattern, PathSet, RoleKeys, RoleType, Root, Signed, Target};\n+use tough::{Prefix, RepositoryLoader, TargetName};\n+\n+/// Returns a date in the future when Rust programs will no longer exist. `MAX_DATETIME` is so huge\n+/// that it serializes to something weird-looking, so we use something that is recognizable to\n+/// humans as a date.\n+fn later() -> DateTime<Utc> {\n+    Utc.ymd(2999, 1, 1).and_hms(0, 0, 0)\n+}\n+\n+/// This test ensures that we can safely handle path-like target names with ../'s in them.\n+fn create_root(root_path: &Path, consistent_snapshot: bool) -> Vec<Box<dyn KeySource>> {\n+    let keys: Vec<Box<dyn KeySource>> = vec![Box::new(LocalKeySource {\n+        path: test_data().join(\"snakeoil.pem\"),\n+    })];\n+\n+    let key_pair = keys.iter().next().unwrap().as_sign().unwrap().tuf_key();\n+    let key_id = key_pair.key_id().unwrap();\n+\n+    let empty_keys = RoleKeys {\n+        keyids: vec![key_id.clone()],\n+        threshold: NonZeroU64::new(1).unwrap(),\n+        _extra: Default::default(),\n+    };\n+\n+    let mut root = Signed {\n+        signed: Root {\n+            spec_version: \"1.0.0\".into(),\n+            consistent_snapshot,\n+            version: NonZeroU64::new(1).unwrap(),\n+            expires: later(),\n+            keys: HashMap::new(),\n+            roles: hashmap! {\n+                RoleType::Root => empty_keys.clone(),\n+                RoleType::Snapshot => empty_keys.clone(),\n+                RoleType::Targets => empty_keys.clone(),\n+                RoleType::Timestamp => empty_keys.clone(),\n+                // RoleType::DelegatedTargets => empty_keys.clone(),\n+            },\n+            _extra: HashMap::new(),\n+        },\n+        signatures: Vec::new(),\n+    };\n+\n+    root.signed.keys.insert(key_id.clone(), key_pair.clone());\n+\n+    let signed_root = SignedRole::new(\n+        root.signed.clone(),\n+        &KeyHolder::Root(root.signed.clone()),\n+        &keys,\n+        &SystemRandom::new(),\n+    )\n+    .unwrap();\n+\n+    std::fs::write(&root_path, signed_root.buffer()).unwrap();\n+\n+    keys\n+}\n+\n+#[test]\n+fn safe_target_paths() {\n+    let tempdir = TempDir::new().unwrap();\n+    let root_path = tempdir.path().join(\"root.json\");\n+    let keys = create_root(&root_path, false);\n+    let one = NonZeroU64::new(1).unwrap();\n+\n+    let mut editor = RepositoryEditor::new(&root_path).unwrap();\n+    editor\n+        .snapshot_version(one)\n+        .snapshot_expires(later())\n+        .timestamp_version(one)\n+        .timestamp_expires(later())\n+        .delegate_role(\n+            \"delegated\",\n+            &keys,\n+            PathSet::Paths(vec![PathPattern::new(\"delegated/*\").unwrap()]),\n+            one,\n+            later(),\n+            one,\n+        )\n+        .unwrap();\n+    let repo_dir = tempdir.path().join(\"repo\");\n+    let targets_dir = repo_dir.join(\"targets\");\n+    fs::create_dir_all(targets_dir.join(\"foo/bar\")).unwrap();\n+    fs::create_dir_all(targets_dir.join(\"delegated/subdir\")).unwrap();\n+    let targets_file_1 = targets_dir.join(\"data1.txt\");\n+    let targets_file_2 = targets_dir.join(\"foo/bar/data2.txt\");\n+    let targets_file_3 = targets_dir.join(\"delegated/subdir/data3.txt\");\n+    fs::write(&targets_file_1, DATA_1).unwrap();\n+    fs::write(&targets_file_2, DATA_2).unwrap();\n+    fs::write(&targets_file_3, DATA_3).unwrap();\n+\n+    let target_name_1 = TargetName::new(\"foo/../bar/../baz/../../../../data1.txt\").unwrap();\n+    let target_1 = Target::from_path(&targets_file_1).unwrap();\n+    let target_name_2 = TargetName::new(\"foo/bar/baz/../data2.txt\").unwrap();\n+    let target_2 = Target::from_path(&targets_file_2).unwrap();\n+    let target_name_3 = TargetName::new(\"../delegated/foo/../subdir/data3.txt\").unwrap();\n+    let target_3 = Target::from_path(&targets_file_3).unwrap();\n+\n+    editor.add_target(target_name_1.clone(), target_1).unwrap();\n+    editor.add_target(target_name_2.clone(), target_2).unwrap();\n+    editor\n+        .targets_version(one)\n+        .unwrap()\n+        .targets_expires(later())\n+        .unwrap()\n+        .sign_targets_editor(&keys)\n+        .unwrap()\n+        .change_delegated_targets(\"delegated\")\n+        .unwrap()\n+        .add_target(target_name_3.clone(), target_3)\n+        .unwrap()\n+        .targets_version(one)\n+        .unwrap()\n+        .targets_expires(later())\n+        .unwrap()\n+        .sign_targets_editor(&keys)\n+        .unwrap();\n+\n+    let signed_repo = editor.sign(&keys).unwrap();\n+    let metadata_dir = repo_dir.join(\"metadata\");\n+    signed_repo.write(&metadata_dir).unwrap();\n+\n+    let loaded_repo = RepositoryLoader::new(\n+        File::open(&root_path).unwrap(),\n+        dir_url(&metadata_dir),\n+        dir_url(&targets_dir),\n+    )\n+    .load()\n+    .unwrap();\n+\n+    let outdir = tempdir.path().join(\"outdir\");\n+    create_dir_all(&outdir).unwrap();\n+    loaded_repo\n+        .save_target(&target_name_1, &outdir, Prefix::None)\n+        .unwrap();\n+    loaded_repo\n+        .save_target(&target_name_2, &outdir, Prefix::None)\n+        .unwrap();\n+    loaded_repo\n+        .save_target(&target_name_3, &outdir, Prefix::None)\n+        .unwrap();\n+\n+    // These might be created if we didn't safely clean the target names as paths.\n+    assert!(!outdir.join(\"bar\").exists());\n+    assert!(!outdir.join(\"baz\").exists());\n+    assert!(!outdir.join(\"foo/bar/baz\").exists());\n+    assert!(!outdir.join(\"../delegated/foo/../subdir/data3.txt\").exists());\n+\n+    // The targets should end up at these paths.\n+    assert_eq!(\n+        fs::read_to_string(outdir.join(\"data1.txt\")).unwrap(),\n+        DATA_1\n+    );\n+    assert_eq!(\n+        fs::read_to_string(outdir.join(\"foo/bar/data2.txt\")).unwrap(),\n+        DATA_2\n+    );\n+    assert_eq!(\n+        fs::read_to_string(outdir.join(\"delegated/subdir/data3.txt\")).unwrap(),\n+        DATA_3\n+    );\n+}"
        },
        {
          "filename": "tough/tests/test_utils.rs",
          "status": "modified",
          "additions": 8,
          "deletions": 3,
          "patch": "@@ -1,28 +1,33 @@\n // Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n // SPDX-License-Identifier: MIT OR Apache-2.0\n \n+// An integration test might want to use some, but not all of, the symbols herein. To do so would\n+// cause compiler warnings for unused code, so we suppress them.\n+#![allow(unused)]\n+\n use std::io::Read;\n use std::path::{Path, PathBuf};\n use url::Url;\n \n /// Utilities for tests. Not every test module uses every function, so we suppress unused warnings.\n \n+pub const DATA_1: &str = \"123\\n456\\n789\\n0\\n\";\n+pub const DATA_2: &str = \"abc\\ndef\\nhij\\nk\\n\";\n+pub const DATA_3: &str = \"!@#\\n$%^\\n&*(\\n)\\n\";\n+\n /// Returns the path to our test data directory\n-#[allow(unused)]\n pub fn test_data() -> PathBuf {\n     PathBuf::from(env!(\"CARGO_MANIFEST_DIR\"))\n         .join(\"tests\")\n         .join(\"data\")\n }\n \n /// Converts a filepath into a URI formatted string\n-#[allow(unused)]\n pub fn dir_url<P: AsRef<Path>>(path: P) -> Url {\n     Url::from_directory_path(path).unwrap()\n }\n \n /// Gets the goods from a read and makes a Vec\n-#[allow(unused)]\n pub fn read_to_end<R: Read>(mut reader: R) -> Vec<u8> {\n     let mut v = Vec::new();\n     reader.read_to_end(&mut v).unwrap();"
        },
        {
          "filename": "tuftool/CHANGELOG.md",
          "status": "modified",
          "additions": 11,
          "deletions": 1,
          "patch": "@@ -4,6 +4,15 @@ All notable changes to this project will be documented in this file.\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n+## [0.7.0] - 2021-10-19\n+### Breaking Changes\n+- `tuftool download` now requires `outdir` to not exist.\n+\n+### Changes\n+- Fix an issue where delegated role names with path traversal constructs could cause files to be written in unexpected locations.\n+- Fix a similar issue with path traversal constructs in target names.\n+- Update dependencies.\n+\n ## [0.6.4] - 2021-09-15\n ### Changes\n - Add ignore threshold flag.  [#412]\n@@ -145,7 +154,8 @@ Major update: much of the logic in `tuftool` has been factored out and added to\n ### Added\n - Everything!\n \n-[Unreleased]: https://github.com/awslabs/tough/compare/tuftool-v0.6.4...develop\n+[Unreleased]: https://github.com/awslabs/tough/compare/tuftool-v0.7.0...develop\n+[0.7.0]: https://github.com/awslabs/tough/compare/tuftool-v0.6.4...tuftool-v0.7.0\n [0.6.4]: https://github.com/awslabs/tough/compare/tuftool-v0.6.3...tuftool-v0.6.4\n [0.6.3]: https://github.com/awslabs/tough/compare/tuftool-v0.6.2...tuftool-v0.6.3\n [0.6.2]: https://github.com/awslabs/tough/compare/tuftool-v0.6.1...tuftool-v0.6.2"
        },
        {
          "filename": "tuftool/Cargo.toml",
          "status": "modified",
          "additions": 4,
          "deletions": 4,
          "patch": "@@ -1,6 +1,6 @@\n [package]\n name = \"tuftool\"\n-version = \"0.6.4\"\n+version = \"0.7.0\"\n description = \"Utility for creating and signing The Update Framework (TUF) repositories\"\n authors = [\"iliana destroyer of worlds <iweller@amazon.com>\"]\n license = \"MIT OR Apache-2.0\"\n@@ -36,9 +36,9 @@ snafu = { version = \"0.6.10\", features = [\"backtraces-impl-backtrace-crate\"] }\n structopt = \"0.3\"\n tempfile = \"3.1.0\"\n tokio = \"~1.8\"  # LTS\n-tough = { version = \"0.11.3\", path = \"../tough\", features = [\"http\"] }\n-tough-ssm = { version = \"0.6.3\", path = \"../tough-ssm\" }\n-tough-kms = { version = \"0.3.3\", path = \"../tough-kms\" }\n+tough = { version = \"0.12.0\", path = \"../tough\", features = [\"http\"] }\n+tough-ssm = { version = \"0.6.4\", path = \"../tough-ssm\" }\n+tough-kms = { version = \"0.3.4\", path = \"../tough-kms\" }\n url = \"2.1.0\"\n walkdir = \"2.3.2\"\n "
        },
        {
          "filename": "tuftool/src/add_role.rs",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -12,7 +12,7 @@ use std::path::PathBuf;\n use structopt::StructOpt;\n use tough::editor::{targets::TargetsEditor, RepositoryEditor};\n use tough::key_source::KeySource;\n-use tough::schema::PathSet;\n+use tough::schema::{PathHashPrefix, PathPattern, PathSet};\n use url::Url;\n \n #[derive(Debug, StructOpt)]\n@@ -56,11 +56,11 @@ pub(crate) struct AddRoleArgs {\n \n     /// The delegated paths\n     #[structopt(short = \"p\", long = \"paths\", conflicts_with = \"path-hash-prefixes\")]\n-    paths: Option<Vec<String>>,\n+    paths: Option<Vec<PathPattern>>,\n \n     /// The delegated paths hash prefixes\n     #[structopt(short = \"hp\", long = \"path-hash-prefixes\")]\n-    path_hash_prefixes: Option<Vec<String>>,\n+    path_hash_prefixes: Option<Vec<PathHashPrefix>>,\n \n     /// Determines if entire repo should be signed\n     #[structopt(long = \"sign-all\")]"
        },
        {
          "filename": "tuftool/src/create.rs",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -100,9 +100,9 @@ impl CreateArgs {\n             .timestamp_version(self.timestamp_version)\n             .timestamp_expires(self.timestamp_expires);\n \n-        for (filename, target) in targets {\n+        for (target_name, target) in targets {\n             editor\n-                .add_target(&filename, target)\n+                .add_target(target_name, target)\n                 .context(error::DelegationStructure)?;\n         }\n "
        },
        {
          "filename": "tuftool/src/download.rs",
          "status": "modified",
          "additions": 22,
          "deletions": 16,
          "patch": "@@ -3,13 +3,12 @@\n \n use crate::download_root::download_root;\n use crate::error::{self, Result};\n-use snafu::{OptionExt, ResultExt};\n+use snafu::{ensure, ResultExt};\n use std::fs::File;\n-use std::io;\n use std::num::NonZeroU64;\n use std::path::{Path, PathBuf};\n use structopt::StructOpt;\n-use tough::{ExpirationEnforcement, Repository, RepositoryLoader};\n+use tough::{ExpirationEnforcement, Prefix, Repository, RepositoryLoader, TargetName};\n use url::Url;\n \n #[derive(Debug, StructOpt)]\n@@ -38,7 +37,7 @@ pub(crate) struct DownloadArgs {\n     #[structopt(short = \"n\", long = \"target-name\")]\n     target_names: Vec<String>,\n \n-    /// Output directory of targets\n+    /// Output directory for targets (will be created and must not already exist)\n     outdir: PathBuf,\n \n     /// Allow repo download for expired metadata\n@@ -58,6 +57,12 @@ WARNING: `--allow-expired-repo` was passed; this is unsafe and will not establis\n \n impl DownloadArgs {\n     pub(crate) fn run(&self) -> Result<()> {\n+        // To help ensure that downloads are safe, we require that the outdir does not exist.\n+        ensure!(\n+            !self.outdir.exists(),\n+            error::DownloadOutdirExists { path: &self.outdir }\n+        );\n+\n         // use local root.json or download from repository\n         let root_path = if let Some(path) = &self.root {\n             PathBuf::from(path)\n@@ -90,21 +95,22 @@ impl DownloadArgs {\n     }\n }\n \n-fn handle_download(repository: &Repository, outdir: &Path, target_names: &[String]) -> Result<()> {\n-    let download_target = |target: &str| -> Result<()> {\n-        let path = PathBuf::from(outdir).join(target);\n-        println!(\"\\t-> {}\", &target);\n-        let mut reader = repository\n-            .read_target(target)\n-            .context(error::Metadata)?\n-            .context(error::TargetNotFound { target })?;\n-        let mut f = File::create(&path).context(error::OpenFile { path: &path })?;\n-        io::copy(&mut reader, &mut f).context(error::WriteTarget)?;\n+fn handle_download(repository: &Repository, outdir: &Path, raw_names: &[String]) -> Result<()> {\n+    let target_names: Result<Vec<TargetName>> = raw_names\n+        .iter()\n+        .map(|s| TargetName::new(s).context(error::InvalidTargetName))\n+        .collect();\n+    let target_names = target_names?;\n+    let download_target = |name: &TargetName| -> Result<()> {\n+        println!(\"\\t-> {}\", name.raw());\n+        repository\n+            .save_target(name, outdir, Prefix::None)\n+            .context(error::Metadata)?;\n         Ok(())\n     };\n \n     // copy requested targets, or all available targets if not specified\n-    let targets = if target_names.is_empty() {\n+    let targets: Vec<TargetName> = if target_names.is_empty() {\n         repository\n             .targets()\n             .signed\n@@ -113,7 +119,7 @@ fn handle_download(repository: &Repository, outdir: &Path, target_names: &[Strin\n             .cloned()\n             .collect()\n     } else {\n-        target_names.to_owned()\n+        target_names\n     };\n \n     println!(\"Downloading targets to {:?}\", outdir);"
        },
        {
          "filename": "tuftool/src/error.rs",
          "status": "modified",
          "additions": 6,
          "deletions": 0,
          "patch": "@@ -79,6 +79,9 @@ pub(crate) enum Error {\n         backtrace: Backtrace,\n     },\n \n+    #[snafu(display(\"The a file or directory already exists at '{}'\", path.display()))]\n+    DownloadOutdirExists { path: PathBuf, backtrace: Backtrace },\n+\n     #[snafu(display(\n         \"Failed to create a Repository Editor with root.json '{}': {}\",\n         path.display(),\n@@ -144,6 +147,9 @@ pub(crate) enum Error {\n         backtrace: Backtrace,\n     },\n \n+    #[snafu(display(\"Invalid target name: {}\", source))]\n+    InvalidTargetName { source: tough::error::Error },\n+\n     #[snafu(display(\"Failed to serialize to JSON: {}\", source))]\n     JsonSerialization {\n         source: tough::schema::Error,"
        },
        {
          "filename": "tuftool/src/main.rs",
          "status": "modified",
          "additions": 12,
          "deletions": 9,
          "patch": "@@ -39,6 +39,7 @@ use std::path::Path;\n use structopt::StructOpt;\n use tempfile::NamedTempFile;\n use tough::schema::Target;\n+use tough::TargetName;\n use walkdir::WalkDir;\n \n static SPEC_VERSION: &str = \"1.0.0\";\n@@ -127,7 +128,7 @@ where\n \n // Walk the directory specified, building a map of filename to Target structs.\n // Hashing of the targets is done in parallel\n-fn build_targets<P>(indir: P, follow_links: bool) -> Result<HashMap<String, Target>>\n+fn build_targets<P>(indir: P, follow_links: bool) -> Result<HashMap<TargetName, Target>>\n where\n     P: AsRef<Path>,\n {\n@@ -149,17 +150,19 @@ where\n         .collect()\n }\n \n-fn process_target(path: &Path) -> Result<(String, Target)> {\n+fn process_target(path: &Path) -> Result<(TargetName, Target)> {\n+    // Get the file name as a TargetName\n+    let target_name = TargetName::new(\n+        path.file_name()\n+            .context(error::NoFileName { path })?\n+            .to_str()\n+            .context(error::PathUtf8 { path })?,\n+    )\n+    .context(error::InvalidTargetName)?;\n+\n     // Build a Target from the path given. If it is not a file, this will fail\n     let target = Target::from_path(path).context(error::TargetFromPath { path })?;\n \n-    // Get the file name as a string\n-    let target_name = path\n-        .file_name()\n-        .context(error::NoFileName { path })?\n-        .to_str()\n-        .context(error::PathUtf8 { path })?\n-        .to_owned();\n     Ok((target_name, target))\n }\n "
        },
        {
          "filename": "tuftool/src/update.rs",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -151,9 +151,9 @@ impl UpdateArgs {\n \n             let new_targets = build_targets(&targets_indir, self.follow)?;\n \n-            for (filename, target) in new_targets {\n+            for (target_name, target) in new_targets {\n                 editor\n-                    .add_target(&filename, target)\n+                    .add_target(target_name, target)\n                     .context(error::DelegationStructure)?;\n             }\n         };"
        },
        {
          "filename": "tuftool/src/update_targets.rs",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -92,8 +92,10 @@ impl UpdateTargetsArgs {\n \n             let new_targets = build_targets(&targets_indir, self.follow)?;\n \n-            for (filename, target) in new_targets {\n-                editor.add_target(&filename, target);\n+            for (target_name, target) in new_targets {\n+                editor\n+                    .add_target(target_name, target)\n+                    .context(error::InvalidTargetName)?;\n             }\n         };\n "
        },
        {
          "filename": "tuftool/tests/create_command.rs",
          "status": "modified",
          "additions": 10,
          "deletions": 7,
          "patch": "@@ -8,7 +8,7 @@ use chrono::{Duration, Utc};\n use std::fs::File;\n use tempfile::TempDir;\n use test_utils::dir_url;\n-use tough::RepositoryLoader;\n+use tough::{RepositoryLoader, TargetName};\n \n #[test]\n // Ensure we can read a repo created by the `tuftool` binary using the `tough` library\n@@ -65,26 +65,29 @@ fn create_command() {\n     .unwrap();\n \n     // Ensure we can read the targets\n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file1.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file1).unwrap().unwrap()),\n         &b\"This is an example target file.\"[..]\n     );\n+    let file2 = TargetName::new(\"file2.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file2.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file2).unwrap().unwrap()),\n         &b\"This is an another example target file.\"[..]\n     );\n+    let file3 = TargetName::new(\"file3.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file3.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file3).unwrap().unwrap()),\n         &b\"This is role1's target file.\"[..]\n     );\n \n     // Ensure the targets.json file is correct\n     assert_eq!(repo.targets().signed.version.get(), targets_version);\n     assert_eq!(repo.targets().signed.expires, targets_expiration);\n     assert_eq!(repo.targets().signed.targets.len(), 3);\n-    assert_eq!(repo.targets().signed.targets[\"file1.txt\"].length, 31);\n-    assert_eq!(repo.targets().signed.targets[\"file2.txt\"].length, 39);\n-    assert_eq!(repo.targets().signed.targets[\"file3.txt\"].length, 28);\n+    assert_eq!(repo.targets().signed.targets[&file1].length, 31);\n+    assert_eq!(repo.targets().signed.targets[&file2].length, 39);\n+    assert_eq!(repo.targets().signed.targets[&file3].length, 28);\n     assert_eq!(repo.targets().signatures.len(), 1);\n \n     // Ensure the snapshot.json file is correct"
        },
        {
          "filename": "tuftool/tests/create_repository_integration.rs",
          "status": "modified",
          "additions": 10,
          "deletions": 7,
          "patch": "@@ -8,7 +8,7 @@ use std::env;\n use std::fs::File;\n use tempfile::TempDir;\n use test_utils::dir_url;\n-use tough::RepositoryLoader;\n+use tough::{RepositoryLoader, TargetName};\n \n // This file include integration tests for KeySources: tough-ssm, tough-kms and local file key.\n // Since the tests are run using the actual \"AWS SSM and AWS KMS\", you would have to configure\n@@ -165,26 +165,29 @@ fn create_repository(root_key: &str, auto_generate: bool) {\n     .unwrap();\n \n     // Ensure we can read the targets\n+    let file1 = TargetName::new(\"file1.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file1.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file1).unwrap().unwrap()),\n         &b\"This is an example target file.\"[..]\n     );\n+    let file2 = TargetName::new(\"file2.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file2.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file2).unwrap().unwrap()),\n         &b\"This is an another example target file.\"[..]\n     );\n+    let file3 = TargetName::new(\"file3.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file3.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file3).unwrap().unwrap()),\n         &b\"This is role1's target file.\"[..]\n     );\n \n     // Ensure the targets.json file is correct\n     assert_eq!(repo.targets().signed.version.get(), targets_version);\n     assert_eq!(repo.targets().signed.expires, targets_expiration);\n     assert_eq!(repo.targets().signed.targets.len(), 3);\n-    assert_eq!(repo.targets().signed.targets[\"file1.txt\"].length, 31);\n-    assert_eq!(repo.targets().signed.targets[\"file2.txt\"].length, 39);\n-    assert_eq!(repo.targets().signed.targets[\"file3.txt\"].length, 28);\n+    assert_eq!(repo.targets().signed.targets[&file1].length, 31);\n+    assert_eq!(repo.targets().signed.targets[&file2].length, 39);\n+    assert_eq!(repo.targets().signed.targets[&file3].length, 28);\n     assert_eq!(repo.targets().signatures.len(), 1);\n \n     // Ensure the snapshot.json file is correct"
        },
        {
          "filename": "tuftool/tests/delegation_commands.rs",
          "status": "modified",
          "additions": 233,
          "deletions": 2,
          "patch": "@@ -9,7 +9,7 @@ use std::fs::File;\n use std::path::Path;\n use tempfile::TempDir;\n use test_utils::dir_url;\n-use tough::RepositoryLoader;\n+use tough::{RepositoryLoader, TargetName};\n \n fn create_repo<P: AsRef<Path>>(repo_dir: P) {\n     let timestamp_expiration = Utc::now().checked_add_signed(Duration::days(1)).unwrap();\n@@ -423,8 +423,9 @@ fn update_target_command() {\n     .unwrap();\n \n     // Make sure we can read new target\n+    let file4 = TargetName::new(\"file4.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file4.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file4).unwrap().unwrap()),\n         &b\"This is an example target file.\"[..]\n     );\n }\n@@ -1430,3 +1431,233 @@ fn remove_role_recursive_command() {\n     assert!(repo.delegated_role(\"A\").is_none());\n     assert!(repo.delegated_role(\"B\").is_none());\n }\n+\n+#[test]\n+/// Ensure we that we percent encode path traversal characters when adding a role name such as\n+/// `../../strange/role/../name` and that we don't write files in unexpected places.\n+fn dubious_role_name() {\n+    let dubious_role_name = \"../../strange/role/../name\";\n+    let dubious_name_encoded = \"..%2F..%2Fstrange%2Frole%2F..%2Fname\";\n+    let funny_role_name = \"../\ud83c\udf7a/( \u0361\u00b0 \u035c\u0296 \u0361\u00b0)\";\n+    let funny_name_encoded =\n+        \"..%2F%F0%9F%8D%BA%2F%28%20%CD%A1%C2%B0%20%CD%9C%CA%96%20%CD%A1%C2%B0%29\";\n+    let root_json = test_utils::test_data().join(\"simple-rsa\").join(\"root.json\");\n+    let root_key = test_utils::test_data().join(\"snakeoil.pem\");\n+    let targets_key = test_utils::test_data().join(\"targetskey\");\n+    let targets_key1 = test_utils::test_data().join(\"targetskey-1\");\n+    let repo_dir = TempDir::new().unwrap();\n+\n+    // Set new expiration dates and version numbers for the update command\n+    let new_timestamp_expiration = Utc::now().checked_add_signed(Duration::days(4)).unwrap();\n+    let new_timestamp_version: u64 = 310;\n+    let new_snapshot_expiration = Utc::now().checked_add_signed(Duration::days(5)).unwrap();\n+    let new_snapshot_version: u64 = 250;\n+    let new_targets_expiration = Utc::now().checked_add_signed(Duration::days(6)).unwrap();\n+    let new_targets_version: u64 = 170;\n+\n+    // Create a repo using tuftool and the reference tuf implementation data\n+    create_repo(repo_dir.path());\n+\n+    // Set new expiration date for the new role\n+    let expiration = Utc::now().checked_add_signed(Duration::days(4)).unwrap();\n+    let metadata_base_url = &dir_url(repo_dir.path().join(\"metadata\"));\n+    let meta_out = TempDir::new().unwrap();\n+\n+    // create role A\n+    Command::cargo_bin(\"tuftool\")\n+        .unwrap()\n+        .args(&[\n+            \"delegation\",\n+            \"--signing-role\",\n+            dubious_role_name,\n+            \"create-role\",\n+            \"-o\",\n+            meta_out.path().to_str().unwrap(),\n+            \"-k\",\n+            targets_key.to_str().unwrap(),\n+            \"-e\",\n+            expiration.to_rfc3339().as_str(),\n+            \"-v\",\n+            \"1\",\n+        ])\n+        .assert()\n+        .success();\n+\n+    let new_repo_dir = TempDir::new().unwrap();\n+    // add role to targets metadata and sign entire repo\n+    Command::cargo_bin(\"tuftool\")\n+        .unwrap()\n+        .args(&[\n+            \"delegation\",\n+            \"--signing-role\",\n+            \"targets\",\n+            \"add-role\",\n+            \"-o\",\n+            new_repo_dir.path().to_str().unwrap(),\n+            \"-i\",\n+            dir_url(&meta_out.path().join(\"metadata\")).as_str(),\n+            \"-k\",\n+            root_key.to_str().unwrap(),\n+            \"--root\",\n+            root_json.to_str().unwrap(),\n+            \"--metadata-url\",\n+            metadata_base_url.as_str(),\n+            \"-e\",\n+            expiration.to_rfc3339().as_str(),\n+            \"--delegated-role\",\n+            dubious_role_name,\n+            \"-t\",\n+            \"1\",\n+            \"-v\",\n+            \"2\",\n+            \"--sign-all\",\n+            \"--snapshot-expires\",\n+            new_snapshot_expiration.to_rfc3339().as_str(),\n+            \"--snapshot-version\",\n+            format!(\"{}\", new_snapshot_version).as_str(),\n+            \"--timestamp-expires\",\n+            new_timestamp_expiration.to_rfc3339().as_str(),\n+            \"--timestamp-version\",\n+            format!(\"{}\", new_timestamp_version).as_str(),\n+        ])\n+        .assert()\n+        .success();\n+\n+    // Load the updated repo\n+    let updated_metadata_base_url = &dir_url(new_repo_dir.path().join(\"metadata\"));\n+    let updated_targets_base_url = &dir_url(new_repo_dir.path().join(\"targets\"));\n+    let repo = RepositoryLoader::new(\n+        File::open(&root_json).unwrap(),\n+        updated_metadata_base_url.clone(),\n+        updated_targets_base_url.clone(),\n+    )\n+    .load()\n+    .unwrap();\n+    // Make sure `A` is added as a role\n+    assert!(repo.delegated_role(dubious_role_name).is_some());\n+\n+    let create_out = TempDir::new().unwrap();\n+    // create role B\n+    Command::cargo_bin(\"tuftool\")\n+        .unwrap()\n+        .args(&[\n+            \"delegation\",\n+            \"--signing-role\",\n+            funny_role_name,\n+            \"create-role\",\n+            \"-o\",\n+            create_out.path().to_str().unwrap(),\n+            \"-k\",\n+            targets_key1.to_str().unwrap(),\n+            \"-e\",\n+            expiration.to_rfc3339().as_str(),\n+            \"-v\",\n+            \"1\",\n+        ])\n+        .assert()\n+        .success();\n+\n+    let add_b_out = TempDir::new().unwrap();\n+    // add role B to A metadata and sign A meta\n+    Command::cargo_bin(\"tuftool\")\n+        .unwrap()\n+        .args(&[\n+            \"delegation\",\n+            \"--signing-role\",\n+            dubious_role_name,\n+            \"add-role\",\n+            \"-o\",\n+            add_b_out.path().to_str().unwrap(),\n+            \"-i\",\n+            dir_url(&create_out.path().join(\"metadata\")).as_str(),\n+            \"-k\",\n+            targets_key.to_str().unwrap(),\n+            \"--root\",\n+            root_json.to_str().unwrap(),\n+            \"--metadata-url\",\n+            updated_metadata_base_url.as_str(),\n+            \"-e\",\n+            expiration.to_rfc3339().as_str(),\n+            \"--delegated-role\",\n+            funny_role_name,\n+            \"-t\",\n+            \"1\",\n+            \"-v\",\n+            \"2\",\n+        ])\n+        .assert()\n+        .success();\n+\n+    // Make sure the metadata files are in the right directory\n+    assert!(add_b_out\n+        .path()\n+        .join(\"metadata\")\n+        .join(format!(\"{}.json\", dubious_name_encoded))\n+        .is_file());\n+    assert!(add_b_out\n+        .path()\n+        .join(\"metadata\")\n+        .join(format!(\"{}.json\", funny_name_encoded))\n+        .is_file());\n+\n+    // update repo with new metadata\n+\n+    let update_out = TempDir::new().unwrap();\n+\n+    // Update the repo we just created\n+    Command::cargo_bin(\"tuftool\")\n+        .unwrap()\n+        .args(&[\n+            \"update\",\n+            \"-o\",\n+            update_out.path().to_str().unwrap(),\n+            \"-k\",\n+            root_key.to_str().unwrap(),\n+            \"--root\",\n+            root_json.to_str().unwrap(),\n+            \"--metadata-url\",\n+            updated_metadata_base_url.as_str(),\n+            \"--targets-expires\",\n+            new_targets_expiration.to_rfc3339().as_str(),\n+            \"--targets-version\",\n+            format!(\"{}\", new_targets_version).as_str(),\n+            \"--snapshot-expires\",\n+            new_snapshot_expiration.to_rfc3339().as_str(),\n+            \"--snapshot-version\",\n+            format!(\"{}\", new_snapshot_version).as_str(),\n+            \"--timestamp-expires\",\n+            new_timestamp_expiration.to_rfc3339().as_str(),\n+            \"--timestamp-version\",\n+            format!(\"{}\", new_timestamp_version).as_str(),\n+            \"--role\",\n+            dubious_role_name,\n+            \"-i\",\n+            dir_url(&add_b_out.path().join(\"metadata\")).as_str(),\n+        ])\n+        .assert()\n+        .success();\n+\n+    // Load the updated repo\n+    let repo = RepositoryLoader::new(\n+        File::open(root_json).unwrap(),\n+        dir_url(update_out.path().join(\"metadata\")),\n+        dir_url(update_out.path().join(\"targets\")),\n+    )\n+    .load()\n+    .unwrap();\n+\n+    // Make sure `B` is added as a role\n+    assert!(repo.delegated_role(funny_role_name).is_some());\n+\n+    // Make sure the metadata files are in the right directory\n+    assert!(update_out\n+        .path()\n+        .join(\"metadata\")\n+        .join(format!(\"{}.{}.json\", 2, dubious_name_encoded))\n+        .is_file());\n+    assert!(update_out\n+        .path()\n+        .join(\"metadata\")\n+        .join(format!(\"{}.{}.json\", 1, funny_name_encoded))\n+        .is_file());\n+}"
        },
        {
          "filename": "tuftool/tests/download_command.rs",
          "status": "modified",
          "additions": 48,
          "deletions": 33,
          "patch": "@@ -3,8 +3,8 @@ mod test_utils;\n use assert_cmd::assert::Assert;\n use assert_cmd::Command;\n use httptest::{matchers::*, responders::*, Expectation, Server};\n-use std::fs::{read_to_string, OpenOptions};\n-use std::io::Write;\n+use std::fs::read_to_string;\n+use std::path::Path;\n use std::str::FromStr;\n use tempfile::TempDir;\n use url::Url;\n@@ -14,7 +14,7 @@ fn create_successful_get(relative_path: &str) -> httptest::Expectation {\n     let repo_dir = test_utils::test_data().join(\"tuf-reference-impl\");\n     let file_bytes = std::fs::read(&repo_dir.join(relative_path)).unwrap();\n     Expectation::matching(request::method_path(\"GET\", format!(\"/{}\", relative_path)))\n-        .times(2)\n+        .times(1)\n         .respond_with(\n             status_code(200)\n                 .append_header(\"content-type\", \"application/octet-stream\")\n@@ -28,13 +28,13 @@ fn create_successful_get(relative_path: &str) -> httptest::Expectation {\n /// S3 returns `403 Forbidden` when requesting a file that does not exist.\n fn create_unsuccessful_get(relative_path: &str) -> httptest::Expectation {\n     Expectation::matching(request::method_path(\"GET\", format!(\"/{}\", relative_path)))\n-        .times(2)\n+        .times(1)\n         .respond_with(status_code(403))\n }\n \n /// Asserts that the named file in `outdir` exactly matches the file in `tuf-reference-impl/targets`\n-fn assert_file_match(outdir: &TempDir, filename: &str) {\n-    let got = read_to_string(outdir.path().join(filename)).unwrap();\n+fn assert_file_match(outdir: &Path, filename: &str) {\n+    let got = read_to_string(outdir.join(filename)).unwrap();\n     let want = read_to_string(\n         test_utils::test_data()\n             .join(\"tuf-reference-impl\")\n@@ -46,7 +46,8 @@ fn assert_file_match(outdir: &TempDir, filename: &str) {\n }\n \n fn download_command(metadata_base_url: Url, targets_base_url: Url) {\n-    let outdir = TempDir::new().unwrap();\n+    let tempdir = TempDir::new().unwrap();\n+    let outdir = tempdir.path().join(\"outdir\");\n     let root_json = test_utils::test_data()\n         .join(\"tuf-reference-impl\")\n         .join(\"metadata\")\n@@ -63,7 +64,7 @@ fn download_command(metadata_base_url: Url, targets_base_url: Url) {\n             metadata_base_url.as_str(),\n             \"--targets-url\",\n             targets_base_url.as_str(),\n-            outdir.path().to_str().unwrap(),\n+            outdir.to_str().unwrap(),\n         ])\n         .assert()\n         .success();\n@@ -72,15 +73,7 @@ fn download_command(metadata_base_url: Url, targets_base_url: Url) {\n     assert_file_match(&outdir, \"file1.txt\");\n     assert_file_match(&outdir, \"file2.txt\");\n \n-    // Add \"bloop\" to the end of file1.txt so that we can prove that the file is truncated when we\n-    // download the repo a second time into the same outdir.\n-    let mut f = OpenOptions::write(&mut OpenOptions::new(), true)\n-        .append(true)\n-        .open(outdir.path().join(\"file1.txt\"))\n-        .unwrap();\n-    writeln!(f, \"bloop\").unwrap();\n-\n-    // Download again into the same outdir\n+    // Download again into the same outdir, this will fail because the directory exists.\n     Command::cargo_bin(\"tuftool\")\n         .unwrap()\n         .args(&[\n@@ -91,20 +84,16 @@ fn download_command(metadata_base_url: Url, targets_base_url: Url) {\n             metadata_base_url.as_str(),\n             \"--targets-url\",\n             targets_base_url.as_str(),\n-            outdir.path().to_str().unwrap(),\n+            outdir.to_str().unwrap(),\n         ])\n         .assert()\n-        .success();\n-\n-    // Assert the files are exactly correct\n-    assert_file_match(&outdir, \"file1.txt\");\n-    assert_file_match(&outdir, \"file2.txt\");\n+        .failure();\n }\n \n #[test]\n-// Ensure that the download command works with http url, and that we truncate files when downloading into a non-\n-// empty directory (i.e. that issue #173 is fixed).\n-fn download_command_truncates_http() {\n+// Ensure that the download command works with http transport and that we require outdir to\n+// not-exist.\n+fn download_http_transport() {\n     let server = Server::run();\n     server.expect(create_successful_get(\"metadata/role1.json\"));\n     server.expect(create_successful_get(\"metadata/role2.json\"));\n@@ -120,16 +109,16 @@ fn download_command_truncates_http() {\n }\n \n #[test]\n-// Ensure that the download command works with file url, and that we truncate files when downloading into a non-\n-// empty directory (i.e. that issue #173 is fixed).\n-fn download_command_truncates_file() {\n+// Ensure that the download command works with file transport, and that we require outdir to\n+// not-exist.\n+fn download_file_transport() {\n     let repo_dir = test_utils::test_data().join(\"tuf-reference-impl\");\n     let metadata_base_url = test_utils::dir_url(repo_dir.join(\"metadata\").to_str().unwrap());\n     let targets_base_url = test_utils::dir_url(repo_dir.join(\"targets\").to_str().unwrap());\n     download_command(metadata_base_url, targets_base_url);\n }\n \n-fn download_expired_repo(outdir: &TempDir, repo_dir: &TempDir, allow_expired_repo: bool) -> Assert {\n+fn download_expired_repo(outdir: &Path, repo_dir: &TempDir, allow_expired_repo: bool) -> Assert {\n     let root_json = test_utils::test_data().join(\"simple-rsa\").join(\"root.json\");\n     let metadata_base_url = &test_utils::dir_url(repo_dir.path().join(\"metadata\"));\n     let targets_base_url = &test_utils::dir_url(repo_dir.path().join(\"targets\"));\n@@ -142,7 +131,7 @@ fn download_expired_repo(outdir: &TempDir, repo_dir: &TempDir, allow_expired_rep\n         metadata_base_url.as_str(),\n         \"--targets-url\",\n         targets_base_url.as_str(),\n-        outdir.path().to_str().unwrap(),\n+        outdir.to_str().unwrap(),\n     ]);\n     if allow_expired_repo {\n         cmd.arg(\"--allow-expired-repo\").assert()\n@@ -159,13 +148,14 @@ fn download_command_expired_repo_fail() {\n     // Create a expired repo using tuftool\n     test_utils::create_expired_repo(repo_dir.path());\n     // assert failure for download command\n-    download_expired_repo(&outdir, &repo_dir, false).failure();\n+    download_expired_repo(outdir.path(), &repo_dir, false).failure();\n }\n \n #[test]\n // Ensure download command is successful when metadata has expired but --allow-expired-repo flag is passed\n fn download_command_expired_repo_allow() {\n-    let outdir = TempDir::new().unwrap();\n+    let tempdir = TempDir::new().unwrap();\n+    let outdir = tempdir.path().join(\"outdir\");\n     let repo_dir = TempDir::new().unwrap();\n     // Create a expired repo using tuftool\n     test_utils::create_expired_repo(repo_dir.path());\n@@ -175,3 +165,28 @@ fn download_command_expired_repo_allow() {\n     assert_file_match(&outdir, \"file1.txt\");\n     assert_file_match(&outdir, \"file2.txt\");\n }\n+\n+#[test]\n+// Ensure that we handle path-like target names correctly.\n+fn download_safe_target_paths() {\n+    let repo_dir = test_utils::test_data().join(\"safe-target-paths\");\n+    let root = repo_dir.join(\"metadata\").join(\"1.root.json\");\n+    let metadata_base_url = &test_utils::dir_url(repo_dir.join(\"metadata\"));\n+    let targets_base_url = &test_utils::dir_url(repo_dir.join(\"targets\"));\n+    let tempdir = TempDir::new().unwrap();\n+    let outdir = tempdir.path().join(\"outdir\");\n+    let mut cmd = Command::cargo_bin(\"tuftool\").unwrap();\n+    cmd.args(&[\n+        \"download\",\n+        \"-r\",\n+        root.to_str().unwrap(),\n+        \"--metadata-url\",\n+        metadata_base_url.as_str(),\n+        \"--targets-url\",\n+        targets_base_url.as_str(),\n+        outdir.to_str().unwrap(),\n+    ]);\n+    cmd.assert().success();\n+    assert!(outdir.join(\"data1.txt\").is_file());\n+    assert!(outdir.join(\"foo/bar/data2.txt\").is_file())\n+}"
        },
        {
          "filename": "tuftool/tests/update_command.rs",
          "status": "modified",
          "additions": 7,
          "deletions": 4,
          "patch": "@@ -10,7 +10,7 @@ use std::fs::File;\n use std::path::Path;\n use tempfile::TempDir;\n use test_utils::dir_url;\n-use tough::RepositoryLoader;\n+use tough::{RepositoryLoader, TargetName};\n \n fn create_repo<P: AsRef<Path>>(repo_dir: P) {\n     let timestamp_expiration = Utc::now().checked_add_signed(Duration::days(1)).unwrap();\n@@ -191,16 +191,19 @@ fn update_command_with_new_targets() {\n     assert_eq!(repo.targets().signed.targets.len(), 6);\n \n     // Ensure we can read the newly added targets\n+    let file4 = TargetName::new(\"file4.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file4.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file4).unwrap().unwrap()),\n         &b\"This is an example target file.\"[..]\n     );\n+    let file5 = TargetName::new(\"file5.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file5.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file5).unwrap().unwrap()),\n         &b\"This is another example target file.\"[..]\n     );\n+    let file6 = TargetName::new(\"file6.txt\").unwrap();\n     assert_eq!(\n-        test_utils::read_to_end(repo.read_target(\"file6.txt\").unwrap().unwrap()),\n+        test_utils::read_to_end(repo.read_target(&file6).unwrap().unwrap()),\n         &b\"This is yet another example target file.\"[..]\n     );\n "
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 16,
        "dependency_files": 4,
        "test_files": 35,
        "unique_directories": 19,
        "max_directory_depth": 7
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "4849410a1633fcf09fb7e3477b788a1dc48d86df",
            "date": "2024-12-31T19:42:31Z",
            "author_login": "jpculp"
          },
          {
            "sha": "02e6daf4740a534443eff744572eac27df0a2254",
            "date": "2024-12-31T19:02:18Z",
            "author_login": "dependabot[bot]"
          },
          {
            "sha": "b47a3919c886c92d85783033b91870802c65df85",
            "date": "2024-12-23T19:31:18Z",
            "author_login": "mgsharm"
          },
          {
            "sha": "90325a8b117ba65ba71a9eec0d0969c5882064ea",
            "date": "2024-12-20T19:08:15Z",
            "author_login": "mgsharm"
          },
          {
            "sha": "dbde931e7c14fef61512075d72f41d3635f41e73",
            "date": "2024-12-20T19:07:22Z",
            "author_login": "mgsharm"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 8.2,
    "cvss_vector": "CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:C/C:H/I:H/A:N",
    "cwe_id": "CWE-22",
    "description": "Tough provides a set of Rust libraries and tools for using and generating the update framework (TUF) repositories. The tough library, prior to 0.12.0, does not properly sanitize target names when caching a repository, or when saving specific targets to an output directory. When targets are cached or saved, files could be overwritten with arbitrary content anywhere on the system. A fix is available in version 0.12.0. No workarounds to this issue are known.",
    "attack_vector": "NETWORK",
    "attack_complexity": "HIGH"
  },
  "temporal_data": {
    "published_date": "2021-10-19T18:15:08.093",
    "last_modified": "2024-11-21T06:25:36.040",
    "fix_date": "2021-10-19T14:34:15Z"
  },
  "references": [
    {
      "url": "https://github.com/awslabs/tough/commit/1809b9bd1106d78a51fbea3071aa97a3530bac9a",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/awslabs/tough/security/advisories/GHSA-x3r5-q6mj-m485",
      "source": "security-advisories@github.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/awslabs/tough/commit/1809b9bd1106d78a51fbea3071aa97a3530bac9a",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/awslabs/tough/security/advisories/GHSA-x3r5-q6mj-m485",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:02:31.838831",
    "processing_status": "enhanced"
  }
}