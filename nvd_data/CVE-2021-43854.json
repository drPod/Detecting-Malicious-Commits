{
  "cve_id": "CVE-2021-43854",
  "github_data": {
    "repository": "nltk/nltk",
    "fix_commit": "1405aad979c6b8080dbbc8e0858f89b2e3690341",
    "related_commits": [
      "1405aad979c6b8080dbbc8e0858f89b2e3690341",
      "1405aad979c6b8080dbbc8e0858f89b2e3690341"
    ],
    "patch_url": null,
    "fix_commit_details": {
      "sha": "1405aad979c6b8080dbbc8e0858f89b2e3690341",
      "commit_date": "2021-11-26T11:58:19Z",
      "author": {
        "login": "tomaarsen",
        "type": "User",
        "stats": {
          "total_commits": 209,
          "average_weekly_commits": 0.17047308319738988,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 56
        }
      },
      "commit_message": {
        "title": "Resolved serious ReDoS in PunktSentenceTokenizer (#2869)",
        "length": 235,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 66,
        "additions": 61,
        "deletions": 5
      },
      "files": [
        {
          "filename": "nltk/tokenize/punkt.py",
          "status": "modified",
          "additions": 61,
          "deletions": 5,
          "patch": "@@ -266,7 +266,6 @@ def word_tokenize(self, s):\n         return self._word_tokenizer_re().findall(s)\n \n     _period_context_fmt = r\"\"\"\n-        \\S*                          # some word material\n         %(SentEndChars)s             # a potential sentence ending\n         (?=(?P<after_tok>\n             %(NonWord)s              # either other punctuation\n@@ -1284,8 +1283,7 @@ def debug_decisions(self, text):\n         See format_debug_decision() to help make this output readable.\n         \"\"\"\n \n-        for match in self._lang_vars.period_context_re().finditer(text):\n-            decision_text = match.group() + match.group(\"after_tok\")\n+        for match, decision_text in self._match_potential_end_contexts(text):\n             tokens = self._tokenize_words(decision_text)\n             tokens = list(self._annotate_first_pass(tokens))\n             while tokens and not tokens[0].tok.endswith(self._lang_vars.sent_end_chars):\n@@ -1333,10 +1331,68 @@ def sentences_from_text(self, text, realign_boundaries=True):\n         \"\"\"\n         return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n \n+    def _match_potential_end_contexts(self, text):\n+        \"\"\"\n+        Given a text, find the matches of potential sentence breaks,\n+        alongside the contexts surrounding these sentence breaks.\n+\n+        Since the fix for the ReDOS discovered in issue #2866, we no longer match\n+        the word before a potential end of sentence token. Instead, we use a separate\n+        regex for this. As a consequence, `finditer`'s desire to find non-overlapping\n+        matches no longer aids us in finding the single longest match.\n+        Where previously, we could use::\n+\n+            >>> pst = PunktSentenceTokenizer()\n+            >>> text = \"Very bad acting!!! I promise.\"\n+            >>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +SKIP\n+            [<re.Match object; span=(9, 18), match='acting!!!'>]\n+\n+        Now we have to find the word before (i.e. 'acting') separately, and `finditer`\n+        returns::\n+\n+            >>> pst = PunktSentenceTokenizer()\n+            >>> text = \"Very bad acting!!! I promise.\"\n+            >>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +NORMALIZE_WHITESPACE\n+            [<re.Match object; span=(15, 16), match='!'>,\n+            <re.Match object; span=(16, 17), match='!'>,\n+            <re.Match object; span=(17, 18), match='!'>]\n+\n+        So, we need to find the word before the match from right to left, and then manually remove\n+        the overlaps. That is what this method does::\n+\n+            >>> pst = PunktSentenceTokenizer()\n+            >>> text = \"Very bad acting!!! I promise.\"\n+            >>> pst._match_potential_end_contexts(text)\n+            [(<re.Match object; span=(17, 18), match='!'>, 'acting!!! I')]\n+\n+        :param text: String of one or more sentences\n+        :type text: str\n+        :return: List of match-context tuples.\n+        :rtype: List[Tuple[re.Match, str]]\n+        \"\"\"\n+        before_words = {}\n+        matches = []\n+        for match in reversed(list(self._lang_vars.period_context_re().finditer(text))):\n+            # Ignore matches that have already been captured by matches to the right of this match\n+            if matches and match.end() > before_start:\n+                continue\n+            # Find the word before the current match\n+            split = text[: match.start()].rsplit(maxsplit=1)\n+            before_start = len(split[0]) if len(split) == 2 else 0\n+            before_words[match] = split[-1]\n+            matches.append(match)\n+\n+        return [\n+            (\n+                match,\n+                before_words[match] + match.group() + match.group(\"after_tok\"),\n+            )\n+            for match in matches[::-1]\n+        ]\n+\n     def _slices_from_text(self, text):\n         last_break = 0\n-        for match in self._lang_vars.period_context_re().finditer(text):\n-            context = match.group() + match.group(\"after_tok\")\n+        for match, context in self._match_potential_end_contexts(text):\n             if self.text_contains_sentbreak(context):\n                 yield slice(last_break, match.end())\n                 if match.group(\"next_tok\"):"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 1,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "7397ccfed06e7c836d3acb0b9197f6e6b26c6741",
            "date": "2024-11-11T06:24:40Z",
            "author_login": "ekaf"
          },
          {
            "sha": "e7f6724af9fbfee9b8d3bdd636cbd75c57dc9323",
            "date": "2024-10-21T08:33:09Z",
            "author_login": "drewvid"
          },
          {
            "sha": "9a5622f8a5b228df9499cd03181d9f8491e39f17",
            "date": "2024-09-25T08:35:04Z",
            "author_login": "stevenbird"
          },
          {
            "sha": "d1dabecd1fe57035e836d1942e897398c49c42db",
            "date": "2024-09-24T00:50:33Z",
            "author_login": "stevenbird"
          },
          {
            "sha": "1502e55d491d54bcbf5a3e3040ac2415fcda0f6f",
            "date": "2024-09-10T10:47:29Z",
            "author_login": "ekaf"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.5,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H",
    "cwe_id": "CWE-400",
    "description": "NLTK (Natural Language Toolkit) is a suite of open source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing. Versions prior to 3.6.5 are vulnerable to regular expression denial of service (ReDoS) attacks. The vulnerability is present in PunktSentenceTokenizer, sent_tokenize and word_tokenize. Any users of this class, or these two functions, are vulnerable to the ReDoS attack. In short, a specifically crafted long input to any of these vulnerable functions will cause them to take a significant amount of execution time. If your program relies on any of the vulnerable functions for tokenizing unpredictable user input, then we would strongly recommend upgrading to a version of NLTK without the vulnerability. For users unable to upgrade the execution time can be bounded by limiting the maximum length of an input to any of the vulnerable functions. Our recommendation is to implement such a limit.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2021-12-23T18:15:07.327",
    "last_modified": "2024-11-21T06:29:56.200",
    "fix_date": "2021-11-26T11:58:19Z"
  },
  "references": [
    {
      "url": "https://github.com/nltk/nltk/commit/1405aad979c6b8080dbbc8e0858f89b2e3690341",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/issues/2866",
      "source": "security-advisories@github.com",
      "tags": [
        "Exploit",
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/pull/2869",
      "source": "security-advisories@github.com",
      "tags": [
        "Exploit",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/security/advisories/GHSA-f8m6-h2c7-8h9x",
      "source": "security-advisories@github.com",
      "tags": [
        "Exploit",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/commit/1405aad979c6b8080dbbc8e0858f89b2e3690341",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/issues/2866",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Exploit",
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/pull/2869",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Exploit",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/nltk/nltk/security/advisories/GHSA-f8m6-h2c7-8h9x",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Exploit",
        "Patch",
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:02:37.024552",
    "processing_status": "enhanced"
  },
  "repository_context": {
    "name": "nltk",
    "owner": "nltk",
    "created_at": "2009-09-07T10:53:58Z",
    "updated_at": "2025-01-25T21:36:09Z",
    "pushed_at": "2024-11-11T06:24:40Z",
    "size": 353364,
    "stars": 13798,
    "forks": 2907,
    "open_issues": 282,
    "watchers": 13798,
    "has_security_policy": false,
    "default_branch": "develop",
    "protected_branches": [],
    "languages": {
      "Python": 4882494,
      "Jupyter Notebook": 56591,
      "HTML": 24786,
      "Makefile": 7734,
      "Shell": 4707,
      "CSS": 705
    },
    "commit_activity": {
      "total_commits_last_year": 159,
      "avg_commits_per_week": 3.0576923076923075,
      "days_active_last_year": 40
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-26T07:51:31.933403"
  }
}