{
  "cve_id": "CVE-2022-0577",
  "github_data": {
    "repository": "scrapy/scrapy",
    "fix_commit": "8ce01b3b76d4634f55067d6cfdf632ec70ba304a",
    "related_commits": [
      "8ce01b3b76d4634f55067d6cfdf632ec70ba304a",
      "8ce01b3b76d4634f55067d6cfdf632ec70ba304a"
    ],
    "patch_url": null,
    "fix_commit_details": {
      "sha": "8ce01b3b76d4634f55067d6cfdf632ec70ba304a",
      "commit_date": "2022-03-01T11:26:05Z",
      "author": {
        "login": "Gallaecio",
        "type": "User",
        "stats": {
          "total_commits": 665,
          "average_weekly_commits": 0.7670126874279123,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 167
        }
      },
      "commit_message": {
        "title": "Merge pull request from GHSA-cjvr-mfj7-j4j8",
        "length": 307,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 253,
        "additions": 247,
        "deletions": 6
      },
      "files": [
        {
          "filename": "docs/news.rst",
          "status": "modified",
          "additions": 66,
          "deletions": 1,
          "patch": "@@ -5,11 +5,13 @@ Release notes\n \n .. _release-2.6.0:\n \n-Scrapy 2.6.0 (2022-02-??)\n+Scrapy 2.6.0 (2022-03-01)\n -------------------------\n \n Highlights:\n \n+*   :ref:`Security fixes for cookie handling <2.6-security-fixes>`\n+\n *   Python 3.10 support\n \n *   :ref:`asyncio support <using-asyncio>` is no longer considered\n@@ -20,6 +22,37 @@ Highlights:\n     :ref:`item filtering <item-filter>` and\n     :ref:`post-processing <post-processing>`\n \n+.. _2.6-security-fixes:\n+\n+Security bug fixes\n+~~~~~~~~~~~~~~~~~~\n+\n+-   When a :class:`~scrapy.http.Request` object with cookies defined gets a\n+    redirect response causing a new :class:`~scrapy.http.Request` object to be\n+    scheduled, the cookies defined in the original\n+    :class:`~scrapy.http.Request` object are no longer copied into the new\n+    :class:`~scrapy.http.Request` object.\n+\n+    If you manually set the ``Cookie`` header on a\n+    :class:`~scrapy.http.Request` object and the domain name of the redirect\n+    URL is not an exact match for the domain of the URL of the original\n+    :class:`~scrapy.http.Request` object, your ``Cookie`` header is now dropped\n+    from the new :class:`~scrapy.http.Request` object.\n+\n+    The old behavior could be exploited by an attacker to gain access to your\n+    cookies. Please, see the `cjvr-mfj7-j4j8 security advisory`_ for more\n+    information.\n+\n+    .. _cjvr-mfj7-j4j8 security advisory: https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8\n+\n+    .. note:: It is still possible to enable the sharing of cookies between\n+              different domains with a shared domain suffix (e.g.\n+              ``example.com`` and any subdomain) by defining the shared domain\n+              suffix (e.g. ``example.com``) as the cookie domain when defining\n+              your cookies. See the documentation of the\n+              :class:`~scrapy.http.Request` class for more information.\n+\n+\n Modified requirements\n ~~~~~~~~~~~~~~~~~~~~~\n \n@@ -1842,6 +1875,38 @@ affect subclasses:\n \n (:issue:`3884`)\n \n+.. _release-1.8.2:\n+\n+Scrapy 1.8.2 (2022-03-01)\n+-------------------------\n+\n+**Security bug fixes:**\n+\n+-   When a :class:`~scrapy.http.Request` object with cookies defined gets a\n+    redirect response causing a new :class:`~scrapy.http.Request` object to be\n+    scheduled, the cookies defined in the original\n+    :class:`~scrapy.http.Request` object are no longer copied into the new\n+    :class:`~scrapy.http.Request` object.\n+\n+    If you manually set the ``Cookie`` header on a\n+    :class:`~scrapy.http.Request` object and the domain name of the redirect\n+    URL is not an exact match for the domain of the URL of the original\n+    :class:`~scrapy.http.Request` object, your ``Cookie`` header is now dropped\n+    from the new :class:`~scrapy.http.Request` object.\n+\n+    The old behavior could be exploited by an attacker to gain access to your\n+    cookies. Please, see the `cjvr-mfj7-j4j8 security advisory`_ for more\n+    information.\n+\n+    .. _cjvr-mfj7-j4j8 security advisory: https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8\n+\n+    .. note:: It is still possible to enable the sharing of cookies between\n+              different domains with a shared domain suffix (e.g.\n+              ``example.com`` and any subdomain) by defining the shared domain\n+              suffix (e.g. ``example.com``) as the cookie domain when defining\n+              your cookies. See the documentation of the\n+              :class:`~scrapy.http.Request` class for more information.\n+\n \n .. _release-1.8.1:\n "
        },
        {
          "filename": "scrapy/downloadermiddlewares/redirect.py",
          "status": "modified",
          "additions": 26,
          "deletions": 5,
          "patch": "@@ -4,13 +4,29 @@\n from w3lib.url import safe_url_string\n \n from scrapy.http import HtmlResponse\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n \n logger = logging.getLogger(__name__)\n \n \n+def _build_redirect_request(source_request, *, url, method=None, body=None):\n+    redirect_request = source_request.replace(\n+        url=url,\n+        method=method,\n+        body=body,\n+        cookies=None,\n+    )\n+    if 'Cookie' in redirect_request.headers:\n+        source_request_netloc = urlparse_cached(source_request).netloc\n+        redirect_request_netloc = urlparse_cached(redirect_request).netloc\n+        if source_request_netloc != redirect_request_netloc:\n+            del redirect_request.headers['Cookie']\n+    return redirect_request\n+\n+\n class BaseRedirectMiddleware:\n \n     enabled_setting = 'REDIRECT_ENABLED'\n@@ -47,10 +63,15 @@ def _redirect(self, redirected, request, spider, reason):\n             raise IgnoreRequest(\"max redirections reached\")\n \n     def _redirect_request_using_get(self, request, redirect_url):\n-        redirected = request.replace(url=redirect_url, method='GET', body='')\n-        redirected.headers.pop('Content-Type', None)\n-        redirected.headers.pop('Content-Length', None)\n-        return redirected\n+        redirect_request = _build_redirect_request(\n+            request,\n+            url=redirect_url,\n+            method='GET',\n+            body='',\n+        )\n+        redirect_request.headers.pop('Content-Type', None)\n+        redirect_request.headers.pop('Content-Length', None)\n+        return redirect_request\n \n \n class RedirectMiddleware(BaseRedirectMiddleware):\n@@ -80,7 +101,7 @@ def process_response(self, request, response, spider):\n         redirected_url = urljoin(request.url, location)\n \n         if response.status in (301, 307, 308) or request.method == 'HEAD':\n-            redirected = request.replace(url=redirected_url)\n+            redirected = _build_redirect_request(request, url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n         redirected = self._redirect_request_using_get(request, redirected_url)"
        },
        {
          "filename": "tests/test_downloadermiddleware_cookies.py",
          "status": "modified",
          "additions": 155,
          "deletions": 0,
          "patch": "@@ -6,8 +6,10 @@\n \n from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n+from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n+from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n@@ -23,9 +25,11 @@ def split_cookies(cookies):\n     def setUp(self):\n         self.spider = Spider('foo')\n         self.mw = CookiesMiddleware()\n+        self.redirect_middleware = RedirectMiddleware(settings=Settings())\n \n     def tearDown(self):\n         del self.mw\n+        del self.redirect_middleware\n \n     def test_basic(self):\n         req = Request('http://scrapytest.org/')\n@@ -368,3 +372,154 @@ def test_primitive_type_cookies(self):\n         req4 = Request('http://example.org', cookies={'a': 'b'})\n         assert self.mw.process_request(req4, self.spider) is None\n         self.assertCookieValEqual(req4.headers['Cookie'], b'a=b')\n+\n+    def _test_cookie_redirect(\n+        self,\n+        source,\n+        target,\n+        *,\n+        cookies1,\n+        cookies2,\n+    ):\n+        input_cookies = {'a': 'b'}\n+\n+        if not isinstance(source, dict):\n+            source = {'url': source}\n+        if not isinstance(target, dict):\n+            target = {'url': target}\n+        target.setdefault('status', 301)\n+\n+        request1 = Request(cookies=input_cookies, **source)\n+        self.mw.process_request(request1, self.spider)\n+        cookies = request1.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n+\n+        response = Response(\n+            headers={\n+                'Location': target['url'],\n+            },\n+            **target,\n+        )\n+        self.assertEqual(\n+            self.mw.process_response(request1, response, self.spider),\n+            response,\n+        )\n+\n+        request2 = self.redirect_middleware.process_response(\n+            request1,\n+            response,\n+            self.spider,\n+        )\n+        self.assertIsInstance(request2, Request)\n+\n+        self.mw.process_request(request2, self.spider)\n+        cookies = request2.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n+\n+    def test_cookie_redirect_same_domain(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            'https://toscrape.com',\n+            cookies1=True,\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_redirect_same_domain_forcing_get(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://toscrape.com', 'status': 302},\n+            cookies1=True,\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_redirect_different_domain(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            'https://example.com',\n+            cookies1=True,\n+            cookies2=False,\n+        )\n+\n+    def test_cookie_redirect_different_domain_forcing_get(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://example.com', 'status': 302},\n+            cookies1=True,\n+            cookies2=False,\n+        )\n+\n+    def _test_cookie_header_redirect(\n+        self,\n+        source,\n+        target,\n+        *,\n+        cookies2,\n+    ):\n+        \"\"\"Test the handling of a user-defined Cookie header when building a\n+        redirect follow-up request.\n+\n+        We follow RFC 6265 for cookie handling. The Cookie header can only\n+        contain a list of key-value pairs (i.e. no additional cookie\n+        parameters like Domain or Path). Because of that, we follow the same\n+        rules that we would follow for the handling of the Set-Cookie response\n+        header when the Domain is not set: the cookies must be limited to the\n+        target URL domain (not even subdomains can receive those cookies).\n+\n+        .. note:: This method tests the scenario where the cookie middleware is\n+                  disabled. Because of known issue #1992, when the cookies\n+                  middleware is enabled we do not need to be concerned about\n+                  the Cookie header getting leaked to unintended domains,\n+                  because the middleware empties the header from every request.\n+        \"\"\"\n+        if not isinstance(source, dict):\n+            source = {'url': source}\n+        if not isinstance(target, dict):\n+            target = {'url': target}\n+        target.setdefault('status', 301)\n+\n+        request1 = Request(headers={'Cookie': b'a=b'}, **source)\n+\n+        response = Response(\n+            headers={\n+                'Location': target['url'],\n+            },\n+            **target,\n+        )\n+\n+        request2 = self.redirect_middleware.process_response(\n+            request1,\n+            response,\n+            self.spider,\n+        )\n+        self.assertIsInstance(request2, Request)\n+\n+        cookies = request2.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n+\n+    def test_cookie_header_redirect_same_domain(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            'https://toscrape.com',\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_header_redirect_same_domain_forcing_get(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://toscrape.com', 'status': 302},\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_header_redirect_different_domain(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            'https://example.com',\n+            cookies2=False,\n+        )\n+\n+    def test_cookie_header_redirect_different_domain_forcing_get(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://example.com', 'status': 302},\n+            cookies2=False,\n+        )"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 1,
        "unique_directories": 3,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "c03fb2abb8c354c56c4e8363fc602d49f956c280",
            "date": "2025-01-23T16:06:45Z",
            "author_login": "protokoul"
          },
          {
            "sha": "d4b152bbf64591317d2d7ec9dfed0746e7bdb8e1",
            "date": "2025-01-23T08:22:18Z",
            "author_login": "wRAR"
          },
          {
            "sha": "7e61ff352439d3e5c85785fc26b5503e4fed67b8",
            "date": "2025-01-22T17:09:42Z",
            "author_login": "Rotzbua"
          },
          {
            "sha": "499b6c66b40b6f66a0f32e2689affe5c7122736d",
            "date": "2025-01-22T11:14:33Z",
            "author_login": "wRAR"
          },
          {
            "sha": "9bc0029d27d8ed719e2cc2e9077a81450b995b96",
            "date": "2025-01-22T11:07:44Z",
            "author_login": "guillermo-bondonno"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 6.5,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N",
    "cwe_id": "CWE-200",
    "description": "Exposure of Sensitive Information to an Unauthorized Actor in GitHub repository scrapy/scrapy prior to 2.6.1.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2022-03-02T04:15:06.960",
    "last_modified": "2024-11-21T06:38:57.217",
    "fix_date": "2022-03-01T11:26:05Z"
  },
  "references": [
    {
      "url": "https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a",
      "source": "security@huntr.dev",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585",
      "source": "security@huntr.dev",
      "tags": [
        "Exploit",
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html",
      "source": "security@huntr.dev",
      "tags": [
        "Mailing List",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Exploit",
        "Issue Tracking",
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Mailing List",
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:02:59.766935",
    "processing_status": "enhanced"
  },
  "repository_context": {
    "name": "scrapy",
    "owner": "scrapy",
    "created_at": "2010-02-22T02:01:14Z",
    "updated_at": "2025-01-26T06:40:18Z",
    "pushed_at": "2025-01-23T16:06:46Z",
    "size": 27029,
    "stars": 53905,
    "forks": 10617,
    "open_issues": 610,
    "watchers": 53905,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "Python": 2376011,
      "HTML": 3290,
      "Roff": 2010,
      "Shell": 252
    },
    "commit_activity": {
      "total_commits_last_year": 394,
      "avg_commits_per_week": 7.576923076923077,
      "days_active_last_year": 126
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "bsd-3-clause"
    },
    "collected_at": "2025-01-26T08:28:13.691377"
  }
}