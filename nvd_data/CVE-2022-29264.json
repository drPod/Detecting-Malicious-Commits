{
  "cve_id": "CVE-2022-29264",
  "github_data": {
    "repository": "coreboot/coreboot",
    "fix_commit": "afb7a814783cda12f5b72167163b9109ee1d15a7",
    "related_commits": [
      "afb7a814783cda12f5b72167163b9109ee1d15a7",
      "afb7a814783cda12f5b72167163b9109ee1d15a7"
    ],
    "patch_url": null,
    "fix_commit_details": {
      "sha": "afb7a814783cda12f5b72167163b9109ee1d15a7",
      "commit_date": "2020-07-21T21:48:48Z",
      "author": {
        "login": "rphagura",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "cpu/x86/smm: Introduce SMM module loader version 2",
        "length": 1970,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 734,
        "additions": 726,
        "deletions": 8
      },
      "files": [
        {
          "filename": "Documentation/releases/coreboot-4.13-relnotes.md",
          "status": "modified",
          "additions": 10,
          "deletions": 0,
          "patch": "@@ -39,4 +39,14 @@ attributes as per their datasheet and convert those attributes into SPD files fo\n the platforms. More details about the tools are added in\n [README.md](https://review.coreboot.org/plugins/gitiles/coreboot/+/refs/heads/master/util/spd_tools/intel/lp4x/README.md).\n \n+### New version of SMM loader\n+\n+A new version of the SMM loader which accomodates platforms with over 32 CPU\n+CPU threads.  The existing version of SMM loader uses a 64K code/data\n+segment and only a limited number of CPU threads can fit into one segment\n+(because of save state, STM, other features, etc). This loader extends beyond\n+the 64K segment to accomodate additional CPUs and in theory allows as many\n+CPU threads as possible limited only by SMRAM space and not by 64K. By default\n+this loader version is disabled. Please see cpu/x86/Kconfig for more info.\n+\n ### Add significant changes here"
        },
        {
          "filename": "src/cpu/x86/Kconfig",
          "status": "modified",
          "additions": 8,
          "deletions": 0,
          "patch": "@@ -121,6 +121,14 @@ config SMM_STUB_STACK_SIZE\n \n endif\n \n+config X86_SMM_LOADER_VERSION2\n+\tbool\n+\tdefault n\n+\tdepends on HAVE_SMI_HANDLER\n+\thelp\n+\t  This option enables SMM module loader that works with server\n+\t  platforms which may contain more than 32 CPU threads.\n+\n config SMM_LAPIC_REMAP_MITIGATION\n \tbool\n \tdefault y if NORTHBRIDGE_INTEL_I945"
        },
        {
          "filename": "src/cpu/x86/mp_init.c",
          "status": "modified",
          "additions": 31,
          "deletions": 8,
          "patch": "@@ -726,12 +726,21 @@ static void asmlinkage smm_do_relocation(void *arg)\n \t * the location of the new SMBASE. If using SMM modules then this\n \t * calculation needs to match that of the module loader.\n \t */\n+#if CONFIG(X86_SMM_LOADER_VERSION2)\n+\tperm_smbase = smm_get_cpu_smbase(cpu);\n+\tmp_state.perm_smbase = perm_smbase;\n+\tif (!perm_smbase) {\n+\t\tprintk(BIOS_ERR, \"%s: bad SMBASE for CPU %d\\n\", __func__, cpu);\n+\t\treturn;\n+\t}\n+#else\n \tperm_smbase = mp_state.perm_smbase;\n \tperm_smbase -= cpu * runtime->save_state_size;\n-\n-\tprintk(BIOS_DEBUG, \"New SMBASE 0x%08lx\\n\", perm_smbase);\n+#endif\n \n \t/* Setup code checks this callback for validity. */\n+\tprintk(BIOS_INFO, \"%s : curr_smbase 0x%x perm_smbase 0x%x, cpu = %d\\n\",\n+\t\t__func__, (int)curr_smbase, (int)perm_smbase, cpu);\n \tmp_state.ops.relocation_handler(cpu, curr_smbase, perm_smbase);\n \n \tif (CONFIG(STM)) {\n@@ -758,9 +767,17 @@ static void adjust_smm_apic_id_map(struct smm_loader_params *smm_params)\n \n static int install_relocation_handler(int num_cpus, size_t save_state_size)\n {\n+\tint cpus = num_cpus;\n+#if CONFIG(X86_SMM_LOADER_VERSION2)\n+\t/* Default SMRAM size is not big enough to concurrently\n+\t * handle relocation for more than ~32 CPU threads\n+\t * therefore, relocate 1 by 1. */\n+\tcpus = 1;\n+#endif\n+\n \tstruct smm_loader_params smm_params = {\n \t\t.per_cpu_stack_size = CONFIG_SMM_STUB_STACK_SIZE,\n-\t\t.num_concurrent_stacks = num_cpus,\n+\t\t.num_concurrent_stacks = cpus,\n \t\t.per_cpu_save_state_size = save_state_size,\n \t\t.num_concurrent_save_states = 1,\n \t\t.handler = smm_do_relocation,\n@@ -770,9 +787,10 @@ static int install_relocation_handler(int num_cpus, size_t save_state_size)\n \tif (mp_state.ops.adjust_smm_params != NULL)\n \t\tmp_state.ops.adjust_smm_params(&smm_params, 0);\n \n-\tif (smm_setup_relocation_handler(&smm_params))\n+\tif (smm_setup_relocation_handler(&smm_params)) {\n+\t\tprintk(BIOS_ERR, \"%s: smm setup failed\\n\", __func__);\n \t\treturn -1;\n-\n+\t}\n \tadjust_smm_apic_id_map(&smm_params);\n \n \treturn 0;\n@@ -781,8 +799,13 @@ static int install_relocation_handler(int num_cpus, size_t save_state_size)\n static int install_permanent_handler(int num_cpus, uintptr_t smbase,\n \t\t\t\t\tsize_t smsize, size_t save_state_size)\n {\n-\t/* There are num_cpus concurrent stacks and num_cpus concurrent save\n-\t * state areas. Lastly, set the stack size to 1KiB. */\n+\t/*\n+\t * All the CPUs will relocate to permanaent handler now. Set parameters\n+\t * needed for all CPUs. The placement of each CPUs entry point is\n+\t * determined by the loader. This code simply provides the beginning of\n+\t * SMRAM region, the number of CPUs who will use the handler, the stack\n+\t * size and save state size for each CPU.\n+\t */\n \tstruct smm_loader_params smm_params = {\n \t\t.per_cpu_stack_size = CONFIG_SMM_MODULE_STACK_SIZE,\n \t\t.num_concurrent_stacks = num_cpus,\n@@ -794,7 +817,7 @@ static int install_permanent_handler(int num_cpus, uintptr_t smbase,\n \tif (mp_state.ops.adjust_smm_params != NULL)\n \t\tmp_state.ops.adjust_smm_params(&smm_params, 1);\n \n-\tprintk(BIOS_DEBUG, \"Installing SMM handler to 0x%08lx\\n\", smbase);\n+\tprintk(BIOS_DEBUG, \"Installing permanent SMM handler to 0x%08lx\\n\", smbase);\n \n \tif (smm_load_module((void *)smbase, smsize, &smm_params))\n \t\treturn -1;"
        },
        {
          "filename": "src/cpu/x86/smm/Makefile.inc",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -1,6 +1,10 @@\n ## SPDX-License-Identifier: GPL-2.0-only\n \n+ifeq ($(CONFIG_X86_SMM_LOADER_VERSION2),y)\n+ramstage-y += smm_module_loaderv2.c\n+else\n ramstage-y += smm_module_loader.c\n+endif\n ramstage-y += smi_trigger.c\n \n ifeq ($(CONFIG_ARCH_RAMSTAGE_X86_32),y)"
        },
        {
          "filename": "src/cpu/x86/smm/smm_module_loaderv2.c",
          "status": "added",
          "additions": 655,
          "deletions": 0,
          "patch": "@@ -0,0 +1,655 @@\n+/* SPDX-License-Identifier: GPL-2.0-only */\n+\n+#include <stdint.h>\n+#include <string.h>\n+#include <rmodule.h>\n+#include <cpu/x86/smm.h>\n+#include <commonlib/helpers.h>\n+#include <console/console.h>\n+#include <security/intel/stm/SmmStm.h>\n+\n+#define FXSAVE_SIZE 512\n+#define SMM_CODE_SEGMENT_SIZE 0x10000\n+/* FXSAVE area during relocation. While it may not be strictly needed the\n+   SMM stub code relies on the FXSAVE area being non-zero to enable SSE\n+   instructions within SMM mode. */\n+static uint8_t fxsave_area_relocation[CONFIG_MAX_CPUS][FXSAVE_SIZE]\n+__attribute__((aligned(16)));\n+\n+/*\n+ * Components that make up the SMRAM:\n+ * 1. Save state - the total save state memory used\n+ * 2. Stack - stacks for the CPUs in the SMM handler\n+ * 3. Stub - SMM stub code for calling into handler\n+ * 4. Handler - C-based SMM handler.\n+ *\n+ * The components are assumed to consist of one consecutive region.\n+ */\n+\n+/* These parameters are used by the SMM stub code. A pointer to the params\n+ * is also passed to the C-base handler. */\n+struct smm_stub_params {\n+\tu32 stack_size;\n+\tu32 stack_top;\n+\tu32 c_handler;\n+\tu32 c_handler_arg;\n+\tu32 fxsave_area;\n+\tu32 fxsave_area_size;\n+\tstruct smm_runtime runtime;\n+} __packed;\n+\n+/*\n+ * The stub is the entry point that sets up protected mode and stacks for each\n+ * CPU. It then calls into the SMM handler module. It is encoded as an rmodule.\n+ */\n+extern unsigned char _binary_smmstub_start[];\n+\n+/* Per CPU minimum stack size. */\n+#define SMM_MINIMUM_STACK_SIZE 32\n+\n+struct cpu_smm_info {\n+\tuint8_t active;\n+\tuintptr_t smbase;\n+\tuintptr_t entry;\n+\tuintptr_t ss_start;\n+\tuintptr_t code_start;\n+\tuintptr_t code_end;\n+};\n+struct cpu_smm_info cpus[CONFIG_MAX_CPUS] = { 0 };\n+\n+/*\n+ * This method creates a map of all the CPU entry points, save state locations\n+ * and the beginning and end of code segments for each CPU. This map is used\n+ * during relocation to properly align as many CPUs that can fit into the SMRAM\n+ * region. For more information on how SMRAM works, refer to the latest Intel\n+ * developer's manuals (volume 3, chapter 34). SMRAM is divided up into the\n+ * following regions:\n+ * +-----------------+ Top of SMRAM\n+ * |                 | <- MSEG, FXSAVE\n+ * +-----------------+\n+ * |    common       |\n+ * |  smi handler    | 64K\n+ * |                 |\n+ * +-----------------+\n+ * | CPU 0 code  seg |\n+ * +-----------------+\n+ * | CPU 1 code seg  |\n+ * +-----------------+\n+ * | CPU x code seg  |\n+ * +-----------------+\n+ * |                 |\n+ * |                 |\n+ * +-----------------+\n+ * |    stacks       |\n+ * +-----------------+ <- START of SMRAM\n+ *\n+ * The code below checks when a code segment is full and begins placing the remainder\n+ * CPUs in the lower segments. The entry point for each CPU is smbase + 0x8000\n+ * and save state is smbase + 0x8000 + (0x8000 - state save size). Save state\n+ * area grows downward into the CPUs entry point.  Therefore staggering too many\n+ * CPUs in one 32K block will corrupt CPU0's entry code as the save states move\n+ * downward.\n+ * input : smbase of first CPU (all other CPUs\n+ *         will go below this address)\n+ * input : num_cpus in the system. The map will\n+ *         be created from 0 to num_cpus.\n+ */\n+static int smm_create_map(uintptr_t smbase, unsigned int num_cpus,\n+\t\t\tconst struct smm_loader_params *params)\n+{\n+\tunsigned int i;\n+\tstruct rmodule smm_stub;\n+\tunsigned int ss_size = params->per_cpu_save_state_size, stub_size;\n+\tunsigned int smm_entry_offset = params->smm_main_entry_offset;\n+\tunsigned int seg_count = 0, segments = 0, available;\n+\tunsigned int cpus_in_segment = 0;\n+\tunsigned int base = smbase;\n+\n+\tif (rmodule_parse(&_binary_smmstub_start, &smm_stub)) {\n+\t\tprintk(BIOS_ERR, \"%s: unable to get SMM module size\\n\", __func__);\n+\t\treturn 0;\n+\t}\n+\n+\tstub_size = rmodule_memory_size(&smm_stub);\n+\t/* How many CPUs can fit into one 64K segment? */\n+\tavailable = 0xFFFF - smm_entry_offset - ss_size - stub_size;\n+\tif (available > 0) {\n+\t\tcpus_in_segment = available / ss_size;\n+\t\t/* minimum segments needed will always be 1 */\n+\t\tsegments = num_cpus / cpus_in_segment + 1;\n+\t\tprintk(BIOS_DEBUG,\n+\t\t\t\"%s: cpus allowed in one segment %d\\n\", __func__, cpus_in_segment);\n+\t\tprintk(BIOS_DEBUG,\n+\t\t\t\"%s: min # of segments needed %d\\n\", __func__, segments);\n+\t} else {\n+\t\tprintk(BIOS_ERR, \"%s: not enough space in SMM to setup all CPUs\\n\", __func__);\n+\t\tprintk(BIOS_ERR, \"    save state & stub size need to be reduced\\n\");\n+\t\tprintk(BIOS_ERR, \"    or increase SMRAM size\\n\");\n+\t\treturn 0;\n+\t}\n+\n+\tif (sizeof(cpus) / sizeof(struct cpu_smm_info) < num_cpus) {\n+\t\tprintk(BIOS_ERR,\n+\t\t\t\"%s: increase MAX_CPUS in Kconfig\\n\", __func__);\n+\t\treturn 0;\n+\t}\n+\n+\tfor (i = 0; i < num_cpus; i++) {\n+\t\tcpus[i].smbase = base;\n+\t\tcpus[i].entry = base + smm_entry_offset;\n+\t\tcpus[i].ss_start = cpus[i].entry + (smm_entry_offset - ss_size);\n+\t\tcpus[i].code_start = cpus[i].entry;\n+\t\tcpus[i].code_end = cpus[i].entry + stub_size;\n+\t\tcpus[i].active = 1;\n+\t\tbase -= ss_size;\n+\t\tseg_count++;\n+\t\tif (seg_count >= cpus_in_segment) {\n+\t\t\tbase -= smm_entry_offset;\n+\t\t\tseg_count = 0;\n+\t\t}\n+\t}\n+\n+\tif (CONFIG_DEFAULT_CONSOLE_LOGLEVEL >= BIOS_DEBUG) {\n+\t\tseg_count = 0;\n+\t\tfor (i = 0; i < num_cpus; i++) {\n+\t\t\tprintk(BIOS_DEBUG, \"CPU 0x%x\\n\", i);\n+\t\t\tprintk(BIOS_DEBUG,\n+\t\t\t\t\"    smbase %zx  entry %zx\\n\",\n+\t\t\t\tcpus[i].smbase, cpus[i].entry);\n+\t\t\tprintk(BIOS_DEBUG,\n+\t\t\t\t\"           ss_start %zx  code_end %zx\\n\",\n+\t\t\t\tcpus[i].ss_start, cpus[i].code_end);\n+\t\t\tseg_count++;\n+\t\t\tif (seg_count >= cpus_in_segment) {\n+\t\t\t\tprintk(BIOS_DEBUG,\n+\t\t\t\t\t\"-------------NEW CODE SEGMENT --------------\\n\");\n+\t\t\t\tseg_count = 0;\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn 1;\n+}\n+\n+/*\n+ * This method expects the smm relocation map to be complete.\n+ * This method does not read any HW registers, it simply uses a\n+ * map that was created during SMM setup.\n+ * input: cpu_num - cpu number which is used as an index into the\n+ *       map to return the smbase\n+ */\n+u32 smm_get_cpu_smbase(unsigned int cpu_num)\n+{\n+\tif (cpu_num < CONFIG_MAX_CPUS) {\n+\t\tif (cpus[cpu_num].active)\n+\t\t\treturn cpus[cpu_num].smbase;\n+\t}\n+\treturn 0;\n+}\n+\n+/*\n+ * This method assumes that at least 1 CPU has been set up from\n+ * which it will place other CPUs below its smbase ensuring that\n+ * save state does not clobber the first CPUs init code segment. The init\n+ * code which is the smm stub code is the same for all CPUs. They enter\n+ * smm, setup stacks (based on their apic id), enter protected mode\n+ * and then jump to the common smi handler.  The stack is allocated\n+ * at the beginning of smram (aka tseg base, not smbase). The stack\n+ * pointer for each CPU is calculated by using its apic id\n+ * (code is in smm_stub.s)\n+ * Each entry point will now have the same stub code which, sets up the CPU\n+ * stack, enters protected mode and then jumps to the smi handler. It is\n+ * important to enter protected mode before the jump because the \"jump to\n+ * address\" might be larger than the 20bit address supported by real mode.\n+ * SMI entry right now is in real mode.\n+ * input: smbase - this is the smbase of the first cpu not the smbase\n+ *        where tseg starts (aka smram_start). All CPUs code segment\n+ *        and stack will be below this point except for the common\n+ *        SMI handler which is one segment above\n+ * input: num_cpus - number of cpus that need relocation including\n+ *        the first CPU (though its code is already loaded)\n+ * input: top of stack (stacks work downward by default in Intel HW)\n+ * output: return -1, if runtime smi code could not be installed. In\n+ *         this case SMM will not work and any SMI's generated will\n+ *         cause a CPU shutdown or general protection fault because\n+ *         the appropriate smi handling code was not installed\n+ */\n+\n+static int smm_place_entry_code(uintptr_t smbase, unsigned int num_cpus,\n+\t\t\t\tunsigned int stack_top, const struct smm_loader_params *params)\n+{\n+\tunsigned int i;\n+\tunsigned int size;\n+\tif (smm_create_map(smbase, num_cpus, params)) {\n+\t\t/*\n+\t\t * Ensure there was enough space and the last CPUs smbase\n+\t\t * did not encroach upon the stack. Stack top is smram start\n+\t\t * + size of stack.\n+\t\t */\n+\t\tif (cpus[num_cpus].active) {\n+\t\t\tif (cpus[num_cpus - 1].smbase +\n+\t\t\t\tparams->smm_main_entry_offset < stack_top) {\n+\t\t\t\tprintk(BIOS_ERR, \"%s: stack encroachment\\n\", __func__);\n+\t\t\t\tprintk(BIOS_ERR, \"%s: smbase %zx, stack_top %x\\n\",\n+\t\t\t\t\t__func__, cpus[num_cpus].smbase, stack_top);\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\tprintk(BIOS_ERR, \"%s: unable to place smm entry code\\n\", __func__);\n+\t\treturn 0;\n+\t}\n+\n+\tprintk(BIOS_INFO, \"%s: smbase %zx, stack_top %x\\n\",\n+\t\t__func__, cpus[num_cpus-1].smbase, stack_top);\n+\n+\t/* start at 1, the first CPU stub code is already there */\n+\tsize = cpus[0].code_end - cpus[0].code_start;\n+\tfor (i = 1; i < num_cpus; i++) {\n+\t\tmemcpy((int *)cpus[i].code_start, (int *)cpus[0].code_start, size);\n+\t\tprintk(BIOS_DEBUG,\n+\t\t\t\"SMM Module: placing smm entry code at %zx,  cpu # 0x%x\\n\",\n+\t\t\tcpus[i].code_start, i);\n+\t\tprintk(BIOS_DEBUG, \"%s: copying from %zx to %zx 0x%x bytes\\n\",\n+\t\t\t__func__, cpus[0].code_start, cpus[i].code_start, size);\n+\t}\n+\treturn 1;\n+}\n+\n+/*\n+ * Place stacks in base -> base + size region, but ensure the stacks don't\n+ * overlap the staggered entry points.\n+ */\n+static void *smm_stub_place_stacks(char *base, size_t size,\n+\t\t\t\t   struct smm_loader_params *params)\n+{\n+\tsize_t total_stack_size;\n+\tchar *stacks_top;\n+\n+\t/* If stack space is requested assume the space lives in the lower\n+\t * half of SMRAM. */\n+\ttotal_stack_size = params->per_cpu_stack_size *\n+\t\t\t   params->num_concurrent_stacks;\n+\tprintk(BIOS_DEBUG, \"%s: cpus: %zx : stack space: needed -> %zx\\n\",\n+\t\t__func__, params->num_concurrent_stacks,\n+\t\ttotal_stack_size);\n+\tprintk(BIOS_DEBUG, \"  available -> %zx : per_cpu_stack_size : %zx\\n\",\n+\t\tsize, params->per_cpu_stack_size);\n+\n+\t/* There has to be at least one stack user. */\n+\tif (params->num_concurrent_stacks < 1)\n+\t\treturn NULL;\n+\n+\t/* Total stack size cannot fit. */\n+\tif (total_stack_size > size)\n+\t\treturn NULL;\n+\n+\t/* Stacks extend down to SMBASE */\n+\tstacks_top = &base[total_stack_size];\n+\tprintk(BIOS_DEBUG, \"%s: exit, stack_top %p\\n\", __func__, stacks_top);\n+\n+\treturn stacks_top;\n+}\n+\n+/*\n+ * Place the staggered entry points for each CPU. The entry points are\n+ * staggered by the per CPU SMM save state size extending down from\n+ * SMM_ENTRY_OFFSET.\n+ */\n+static int smm_stub_place_staggered_entry_points(char *base,\n+\tconst struct smm_loader_params *params, const struct rmodule *smm_stub)\n+{\n+\tsize_t stub_entry_offset;\n+\tint rc = 1;\n+\tstub_entry_offset = rmodule_entry_offset(smm_stub);\n+\t/* Each CPU now has its own stub code, which enters protected mode,\n+\t * sets up the stack, and then jumps to common SMI handler\n+\t */\n+\tif (params->num_concurrent_save_states > 1 || stub_entry_offset != 0) {\n+\t\trc = smm_place_entry_code((unsigned int)base,\n+\t\t\tparams->num_concurrent_save_states,\n+\t\t\t(unsigned int)params->stack_top, params);\n+\t}\n+\treturn rc;\n+}\n+\n+/*\n+ * The stub setup code assumes it is completely contained within the\n+ * default SMRAM size (0x10000) for the default SMI handler (entry at\n+ * 0x30000), but no assumption should be made for the permanent SMI handler.\n+ * The placement of CPU entry points for permanent handler are determined\n+ * by the number of CPUs in the system and the amount of SMRAM.\n+ * There are potentially 3 regions to place\n+ * within the default SMRAM size:\n+ * 1. Save state areas\n+ * 2. Stub code\n+ * 3. Stack areas\n+ *\n+ * The save state and smm stack are treated as contiguous for the number of\n+ * concurrent areas requested. The save state always lives at the top of the\n+ * the CPUS smbase (and the entry point is at offset 0x8000). This allows only a certain\n+ * number of CPUs with staggered entry points until the save state area comes\n+ * down far enough to overwrite/corrupt the entry code (stub code). Therefore,\n+ * an SMM map is created to avoid this corruption, see smm_create_map() above.\n+ * This module setup code works for the default (0x30000) SMM handler setup and the\n+ * permanent SMM handler.\n+ */\n+static int smm_module_setup_stub(void *smbase, size_t smm_size,\n+\t\t\t\t struct smm_loader_params *params,\n+\t\t\t\t void *fxsave_area)\n+{\n+\tsize_t total_save_state_size;\n+\tsize_t smm_stub_size;\n+\tsize_t stub_entry_offset;\n+\tchar *smm_stub_loc;\n+\tvoid *stacks_top;\n+\tsize_t size;\n+\tchar *base;\n+\tsize_t i;\n+\tstruct smm_stub_params *stub_params;\n+\tstruct rmodule smm_stub;\n+\tunsigned int total_size_all;\n+\tbase = smbase;\n+\tsize = smm_size;\n+\n+\t/* The number of concurrent stacks cannot exceed CONFIG_MAX_CPUS. */\n+\tif (params->num_concurrent_stacks > CONFIG_MAX_CPUS) {\n+\t\tprintk(BIOS_ERR, \"%s: not enough stacks\\n\", __func__);\n+\t\treturn -1;\n+\t}\n+\n+\t/* Fail if can't parse the smm stub rmodule. */\n+\tif (rmodule_parse(&_binary_smmstub_start, &smm_stub)) {\n+\t\tprintk(BIOS_ERR, \"%s: unable to parse smm stub\\n\", __func__);\n+\t\treturn -1;\n+\t}\n+\n+\t/* Adjust remaining size to account for save state. */\n+\ttotal_save_state_size = params->per_cpu_save_state_size *\n+\t\t\t\tparams->num_concurrent_save_states;\n+\tif (total_save_state_size > size) {\n+\t\tprintk(BIOS_ERR,\n+\t\t\t\"%s: more state save space needed:need -> %zx:available->%zx\\n\",\n+\t\t\t__func__, total_save_state_size, size);\n+\t\treturn -1;\n+\t}\n+\n+\tsize -= total_save_state_size;\n+\n+\t/* The save state size encroached over the first SMM entry point. */\n+\tif (size <= params->smm_main_entry_offset) {\n+\t\tprintk(BIOS_ERR, \"%s: encroachment over SMM entry point\\n\", __func__);\n+\t\tprintk(BIOS_ERR, \"%s: state save size: %zx : smm_entry_offset -> %x\\n\",\n+\t\t\t__func__, size, params->smm_main_entry_offset);\n+\t\treturn -1;\n+\t}\n+\n+\t/* Need a minimum stack size and alignment. */\n+\tif (params->per_cpu_stack_size <= SMM_MINIMUM_STACK_SIZE ||\n+\t    (params->per_cpu_stack_size & 3) != 0) {\n+\t\tprintk(BIOS_ERR, \"%s: need minimum stack size\\n\", __func__);\n+\t\treturn -1;\n+\t}\n+\n+\tsmm_stub_loc = NULL;\n+\tsmm_stub_size = rmodule_memory_size(&smm_stub);\n+\tstub_entry_offset = rmodule_entry_offset(&smm_stub);\n+\n+\t/* Put the stub at the main entry point */\n+\tsmm_stub_loc = &base[params->smm_main_entry_offset];\n+\n+\t/* Stub is too big to fit. */\n+\tif (smm_stub_size > (size - params->smm_main_entry_offset)) {\n+\t\tprintk(BIOS_ERR, \"%s: stub is too big to fit\\n\", __func__);\n+\t\treturn -1;\n+\t}\n+\n+\t/* The stacks, if requested, live in the lower half of SMRAM space\n+\t * for default handler, but for relocated handler it lives at the beginning\n+\t * of SMRAM which is TSEG base\n+\t */\n+\tsize = params->num_concurrent_stacks * params->per_cpu_stack_size;\n+\tstacks_top = smm_stub_place_stacks((char *)params->smram_start, size, params);\n+\tif (stacks_top == NULL) {\n+\t\tprintk(BIOS_ERR, \"%s: not enough space for stacks\\n\", __func__);\n+\t\tprintk(BIOS_ERR, \"%s: ....need -> %p : available -> %zx\\n\", __func__,\n+\t\t\tbase, size);\n+\t\treturn -1;\n+\t}\n+\tparams->stack_top = stacks_top;\n+\t/* Load the stub. */\n+\tif (rmodule_load(smm_stub_loc, &smm_stub)) {\n+\t\tprintk(BIOS_ERR, \"%s: load module failed\\n\", __func__);\n+\t\treturn -1;\n+\t}\n+\n+\tif (!smm_stub_place_staggered_entry_points(base, params, &smm_stub)) {\n+\t\tprintk(BIOS_ERR, \"%s: staggered entry points failed\\n\", __func__);\n+\t\treturn -1;\n+\t}\n+\n+\t/* Setup the parameters for the stub code. */\n+\tstub_params = rmodule_parameters(&smm_stub);\n+\tstub_params->stack_top = (uintptr_t)stacks_top;\n+\tstub_params->stack_size = params->per_cpu_stack_size;\n+\tstub_params->c_handler = (uintptr_t)params->handler;\n+\tstub_params->c_handler_arg = (uintptr_t)params->handler_arg;\n+\tstub_params->fxsave_area = (uintptr_t)fxsave_area;\n+\tstub_params->fxsave_area_size = FXSAVE_SIZE;\n+\tstub_params->runtime.smbase = (uintptr_t)smbase;\n+\tstub_params->runtime.smm_size = smm_size;\n+\tstub_params->runtime.save_state_size = params->per_cpu_save_state_size;\n+\tstub_params->runtime.num_cpus = params->num_concurrent_stacks;\n+\n+\tprintk(BIOS_DEBUG, \"%s: stack_end = 0x%x\\n\",\n+\t\t__func__, stub_params->runtime.smbase);\n+\tprintk(BIOS_DEBUG,\n+\t\t\"%s: stack_top = 0x%x\\n\", __func__, stub_params->stack_top);\n+\tprintk(BIOS_DEBUG, \"%s: stack_size = 0x%x\\n\",\n+\t\t__func__, stub_params->stack_size);\n+\tprintk(BIOS_DEBUG, \"%s: runtime.smbase = 0x%x\\n\",\n+\t\t__func__, stub_params->runtime.smbase);\n+\tprintk(BIOS_DEBUG, \"%s: runtime.start32_offset = 0x%x\\n\", __func__,\n+\t\tstub_params->runtime.start32_offset);\n+\tprintk(BIOS_DEBUG, \"%s: runtime.smm_size = 0x%zx\\n\",\n+\t\t__func__, smm_size);\n+\tprintk(BIOS_DEBUG, \"%s: per_cpu_save_state_size = 0x%x\\n\",\n+\t\t__func__, stub_params->runtime.save_state_size);\n+\tprintk(BIOS_DEBUG, \"%s: num_cpus = 0x%x\\n\", __func__,\n+\t\tstub_params->runtime.num_cpus);\n+\tprintk(BIOS_DEBUG, \"%s: total_save_state_size = 0x%x\\n\",\n+\t\t__func__, (stub_params->runtime.save_state_size *\n+\t\tstub_params->runtime.num_cpus));\n+\ttotal_size_all = stub_params->stack_size +\n+\t\t(stub_params->runtime.save_state_size *\n+\t\tstub_params->runtime.num_cpus);\n+\tprintk(BIOS_DEBUG, \"%s: total_size_all = 0x%x\\n\", __func__,\n+\t\ttotal_size_all);\n+\n+\t/* Initialize the APIC id to CPU number table to be 1:1 */\n+\tfor (i = 0; i < params->num_concurrent_stacks; i++)\n+\t\tstub_params->runtime.apic_id_to_cpu[i] = i;\n+\n+\t/* Allow the initiator to manipulate SMM stub parameters. */\n+\tparams->runtime = &stub_params->runtime;\n+\n+\tprintk(BIOS_DEBUG, \"SMM Module: stub loaded at %p. Will call %p(%p)\\n\",\n+\t       smm_stub_loc, params->handler, params->handler_arg);\n+\treturn 0;\n+}\n+\n+/*\n+ * smm_setup_relocation_handler assumes the callback is already loaded in\n+ * memory. i.e. Another SMM module isn't chained to the stub. The other\n+ * assumption is that the stub will be entered from the default SMRAM\n+ * location: 0x30000 -> 0x40000.\n+ */\n+int smm_setup_relocation_handler(struct smm_loader_params *params)\n+{\n+\tvoid *smram = (void *)(SMM_DEFAULT_BASE);\n+\tprintk(BIOS_SPEW, \"%s: enter\\n\", __func__);\n+\t/* There can't be more than 1 concurrent save state for the relocation\n+\t * handler because all CPUs default to 0x30000 as SMBASE. */\n+\tif (params->num_concurrent_save_states > 1)\n+\t\treturn -1;\n+\n+\t/* A handler has to be defined to call for relocation. */\n+\tif (params->handler == NULL)\n+\t\treturn -1;\n+\n+\t/* Since the relocation handler always uses stack, adjust the number\n+\t * of concurrent stack users to be CONFIG_MAX_CPUS. */\n+\tif (params->num_concurrent_stacks == 0)\n+\t\tparams->num_concurrent_stacks = CONFIG_MAX_CPUS;\n+\n+\tparams->smm_main_entry_offset = SMM_ENTRY_OFFSET;\n+\tparams->smram_start = SMM_DEFAULT_BASE;\n+\tparams->smram_end = SMM_DEFAULT_BASE + SMM_DEFAULT_SIZE;\n+\treturn smm_module_setup_stub(smram, SMM_DEFAULT_SIZE,\n+\t\t\t\tparams, fxsave_area_relocation);\n+\tprintk(BIOS_SPEW, \"%s: exit\\n\", __func__);\n+}\n+\n+/*\n+ *The SMM module is placed within the provided region in the following\n+ * manner:\n+ * +-----------------+ <- smram + size\n+ * | BIOS resource   |\n+ * | list (STM)      |\n+ * +-----------------+\n+ * |  fxsave area    |\n+ * +-----------------+\n+ * |  smi handler    |\n+ * |      ...        |\n+ * +-----------------+ <- cpu0\n+ * |    stub code    | <- cpu1\n+ * |    stub code    | <- cpu2\n+ * |    stub code    | <- cpu3, etc\n+ * |                 |\n+ * |                 |\n+ * |                 |\n+ * |    stacks       |\n+ * +-----------------+ <- smram start\n+\n+ * It should be noted that this algorithm will not work for\n+ * SMM_DEFAULT_SIZE SMRAM regions such as the A segment. This algorithm\n+ * expects a region large enough to encompass the handler and stacks\n+ * as well as the SMM_DEFAULT_SIZE.\n+ */\n+int smm_load_module(void *smram, size_t size, struct smm_loader_params *params)\n+{\n+\tstruct rmodule smm_mod;\n+\tsize_t total_stack_size;\n+\tsize_t handler_size;\n+\tsize_t module_alignment;\n+\tsize_t alignment_size;\n+\tsize_t fxsave_size;\n+\tvoid *fxsave_area;\n+\tsize_t total_size = 0;\n+\tchar *base;\n+\n+\tif (size <= SMM_DEFAULT_SIZE)\n+\t\treturn -1;\n+\n+\t/* Load main SMI handler at the top of SMRAM\n+\t * everything else will go below\n+\t */\n+\tbase = smram;\n+\tbase += size;\n+\tparams->smram_start = (uintptr_t)smram;\n+\tparams->smram_end = params->smram_start + size;\n+\tparams->smm_main_entry_offset = SMM_ENTRY_OFFSET;\n+\n+\t/* Fail if can't parse the smm rmodule. */\n+\tif (rmodule_parse(&_binary_smm_start, &smm_mod))\n+\t\treturn -1;\n+\n+\t/* Clear SMM region */\n+\tif (CONFIG(DEBUG_SMI))\n+\t\tmemset(smram, 0xcd, size);\n+\n+\ttotal_stack_size = params->per_cpu_stack_size *\n+\t\t\t   params->num_concurrent_stacks;\n+\ttotal_size += total_stack_size;\n+\t/* Stacks are the base of SMRAM */\n+\tparams->stack_top = smram + total_stack_size;\n+\n+\t/* MSEG starts at the top of SMRAM and works down */\n+\tif (CONFIG(STM)) {\n+\t\tbase -= CONFIG_MSEG_SIZE + CONFIG_BIOS_RESOURCE_LIST_SIZE;\n+\t\ttotal_size += CONFIG_MSEG_SIZE + CONFIG_BIOS_RESOURCE_LIST_SIZE;\n+\t}\n+\n+\t/* FXSAVE goes below MSEG */\n+\tif (CONFIG(SSE)) {\n+\t\tfxsave_size = FXSAVE_SIZE * params->num_concurrent_stacks;\n+\t\tfxsave_area = base - fxsave_size;\n+\t\tbase -= fxsave_size;\n+\t\ttotal_size += fxsave_size;\n+\t} else {\n+\t\tfxsave_size = 0;\n+\t\tfxsave_area = NULL;\n+\t}\n+\n+\n+\thandler_size = rmodule_memory_size(&smm_mod);\n+\tbase -= handler_size;\n+\ttotal_size += handler_size;\n+\tmodule_alignment = rmodule_load_alignment(&smm_mod);\n+\talignment_size = module_alignment -\n+\t\t\t\t((uintptr_t)base % module_alignment);\n+\tif (alignment_size != module_alignment) {\n+\t\thandler_size += alignment_size;\n+\t\tbase += alignment_size;\n+\t}\n+\n+\tprintk(BIOS_DEBUG,\n+\t\t\"%s: total_smm_space_needed %zx, available -> %zx\\n\",\n+\t\t __func__, total_size, size);\n+\n+\t/* Does the required amount of memory exceed the SMRAM region size? */\n+\tif (total_size > size) {\n+\t\tprintk(BIOS_ERR, \"%s: need more SMRAM\\n\", __func__);\n+\t\treturn -1;\n+\t}\n+\tif (handler_size > SMM_CODE_SEGMENT_SIZE) {\n+\t\tprintk(BIOS_ERR, \"%s: increase SMM_CODE_SEGMENT_SIZE: handler_size = %zx\\n\",\n+\t\t\t__func__, handler_size);\n+\t\treturn -1;\n+\t}\n+\n+\tif (rmodule_load(base, &smm_mod))\n+\t\treturn -1;\n+\n+\tparams->handler = rmodule_entry(&smm_mod);\n+\tparams->handler_arg = rmodule_parameters(&smm_mod);\n+\n+\tprintk(BIOS_DEBUG, \"%s: smram_start: 0x%p\\n\",\n+\t\t __func__, smram);\n+\tprintk(BIOS_DEBUG, \"%s: smram_end: %p\\n\",\n+\t\t __func__, smram + size);\n+\tprintk(BIOS_DEBUG, \"%s: stack_top: %p\\n\",\n+\t\t __func__, params->stack_top);\n+\tprintk(BIOS_DEBUG, \"%s: handler start %p\\n\",\n+\t\t __func__, params->handler);\n+\tprintk(BIOS_DEBUG, \"%s: handler_size %zx\\n\",\n+\t\t __func__, handler_size);\n+\tprintk(BIOS_DEBUG, \"%s: handler_arg %p\\n\",\n+\t\t __func__, params->handler_arg);\n+\tprintk(BIOS_DEBUG, \"%s: fxsave_area %p\\n\",\n+\t\t __func__, fxsave_area);\n+\tprintk(BIOS_DEBUG, \"%s: fxsave_size %zx\\n\",\n+\t\t __func__, fxsave_size);\n+\tprintk(BIOS_DEBUG, \"%s: CONFIG_MSEG_SIZE 0x%x\\n\",\n+\t\t __func__, CONFIG_MSEG_SIZE);\n+\tprintk(BIOS_DEBUG, \"%s: CONFIG_BIOS_RESOURCE_LIST_SIZE 0x%x\\n\",\n+\t\t __func__, CONFIG_BIOS_RESOURCE_LIST_SIZE);\n+\n+\t/* CPU 0 smbase goes first, all other CPUs\n+\t * will be staggered below\n+\t */\n+\tbase -= SMM_CODE_SEGMENT_SIZE;\n+\tprintk(BIOS_DEBUG, \"%s: cpu0 entry: %p\\n\",\n+\t\t __func__, base);\n+\tparams->smm_entry = (uintptr_t)base + params->smm_main_entry_offset;\n+\treturn smm_module_setup_stub(base, size, params, fxsave_area);\n+}"
        },
        {
          "filename": "src/include/cpu/x86/smm.h",
          "status": "modified",
          "additions": 18,
          "deletions": 0,
          "patch": "@@ -128,6 +128,12 @@ static inline bool smm_points_to_smram(const void *ptr, const size_t len)\n  *             into this field so the code doing the loading can manipulate the\n  *             runtime's assumptions. e.g. updating the APIC id to CPU map to\n  *             handle sparse APIC id space.\n+ * The following parameters are only used when X86_SMM_LOADER_VERSION2 is enabled.\n+ * - smm_entry - entry address of first CPU thread, all others will be tiled\n+ *               below this address.\n+ * - smm_main_entry_offset - default entry offset (e.g 0x8000)\n+ * - smram_start - smaram starting address\n+ * - smram_end - smram ending address\n  */\n struct smm_loader_params {\n \tvoid *stack_top;\n@@ -141,12 +147,24 @@ struct smm_loader_params {\n \tvoid *handler_arg;\n \n \tstruct smm_runtime *runtime;\n+\n+\t/* The following are only used by X86_SMM_LOADER_VERSION2 */\n+#if CONFIG(X86_SMM_LOADER_VERSION2)\n+\tunsigned int smm_entry;\n+\tunsigned int smm_main_entry_offset;\n+\tunsigned int smram_start;\n+\tunsigned int smram_end;\n+#endif\n };\n \n /* Both of these return 0 on success, < 0 on failure. */\n int smm_setup_relocation_handler(struct smm_loader_params *params);\n int smm_load_module(void *smram, size_t size, struct smm_loader_params *params);\n \n+#if CONFIG(X86_SMM_LOADER_VERSION2)\n+u32 smm_get_cpu_smbase(unsigned int cpu_num);\n+#endif\n+\n /* Backup and restore default SMM region. */\n void *backup_default_smm_area(void);\n void restore_default_smm_area(void *smm_save_area);"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 1,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 4,
        "max_directory_depth": 4
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "ee9201de40711d2cfe5506db999e15c142b67cd1",
            "date": "2025-01-23T15:04:09Z",
            "author_login": "Sean-StarLabs"
          },
          {
            "sha": "1e733b59a990d78948cbabdeae5b4de5e3e60da0",
            "date": "2025-01-23T13:53:43Z",
            "author_login": "Sean-StarLabs"
          },
          {
            "sha": "0c6576ba679602ded05eef75423a13f3eee170be",
            "date": "2025-01-23T13:49:49Z",
            "author_login": "Sean-StarLabs"
          },
          {
            "sha": "5deea4ca73665457a9439fb2e461bf5a45108ad5",
            "date": "2025-01-23T13:47:31Z",
            "author_login": "Sean-StarLabs"
          },
          {
            "sha": "e15c97c56c6bb5cc7c4c9a081276ee03696fe97e",
            "date": "2025-01-21T15:41:02Z",
            "author_login": "Sean-StarLabs"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 9.8,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H",
    "cwe_id": null,
    "description": "An issue was discovered in coreboot 4.13 through 4.16. On APs, arbitrary code execution in SMM may occur.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2022-04-25T07:15:06.913",
    "last_modified": "2024-11-21T06:58:49.897",
    "fix_date": "2020-07-21T21:48:48Z"
  },
  "references": [
    {
      "url": "https://github.com/coreboot/coreboot/commit/afb7a814783cda12f5b72167163b9109ee1d15a7",
      "source": "cve@mitre.org",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://review.coreboot.org/c/coreboot/+/63478",
      "source": "cve@mitre.org",
      "tags": [
        "Vendor Advisory"
      ]
    },
    {
      "url": "https://github.com/coreboot/coreboot/commit/afb7a814783cda12f5b72167163b9109ee1d15a7",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://review.coreboot.org/c/coreboot/+/63478",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Vendor Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:03:04.412208",
    "processing_status": "enhanced"
  },
  "repository_context": {
    "name": "coreboot",
    "owner": "coreboot",
    "created_at": "2014-03-16T08:11:46Z",
    "updated_at": "2025-01-13T17:28:50Z",
    "pushed_at": "2025-01-10T15:13:15Z",
    "size": 242230,
    "stars": 2262,
    "forks": 547,
    "open_issues": 0,
    "watchers": 2262,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [],
    "languages": {
      "C": 113357976,
      "ASL": 2509277,
      "Makefile": 1105913,
      "C++": 946421,
      "Pawn": 565101,
      "Assembly": 466866,
      "Perl": 412480,
      "Go": 404579,
      "Shell": 300622,
      "Python": 264925,
      "Ada": 132471,
      "Yacc": 37970,
      "Roff": 27139,
      "Lex": 15390,
      "Dockerfile": 8230,
      "M4": 6550,
      "Ruby": 6249,
      "Nix": 6045,
      "HTML": 5455,
      "Meson": 2559,
      "sed": 1449,
      "Awk": 491
    },
    "commit_activity": {
      "total_commits_last_year": 2152,
      "avg_commits_per_week": 41.38461538461539,
      "days_active_last_year": 326
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": false,
      "has_issues": false,
      "allow_forking": true,
      "is_template": false,
      "license": "gpl-2.0"
    },
    "collected_at": "2025-01-14T21:05:38.449409"
  }
}