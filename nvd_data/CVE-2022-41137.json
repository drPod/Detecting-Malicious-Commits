{
  "cve_id": "CVE-2022-41137",
  "github_data": {
    "repository": "apache/hive",
    "fix_commit": "60027bb9c91a93affcfebd9068f064bc1f2a74c9",
    "related_commits": [
      "60027bb9c91a93affcfebd9068f064bc1f2a74c9"
    ],
    "patch_url": "https://github.com/apache/hive/commit/60027bb9c91a93affcfebd9068f064bc1f2a74c9.patch",
    "fix_commit_details": {
      "sha": "60027bb9c91a93affcfebd9068f064bc1f2a74c9",
      "commit_date": "2022-10-18T11:54:15Z",
      "author": {
        "login": "dengzhhu653",
        "type": "User",
        "stats": {
          "total_commits": 104,
          "average_weekly_commits": 0.1216374269005848,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 83
        }
      },
      "commit_message": {
        "title": "HIVE-26539: Prevent unsafe deserialization in PartitionExpressionForMetastore (#3605)",
        "length": 85,
        "has_description": false,
        "references_issue": true
      },
      "stats": {
        "total": 89,
        "additions": 64,
        "deletions": 25
      },
      "files": [
        {
          "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java",
          "status": "modified",
          "additions": 36,
          "deletions": 3,
          "patch": "@@ -124,6 +124,10 @@ private static class KryoWithHooks extends Kryo implements Configurable {\n     private Hook globalHook;\n     // this should be set on-the-fly after borrowing this instance and needs to be reset on release\n     private Configuration configuration;\n+    // default false, should be reset on release\n+    private boolean isExprNodeFirst = false;\n+    // total classes we have met during (de)serialization, should be reset on release\n+    private long classCounter = 0;\n \n     @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n     private static final class SerializerWithHook extends com.esotericsoftware.kryo.Serializer {\n@@ -228,6 +232,32 @@ public void setConf(Configuration conf) {\n     public Configuration getConf() {\n       return configuration;\n     }\n+\n+    @Override\n+    public com.esotericsoftware.kryo.Registration getRegistration(Class type) {\n+      // If PartitionExpressionForMetastore performs deserialization at remote HMS,\n+      // the first class encountered during deserialization must be an ExprNodeDesc,\n+      // throw exception to avoid potential security problem if it is not.\n+      if (isExprNodeFirst && classCounter == 0) {\n+        if (!ExprNodeDesc.class.isAssignableFrom(type)) {\n+          throw new UnsupportedOperationException(\n+              \"The object to be deserialized must be an ExprNodeDesc, but encountered: \" + type);\n+        }\n+      }\n+      classCounter++;\n+      return super.getRegistration(type);\n+    }\n+\n+    public void setExprNodeFirst(boolean isPartFilter) {\n+      this.isExprNodeFirst = isPartFilter;\n+    }\n+\n+    // reset the fields on release\n+    public void restore() {\n+      setConf(null);\n+      isExprNodeFirst = false;\n+      classCounter = 0;\n+    }\n   }\n \n   private static final Object FAKE_REFERENCE = new Object();\n@@ -294,7 +324,7 @@ public static Kryo borrowKryo(Configuration configuration) {\n    */\n   public static void releaseKryo(Kryo kryo) {\n     if (kryo != null){\n-      ((KryoWithHooks) kryo).setConf(null);\n+      ((KryoWithHooks) kryo).restore();\n     }\n     kryoPool.free(kryo);\n   }\n@@ -830,10 +860,13 @@ public static byte[] serializeObjectWithTypeInformation(Serializable object) {\n   /**\n    * Deserializes expression from Kryo.\n    * @param bytes Bytes containing the expression.\n+   * @param isPartFilter ture if it is a partition filter\n    * @return Expression; null if deserialization succeeded, but the result type is incorrect.\n    */\n-  public static <T> T deserializeObjectWithTypeInformation(byte[] bytes) {\n-    Kryo kryo = borrowKryo();\n+  public static <T> T deserializeObjectWithTypeInformation(byte[] bytes,\n+      boolean isPartFilter) {\n+    KryoWithHooks kryo = (KryoWithHooks) borrowKryo();\n+    kryo.setExprNodeFirst(isPartFilter);\n     try (Input inp = new Input(new ByteArrayInputStream(bytes))) {\n       return (T) kryo.readClassAndObject(inp);\n     } finally {"
        },
        {
          "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionExpressionForMetastore.java",
          "status": "modified",
          "additions": 1,
          "deletions": 2,
          "patch": "@@ -37,7 +37,6 @@\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils;\n-import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.slf4j.Logger;\n@@ -108,7 +107,7 @@ public boolean filterPartitionsByExpr(List<FieldSchema> partColumns,\n   private ExprNodeDesc deserializeExpr(byte[] exprBytes) throws MetaException {\n     ExprNodeDesc expr = null;\n     try {\n-      expr = SerializationUtilities.deserializeObjectWithTypeInformation(exprBytes);\n+      expr = SerializationUtilities.deserializeObjectWithTypeInformation(exprBytes, true);\n     } catch (Exception ex) {\n       LOG.error(\"Failed to deserialize the expression\", ex);\n       throw new MetaException(ex.getMessage());"
        },
        {
          "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/TestSerializationUtilities.java",
          "status": "modified",
          "additions": 26,
          "deletions": 0,
          "patch": "@@ -22,6 +22,7 @@\n import java.io.IOException;\n import java.io.InputStream;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n@@ -32,10 +33,16 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\n import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n import org.apache.hadoop.hive.ql.plan.MapWork;\n import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -127,6 +134,25 @@ public void testSkippingAppliesToAllPartitions() throws Exception {\n     assertPartitionDescPropertyPresence(mapWork, \"/warehouse/test_table/p=1\", \"serialization.ddl\", false);\n   }\n \n+  @Test\n+  public void testUnsupportedDeserialization() throws Exception {\n+    ArrayList<Long> invalidExpr = new ArrayList<>();\n+    invalidExpr.add(1L);\n+    byte[] buf = SerializationUtilities.serializeObjectWithTypeInformation(invalidExpr);\n+    try {\n+      SerializationUtilities.deserializeObjectWithTypeInformation(buf, true);\n+      Assert.fail(\"Should throw exception as the input is not a valid filter\");\n+    } catch (UnsupportedOperationException e) {\n+      // ignore\n+    }\n+\n+    ExprNodeDesc validExpr = ExprNodeGenericFuncDesc.newInstance(new GenericUDFOPNull(),\n+        Arrays.asList(new ExprNodeColumnDesc(new ColumnInfo(\"_c0\", TypeInfoFactory.stringTypeInfo, \"a\", false))));\n+    buf = SerializationUtilities.serializeObjectWithTypeInformation(validExpr);\n+    ExprNodeDesc desc = SerializationUtilities.deserializeObjectWithTypeInformation(buf, true);\n+    Assert.assertTrue(ExprNodeDescUtils.isSame(validExpr, desc));\n+  }\n+\n   private MapWork doSerDeser(Configuration configuration) throws Exception, IOException {\n     MapWork mapWork = mockMapWorkWithSomePartitionDescProperties();\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();"
        },
        {
          "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
          "status": "modified",
          "additions": 1,
          "deletions": 20,
          "patch": "@@ -422,7 +422,7 @@ private void initialize() {\n     isInitialized = pm != null;\n     if (isInitialized) {\n       dbType = determineDatabaseProduct();\n-      expressionProxy = createExpressionProxy(conf);\n+      expressionProxy = PartFilterExprUtil.createExpressionProxy(conf);\n       if (MetastoreConf.getBoolVar(getConf(), ConfVars.TRY_DIRECT_SQL)) {\n         String schema = PersistenceManagerProvider.getProperty(\"javax.jdo.mapping.Schema\");\n         schema = org.apache.commons.lang3.StringUtils.defaultIfBlank(schema, null);\n@@ -447,25 +447,6 @@ private static String getProductName(PersistenceManager pm) {\n     }\n   }\n \n-  /**\n-   * Creates the proxy used to evaluate expressions. This is here to prevent circular\n-   * dependency - ql -&gt; metastore client &lt;-&gt metastore server -&gt ql. If server and\n-   * client are split, this can be removed.\n-   * @param conf Configuration.\n-   * @return The partition expression proxy.\n-   */\n-  private static PartitionExpressionProxy createExpressionProxy(Configuration conf) {\n-    String className = MetastoreConf.getVar(conf, ConfVars.EXPRESSION_PROXY_CLASS);\n-    try {\n-      Class<? extends PartitionExpressionProxy> clazz =\n-           JavaUtils.getClass(className, PartitionExpressionProxy.class);\n-      return JavaUtils.newInstance(clazz, new Class<?>[0], new Object[0]);\n-    } catch (MetaException e) {\n-      LOG.error(\"Error loading PartitionExpressionProxy\", e);\n-      throw new RuntimeException(\"Error loading PartitionExpressionProxy: \" + e.getMessage());\n-    }\n-  }\n-\n   /**\n    * Configure SSL encryption to the database store.\n    *"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 1,
        "unique_directories": 4,
        "max_directory_depth": 10
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "91e0f1a04012f9fc049d3a9557112fdc8830d65d",
            "date": "2025-01-14T16:50:52Z",
            "author_login": "ramesh0201"
          },
          {
            "sha": "4d012ca515beaac2455c2de6bdca73b0576b1af4",
            "date": "2025-01-14T14:22:13Z",
            "author_login": "difin"
          },
          {
            "sha": "d486f587df0954f43f0ef6e7dabe7aafe25b1cc4",
            "date": "2025-01-13T15:30:06Z",
            "author_login": "okumin"
          },
          {
            "sha": "62ad7d42d6fd4e10ef5b86545ae41011554fd2f5",
            "date": "2025-01-13T05:21:18Z",
            "author_login": "kasakrisz"
          },
          {
            "sha": "a057684db69a60906d386de30efa7ec23d4fb3e8",
            "date": "2025-01-12T09:36:47Z",
            "author_login": "Indhumathi27"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 8.3,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:L/I:H/A:H",
    "cwe_id": "CWE-502",
    "description": "Apache Hive\u00a0Metastore (HMS) uses\u00a0SerializationUtilities#deserializeObjectWithTypeInformation\u00a0method when filtering and fetching partitions that is unsafe and\u00a0can lead\u00a0to Remote Code Execution (RCE) since it allows the deserialization of arbitrary data.\n\nIn real deployments, the vulnerability can be exploited only by authenticated users/clients that were able to successfully establish\u00a0a connection to the Metastore. From an API perspective any code that calls the unsafe method may be vulnerable unless it performs additional prerechecks on the input arguments.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-12-05T10:15:04.450",
    "last_modified": "2024-12-05T17:15:07.033",
    "fix_date": "2022-10-18T11:54:15Z"
  },
  "references": [
    {
      "url": "https://github.com/apache/hive",
      "source": "security@apache.org",
      "tags": []
    },
    {
      "url": "https://github.com/apache/hive/commit/60027bb9c91a93affcfebd9068f064bc1f2a74c9",
      "source": "security@apache.org",
      "tags": []
    },
    {
      "url": "https://issues.apache.org/jira/browse/HIVE-26539",
      "source": "security@apache.org",
      "tags": []
    },
    {
      "url": "https://lists.apache.org/thread/jwtr3d9yovf2wo0qlxvkhoxnwxxyzgts",
      "source": "security@apache.org",
      "tags": []
    },
    {
      "url": "http://www.openwall.com/lists/oss-security/2024/12/04/2",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:09:33.093218",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "hive",
    "owner": "apache",
    "created_at": "2009-05-21T02:31:01Z",
    "updated_at": "2025-01-14T16:51:01Z",
    "pushed_at": "2025-01-14T16:50:52Z",
    "size": 731849,
    "stars": 5608,
    "forks": 4705,
    "open_issues": 61,
    "watchers": 5608,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "Java": 63055932,
      "HiveQL": 9103887,
      "Python": 384806,
      "Perl": 319742,
      "Shell": 278655,
      "GAP": 201790,
      "Thrift": 170944,
      "PHP": 148097,
      "C++": 98266,
      "ANTLR": 59134,
      "JavaScript": 45704,
      "q": 29163,
      "C": 28218,
      "HTML": 16027,
      "PigLatin": 12333,
      "TSQL": 10497,
      "PLSQL": 9105,
      "CSS": 5758,
      "Roff": 5379,
      "Dockerfile": 3637,
      "M4": 2276,
      "XSLT": 1329,
      "SQLPL": 1190,
      "Batchfile": 845,
      "ReScript": 240
    },
    "commit_activity": {
      "total_commits_last_year": 380,
      "avg_commits_per_week": 7.3076923076923075,
      "days_active_last_year": 190
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": false,
      "has_issues": false,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-14T21:17:35.965259"
  }
}