{
  "cve_id": "CVE-2022-41917",
  "github_data": {
    "repository": "opensearch-project/OpenSearch",
    "fix_commit": "6d20423f5920745463b1abc5f1daf6a786c41aa0",
    "related_commits": [
      "6d20423f5920745463b1abc5f1daf6a786c41aa0",
      "6d20423f5920745463b1abc5f1daf6a786c41aa0"
    ],
    "patch_url": "https://github.com/opensearch-project/OpenSearch/commit/6d20423f5920745463b1abc5f1daf6a786c41aa0.patch",
    "fix_commit_details": {
      "sha": "6d20423f5920745463b1abc5f1daf6a786c41aa0",
      "commit_date": "2022-11-08T00:52:52Z",
      "author": {
        "login": "adnapibar",
        "type": "User",
        "stats": {
          "total_commits": 192,
          "average_weekly_commits": 0.3609022556390977,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 41
        }
      },
      "commit_message": {
        "title": "Merge pull request from GHSA-w3rx-m34v-wrqx",
        "length": 484,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 547,
        "additions": 393,
        "deletions": 154
      },
      "files": [
        {
          "filename": "CHANGELOG.md",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -212,6 +212,7 @@ Inspired from [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)\n - Fix flaky test ResourceAwareTasksTests on Windows ([#5077](https://github.com/opensearch-project/OpenSearch/pull/5077))\n - Length calculation for block based fetching ([#5055](https://github.com/opensearch-project/OpenSearch/pull/5055))\n - [Segment Replication] Fix for AlreadyClosedException for engine ([#4743](https://github.com/opensearch-project/OpenSearch/pull/4743))\n+- Fix error handling while reading analyzer mapping rules\n ### Security\n - CVE-2022-25857 org.yaml:snakeyaml DOS vulnerability ([#4341](https://github.com/opensearch-project/OpenSearch/pull/4341))\n "
        },
        {
          "filename": "modules/analysis-common/src/main/java/org/opensearch/analysis/common/HyphenationCompoundWordTokenFilterFactory.java",
          "status": "modified",
          "additions": 6,
          "deletions": 2,
          "patch": "@@ -32,12 +32,14 @@\n \n package org.opensearch.analysis.common;\n \n+import org.apache.logging.log4j.LogManager;\n import org.apache.lucene.analysis.TokenStream;\n import org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter;\n import org.apache.lucene.analysis.compound.hyphenation.HyphenationTree;\n import org.opensearch.common.settings.Settings;\n import org.opensearch.env.Environment;\n import org.opensearch.index.IndexSettings;\n+import org.opensearch.index.analysis.Analysis;\n import org.xml.sax.InputSource;\n \n import java.io.InputStream;\n@@ -61,13 +63,15 @@ public class HyphenationCompoundWordTokenFilterFactory extends AbstractCompoundW\n             throw new IllegalArgumentException(\"hyphenation_patterns_path is a required setting.\");\n         }\n \n-        Path hyphenationPatternsFile = env.configDir().resolve(hyphenationPatternsPath);\n+        Path hyphenationPatternsFile = Analysis.resolveAnalyzerPath(env, hyphenationPatternsPath);\n \n         try {\n             InputStream in = Files.newInputStream(hyphenationPatternsFile);\n             hyphenationTree = HyphenationCompoundWordTokenFilter.getHyphenationTree(new InputSource(in));\n         } catch (Exception e) {\n-            throw new IllegalArgumentException(\"Exception while reading hyphenation_patterns_path.\", e);\n+            LogManager.getLogger(HyphenationCompoundWordTokenFilterFactory.class)\n+                .error(\"Exception while reading hyphenation_patterns_path \", e);\n+            throw new IllegalArgumentException(\"Exception while reading hyphenation_patterns_path.\");\n         }\n     }\n "
        },
        {
          "filename": "modules/analysis-common/src/main/java/org/opensearch/analysis/common/MappingCharFilterFactory.java",
          "status": "modified",
          "additions": 10,
          "deletions": 14,
          "patch": "@@ -39,6 +39,7 @@\n import org.opensearch.index.IndexSettings;\n import org.opensearch.index.analysis.AbstractCharFilterFactory;\n import org.opensearch.index.analysis.Analysis;\n+import org.opensearch.index.analysis.MappingRule;\n import org.opensearch.index.analysis.NormalizingCharFilterFactory;\n \n import java.io.Reader;\n@@ -53,13 +54,13 @@ public class MappingCharFilterFactory extends AbstractCharFilterFactory implemen\n     MappingCharFilterFactory(IndexSettings indexSettings, Environment env, String name, Settings settings) {\n         super(indexSettings, name);\n \n-        List<String> rules = Analysis.getWordList(env, settings, \"mappings\");\n+        List<MappingRule<String, String>> rules = Analysis.parseWordList(env, settings, \"mappings\", this::parse);\n         if (rules == null) {\n             throw new IllegalArgumentException(\"mapping requires either `mappings` or `mappings_path` to be configured\");\n         }\n \n         NormalizeCharMap.Builder normMapBuilder = new NormalizeCharMap.Builder();\n-        parseRules(rules, normMapBuilder);\n+        rules.forEach(rule -> normMapBuilder.add(rule.getLeft(), rule.getRight()));\n         normMap = normMapBuilder.build();\n     }\n \n@@ -71,18 +72,13 @@ public Reader create(Reader tokenStream) {\n     // source => target\n     private static Pattern rulePattern = Pattern.compile(\"(.*)\\\\s*=>\\\\s*(.*)\\\\s*$\");\n \n-    /**\n-     * parses a list of MappingCharFilter style rules into a normalize char map\n-     */\n-    private void parseRules(List<String> rules, NormalizeCharMap.Builder map) {\n-        for (String rule : rules) {\n-            Matcher m = rulePattern.matcher(rule);\n-            if (!m.find()) throw new RuntimeException(\"Invalid Mapping Rule : [\" + rule + \"]\");\n-            String lhs = parseString(m.group(1).trim());\n-            String rhs = parseString(m.group(2).trim());\n-            if (lhs == null || rhs == null) throw new RuntimeException(\"Invalid Mapping Rule : [\" + rule + \"]. Illegal mapping.\");\n-            map.add(lhs, rhs);\n-        }\n+    private MappingRule<String, String> parse(String rule) {\n+        Matcher m = rulePattern.matcher(rule);\n+        if (!m.find()) throw new RuntimeException(\"Invalid mapping rule : [\" + rule + \"]\");\n+        String lhs = parseString(m.group(1).trim());\n+        String rhs = parseString(m.group(2).trim());\n+        if (lhs == null || rhs == null) throw new RuntimeException(\"Invalid mapping rule: [\" + rule + \"]. Illegal mapping.\");\n+        return new MappingRule<>(lhs, rhs);\n     }\n \n     char[] out = new char[256];"
        },
        {
          "filename": "modules/analysis-common/src/main/java/org/opensearch/analysis/common/StemmerOverrideTokenFilterFactory.java",
          "status": "modified",
          "additions": 26,
          "deletions": 20,
          "patch": "@@ -40,24 +40,31 @@\n import org.opensearch.index.IndexSettings;\n import org.opensearch.index.analysis.AbstractTokenFilterFactory;\n import org.opensearch.index.analysis.Analysis;\n+import org.opensearch.index.analysis.MappingRule;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.List;\n \n public class StemmerOverrideTokenFilterFactory extends AbstractTokenFilterFactory {\n \n+    private static final String MAPPING_SEPARATOR = \"=>\";\n     private final StemmerOverrideMap overrideMap;\n \n     StemmerOverrideTokenFilterFactory(IndexSettings indexSettings, Environment env, String name, Settings settings) throws IOException {\n         super(indexSettings, name, settings);\n \n-        List<String> rules = Analysis.getWordList(env, settings, \"rules\");\n+        List<MappingRule<List<String>, String>> rules = Analysis.parseWordList(env, settings, \"rules\", this::parse);\n         if (rules == null) {\n             throw new IllegalArgumentException(\"stemmer override filter requires either `rules` or `rules_path` to be configured\");\n         }\n \n         StemmerOverrideFilter.Builder builder = new StemmerOverrideFilter.Builder(false);\n-        parseRules(rules, builder, \"=>\");\n+        for (MappingRule<List<String>, String> rule : rules) {\n+            for (String key : rule.getLeft()) {\n+                builder.add(key, rule.getRight());\n+            }\n+        }\n         overrideMap = builder.build();\n \n     }\n@@ -67,27 +74,26 @@ public TokenStream create(TokenStream tokenStream) {\n         return new StemmerOverrideFilter(tokenStream, overrideMap);\n     }\n \n-    static void parseRules(List<String> rules, StemmerOverrideFilter.Builder builder, String mappingSep) {\n-        for (String rule : rules) {\n-            String[] sides = rule.split(mappingSep, -1);\n-            if (sides.length != 2) {\n-                throw new RuntimeException(\"Invalid Keyword override Rule:\" + rule);\n-            }\n+    private MappingRule<List<String>, String> parse(String rule) {\n+        String[] sides = rule.split(MAPPING_SEPARATOR, -1);\n+        if (sides.length != 2) {\n+            throw new RuntimeException(\"Invalid keyword override rule: \" + rule);\n+        }\n \n-            String[] keys = sides[0].split(\",\", -1);\n-            String override = sides[1].trim();\n-            if (override.isEmpty() || override.indexOf(',') != -1) {\n-                throw new RuntimeException(\"Invalid Keyword override Rule:\" + rule);\n-            }\n+        String[] keys = sides[0].split(\",\", -1);\n+        String override = sides[1].trim();\n+        if (override.isEmpty() || override.indexOf(',') != -1) {\n+            throw new RuntimeException(\"Invalid keyword override rule: \" + rule);\n+        }\n \n-            for (String key : keys) {\n-                String trimmedKey = key.trim();\n-                if (trimmedKey.isEmpty()) {\n-                    throw new RuntimeException(\"Invalid Keyword override Rule:\" + rule);\n-                }\n-                builder.add(trimmedKey, override);\n+        List<String> trimmedKeys = new ArrayList<>();\n+        for (String key : keys) {\n+            String trimmedKey = key.trim();\n+            if (trimmedKey.isEmpty()) {\n+                throw new RuntimeException(\"Invalid keyword override rule: \" + rule);\n             }\n+            trimmedKeys.add(trimmedKey);\n         }\n+        return new MappingRule<>(trimmedKeys, override);\n     }\n-\n }"
        },
        {
          "filename": "modules/analysis-common/src/main/java/org/opensearch/analysis/common/SynonymTokenFilterFactory.java",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -32,6 +32,7 @@\n \n package org.opensearch.analysis.common;\n \n+import org.apache.logging.log4j.LogManager;\n import org.apache.lucene.analysis.Analyzer;\n import org.apache.lucene.analysis.TokenStream;\n import org.apache.lucene.analysis.synonym.SynonymFilter;\n@@ -155,14 +156,15 @@ SynonymMap buildSynonyms(Analyzer analyzer, Reader rules) {\n             }\n             return parser.build();\n         } catch (Exception e) {\n-            throw new IllegalArgumentException(\"failed to build synonyms\", e);\n+            LogManager.getLogger(SynonymTokenFilterFactory.class).error(\"Failed to build synonyms: \", e);\n+            throw new IllegalArgumentException(\"Failed to build synonyms\");\n         }\n     }\n \n     Reader getRulesFromSettings(Environment env) {\n         Reader rulesReader;\n         if (settings.getAsList(\"synonyms\", null) != null) {\n-            List<String> rulesList = Analysis.getWordList(env, settings, \"synonyms\");\n+            List<String> rulesList = Analysis.parseWordList(env, settings, \"synonyms\", s -> s);\n             StringBuilder sb = new StringBuilder();\n             for (String line : rulesList) {\n                 sb.append(line).append(System.lineSeparator());"
        },
        {
          "filename": "modules/analysis-common/src/main/java/org/opensearch/analysis/common/WordDelimiterGraphTokenFilterFactory.java",
          "status": "modified",
          "additions": 7,
          "deletions": 1,
          "patch": "@@ -41,6 +41,7 @@\n import org.opensearch.index.IndexSettings;\n import org.opensearch.index.analysis.AbstractTokenFilterFactory;\n import org.opensearch.index.analysis.Analysis;\n+import org.opensearch.index.analysis.MappingRule;\n import org.opensearch.index.analysis.TokenFilterFactory;\n \n import java.util.List;\n@@ -73,7 +74,12 @@ public WordDelimiterGraphTokenFilterFactory(IndexSettings indexSettings, Environ\n         // . => DIGIT\n         // \\u002C => DIGIT\n         // \\u200D => ALPHANUM\n-        List<String> charTypeTableValues = Analysis.getWordList(env, settings, \"type_table\");\n+        List<MappingRule<Character, Byte>> charTypeTableValues = Analysis.parseWordList(\n+            env,\n+            settings,\n+            \"type_table\",\n+            WordDelimiterTokenFilterFactory::parse\n+        );\n         if (charTypeTableValues == null) {\n             this.charTypeTable = WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE;\n         } else {"
        },
        {
          "filename": "modules/analysis-common/src/main/java/org/opensearch/analysis/common/WordDelimiterTokenFilterFactory.java",
          "status": "modified",
          "additions": 20,
          "deletions": 10,
          "patch": "@@ -41,6 +41,7 @@\n import org.opensearch.index.IndexSettings;\n import org.opensearch.index.analysis.AbstractTokenFilterFactory;\n import org.opensearch.index.analysis.Analysis;\n+import org.opensearch.index.analysis.MappingRule;\n import org.opensearch.index.analysis.TokenFilterFactory;\n \n import java.util.Collection;\n@@ -76,7 +77,12 @@ public WordDelimiterTokenFilterFactory(IndexSettings indexSettings, Environment\n         // . => DIGIT\n         // \\u002C => DIGIT\n         // \\u200D => ALPHANUM\n-        List<String> charTypeTableValues = Analysis.getWordList(env, settings, \"type_table\");\n+        List<MappingRule<Character, Byte>> charTypeTableValues = Analysis.parseWordList(\n+            env,\n+            settings,\n+            \"type_table\",\n+            WordDelimiterTokenFilterFactory::parse\n+        );\n         if (charTypeTableValues == null) {\n             this.charTypeTable = WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE;\n         } else {\n@@ -127,19 +133,23 @@ public int getFlag(int flag, Settings settings, String key, boolean defaultValue\n     // source => type\n     private static Pattern typePattern = Pattern.compile(\"(.*)\\\\s*=>\\\\s*(.*)\\\\s*$\");\n \n+    static MappingRule<Character, Byte> parse(String rule) {\n+        Matcher m = typePattern.matcher(rule);\n+        if (!m.find()) throw new RuntimeException(\"Invalid mapping rule: [\" + rule + \"]\");\n+        String lhs = parseString(m.group(1).trim());\n+        Byte rhs = parseType(m.group(2).trim());\n+        if (lhs.length() != 1) throw new RuntimeException(\"Invalid mapping rule: [\" + rule + \"]. Only a single character is allowed.\");\n+        if (rhs == null) throw new RuntimeException(\"Invalid mapping rule: [\" + rule + \"]. Illegal type.\");\n+        return new MappingRule<>(lhs.charAt(0), rhs);\n+    }\n+\n     /**\n      * parses a list of MappingCharFilter style rules into a custom byte[] type table\n      */\n-    static byte[] parseTypes(Collection<String> rules) {\n+    static byte[] parseTypes(Collection<MappingRule<Character, Byte>> rules) {\n         SortedMap<Character, Byte> typeMap = new TreeMap<>();\n-        for (String rule : rules) {\n-            Matcher m = typePattern.matcher(rule);\n-            if (!m.find()) throw new RuntimeException(\"Invalid Mapping Rule : [\" + rule + \"]\");\n-            String lhs = parseString(m.group(1).trim());\n-            Byte rhs = parseType(m.group(2).trim());\n-            if (lhs.length() != 1) throw new RuntimeException(\"Invalid Mapping Rule : [\" + rule + \"]. Only a single character is allowed.\");\n-            if (rhs == null) throw new RuntimeException(\"Invalid Mapping Rule : [\" + rule + \"]. Illegal type.\");\n-            typeMap.put(lhs.charAt(0), rhs);\n+        for (MappingRule<Character, Byte> rule : rules) {\n+            typeMap.put(rule.getLeft(), rule.getRight());\n         }\n \n         // ensure the table is always at least as big as DEFAULT_WORD_DELIM_TABLE for performance"
        },
        {
          "filename": "modules/analysis-common/src/test/java/org/opensearch/analysis/common/BaseWordDelimiterTokenFilterFactoryTestCase.java",
          "status": "modified",
          "additions": 20,
          "deletions": 0,
          "patch": "@@ -195,4 +195,24 @@ public void testStemEnglishPossessive() throws IOException {\n         tokenizer.setReader(new StringReader(source));\n         assertTokenStreamContents(tokenFilter.create(tokenizer), expected);\n     }\n+\n+    private void createTokenFilterFactoryWithTypeTable(String[] rules) throws IOException {\n+        OpenSearchTestCase.TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(\n+            Settings.builder()\n+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())\n+                .put(\"index.analysis.filter.my_word_delimiter.type\", type)\n+                .putList(\"index.analysis.filter.my_word_delimiter.type_table\", rules)\n+                .put(\"index.analysis.filter.my_word_delimiter.catenate_words\", \"true\")\n+                .put(\"index.analysis.filter.my_word_delimiter.generate_word_parts\", \"true\")\n+                .build(),\n+            new CommonAnalysisModulePlugin()\n+        );\n+        analysis.tokenFilter.get(\"my_word_delimiter\");\n+    }\n+\n+    public void testTypeTableParsingError() {\n+        String[] rules = { \"# This is a comment\", \"$ => DIGIT\", \"\\\\u200D => ALPHANUM\", \"abc => ALPHA\" };\n+        RuntimeException ex = expectThrows(RuntimeException.class, () -> createTokenFilterFactoryWithTypeTable(rules));\n+        assertEquals(\"Line [4]: Invalid mapping rule: [abc => ALPHA]. Only a single character is allowed.\", ex.getMessage());\n+    }\n }"
        },
        {
          "filename": "modules/analysis-common/src/test/java/org/opensearch/analysis/common/MappingCharFilterFactoryTests.java",
          "status": "added",
          "additions": 70,
          "deletions": 0,
          "patch": "@@ -0,0 +1,70 @@\n+/*\n+ * SPDX-License-Identifier: Apache-2.0\n+ *\n+ * The OpenSearch Contributors require contributions made to\n+ * this file be licensed under the Apache-2.0 license or a\n+ * compatible open source license.\n+ */\n+\n+package org.opensearch.analysis.common;\n+\n+import org.apache.lucene.analysis.CharFilter;\n+import org.opensearch.common.settings.Settings;\n+import org.opensearch.env.Environment;\n+import org.opensearch.index.analysis.AnalysisTestsHelper;\n+import org.opensearch.index.analysis.CharFilterFactory;\n+import org.opensearch.test.OpenSearchTestCase;\n+\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.util.Arrays;\n+\n+public class MappingCharFilterFactoryTests extends OpenSearchTestCase {\n+    public static CharFilterFactory create(String... rules) throws IOException {\n+        OpenSearchTestCase.TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(\n+            Settings.builder()\n+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())\n+                .put(\"index.analysis.analyzer.my_analyzer.tokenizer\", \"standard\")\n+                .put(\"index.analysis.analyzer.my_analyzer.char_filter\", \"my_mappings_char_filter\")\n+                .put(\"index.analysis.char_filter.my_mappings_char_filter.type\", \"mapping\")\n+                .putList(\"index.analysis.char_filter.my_mappings_char_filter.mappings\", rules)\n+                .build(),\n+            new CommonAnalysisModulePlugin()\n+        );\n+\n+        return analysis.charFilter.get(\"my_mappings_char_filter\");\n+    }\n+\n+    public void testRulesOk() throws IOException {\n+        MappingCharFilterFactory mappingCharFilterFactory = (MappingCharFilterFactory) create(\n+            \"# This is a comment\",\n+            \":) => _happy_\",\n+            \":( => _sad_\"\n+        );\n+        CharFilter inputReader = (CharFilter) mappingCharFilterFactory.create(new StringReader(\"I'm so :)\"));\n+        char[] tempBuff = new char[14];\n+        StringBuilder output = new StringBuilder();\n+        while (true) {\n+            int length = inputReader.read(tempBuff);\n+            if (length == -1) break;\n+            output.append(tempBuff, 0, length);\n+        }\n+        assertEquals(\"I'm so _happy_\", output.toString());\n+    }\n+\n+    public void testRuleError() {\n+        for (String rule : Arrays.asList(\n+            \"\",        // empty\n+            \"a\",       // no arrow\n+            \"a:>b\"     // invalid delimiter\n+        )) {\n+            RuntimeException ex = expectThrows(RuntimeException.class, () -> create(rule));\n+            assertEquals(\"Line [1]: Invalid mapping rule : [\" + rule + \"]\", ex.getMessage());\n+        }\n+    }\n+\n+    public void testRulePartError() {\n+        RuntimeException ex = expectThrows(RuntimeException.class, () -> create(\"# This is a comment\", \":) => _happy_\", \"a:b\"));\n+        assertEquals(\"Line [3]: Invalid mapping rule : [a:b]\", ex.getMessage());\n+    }\n+}"
        },
        {
          "filename": "modules/analysis-common/src/test/java/org/opensearch/analysis/common/StemmerOverrideTokenFilterFactoryTests.java",
          "status": "modified",
          "additions": 7,
          "deletions": 6,
          "patch": "@@ -46,7 +46,6 @@\n import java.io.IOException;\n import java.io.StringReader;\n import java.util.Arrays;\n-import java.util.Locale;\n \n public class StemmerOverrideTokenFilterFactoryTests extends OpenSearchTokenStreamTestCase {\n     @Rule\n@@ -76,11 +75,8 @@ public void testRuleError() {\n             \"=>a\",     // no keys\n             \"a,=>b\"    // empty key\n         )) {\n-            expectThrows(\n-                RuntimeException.class,\n-                String.format(Locale.ROOT, \"Should fail for invalid rule: '%s'\", rule),\n-                () -> create(rule)\n-            );\n+            RuntimeException ex = expectThrows(RuntimeException.class, () -> create(rule));\n+            assertEquals(\"Line [1]: Invalid keyword override rule: \" + rule, ex.getMessage());\n         }\n     }\n \n@@ -90,4 +86,9 @@ public void testRulesOk() throws IOException {\n         tokenizer.setReader(new StringReader(\"a b c\"));\n         assertTokenStreamContents(tokenFilterFactory.create(tokenizer), new String[] { \"1\", \"2\", \"2\" });\n     }\n+\n+    public void testRulePartError() {\n+        RuntimeException ex = expectThrows(RuntimeException.class, () -> create(\"a => 1\", \"b,c => 2\", \"# This is a comment\", \"=>a=>b\"));\n+        assertEquals(\"Line [4]: Invalid keyword override rule: =>a=>b\", ex.getMessage());\n+    }\n }"
        },
        {
          "filename": "modules/analysis-common/src/test/java/org/opensearch/analysis/common/SynonymsAnalysisTests.java",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -116,7 +116,7 @@ public void testSynonymWordDeleteByAnalyzer() throws IOException {\n             fail(\"fail! due to synonym word deleted by analyzer\");\n         } catch (Exception e) {\n             assertThat(e, instanceOf(IllegalArgumentException.class));\n-            assertThat(e.getMessage(), startsWith(\"failed to build synonyms\"));\n+            assertThat(e.getMessage(), startsWith(\"Failed to build synonyms\"));\n         }\n     }\n \n@@ -137,7 +137,7 @@ public void testExpandSynonymWordDeleteByAnalyzer() throws IOException {\n             fail(\"fail! due to synonym word deleted by analyzer\");\n         } catch (Exception e) {\n             assertThat(e, instanceOf(IllegalArgumentException.class));\n-            assertThat(e.getMessage(), startsWith(\"failed to build synonyms\"));\n+            assertThat(e.getMessage(), startsWith(\"Failed to build synonyms\"));\n         }\n     }\n "
        },
        {
          "filename": "plugins/analysis-icu/src/main/java/org/opensearch/index/analysis/IcuCollationTokenFilterFactory.java",
          "status": "modified",
          "additions": 6,
          "deletions": 2,
          "patch": "@@ -37,6 +37,7 @@\n import java.nio.file.Files;\n import java.nio.file.InvalidPathException;\n \n+import org.apache.logging.log4j.LogManager;\n import org.apache.lucene.analysis.TokenStream;\n import org.opensearch.common.io.Streams;\n import org.opensearch.common.settings.Settings;\n@@ -80,9 +81,12 @@ public IcuCollationTokenFilterFactory(IndexSettings indexSettings, Environment e\n                 collator = new RuleBasedCollator(rules);\n             } catch (Exception e) {\n                 if (failureToResolve != null) {\n-                    throw new IllegalArgumentException(\"Failed to resolve collation rules location\", failureToResolve);\n+                    LogManager.getLogger(IcuCollationTokenFilterFactory.class)\n+                        .error(\"Failed to resolve collation rules location\", failureToResolve);\n+                    throw new IllegalArgumentException(\"Failed to resolve collation rules location\");\n                 } else {\n-                    throw new IllegalArgumentException(\"Failed to parse collation rules\", e);\n+                    LogManager.getLogger(IcuCollationTokenFilterFactory.class).error(\"Failed to parse collation rules\", e);\n+                    throw new IllegalArgumentException(\"Failed to parse collation rules\");\n                 }\n             }\n         } else {"
        },
        {
          "filename": "plugins/analysis-kuromoji/src/main/java/org/opensearch/index/analysis/KuromojiPartOfSpeechFilterFactory.java",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -49,7 +49,7 @@ public class KuromojiPartOfSpeechFilterFactory extends AbstractTokenFilterFactor\n \n     public KuromojiPartOfSpeechFilterFactory(IndexSettings indexSettings, Environment env, String name, Settings settings) {\n         super(indexSettings, name, settings);\n-        List<String> wordList = Analysis.getWordList(env, settings, \"stoptags\");\n+        List<String> wordList = Analysis.parseWordList(env, settings, \"stoptags\", s -> s);\n         if (wordList != null) {\n             stopTags.addAll(wordList);\n         } else {"
        },
        {
          "filename": "plugins/analysis-kuromoji/src/main/java/org/opensearch/index/analysis/KuromojiTokenizerFactory.java",
          "status": "modified",
          "additions": 22,
          "deletions": 16,
          "patch": "@@ -32,6 +32,8 @@\n \n package org.opensearch.index.analysis;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n import org.apache.lucene.analysis.Tokenizer;\n import org.apache.lucene.analysis.ja.JapaneseTokenizer;\n import org.apache.lucene.analysis.ja.JapaneseTokenizer.Mode;\n@@ -50,6 +52,7 @@\n \n public class KuromojiTokenizerFactory extends AbstractTokenizerFactory {\n \n+    private static final Logger LOGGER = LogManager.getLogger(KuromojiTokenizerFactory.class);\n     private static final String USER_DICT_PATH_OPTION = \"user_dictionary\";\n     private static final String USER_DICT_RULES_OPTION = \"user_dictionary_rules\";\n     private static final String NBEST_COST = \"nbest_cost\";\n@@ -74,38 +77,41 @@ public KuromojiTokenizerFactory(IndexSettings indexSettings, Environment env, St\n         discardCompoundToken = settings.getAsBoolean(DISCARD_COMPOUND_TOKEN, false);\n     }\n \n+    private static String parse(String rule, Set<String> dup) {\n+        String[] values = CSVUtil.parse(rule);\n+        if (dup.add(values[0]) == false) {\n+            throw new IllegalArgumentException(\"Found duplicate term [\" + values[0] + \"] in user dictionary.\");\n+        }\n+        return rule;\n+    }\n+\n     public static UserDictionary getUserDictionary(Environment env, Settings settings) {\n         if (settings.get(USER_DICT_PATH_OPTION) != null && settings.get(USER_DICT_RULES_OPTION) != null) {\n             throw new IllegalArgumentException(\n                 \"It is not allowed to use [\" + USER_DICT_PATH_OPTION + \"] in conjunction\" + \" with [\" + USER_DICT_RULES_OPTION + \"]\"\n             );\n         }\n         try {\n-            List<String> ruleList = Analysis.getWordList(env, settings, USER_DICT_PATH_OPTION, USER_DICT_RULES_OPTION, false);\n+            Set<String> dup = new HashSet<>();\n+            List<String> ruleList = Analysis.parseWordList(\n+                env,\n+                settings,\n+                USER_DICT_PATH_OPTION,\n+                USER_DICT_RULES_OPTION,\n+                s -> parse(s, dup)\n+            );\n             if (ruleList == null || ruleList.isEmpty()) {\n                 return null;\n             }\n-            Set<String> dup = new HashSet<>();\n-            int lineNum = 0;\n-            for (String line : ruleList) {\n-                // ignore comments\n-                if (line.startsWith(\"#\") == false) {\n-                    String[] values = CSVUtil.parse(line);\n-                    if (dup.add(values[0]) == false) {\n-                        throw new IllegalArgumentException(\n-                            \"Found duplicate term [\" + values[0] + \"] in user dictionary \" + \"at line [\" + lineNum + \"]\"\n-                        );\n-                    }\n-                }\n-                ++lineNum;\n-            }\n+\n             StringBuilder sb = new StringBuilder();\n             for (String line : ruleList) {\n                 sb.append(line).append(System.lineSeparator());\n             }\n             return UserDictionary.open(new StringReader(sb.toString()));\n         } catch (IOException e) {\n-            throw new OpenSearchException(\"failed to load kuromoji user dictionary\", e);\n+            LOGGER.error(\"Failed to load kuromoji user dictionary\", e);\n+            throw new OpenSearchException(\"Failed to load kuromoji user dictionary\");\n         }\n     }\n "
        },
        {
          "filename": "plugins/analysis-kuromoji/src/test/java/org/opensearch/index/analysis/KuromojiAnalysisTests.java",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -390,8 +390,8 @@ public void testKuromojiAnalyzerDuplicateUserDictRule() throws Exception {\n                 \"\u5236\u9650\u30b9\u30d4\u30fc\u30c9,\u5236\u9650\u30b9\u30d4\u30fc\u30c9,\u30bb\u30a4\u30b2\u30f3\u30b9\u30d4\u30fc\u30c9,\u30c6\u30b9\u30c8\u540d\u8a5e\"\n             )\n             .build();\n-        IllegalArgumentException exc = expectThrows(IllegalArgumentException.class, () -> createTestAnalysis(settings));\n-        assertThat(exc.getMessage(), containsString(\"[\u5236\u9650\u30b9\u30d4\u30fc\u30c9] in user dictionary at line [3]\"));\n+        RuntimeException exc = expectThrows(RuntimeException.class, () -> createTestAnalysis(settings));\n+        assertThat(exc.getMessage(), equalTo(\"Line [4]: Found duplicate term [\u5236\u9650\u30b9\u30d4\u30fc\u30c9] in user dictionary.\"));\n     }\n \n     public void testDiscardCompoundToken() throws Exception {"
        },
        {
          "filename": "plugins/analysis-nori/src/main/java/org/opensearch/index/analysis/NoriAnalyzerProvider.java",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -52,7 +52,7 @@ public NoriAnalyzerProvider(IndexSettings indexSettings, Environment env, String\n         super(indexSettings, name, settings);\n         final KoreanTokenizer.DecompoundMode mode = NoriTokenizerFactory.getMode(settings);\n         final UserDictionary userDictionary = NoriTokenizerFactory.getUserDictionary(env, settings);\n-        final List<String> tagList = Analysis.getWordList(env, settings, \"stoptags\");\n+        final List<String> tagList = Analysis.parseWordList(env, settings, \"stoptags\", s -> s);\n         final Set<POS.Tag> stopTags = tagList != null ? resolvePOSList(tagList) : KoreanPartOfSpeechStopFilter.DEFAULT_STOP_TAGS;\n         analyzer = new KoreanAnalyzer(userDictionary, mode, stopTags, false);\n     }"
        },
        {
          "filename": "plugins/analysis-nori/src/main/java/org/opensearch/index/analysis/NoriPartOfSpeechStopFilterFactory.java",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -48,7 +48,7 @@ public class NoriPartOfSpeechStopFilterFactory extends AbstractTokenFilterFactor\n \n     public NoriPartOfSpeechStopFilterFactory(IndexSettings indexSettings, Environment env, String name, Settings settings) {\n         super(indexSettings, name, settings);\n-        List<String> tagList = Analysis.getWordList(env, settings, \"stoptags\");\n+        List<String> tagList = Analysis.parseWordList(env, settings, \"stoptags\", s -> s);\n         this.stopTags = tagList != null ? resolvePOSList(tagList) : KoreanPartOfSpeechStopFilter.DEFAULT_STOP_TAGS;\n     }\n "
        },
        {
          "filename": "plugins/analysis-nori/src/main/java/org/opensearch/index/analysis/NoriTokenizerFactory.java",
          "status": "modified",
          "additions": 6,
          "deletions": 2,
          "patch": "@@ -32,6 +32,8 @@\n \n package org.opensearch.index.analysis;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n import org.apache.lucene.analysis.Tokenizer;\n import org.apache.lucene.analysis.ko.KoreanTokenizer;\n import org.apache.lucene.analysis.ko.dict.UserDictionary;\n@@ -47,6 +49,7 @@\n import java.util.Locale;\n \n public class NoriTokenizerFactory extends AbstractTokenizerFactory {\n+    private static final Logger LOGGER = LogManager.getLogger(NoriTokenizerFactory.class);\n     private static final String USER_DICT_PATH_OPTION = \"user_dictionary\";\n     private static final String USER_DICT_RULES_OPTION = \"user_dictionary_rules\";\n \n@@ -67,7 +70,7 @@ public static UserDictionary getUserDictionary(Environment env, Settings setting\n                 \"It is not allowed to use [\" + USER_DICT_PATH_OPTION + \"] in conjunction\" + \" with [\" + USER_DICT_RULES_OPTION + \"]\"\n             );\n         }\n-        List<String> ruleList = Analysis.getWordList(env, settings, USER_DICT_PATH_OPTION, USER_DICT_RULES_OPTION, true);\n+        List<String> ruleList = Analysis.parseWordList(env, settings, USER_DICT_PATH_OPTION, USER_DICT_RULES_OPTION, s -> s);\n         StringBuilder sb = new StringBuilder();\n         if (ruleList == null || ruleList.isEmpty()) {\n             return null;\n@@ -78,7 +81,8 @@ public static UserDictionary getUserDictionary(Environment env, Settings setting\n         try (Reader rulesReader = new StringReader(sb.toString())) {\n             return UserDictionary.open(rulesReader);\n         } catch (IOException e) {\n-            throw new OpenSearchException(\"failed to load nori user dictionary\", e);\n+            LOGGER.error(\"Failed to load nori user dictionary\", e);\n+            throw new OpenSearchException(\"Failed to load nori user dictionary\");\n         }\n     }\n "
        },
        {
          "filename": "server/src/main/java/org/opensearch/index/analysis/Analysis.java",
          "status": "modified",
          "additions": 75,
          "deletions": 38,
          "patch": "@@ -32,6 +32,8 @@\n \n package org.opensearch.index.analysis;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n import org.apache.lucene.analysis.CharArraySet;\n import org.apache.lucene.analysis.ar.ArabicAnalyzer;\n import org.apache.lucene.analysis.bg.BulgarianAnalyzer;\n@@ -94,6 +96,7 @@\n  * @opensearch.internal\n  */\n public class Analysis {\n+    private static final Logger LOGGER = LogManager.getLogger(Analysis.class);\n \n     public static CharArraySet parseStemExclusion(Settings settings, CharArraySet defaultStemExclusion) {\n         String value = settings.get(\"stem_exclusion\");\n@@ -166,7 +169,7 @@ public static CharArraySet parseWords(\n                 return resolveNamedWords(settings.getAsList(name), namedWords, ignoreCase);\n             }\n         }\n-        List<String> pathLoadedWords = getWordList(env, settings, name);\n+        List<String> pathLoadedWords = parseWordList(env, settings, name, s -> s);\n         if (pathLoadedWords != null) {\n             return resolveNamedWords(pathLoadedWords, namedWords, ignoreCase);\n         }\n@@ -207,23 +210,56 @@ private static CharArraySet resolveNamedWords(Collection<String> words, Map<Stri\n     }\n \n     public static CharArraySet getWordSet(Environment env, Settings settings, String settingsPrefix) {\n-        List<String> wordList = getWordList(env, settings, settingsPrefix);\n+        List<String> wordList = parseWordList(env, settings, settingsPrefix, s -> s);\n         if (wordList == null) {\n             return null;\n         }\n         boolean ignoreCase = settings.getAsBoolean(settingsPrefix + \"_case\", false);\n         return new CharArraySet(wordList, ignoreCase);\n     }\n \n+    public static <T> List<T> parseWordList(Environment env, Settings settings, String settingPrefix, CustomMappingRuleParser<T> parser) {\n+        return parseWordList(env, settings, settingPrefix + \"_path\", settingPrefix, parser);\n+    }\n+\n     /**\n-     * Fetches a list of words from the specified settings file. The list should either be available at the key\n-     * specified by settingsPrefix or in a file specified by settingsPrefix + _path.\n+     * Parses a list of words from the specified settings or from a file, with the given parser.\n      *\n      * @throws IllegalArgumentException\n      *          If the word list cannot be found at either key.\n+     * @throws RuntimeException\n+     *          If there is error parsing the words\n      */\n-    public static List<String> getWordList(Environment env, Settings settings, String settingPrefix) {\n-        return getWordList(env, settings, settingPrefix + \"_path\", settingPrefix, true);\n+    public static <T> List<T> parseWordList(\n+        Environment env,\n+        Settings settings,\n+        String settingPath,\n+        String settingList,\n+        CustomMappingRuleParser<T> parser\n+    ) {\n+        List<String> words = getWordList(env, settings, settingPath, settingList);\n+        if (words == null) {\n+            return null;\n+        }\n+        List<T> rules = new ArrayList<>();\n+        int lineNum = 0;\n+        for (String word : words) {\n+            lineNum++;\n+            if (word.startsWith(\"#\") == false) {\n+                try {\n+                    rules.add(parser.apply(word));\n+                } catch (RuntimeException ex) {\n+                    String wordListPath = settings.get(settingPath, null);\n+                    if (wordListPath == null || isUnderConfig(env, wordListPath)) {\n+                        throw new RuntimeException(\"Line [\" + lineNum + \"]: \" + ex.getMessage());\n+                    } else {\n+                        LOGGER.error(\"Line [{}]: {}\", lineNum, ex);\n+                        throw new RuntimeException(\"Line [\" + lineNum + \"]: \" + \"Invalid rule\");\n+                    }\n+                }\n+            }\n+        }\n+        return rules;\n     }\n \n     /**\n@@ -233,53 +269,41 @@ public static List<String> getWordList(Environment env, Settings settings, Strin\n      * @throws IllegalArgumentException\n      *          If the word list cannot be found at either key.\n      */\n-    public static List<String> getWordList(\n-        Environment env,\n-        Settings settings,\n-        String settingPath,\n-        String settingList,\n-        boolean removeComments\n-    ) {\n+    private static List<String> getWordList(Environment env, Settings settings, String settingPath, String settingList) {\n         String wordListPath = settings.get(settingPath, null);\n \n         if (wordListPath == null) {\n-            List<String> explicitWordList = settings.getAsList(settingList, null);\n-            if (explicitWordList == null) {\n-                return null;\n-            } else {\n-                return explicitWordList;\n-            }\n+            return settings.getAsList(settingList, null);\n         }\n \n-        final Path path = env.configDir().resolve(wordListPath);\n+        final Path path = resolveAnalyzerPath(env, wordListPath);\n \n         try {\n-            return loadWordList(path, removeComments);\n+            return loadWordList(path);\n         } catch (CharacterCodingException ex) {\n             String message = String.format(\n                 Locale.ROOT,\n-                \"Unsupported character encoding detected while reading %s: %s - files must be UTF-8 encoded\",\n-                settingPath,\n-                path.toString()\n+                \"Unsupported character encoding detected while reading %s: files must be UTF-8 encoded\",\n+                settingPath\n             );\n-            throw new IllegalArgumentException(message, ex);\n+            LOGGER.error(\"{}: from file: {}, exception is: {}\", message, path.toString(), ex);\n+            throw new IllegalArgumentException(message);\n         } catch (IOException ioe) {\n-            String message = String.format(Locale.ROOT, \"IOException while reading %s: %s\", settingPath, path.toString());\n-            throw new IllegalArgumentException(message, ioe);\n+            String message = String.format(Locale.ROOT, \"IOException while reading %s: file not readable\", settingPath);\n+            LOGGER.error(\"{}, from file: {}, exception is: {}\", message, path.toString(), ioe);\n+            throw new IllegalArgumentException(message);\n         }\n     }\n \n-    private static List<String> loadWordList(Path path, boolean removeComments) throws IOException {\n+    private static List<String> loadWordList(Path path) throws IOException {\n         final List<String> result = new ArrayList<>();\n         try (BufferedReader br = Files.newBufferedReader(path, StandardCharsets.UTF_8)) {\n             String word;\n             while ((word = br.readLine()) != null) {\n                 if (Strings.hasText(word) == false) {\n                     continue;\n                 }\n-                if (removeComments == false || word.startsWith(\"#\") == false) {\n-                    result.add(word.trim());\n-                }\n+                result.add(word.trim());\n             }\n         }\n         return result;\n@@ -296,21 +320,34 @@ public static Reader getReaderFromFile(Environment env, Settings settings, Strin\n         if (filePath == null) {\n             return null;\n         }\n-        final Path path = env.configDir().resolve(filePath);\n+        final Path path = resolveAnalyzerPath(env, filePath);\n         try {\n             return Files.newBufferedReader(path, StandardCharsets.UTF_8);\n         } catch (CharacterCodingException ex) {\n             String message = String.format(\n                 Locale.ROOT,\n-                \"Unsupported character encoding detected while reading %s_path: %s files must be UTF-8 encoded\",\n-                settingPrefix,\n-                path.toString()\n+                \"Unsupported character encoding detected while reading %s_path: files must be UTF-8 encoded\",\n+                settingPrefix\n             );\n-            throw new IllegalArgumentException(message, ex);\n+            LOGGER.error(\"{}: from file: {}, exception is: {}\", message, path.toString(), ex);\n+            throw new IllegalArgumentException(message);\n         } catch (IOException ioe) {\n-            String message = String.format(Locale.ROOT, \"IOException while reading %s_path: %s\", settingPrefix, path.toString());\n-            throw new IllegalArgumentException(message, ioe);\n+            String message = String.format(Locale.ROOT, \"IOException while reading %s_path: file not readable\", settingPrefix);\n+            LOGGER.error(\"{}, from file: {}, exception is: {}\", message, path.toString(), ioe);\n+            throw new IllegalArgumentException(message);\n         }\n     }\n \n+    public static Path resolveAnalyzerPath(Environment env, String wordListPath) {\n+        return env.configDir().resolve(wordListPath).normalize();\n+    }\n+\n+    private static boolean isUnderConfig(Environment env, String wordListPath) {\n+        try {\n+            final Path path = env.configDir().resolve(wordListPath).normalize();\n+            return path.startsWith(env.configDir().toAbsolutePath());\n+        } catch (Exception ex) {\n+            return false;\n+        }\n+    }\n }"
        },
        {
          "filename": "server/src/main/java/org/opensearch/index/analysis/CustomMappingRuleParser.java",
          "status": "added",
          "additions": 21,
          "deletions": 0,
          "patch": "@@ -0,0 +1,21 @@\n+/*\n+ * SPDX-License-Identifier: Apache-2.0\n+ *\n+ * The OpenSearch Contributors require contributions made to\n+ * this file be licensed under the Apache-2.0 license or a\n+ * compatible open source license.\n+ */\n+\n+package org.opensearch.index.analysis;\n+\n+import java.util.function.Function;\n+\n+/**\n+ * A parser that takes a raw string and returns the parsed data of type T.\n+ *\n+ * @param <T> type of parsed data\n+ */\n+@FunctionalInterface\n+public interface CustomMappingRuleParser<T> extends Function<String, T> {\n+\n+}"
        },
        {
          "filename": "server/src/main/java/org/opensearch/index/analysis/MappingRule.java",
          "status": "added",
          "additions": 30,
          "deletions": 0,
          "patch": "@@ -0,0 +1,30 @@\n+/*\n+ * SPDX-License-Identifier: Apache-2.0\n+ *\n+ * The OpenSearch Contributors require contributions made to\n+ * this file be licensed under the Apache-2.0 license or a\n+ * compatible open source license.\n+ */\n+\n+package org.opensearch.index.analysis;\n+\n+/**\n+ * Represents a mapping between two objects.\n+ */\n+public class MappingRule<L, R> {\n+    private final L left;\n+    private final R right;\n+\n+    public MappingRule(L left, R right) {\n+        this.left = left;\n+        this.right = right;\n+    }\n+\n+    public L getLeft() {\n+        return left;\n+    }\n+\n+    public R getRight() {\n+        return right;\n+    }\n+}"
        },
        {
          "filename": "server/src/main/java/org/opensearch/indices/analysis/HunspellService.java",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -123,7 +123,8 @@ public HunspellService(final Settings settings, final Environment env, final Map\n             try {\n                 return loadDictionary(locale, settings, env);\n             } catch (Exception e) {\n-                throw new IllegalStateException(\"failed to load hunspell dictionary for locale: \" + locale, e);\n+                logger.error(\"Failed to load hunspell dictionary for locale: \" + locale, e);\n+                throw new IllegalStateException(\"Failed to load hunspell dictionary for locale: \" + locale);\n             }\n         };\n         if (!HUNSPELL_LAZY_LOAD.get(settings)) {"
        },
        {
          "filename": "server/src/test/java/org/opensearch/index/analysis/AnalysisTests.java",
          "status": "modified",
          "additions": 41,
          "deletions": 22,
          "patch": "@@ -39,14 +39,10 @@\n import org.opensearch.test.OpenSearchTestCase;\n \n import java.io.BufferedWriter;\n-import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.OutputStream;\n-import java.nio.charset.CharacterCodingException;\n-import java.nio.charset.MalformedInputException;\n import java.nio.charset.StandardCharsets;\n import java.nio.file.Files;\n-import java.nio.file.NoSuchFileException;\n import java.nio.file.Path;\n import java.util.Arrays;\n import java.util.List;\n@@ -79,13 +75,10 @@ public void testParseNonExistingFile() {\n         Environment env = TestEnvironment.newEnvironment(nodeSettings);\n         IllegalArgumentException ex = expectThrows(\n             IllegalArgumentException.class,\n-            () -> Analysis.getWordList(env, nodeSettings, \"foo.bar\")\n-        );\n-        assertEquals(\"IOException while reading foo.bar_path: \" + tempDir.resolve(\"foo.dict\").toString(), ex.getMessage());\n-        assertTrue(\n-            ex.getCause().toString(),\n-            ex.getCause() instanceof FileNotFoundException || ex.getCause() instanceof NoSuchFileException\n+            () -> Analysis.parseWordList(env, nodeSettings, \"foo.bar\", s -> s)\n         );\n+        assertEquals(\"IOException while reading foo.bar_path: file not readable\", ex.getMessage());\n+        assertNull(ex.getCause());\n     }\n \n     public void testParseFalseEncodedFile() throws IOException {\n@@ -99,18 +92,10 @@ public void testParseFalseEncodedFile() throws IOException {\n         Environment env = TestEnvironment.newEnvironment(nodeSettings);\n         IllegalArgumentException ex = expectThrows(\n             IllegalArgumentException.class,\n-            () -> Analysis.getWordList(env, nodeSettings, \"foo.bar\")\n-        );\n-        assertEquals(\n-            \"Unsupported character encoding detected while reading foo.bar_path: \"\n-                + tempDir.resolve(\"foo.dict\").toString()\n-                + \" - files must be UTF-8 encoded\",\n-            ex.getMessage()\n-        );\n-        assertTrue(\n-            ex.getCause().toString(),\n-            ex.getCause() instanceof MalformedInputException || ex.getCause() instanceof CharacterCodingException\n+            () -> Analysis.parseWordList(env, nodeSettings, \"foo.bar\", s -> s)\n         );\n+        assertEquals(\"Unsupported character encoding detected while reading foo.bar_path: files must be UTF-8 encoded\", ex.getMessage());\n+        assertNull(ex.getCause());\n     }\n \n     public void testParseWordList() throws IOException {\n@@ -124,8 +109,42 @@ public void testParseWordList() throws IOException {\n             writer.write('\\n');\n         }\n         Environment env = TestEnvironment.newEnvironment(nodeSettings);\n-        List<String> wordList = Analysis.getWordList(env, nodeSettings, \"foo.bar\");\n+        List<String> wordList = Analysis.parseWordList(env, nodeSettings, \"foo.bar\", s -> s);\n         assertEquals(Arrays.asList(\"hello\", \"world\"), wordList);\n+    }\n \n+    public void testParseWordListError() throws IOException {\n+        Path home = createTempDir();\n+        Path config = home.resolve(\"config\");\n+        Files.createDirectory(config);\n+        Path dict = config.resolve(\"foo.dict\");\n+        Settings nodeSettings = Settings.builder().put(\"foo.bar_path\", dict).put(Environment.PATH_HOME_SETTING.getKey(), home).build();\n+        try (BufferedWriter writer = Files.newBufferedWriter(dict, StandardCharsets.UTF_8)) {\n+            writer.write(\"abcd\");\n+            writer.write('\\n');\n+        }\n+        Environment env = TestEnvironment.newEnvironment(nodeSettings);\n+        RuntimeException ex = expectThrows(\n+            RuntimeException.class,\n+            () -> Analysis.parseWordList(\n+                env,\n+                nodeSettings,\n+                \"foo.bar\",\n+                s -> { throw new RuntimeException(\"Error while parsing rule = \" + s); }\n+            )\n+        );\n+        assertEquals(\"Line [1]: Error while parsing rule = abcd\", ex.getMessage());\n+    }\n+\n+    public void testParseWordListOutsideConfigDirError() {\n+        Path home = createTempDir();\n+        Path dict = home.resolve(\"/etc/os-release\");\n+        Settings nodeSettings = Settings.builder().put(\"foo.bar_path\", dict).put(Environment.PATH_HOME_SETTING.getKey(), home).build();\n+        Environment env = TestEnvironment.newEnvironment(nodeSettings);\n+        RuntimeException ex = expectThrows(\n+            RuntimeException.class,\n+            () -> Analysis.parseWordList(env, nodeSettings, \"foo.bar\", s -> { throw new RuntimeException(\"Error while parsing\"); })\n+        );\n+        assertEquals(\"Line [1]: Invalid rule\", ex.getMessage());\n     }\n }"
        },
        {
          "filename": "server/src/test/java/org/opensearch/indices/analysis/AnalysisModuleTests.java",
          "status": "modified",
          "additions": 7,
          "deletions": 4,
          "patch": "@@ -167,11 +167,12 @@ private void testSimpleConfiguration(Settings settings) throws IOException {\n     }\n \n     public void testWordListPath() throws Exception {\n-        Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();\n+        Path home = createTempDir();\n+        Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), home.toString()).build();\n         Environment env = TestEnvironment.newEnvironment(settings);\n         String[] words = new String[] { \"donau\", \"dampf\", \"schiff\", \"spargel\", \"creme\", \"suppe\" };\n \n-        Path wordListFile = generateWordList(words);\n+        Path wordListFile = generateWordList(home, words);\n         settings = Settings.builder()\n             .loadFromSource(\"index: \\n  word_list_path: \" + wordListFile.toAbsolutePath(), XContentType.YAML)\n             .build();\n@@ -182,8 +183,10 @@ public void testWordListPath() throws Exception {\n         Files.delete(wordListFile);\n     }\n \n-    private Path generateWordList(String[] words) throws Exception {\n-        Path wordListFile = createTempDir().resolve(\"wordlist.txt\");\n+    private Path generateWordList(Path home, String[] words) throws Exception {\n+        Path config = home.resolve(\"config\");\n+        Files.createDirectory(config);\n+        Path wordListFile = config.resolve(\"wordlist.txt\");\n         try (BufferedWriter writer = Files.newBufferedWriter(wordListFile, StandardCharsets.UTF_8)) {\n             for (String word : words) {\n                 writer.write(word);"
        },
        {
          "filename": "server/src/test/java/org/opensearch/indices/analyze/HunspellServiceTests.java",
          "status": "modified",
          "additions": 5,
          "deletions": 7,
          "patch": "@@ -42,8 +42,6 @@\n import static java.util.Collections.emptyMap;\n import static org.opensearch.indices.analysis.HunspellService.HUNSPELL_IGNORE_CASE;\n import static org.opensearch.indices.analysis.HunspellService.HUNSPELL_LAZY_LOAD;\n-import static org.hamcrest.Matchers.containsString;\n-import static org.hamcrest.Matchers.hasToString;\n import static org.hamcrest.Matchers.notNullValue;\n \n public class HunspellServiceTests extends OpenSearchTestCase {\n@@ -91,11 +89,11 @@ public void testDicWithNoAff() throws Exception {\n             final Environment environment = new Environment(settings, getDataPath(\"/indices/analyze/no_aff_conf_dir\"));\n             new HunspellService(settings, environment, emptyMap()).getDictionary(\"en_US\");\n         });\n-        assertEquals(\"failed to load hunspell dictionary for locale: en_US\", e.getMessage());\n-        assertThat(e.getCause(), hasToString(containsString(\"Missing affix file\")));\n+        assertEquals(\"Failed to load hunspell dictionary for locale: en_US\", e.getMessage());\n+        assertNull(e.getCause());\n     }\n \n-    public void testDicWithTwoAffs() throws Exception {\n+    public void testDicWithTwoAffs() {\n         Settings settings = Settings.builder()\n             .put(HUNSPELL_LAZY_LOAD.getKey(), randomBoolean())\n             .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())\n@@ -105,7 +103,7 @@ public void testDicWithTwoAffs() throws Exception {\n             final Environment environment = new Environment(settings, getDataPath(\"/indices/analyze/two_aff_conf_dir\"));\n             new HunspellService(settings, environment, emptyMap()).getDictionary(\"en_US\");\n         });\n-        assertEquals(\"failed to load hunspell dictionary for locale: en_US\", e.getMessage());\n-        assertThat(e.getCause(), hasToString(containsString(\"Too many affix files\")));\n+        assertEquals(\"Failed to load hunspell dictionary for locale: en_US\", e.getMessage());\n+        assertNull(e.getCause());\n     }\n }"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 8,
        "unique_directories": 12,
        "max_directory_depth": 9
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "a609e634a348b76386fb11936bbe8c4b38ea72d0",
            "date": "2025-01-13T21:12:13Z",
            "author_login": "rursprung"
          },
          {
            "sha": "f98f4267c35ec2d5567f189944dda5671425e46e",
            "date": "2025-01-13T16:09:34Z",
            "author_login": "dependabot[bot]"
          },
          {
            "sha": "26465e87ed60acc8fd7c65e3559b14e8ccd59fbf",
            "date": "2025-01-13T15:41:36Z",
            "author_login": "dependabot[bot]"
          },
          {
            "sha": "8d5e1a3972ac34d769fff6618d26f9f9e36b06b7",
            "date": "2025-01-11T06:16:34Z",
            "author_login": "brusic"
          },
          {
            "sha": "fccd6c54c14dabc46483f1b6ec3f3b02d08edfdd",
            "date": "2025-01-10T22:45:52Z",
            "author_login": "kkewwei"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 4.3,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:L/I:N/A:N",
    "cwe_id": "CWE-200",
    "description": "OpenSearch is a community-driven, open source fork of Elasticsearch and Kibana. OpenSearch allows users to specify a local file when defining text analyzers to process data for text analysis. An issue in the implementation of this feature allows certain specially crafted queries to return a response containing the first line of text from arbitrary files. The list of potentially impacted files is limited to text files with read permissions allowed in the Java Security Manager policy configuration. OpenSearch version 1.3.7 and 2.4.0 contain a fix for this issue. Users are advised to upgrade. There are no known workarounds for this issue.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2022-11-16T00:15:11.283",
    "last_modified": "2024-11-21T07:24:03.857",
    "fix_date": "2022-11-08T00:52:52Z"
  },
  "references": [
    {
      "url": "https://github.com/opensearch-project/OpenSearch/commit/6d20423f5920745463b1abc5f1daf6a786c41aa0",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/opensearch-project/OpenSearch/security/advisories/GHSA-w3rx-m34v-wrqx",
      "source": "security-advisories@github.com",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/opensearch-project/OpenSearch/commit/6d20423f5920745463b1abc5f1daf6a786c41aa0",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/opensearch-project/OpenSearch/security/advisories/GHSA-w3rx-m34v-wrqx",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:04:01.022072",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "OpenSearch",
    "owner": "opensearch-project",
    "created_at": "2021-01-29T22:10:00Z",
    "updated_at": "2025-01-14T13:31:56Z",
    "pushed_at": "2025-01-14T13:01:39Z",
    "size": 572091,
    "stars": 10055,
    "forks": 1892,
    "open_issues": 1997,
    "watchers": 10055,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [
      "1.x",
      "1.0",
      "1.1",
      "1.2",
      "1.3",
      "2.x",
      "2.0",
      "2.1",
      "2.2",
      "2.3",
      "2.4",
      "2.5-transfer",
      "2.5",
      "2.6",
      "2.7",
      "2.8",
      "2.9",
      "2.10",
      "2.11",
      "2.12",
      "2.13",
      "2.14",
      "2.15",
      "2.16",
      "2.17",
      "2.18",
      "14435-refactor-range-agg-optimization",
      "Bukhtawar-patch-1",
      "Issue-6778"
    ],
    "languages": {
      "Java": 87234001,
      "Groovy": 267277,
      "Shell": 38711,
      "Batchfile": 18349,
      "ANTLR": 10108,
      "Dockerfile": 6111,
      "Python": 5008,
      "Emacs Lisp": 3341,
      "Standard ML": 3134,
      "HTML": 2177,
      "FreeMarker": 45
    },
    "commit_activity": {
      "total_commits_last_year": 1494,
      "avg_commits_per_week": 28.73076923076923,
      "days_active_last_year": 277
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-14T13:35:09.572020"
  }
}