{
  "cve_id": "CVE-2023-25658",
  "github_data": {
    "repository": "tensorflow/tensorflow",
    "fix_commit": "ff459137c2716a2a60f7d441b855fcb466d778cb",
    "related_commits": [
      "ff459137c2716a2a60f7d441b855fcb466d778cb",
      "ff459137c2716a2a60f7d441b855fcb466d778cb"
    ],
    "patch_url": "https://github.com/tensorflow/tensorflow/commit/ff459137c2716a2a60f7d441b855fcb466d778cb.patch",
    "fix_commit_details": {
      "sha": "ff459137c2716a2a60f7d441b855fcb466d778cb",
      "commit_date": "2023-01-25T23:59:30Z",
      "author": {
        "login": "tensorflower-gardener",
        "type": "User",
        "stats": {
          "total_commits": 51278,
          "average_weekly_commits": 106.82916666666667,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 450
        }
      },
      "commit_message": {
        "title": "Merged commit includes the following changes:",
        "length": 4069,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 956,
        "additions": 472,
        "deletions": 484
      },
      "files": [
        {
          "filename": ".bazelrc",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -242,7 +242,6 @@ build:mkl_aarch64 -c opt\n # Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).\n # with Eigen threadpool support\n build:mkl_aarch64_threadpool --define=build_with_mkl_aarch64=true\n-build:mkl_aarch64_threadpool --define=build_with_acl=true\n build:mkl_aarch64_threadpool -c opt\n \n # This config refers to building CUDA op kernels with nvcc."
        },
        {
          "filename": "tensorflow/compiler/jit/BUILD",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -411,6 +411,7 @@ cc_library(\n         \":internal\",\n         # We reuse VariableInfo in TFRT's implementation of TpuExecuteOp.\n         \"//learning/brain/tfrt/tf_tpu:__pkg__\",\n+        \"//learning/brain/tfrt/tpu_plugin:__pkg__\",\n         \"//learning/brain/tfrt/tpu_common:__pkg__\",\n         \"//tensorflow/core/common_runtime/next_pluggable_device:__pkg__\",\n     ],"
        },
        {
          "filename": "tensorflow/compiler/xla/backends/interpreter/BUILD",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -138,7 +138,7 @@ cc_library(\n         \":executor\",\n         \":platform_id\",\n         \"//tensorflow/compiler/xla/stream_executor\",\n-        \"//tensorflow/compiler/xla/stream_executor/lib\",\n+        \"//tensorflow/compiler/xla/stream_executor/platform\",\n         \"//tensorflow/tsl/platform:status\",\n         \"@com_google_absl//absl/strings:str_format\",\n     ],"
        },
        {
          "filename": "tensorflow/compiler/xla/backends/interpreter/platform.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -21,9 +21,9 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/backends/interpreter/executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_options.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/multi_platform_manager.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n \n namespace stream_executor {"
        },
        {
          "filename": "tensorflow/compiler/xla/hlo/evaluator/hlo_evaluator.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -222,7 +222,7 @@ Status MakeEvalErrorDueToParamOrInfeed(const HloInstruction& eval_instruction) {\n   absl::little_endian::Store32(\n       const_cast<char*>(error_payload.data()),\n       static_cast<uint32_t>(EvalErrorDetail::kDynamicValueDependence));\n-  error.SetPayload(kEvalErrorDetailUrl, error_payload);\n+  error.SetPayload(kEvalErrorDetailUrl, absl::Cord(error_payload));\n   return error;\n }\n "
        },
        {
          "filename": "tensorflow/compiler/xla/mlir/tools/mlir_replay/BUILD",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -9,6 +9,7 @@ xla_cc_binary(\n         \"//tensorflow/compiler/xla:debug_options_flags\",\n         \"//tensorflow/compiler/xla/mlir/runtime/ir:rt\",\n         \"//tensorflow/compiler/xla/mlir/tools/mlir_replay/public:compiler_trace_proto_cc\",\n+        \"//tensorflow/compiler/xla/mlir/tools/mlir_replay/public:compiler_trace_proto_cc_impl\",\n         \"//tensorflow/compiler/xla/mlir/tools/mlir_replay/public:execution_trace_proto_cc\",\n         \"//tensorflow/compiler/xla/mlir_hlo:gml_st\",\n         \"//tensorflow/compiler/xla/mlir_hlo:hlo_dialect_registration\","
        },
        {
          "filename": "tensorflow/compiler/xla/service/hlo_graph_dumper.cc",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -1251,6 +1251,10 @@ ExtractGemmBackendConfigProps(const gpu::GemmBackendConfig& config,\n   if (config.algorithm_case() == gpu::GemmBackendConfig::kSelectedAlgorithm) {\n     props.emplace_back(\"algorithm\", StrCat(config.selected_algorithm()));\n   }\n+  if (config.epilogue() != gpu::GemmBackendConfig::DEFAULT) {\n+    props.emplace_back(\n+        \"epilogue\", gpu::GemmBackendConfig::Epilogue_Name(config.epilogue()));\n+  }\n   return props;\n }\n "
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/BUILD",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -239,6 +239,7 @@ cc_library(\n         \":stream_executor_headers\",\n         \"//tensorflow/compiler/xla/stream_executor/lib\",\n         \"//tensorflow/compiler/xla/stream_executor/platform\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:status\",\n         \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings\",\n@@ -450,6 +451,7 @@ tsl_gpu_library(\n         \"//tensorflow/compiler/xla/stream_executor/lib\",\n         \"//tensorflow/compiler/xla/stream_executor/platform\",\n         \"//tensorflow/tsl/platform:env\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:logging\",\n         \"//tensorflow/tsl/platform:stacktrace\",\n         \"//tensorflow/tsl/platform:status\",\n@@ -661,7 +663,7 @@ cc_library(\n         \":platform\",\n         \":plugin\",\n         \":stream_executor_headers\",\n-        \"//tensorflow/compiler/xla/stream_executor/lib\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:status\",\n         \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\","
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -63,7 +63,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_types.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc",
          "status": "modified",
          "additions": 58,
          "deletions": 59,
          "patch": "@@ -36,8 +36,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/dnn.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n #include \"tensorflow/compiler/xla/stream_executor/scratch_allocator.h\"\n@@ -85,7 +84,7 @@ static_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n       std::ostringstream oss;                                           \\\n       oss << CudnnStatusToString(_status) << \"\\nin \" << __FILE__ << \"(\" \\\n           << __LINE__ << \"): '\" << #expr << \"'\";                        \\\n-      return tsl::Status(port::error::UNKNOWN, oss.str());              \\\n+      return tsl::Status(tsl::error::UNKNOWN, oss.str());               \\\n     }                                                                   \\\n   } while (false)\n \n@@ -96,7 +95,7 @@ static_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n       std::ostringstream oss;                                           \\\n       oss << CudnnStatusToString(_status) << \"\\nin \" << __FILE__ << \"(\" \\\n           << __LINE__ << \"): '\" << #expr << \"' \" << (expr).get_error(); \\\n-      return tsl::Status(port::error::UNKNOWN, oss.str());              \\\n+      return tsl::Status(tsl::error::UNKNOWN, oss.str());               \\\n     }                                                                   \\\n   } while (false)\n \n@@ -417,7 +416,7 @@ tsl::Status CudnnSupport::Init() {\n           \"configuration.\");\n       LOG(ERROR) << error;\n       cudnnDestroy(cudnn_handle);\n-      return tsl::Status(port::error::INTERNAL, error);\n+      return tsl::Status(tsl::error::INTERNAL, error);\n     }\n \n     cudnn_.reset(new CudnnAccess(cudnn_handle));\n@@ -441,7 +440,7 @@ tsl::Status CudnnSupport::Init() {\n     }\n   }\n \n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      absl::StrCat(\"cudnn library could not create a handle: \",\n                                   CudnnStatusToString(status)));\n }\n@@ -1299,7 +1298,7 @@ class CudnnRnnDescriptor : public dnn::RnnDescriptor {\n             ? algorithm_config.algorithm()->tensor_ops_enabled()\n             : allow_tensor_ops;\n     if (use_tensor_ops && !allow_tensor_ops) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT,\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                          \"Algo requests disallowed tensor op evaluation.\");\n     }\n \n@@ -1658,7 +1657,7 @@ class CudnnRnnSequenceTensorDescriptor\n       GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n       cudnnDataType_t data_type) {\n     if (max_seq_length <= 0) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n     }\n     int dims[] = {batch_size, data_size, 1};\n     int strides[] = {dims[1] * dims[2], dims[2], 1};\n@@ -1677,7 +1676,7 @@ class CudnnRnnSequenceTensorDescriptor\n       const absl::Span<const int>& seq_lengths, bool time_major,\n       cudnnDataType_t data_type) {\n     if (max_seq_length <= 0) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n     }\n     int dims[] = {batch_size, data_size, 1};\n     int strides[] = {dims[1] * dims[2], dims[2], 1};\n@@ -1804,30 +1803,30 @@ tsl::StatusOr<RnnModelDims> ExtractAndCheckRnnForward(\n             model_dims.num_layers * model_dims.dir_count &&\n         input_h_desc.batch_size() == model_dims.batch_size &&\n         input_h_desc.data_size() == model_dims.hidden_size)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n   }\n   // The LSTM projection will be used if input_h_desc.data_size() <\n   // input_c_desc.data_size()\n   if (!(input_h_desc.num_layers() == input_c_desc.num_layers() &&\n         input_h_desc.batch_size() == input_c_desc.batch_size() &&\n         input_h_desc.data_size() <= input_c_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n   }\n   if (!(output_desc.max_seq_length() == model_dims.max_seq_length &&\n         output_desc.batch_size() == model_dims.batch_size &&\n         output_desc.data_size() ==\n             model_dims.hidden_size * model_dims.dir_count)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output shape\");\n   }\n   if (!(input_h_desc.num_layers() == output_h_desc.num_layers() &&\n         input_h_desc.batch_size() == output_h_desc.batch_size() &&\n         input_h_desc.data_size() == output_h_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output_h shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output_h shape\");\n   }\n   if (!(input_h_desc.num_layers() == output_c_desc.num_layers() &&\n         input_h_desc.batch_size() == output_c_desc.batch_size() &&\n         input_h_desc.data_size() <= output_c_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output_c shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output_c shape\");\n   }\n \n   return model_dims;\n@@ -1849,7 +1848,7 @@ tsl::Status CheckRNNParameterSize(\n #endif\n   if (static_cast<int64_t>(params_size_in_bytes) !=\n       rnn_desc.ParamsSizeInBytes()) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"Mismatching RNN parameter size\");\n   }\n   return ::tsl::OkStatus();\n@@ -1997,7 +1996,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -2020,7 +2019,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n       output_profile_result->set_algorithm(algo_desc);\n@@ -2058,7 +2057,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n     // possible. It is still possible for other threads to issue workload on\n     // to this stream. So it could take multiple profiling measurements.\n     if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n     }\n   }\n \n@@ -2130,7 +2129,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n \n   if (is_profiling) {\n     if (!timer->Stop(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n     }\n     auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n     output_profile_result->set_algorithm(algo_desc);\n@@ -2204,7 +2203,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -2253,7 +2252,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n       output_profile_result->set_algorithm(algo_desc);\n@@ -2275,7 +2274,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n     // possible. It is still possible for other threads to issue workload on\n     // to this stream. So it could take multiple profiling measurements.\n     if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n     }\n   }\n \n@@ -2362,7 +2361,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n \n   if (is_profiling) {\n     if (!timer->Stop(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n     }\n     auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n     output_profile_result->set_algorithm(algo_desc);\n@@ -2404,7 +2403,7 @@ tsl::Status CudnnSupport::DoCtcLossImpl(\n       /*workspace=*/scratch_memory.opaque(),\n       /*workSpaceSizeInBytes=*/scratch_memory.size()));\n #else\n-  return tsl::Status(port::error::INVALID_ARGUMENT,\n+  return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                      \"No supported cudnnCTCLoss when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n #endif\n@@ -2786,7 +2785,7 @@ tsl::StatusOr<cudnnConvolutionFwdAlgo_t> GetCudnnConvolutionForwardAlgo(\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionForwardAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2828,7 +2827,7 @@ GetCudnnConvolutionBackwardDataAlgo(const CudnnHandle& cudnn,\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardDataAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2870,7 +2869,7 @@ GetCudnnConvolutionBackwardFilterAlgo(const CudnnHandle& cudnn,\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardFilterAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2895,7 +2894,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -2917,7 +2916,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -2927,7 +2926,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -2944,7 +2943,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -2967,7 +2966,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionBackwardDataWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -2977,7 +2976,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -2994,7 +2993,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -3017,7 +3016,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionBackwardFilterWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -3027,7 +3026,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -3040,7 +3039,7 @@ tsl::StatusOr<bool> UseTensorOps(Stream* stream, dnn::DataType type,\n   if (desc.has_value()) {\n     use_tensor_ops = desc->tensor_ops_enabled();\n     if (use_tensor_ops && !IsTensorMathEnabled(stream, type)) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT,\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                          \"Algo requests disabled tensor op evaluation.\");\n     }\n   } else {\n@@ -3162,7 +3161,7 @@ tsl::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardDataAlgorithm(\n   // no_scratch algorithm.\n   if (!algo_desc.has_value()) {\n     return tsl::Status(\n-        port::error::INVALID_ARGUMENT,\n+        tsl::error::INVALID_ARGUMENT,\n         \"The primary convolution algorithm failed memory allocation, \"\n         \"while a secondary algorithm is not provided.\");\n   }\n@@ -3224,7 +3223,7 @@ tsl::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardFilterAlgorithm(\n   // no_scratch algorithm.\n   if (!algo_desc.has_value()) {\n     return tsl::Status(\n-        port::error::INVALID_ARGUMENT,\n+        tsl::error::INVALID_ARGUMENT,\n         absl::StrCat(\n             \"The primary convolution algorithm failed memory allocation, \"\n             \"while a secondary algorithm is not provided. Actual error: \",\n@@ -4254,7 +4253,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -4264,7 +4263,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n           ToCudnnDataType(input_type_) == CUDNN_DATA_INT8 &&\n           ToCudnnDataType(output_type_) == CUDNN_DATA_FLOAT) {\n         return tsl::Status(\n-            port::error::FAILED_PRECONDITION,\n+            tsl::error::FAILED_PRECONDITION,\n             \"This configuration potentially produces incorrect results.\");\n       }\n #else\n@@ -4336,7 +4335,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       profile_result->set_algorithm(algo);\n       profile_result->set_elapsed_time_in_ms(timer->GetElapsedMilliseconds());\n@@ -4631,7 +4630,7 @@ class CudnnExecutionPlanRunner<void(Args...)>\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -4641,7 +4640,7 @@ class CudnnExecutionPlanRunner<void(Args...)>\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       TF_ASSIGN_OR_RETURN(auto desc, ToAlgorithmDesc());\n       profile_result->set_algorithm(desc);\n@@ -4868,7 +4867,7 @@ tsl::Status CudnnSupport::GetConvolveRunners(\n     }\n     if (!got_algos) {\n       return tsl::Status(\n-          port::error::UNKNOWN,\n+          tsl::error::UNKNOWN,\n           absl::StrFormat(\"Listing algorithms failed for kind %d\", kind));\n     }\n \n@@ -5037,7 +5036,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n     auto side_input_data_ptr = (side_input_scale_ == 0)\n@@ -5065,7 +5064,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n             << \"\\noutput_data.opaque() = \" << output_data.opaque();\n \n     if (IsTensorMathOpSet(conv_) != tensor_ops_enabled_) {\n-      return tsl::Status(port::error::FAILED_PRECONDITION,\n+      return tsl::Status(tsl::error::FAILED_PRECONDITION,\n                          \"Tensor op math type in dnn::AlgorithmDesc does not \"\n                          \"match that of the CudnnConvolutionDescriptor\");\n     }\n@@ -5095,7 +5094,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n \n     if (profile_result) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       profile_result->set_algorithm(algo);\n       profile_result->set_elapsed_time_in_ms(timer->GetElapsedMilliseconds());\n@@ -5308,7 +5307,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n       activation_mode != dnn::ActivationMode::kElu &&\n       activation_mode != dnn::ActivationMode::kLeakyRelu &&\n       activation_mode != dnn::ActivationMode::kNone) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"CuDNN fusion only supports activations of \"\n                        \"{Relu, Relu6, Elu, <None>}.\");\n   }\n@@ -5319,7 +5318,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n     auto cuda_compute_capability = stream->GetCudaComputeCapability();\n     if (!GetConvolveAlgorithms(cuda_compute_capability, input_type,\n                                &algorithms)) {\n-      return tsl::Status(port::error::UNKNOWN,\n+      return tsl::Status(tsl::error::UNKNOWN,\n                          \"Listing fused convolve algorithms failed.\");\n     }\n \n@@ -5354,7 +5353,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n       leakyrelu_alpha, input_descriptor, filter_descriptor, bias_descriptor,\n       output_descriptor, convolution_descriptor, activation_mode, cudnn);\n   if (!op_graph_status.status().ok()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        absl::StrCat(\"Cudnn graph failed to build: \",\n                                     op_graph_status.status().ToString()));\n   }\n@@ -5391,7 +5390,7 @@ tsl::Status CudnnSupport::GetFusedMatmulRunners(\n       input_type, bias_type, output_type, trans_a, trans_b, m, n, k, lda, ldb,\n       ldc, activation_mode, cudnn);\n   if (!op_graph_status.status().ok()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        absl::StrCat(\"Cudnn graph failed to build: \",\n                                     op_graph_status.status().ToString()));\n   }\n@@ -5685,7 +5684,7 @@ tsl::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n     if (activation_mode != dnn::ActivationMode::kNone ||\n         !side_input.is_null()) {\n       return tsl::Status(\n-          port::error::INTERNAL,\n+          tsl::error::INTERNAL,\n           absl::StrCat(\n               \"Side input and activation are not supported by cuDNN version: \",\n               CUDNN_VERSION));\n@@ -5968,7 +5967,7 @@ tsl::Status CudnnSupport::DoFusedConvolve(\n \n   if (activation_mode != dnn::ActivationMode::kRelu &&\n       activation_mode != dnn::ActivationMode::kNone) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"cudnnConvolutionBiasActivationForward() only supports \"\n                        \"Relu or None activation.\");\n   }\n@@ -6070,7 +6069,7 @@ tsl::Status CudnnSupport::DoPrepareForCtcLoss(\n   }\n   *ctc_loss_algo_id = algo;\n #else\n-  return tsl::Status(port::error::INVALID_ARGUMENT,\n+  return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                      \"No supported cudnnGetCTCLossWorkspaceSize when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n #endif\n@@ -6100,7 +6099,7 @@ tsl::Status CudnnSupport::DoCtcLoss(\n     int ctc_loss_algo_id) {\n   // Current cuDNN CTC Loss only supports the float datatype\n   if (CUDNN_VERSION < 7603 || element_type != dnn::DataType::kFloat) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"CudnnCtcLossDescriptor is supported only when the \"\n                        \"CUDNN_VERSION >= 7.6.3 and DataType is float\");\n   }\n@@ -6382,7 +6381,7 @@ tsl::StatusOr<std::vector<PoolingSplitsSpec>> GetTensorSplits(\n \n   if (max_batches_per_split == 0) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrCat(\n             \"Tensor has too many elements for int32 indexing: batches=\",\n             num_batches, \" elements_per_batch=\", elements_per_batch_input,\n@@ -6442,7 +6441,7 @@ tsl::Status CudnnSupport::DoPoolForward(\n   auto splits_or =\n       GetTensorSplits(input_dimensions, output_dimensions, element_type);\n   if (!splits_or.ok()) {\n-    return tsl::Status(port::error::INTERNAL, \"Cudnn pooling failed to split\");\n+    return tsl::Status(tsl::error::INTERNAL, \"Cudnn pooling failed to split\");\n   }\n   auto splits = std::move(splits_or.value());\n \n@@ -6511,7 +6510,7 @@ tsl::Status CudnnSupport::DoPoolBackward(\n   auto splits_or =\n       GetTensorSplits(input_dimensions, output_dimensions, element_type);\n   if (!splits_or.ok()) {\n-    return tsl::Status(port::error::INTERNAL, \"Cudnn pooling failed to split\");\n+    return tsl::Status(tsl::error::INTERNAL, \"Cudnn pooling failed to split\");\n   }\n   auto splits = std::move(splits_or.value());\n "
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc",
          "status": "modified",
          "additions": 20,
          "deletions": 20,
          "patch": "@@ -35,10 +35,10 @@ limitations under the License.\n #include \"absl/synchronization/notification.h\"\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/stacktrace.h\"\n #include \"tensorflow/tsl/platform/static_threadlocal.h\"\n #include \"tensorflow/tsl/platform/threadpool.h\"\n@@ -267,7 +267,7 @@ static tsl::Status InternalInit() {\n   }\n \n   Diagnostician::LogDiagnosticInformation();\n-  return tsl::Status(port::error::ABORTED,\n+  return tsl::Status(tsl::error::ABORTED,\n                      absl::StrCat(\"failed call to cuInit: \", ToString(res)));\n }\n \n@@ -400,7 +400,7 @@ bool DeviceOptionsToContextFlags(const DeviceOptions& device_options,\n     }\n   }\n \n-  return tsl::Status(port::error::INTERNAL, message);\n+  return tsl::Status(tsl::error::INTERNAL, message);\n }\n \n /* static */ void GpuDriver::DestroyContext(GpuContext* context) {\n@@ -673,7 +673,7 @@ bool DeviceOptionsToContextFlags(const DeviceOptions& device_options,\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrCat(\"failed to get device for context: \", ToString(result)));\n }\n \n@@ -972,7 +972,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n /* static */ tsl::Status GpuDriver::DestroyEvent(GpuContext* context,\n                                                  CUevent* event) {\n   if (*event == nullptr) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"input event cannot be null\");\n   }\n \n@@ -997,7 +997,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n   CUresult res = cuEventQuery(event);\n   if (res != CUDA_SUCCESS && res != CUDA_ERROR_NOT_READY) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to query event: %s\", ToString(res)));\n   }\n \n@@ -1263,11 +1263,11 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n   if (res == CUDA_SUCCESS) {\n     return ::tsl::OkStatus();\n   } else if (res == CUDA_ERROR_OUT_OF_MEMORY) {\n-    return tsl::Status(port::error::RESOURCE_EXHAUSTED,\n+    return tsl::Status(tsl::error::RESOURCE_EXHAUSTED,\n                        \"could not create CUDA event: out of device memory\");\n   } else {\n     return tsl::Status(\n-        port::error::FAILED_PRECONDITION,\n+        tsl::error::FAILED_PRECONDITION,\n         absl::StrCat(\"could not create CUDA event: \", ToString(res)));\n   }\n }\n@@ -1299,14 +1299,14 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n     // error then the original one.\n     if (context == nullptr) {\n       return tsl::Status(\n-          port::error::UNAVAILABLE,\n+          tsl::error::UNAVAILABLE,\n           \"Empty context returned while querying context for device pointer\");\n     }\n     return context;\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrCat(\"failed to query context for device pointer: \",\n                    ToString(result)));\n }\n@@ -1324,13 +1324,13 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n         return MemorySpace::kHost;\n       default:\n         return tsl::Status(\n-            port::error::INTERNAL,\n+            tsl::error::INTERNAL,\n             absl::StrCat(\"unknown memory space provided by CUDA API: \", value));\n     }\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrCat(\"failed to query device pointer for memory space: \",\n                    ToString(result)));\n }\n@@ -1346,13 +1346,13 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n     // \"there was an internal error while performing this operation\" (return\n     // below).\n     return tsl::Status(\n-        port::error::NOT_FOUND,\n+        tsl::error::NOT_FOUND,\n         absl::StrFormat(\"not a device pointer %p; %s\",\n                         reinterpret_cast<void*>(dptr), ToString(result)));\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrFormat(\"failed to get pointer into for device pointer %p; %s\",\n                       reinterpret_cast<void*>(dptr), ToString(result)));\n }\n@@ -1377,7 +1377,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n       cc_major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device);\n   if (res != CUDA_SUCCESS) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed to get compute capability major for device: %s; %d\",\n             ToString(res), device));\n@@ -1387,7 +1387,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n       cc_minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, device);\n   if (res != CUDA_SUCCESS) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed to get compute capability minor for device: %s; %d\",\n             ToString(res), device));\n@@ -1399,13 +1399,13 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n /* static */ tsl::Status GpuDriver::GetGpuISAVersion(int* version,\n                                                      CUdevice device) {\n   return tsl::Status{\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       \"Feature not supported on CUDA platform (GetGpuISAVersion)\"};\n }\n \n /* static */ tsl::Status GpuDriver::GetGpuGCNArchName(CUdevice, std::string*) {\n   return tsl::Status{\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       \"Feature not supported on CUDA platform (GetGpuGCNArchName)\"};\n }\n \n@@ -1519,7 +1519,7 @@ static tsl::StatusOr<T> GetSimpleAttribute(CUdevice device,\n   CUresult res = cuDeviceGetAttribute(&val, attribute, device);\n   if (res != CUDA_SUCCESS) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to get device attribute %d for device %d: %s\",\n                         attribute, device, ToString(res)));\n   }\n@@ -1628,7 +1628,7 @@ static tsl::StatusOr<T> GetSimpleAttribute(CUdevice device,\n   if (result != CUDA_SUCCESS &&\n       result != CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to enable peer access from %p to %p: %s\", from,\n                         to, ToString(result)));\n   }"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_platform_id.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_memory.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -42,9 +42,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/kernel_cache_config.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n@@ -53,6 +52,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n #include \"tensorflow/compiler/xla/stream_executor/timer.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/numbers.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n \n@@ -745,7 +745,7 @@ tsl::Status GpuExecutor::WaitForEvent(Stream* stream, Event* event) {\n     return ::tsl::OkStatus();\n   } else {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"error recording waiting for CUDA event on stream %p\",\n                         stream));\n   }"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/cuda/cuda_platform.cc",
          "status": "modified",
          "additions": 4,
          "deletions": 4,
          "patch": "@@ -23,8 +23,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n \n namespace stream_executor {\n@@ -117,7 +117,7 @@ tsl::StatusOr<StreamExecutor*> CudaPlatform::FirstExecutorForBus(\n   }\n \n   return tsl::Status(\n-      port::error::NOT_FOUND,\n+      tsl::error::NOT_FOUND,\n       absl::StrFormat(\"Executor for bus %d not found.\", bus_ordinal));\n }\n \n@@ -177,7 +177,7 @@ CudaPlatform::GetUncachedExecutor(const StreamExecutorConfig& config) {\n   auto init_status = executor->Init(config.device_options);\n   if (!init_status.ok()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed initializing StreamExecutor for CUDA device ordinal %d: %s\",\n             config.ordinal, init_status.ToString()));"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/cuda/cuda_rng.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_platform_id.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_memory.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/tsl/platform/status.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/host/BUILD",
          "status": "modified",
          "additions": 3,
          "deletions": 4,
          "patch": "@@ -46,7 +46,7 @@ cc_library(\n         \"//tensorflow/compiler/xla/stream_executor:stream_executor_headers\",\n         \"//tensorflow/compiler/xla/stream_executor/lib\",\n         \"//tensorflow/compiler/xla/stream_executor/platform\",\n-        \"@com_google_absl//absl/base:core_headers\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/strings:str_format\",\n     ],\n@@ -104,9 +104,8 @@ cc_library(\n         \"//tensorflow/compiler/xla/stream_executor:kernel\",\n         \"//tensorflow/compiler/xla/stream_executor:rng\",\n         \"//tensorflow/compiler/xla/stream_executor:stream_executor_internal\",\n-        \"//tensorflow/compiler/xla/stream_executor:stream_executor_pimpl\",\n-        \"//tensorflow/compiler/xla/stream_executor:timer\",\n-        \"//tensorflow/compiler/xla/stream_executor/lib\",\n+        \"//tensorflow/compiler/xla/stream_executor:stream_executor_pimpl\",  # fixdeps: keep\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:platform_port\",\n         \"//tensorflow/tsl/platform/profile_utils:profile_utils_cpu_utils\",\n         \"@com_google_absl//absl/functional:any_invocable\","
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/host/host_gpu_executor.h",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -25,10 +25,10 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/blas.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_timer.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_internal.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n namespace host {"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/host/host_platform.cc",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -21,8 +21,8 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_platform_id.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n namespace host {\n@@ -75,7 +75,7 @@ HostPlatform::GetUncachedExecutor(const StreamExecutorConfig& config) {\n   auto init_status = executor->Init(config.device_options);\n   if (!init_status.ok()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed initializing StreamExecutor for device ordinal %d: %s\",\n             config.ordinal, init_status.ToString().c_str()));"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/lib/BUILD",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -31,7 +31,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"//tensorflow/compiler/xla/stream_executor/platform\",\n         \"//tensorflow/tsl/platform:env\",\n         \"//tensorflow/tsl/platform:stacktrace\",\n         \"//tensorflow/tsl/platform:status\","
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/lib/error.h",
          "status": "removed",
          "additions": 0,
          "deletions": 30,
          "patch": "@@ -1,30 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-\n-#ifndef TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_ERROR_H_\n-#define TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_ERROR_H_\n-\n-#include \"tensorflow/tsl/protobuf/error_codes.pb.h\"  // IWYU pragma: export\n-\n-namespace stream_executor {\n-namespace port {\n-\n-namespace error = tensorflow::error;\n-\n-}  // namespace port\n-}  // namespace stream_executor\n-\n-#endif  // TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_ERROR_H_"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/lib/initialize.h",
          "status": "removed",
          "additions": 0,
          "deletions": 21,
          "patch": "@@ -1,21 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_INITIALIZE_H_\n-#define TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_INITIALIZE_H_\n-\n-#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n-\n-#endif  // TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_INITIALIZE_H_"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/multi_platform_manager.cc",
          "status": "modified",
          "additions": 6,
          "deletions": 7,
          "patch": "@@ -24,8 +24,7 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n@@ -96,7 +95,7 @@ tsl::Status MultiPlatformManagerImpl::RegisterPlatform(\n   std::string key = absl::AsciiStrToLower(platform->Name());\n   absl::MutexLock lock(&mu_);\n   if (name_map_.find(key) != name_map_.end()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        \"platform is already registered with name: \\\"\" +\n                            platform->Name() + \"\\\"\");\n   }\n@@ -156,7 +155,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::InitializePlatformWithName(\n   TF_ASSIGN_OR_RETURN(Platform * platform, LookupByNameLocked(target));\n   if (platform->Initialized()) {\n     return tsl::Status(\n-        port::error::FAILED_PRECONDITION,\n+        tsl::error::FAILED_PRECONDITION,\n         absl::StrCat(\"platform \\\"\", target, \"\\\" is already initialized\"));\n   }\n \n@@ -172,7 +171,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::InitializePlatformWithId(\n   TF_ASSIGN_OR_RETURN(Platform * platform, LookupByIdLocked(id));\n   if (platform->Initialized()) {\n     return tsl::Status(\n-        port::error::FAILED_PRECONDITION,\n+        tsl::error::FAILED_PRECONDITION,\n         absl::StrFormat(\"platform with id %p is already initialized\", id));\n   }\n \n@@ -232,7 +231,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::LookupByNameLocked(\n   auto it = name_map_.find(absl::AsciiStrToLower(target));\n   if (it == name_map_.end()) {\n     return tsl::Status(\n-        port::error::NOT_FOUND,\n+        tsl::error::NOT_FOUND,\n         absl::StrCat(\"Could not find registered platform with name: \\\"\", target,\n                      \"\\\". Available platform names are: \",\n                      absl::StrJoin(InitializedPlatformNamesWithFilter(), \" \")));\n@@ -245,7 +244,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::LookupByIdLocked(\n   auto it = id_map_.find(id);\n   if (it == id_map_.end()) {\n     return tsl::Status(\n-        port::error::NOT_FOUND,\n+        tsl::error::NOT_FOUND,\n         absl::StrFormat(\"could not find registered platform with id: %p\", id));\n   }\n   return it->second;"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/multi_platform_manager.h",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -70,8 +70,8 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/strings/string_view.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n #include \"tensorflow/tsl/platform/statusor.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/platform.cc",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -16,10 +16,10 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n \n #include \"absl/strings/str_cat.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n \n@@ -92,14 +92,14 @@ bool Platform::Initialized() const { return true; }\n tsl::Status Platform::Initialize(\n     const std::map<std::string, std::string> &platform_options) {\n   if (!platform_options.empty()) {\n-    return tsl::Status(port::error::UNIMPLEMENTED,\n+    return tsl::Status(tsl::error::UNIMPLEMENTED,\n                        \"this platform does not support custom initialization\");\n   }\n   return ::tsl::OkStatus();\n }\n \n tsl::Status Platform::ForceExecutorShutdown() {\n-  return tsl::Status(port::error::UNIMPLEMENTED,\n+  return tsl::Status(tsl::error::UNIMPLEMENTED,\n                      \"executor shutdown is not supported on this platform\");\n }\n "
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/plugin_registry.cc",
          "status": "modified",
          "additions": 5,
          "deletions": 5,
          "patch": "@@ -19,8 +19,8 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/multi_platform_manager.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n \n@@ -76,7 +76,7 @@ tsl::Status PluginRegistry::RegisterFactoryInternal(\n \n   if (factories->find(plugin_id) != factories->end()) {\n     return tsl::Status(\n-        port::error::ALREADY_EXISTS,\n+        tsl::error::ALREADY_EXISTS,\n         absl::StrFormat(\"Attempting to register factory for plugin %s when \"\n                         \"one has already been registered\",\n                         plugin_name));\n@@ -96,7 +96,7 @@ tsl::StatusOr<FACTORY_TYPE> PluginRegistry::GetFactoryInternal(\n     iter = generic_factories.find(plugin_id);\n     if (iter == generic_factories.end()) {\n       return tsl::Status(\n-          port::error::NOT_FOUND,\n+          tsl::error::NOT_FOUND,\n           absl::StrFormat(\"Plugin ID %p not registered.\", plugin_id));\n     }\n   }\n@@ -217,7 +217,7 @@ bool PluginRegistry::HasFactory(Platform::Id platform_id,\n                                                                               \\\n       if (plugin_id == kNullPlugin) {                                         \\\n         return tsl::Status(                                                   \\\n-            port::error::FAILED_PRECONDITION,                                 \\\n+            tsl::error::FAILED_PRECONDITION,                                  \\\n             \"No suitable \" PLUGIN_STRING                                      \\\n             \" plugin registered. Have you linked in a \" PLUGIN_STRING         \\\n             \"-providing plugin?\");                                            \\\n@@ -236,7 +236,7 @@ bool PluginRegistry::HasFactory(Platform::Id platform_id,\n       PlatformKind platform_kind, PluginId plugin_id) {                       \\\n     auto iter = platform_id_by_kind_.find(platform_kind);                     \\\n     if (iter == platform_id_by_kind_.end()) {                                 \\\n-      return tsl::Status(port::error::FAILED_PRECONDITION,                    \\\n+      return tsl::Status(tsl::error::FAILED_PRECONDITION,                     \\\n                          absl::StrFormat(\"Platform kind %d not registered.\",  \\\n                                          static_cast<int>(platform_kind)));   \\\n     }                                                                         \\"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_blas.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -32,8 +32,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_diagnostics.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -35,8 +35,8 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/strip.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/host_info.h\"\n \n namespace stream_executor {"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_dnn.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -31,9 +31,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_diagnostics.h\"\n@@ -42,6 +41,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/hash.h\"\n #include \"tensorflow/tsl/util/determinism.h\"\n #include \"tensorflow/tsl/util/env_var.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_driver.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -28,11 +28,11 @@ limitations under the License.\n #include \"absl/synchronization/notification.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_diagnostics.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_driver.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_driver_wrapper.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/numbers.h\"\n #include \"tensorflow/tsl/platform/stacktrace.h\"\n #include \"tensorflow/tsl/platform/static_threadlocal.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_fft.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -28,10 +28,9 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/kernel_cache_config.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n@@ -42,6 +41,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n #include \"tensorflow/compiler/xla/stream_executor/timer.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n #ifdef PLATFORMS_GPUS_ROCM_DYNAMIC_LIBROCM_DYNAMIC_LIBROCM_H_\n #error \\"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_platform.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -21,9 +21,9 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_driver.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_platform_id.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n namespace gpu {"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/rocm/rocm_rng.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -20,8 +20,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_platform_id.h\""
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.cc",
          "status": "modified",
          "additions": 8,
          "deletions": 8,
          "patch": "@@ -32,11 +32,11 @@ limitations under the License.\n #include \"absl/synchronization/notification.h\"\n #include \"tensorflow/compiler/xla/stream_executor/blas.h\"\n #include \"tensorflow/compiler/xla/stream_executor/fft.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_internal.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/stacktrace.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n #include \"tensorflow/tsl/platform/threadpool.h\"\n@@ -405,7 +405,7 @@ StreamExecutor::createRnnDescriptor(\n     bool use_padded_io) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnDescriptor(\n@@ -420,7 +420,7 @@ StreamExecutor::createRnnSequenceTensorDescriptor(int max_seq_length,\n                                                   dnn::DataType data_type) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnSequenceTensorDescriptor(\n@@ -434,7 +434,7 @@ StreamExecutor::createRnnSequenceTensorDescriptor(\n     dnn::DataType data_type) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnSequenceTensorDescriptor(\n@@ -448,7 +448,7 @@ StreamExecutor::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n                                                dnn::DataType data_type) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnStateTensorDescriptor(num_layer, batch_size,\n@@ -546,7 +546,7 @@ tsl::StatusOr<DeviceMemoryBase> StreamExecutor::GetUntypedSymbol(\n   }\n \n   return tsl::Status(\n-      port::error::NOT_FOUND,\n+      tsl::error::NOT_FOUND,\n       absl::StrCat(\"Check if module containing symbol \", symbol_name,\n                    \" is loaded (module_handle = \",\n                    reinterpret_cast<uintptr_t>(module_handle.id()), \")\"));\n@@ -691,7 +691,7 @@ tsl::Status StreamExecutor::SynchronousMemcpyD2H(\n   result = implementation_->SynchronousMemcpy(host_dst, device_src, size);\n   if (!result.ok()) {\n     result = tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to synchronously memcpy device-to-host: device \"\n                         \"%p to host %p size %d: %s\",\n                         device_src.opaque(), host_dst, size,\n@@ -715,7 +715,7 @@ tsl::Status StreamExecutor::SynchronousMemcpyH2D(const void* host_src,\n   result = implementation_->SynchronousMemcpy(device_dst, host_src, size);\n   if (!result.ok()) {\n     result = tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to synchronously memcpy host-to-device: host \"\n                         \"%p to device %p size %d: %s\",\n                         host_src, device_dst->opaque(), size,"
        },
        {
          "filename": "tensorflow/compiler/xla/stream_executor/tf_allocator_adapter.cc",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/tf_allocator_adapter.h\"\n \n #include \"absl/synchronization/mutex.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor.h\"\n #include \"tensorflow/tsl/platform/errors.h\""
        },
        {
          "filename": "tensorflow/core/common_runtime/function.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -535,7 +535,7 @@ class CallOp : public AsyncOpKernel {\n     OP_REQUIRES_ASYNC(ctx, lib != nullptr,\n                       errors::Internal(\"No function library is provided.\"),\n                       done);\n-    FunctionLibraryRuntime::Options opts;\n+    FunctionLibraryRuntime::Options opts(ctx->step_id());\n     opts.rendezvous = ctx->rendezvous();\n     opts.cancellation_manager = ctx->cancellation_manager();\n     opts.step_container = ctx->step_container();"
        },
        {
          "filename": "tensorflow/core/data/BUILD",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -257,6 +257,7 @@ cc_library(\n     # copybara:uncomment copts = [\"-Wthread-safety-analysis\"],\n     visibility = [\"//visibility:public\"],\n     deps = [\n+        \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core/platform:env\",\n         \"//tensorflow/core/platform:mutex\","
        },
        {
          "filename": "tensorflow/core/data/service/server_lib.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -37,7 +37,7 @@ constexpr char kPortPlaceholder[] = \"%port%\";\n }\n \n GrpcDataServerBase::GrpcDataServerBase(\n-    int port, const std::string& protocol, const std::string server_type,\n+    int port, const std::string& protocol, const std::string& server_type,\n     std::vector<std::unique_ptr<::grpc::ServerBuilderOption>> options)\n     : requested_port_(port),\n       protocol_(protocol),"
        },
        {
          "filename": "tensorflow/core/data/service/server_lib.h",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -44,7 +44,7 @@ class GrpcDataServerBase {\n   // found by calling `BoundPort()`.\n   GrpcDataServerBase(\n       int requested_port, const std::string& protocol,\n-      const std::string server_type,\n+      const std::string& server_type,\n       std::vector<std::unique_ptr<::grpc::ServerBuilderOption>> options = {});\n   virtual ~GrpcDataServerBase() = default;\n "
        },
        {
          "filename": "tensorflow/core/data/service/snapshot/BUILD",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -84,7 +84,9 @@ cc_library(\n     hdrs = [\"path_utils.h\"],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:path\",\n+        \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings\",\n     ],\n )\n@@ -96,6 +98,8 @@ tf_cc_test(\n         \":path_utils\",\n         \"//tensorflow/core:test\",\n         \"//tensorflow/core:test_main\",\n+        \"//tensorflow/tsl/platform:status_matchers\",\n+        \"//tensorflow/tsl/protobuf:protos_all_cc\",\n     ],\n )\n "
        },
        {
          "filename": "tensorflow/core/data/service/snapshot/path_utils.cc",
          "status": "modified",
          "additions": 27,
          "deletions": 0,
          "patch": "@@ -15,10 +15,15 @@ limitations under the License.\n #include \"tensorflow/core/data/service/snapshot/path_utils.h\"\n \n #include <string>\n+#include <utility>\n+#include <vector>\n \n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/path.h\"\n+#include \"tensorflow/tsl/platform/statusor.h\"\n \n namespace tensorflow {\n namespace data {\n@@ -65,6 +70,28 @@ std::string SplitPath(absl::string_view snapshot_path, int64_t stream_index,\n       absl::StrCat(\"split_\", local_index, \"_\", global_index));\n }\n \n+tsl::StatusOr<std::pair<int64_t, int64_t>> SplitIndex(\n+    absl::string_view split_path) {\n+  std::vector<std::string> tokens = absl::StrSplit(split_path, '_');\n+  int64_t local_split_index = 0, global_split_index = 0;\n+  if (tokens.size() != 3 || tokens[0] != \"split\" ||\n+      !absl::SimpleAtoi(tokens[1], &local_split_index) ||\n+      local_split_index < 0 ||\n+      !absl::SimpleAtoi(tokens[2], &global_split_index) ||\n+      global_split_index < 0) {\n+    return tsl::errors::InvalidArgument(\n+        \"Invalid split file name: \", split_path,\n+        \". Expected split_<local_split_index>_<global_split_index>.\");\n+  }\n+  if (local_split_index > global_split_index) {\n+    return tsl::errors::InvalidArgument(\n+        \"Invalid split file name: \", split_path, \". The local split index \",\n+        local_split_index, \" exceeds the global split index \",\n+        global_split_index, \".\");\n+  }\n+  return std::make_pair(local_split_index, global_split_index);\n+}\n+\n std::string SnapshotMetadataFilePath(absl::string_view snapshot_path_) {\n   return tsl::io::JoinPath(snapshot_path_, kSnapshotMetadataFileName);\n }"
        },
        {
          "filename": "tensorflow/core/data/service/snapshot/path_utils.h",
          "status": "modified",
          "additions": 8,
          "deletions": 0,
          "patch": "@@ -16,8 +16,10 @@ limitations under the License.\n #define TENSORFLOW_CORE_DATA_SERVICE_SNAPSHOT_PATH_UTILS_H_\n \n #include <string>\n+#include <utility>\n \n #include \"absl/strings/string_view.h\"\n+#include \"tensorflow/tsl/platform/statusor.h\"\n \n namespace tensorflow {\n namespace data {\n@@ -45,6 +47,12 @@ std::string SplitPath(absl::string_view snapshot_path, int64_t stream_index,\n                       int64_t source_id, int64_t local_index,\n                       int64_t global_index);\n \n+// Returns a pair of {local_split_index, global_split_index} of the split. The\n+// expected format of `split_path` is:\n+// split_<local_split_index>_<global_split_index>\n+tsl::StatusOr<std::pair<int64_t, int64_t>> SplitIndex(\n+    absl::string_view split_path);\n+\n // Returns the path of the DONE file of a snapshot stream.\n std::string StreamDoneFilePath(absl::string_view snapshot_path,\n                                int64_t stream_index);"
        },
        {
          "filename": "tensorflow/core/data/service/snapshot/path_utils_test.cc",
          "status": "modified",
          "additions": 34,
          "deletions": 0,
          "patch": "@@ -14,13 +14,19 @@ limitations under the License.\n ==============================================================================*/\n #include \"tensorflow/core/data/service/snapshot/path_utils.h\"\n \n+#include \"tensorflow/tsl/platform/status_matchers.h\"\n #include \"tensorflow/tsl/platform/test.h\"\n+#include \"tensorflow/tsl/protobuf/error_codes.pb.h\"\n \n namespace tensorflow {\n namespace data {\n namespace {\n \n+using ::testing::HasSubstr;\n using ::testing::MatchesRegex;\n+using ::testing::Pair;\n+using tsl::testing::IsOkAndHolds;\n+using tsl::testing::StatusIs;\n \n TEST(PathUtilsTest, StreamsDirectory) {\n   EXPECT_THAT(StreamsDirectory(\"/path/to/snapshot\"),\n@@ -51,6 +57,34 @@ TEST(PathUtilsTest, SplitPath) {\n           \"/path/to/snapshot.streams.stream_0.splits.source_1.split_2_3\"));\n }\n \n+TEST(PathUtilsTest, SplitIndex) {\n+  EXPECT_THAT(SplitIndex(\"split_0_1\"), IsOkAndHolds(Pair(0, 1)));\n+}\n+\n+TEST(PathUtilsTest, InvalidSplitFile) {\n+  EXPECT_THAT(\n+      SplitIndex(\"\"),\n+      StatusIs(error::INVALID_ARGUMENT,\n+               HasSubstr(\n+                   \"Expected split_<local_split_index>_<global_split_index>\")));\n+  EXPECT_THAT(\n+      SplitIndex(\"split_123\"),\n+      StatusIs(error::INVALID_ARGUMENT,\n+               HasSubstr(\n+                   \"Expected split_<local_split_index>_<global_split_index>\")));\n+  EXPECT_THAT(\n+      SplitIndex(\"split_-1_(-1)\"),\n+      StatusIs(error::INVALID_ARGUMENT,\n+               HasSubstr(\n+                   \"Expected split_<local_split_index>_<global_split_index>\")));\n+  EXPECT_THAT(\n+      SplitIndex(\"split_5_0\"),\n+      StatusIs(\n+          error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"The local split index 5 exceeds the global split index 0\")));\n+}\n+\n TEST(PathUtilsTest, StreamDoneFilePath) {\n   EXPECT_THAT(StreamDoneFilePath(\"/path/to/snapshot\", /*stream_index=*/0),\n               MatchesRegex(\"/path/to/snapshot.streams.stream_0.DONE\"));"
        },
        {
          "filename": "tensorflow/core/data/service/snapshot/snapshot_manager.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 15,
          "patch": "@@ -187,21 +187,8 @@ Status SnapshotManager::ReadOnDiskSource(\n \n     // `split_filename` must have this format:\n     // \"split_<local_split_index>_<global_split_index>\".\n-    std::vector<std::string> tokens = absl::StrSplit(split_filename, '_');\n-    int64_t local_split_index;\n-    int64_t global_split_index;\n-    if (tokens.size() != 3 ||\n-        !absl::SimpleAtoi(tokens[1], &local_split_index) ||\n-        local_split_index < 0 ||\n-        !absl::SimpleAtoi(tokens[2], &global_split_index) ||\n-        global_split_index < 0) {\n-      return InvalidArgument(\"can't parse the name of \", split_path);\n-    }\n-    if (local_split_index > global_split_index) {\n-      return InvalidArgument(\n-          \"found conflict between local split index and global split index in \",\n-          \"name of \", split_path);\n-    }\n+    TF_ASSIGN_OR_RETURN(auto split_index, SplitIndex(split_filename));\n+    auto [local_split_index, global_split_index] = split_index;\n     if (local_split_index > split_filenames.size() - 1) {\n       return InvalidArgument(\n           \"found conflict between the number of splits and name of \","
        },
        {
          "filename": "tensorflow/core/data/tfdataz_metrics.cc",
          "status": "modified",
          "additions": 7,
          "deletions": 2,
          "patch": "@@ -88,8 +88,9 @@ absl::Duration ApproximateLatencyEstimator::GetAverageLatency(Duration duration)\n   return absl::Duration(absl::Microseconds(interval_latency)) / interval_count;\n }\n \n-TfDatazMetricsCollector::TfDatazMetricsCollector(const Env& env)\n-    : latency_estimator_(env) {}\n+TfDatazMetricsCollector::TfDatazMetricsCollector(const Env& env,\n+                                                 IteratorBase* iterator)\n+    : iterator_(iterator), latency_estimator_(env) {}\n \n void TfDatazMetricsCollector::RecordGetNextLatency(\n     int64_t get_next_latency_usec) {\n@@ -113,6 +114,10 @@ absl::Duration TfDatazMetricsCollector::GetAverageLatencyForLastSixtyMinutes() {\n       ApproximateLatencyEstimator::Duration::kSixtyMinutes);\n }\n \n+int64_t TfDatazMetricsCollector::GetIteratorTotalMemoryUsage() {\n+  return iterator_->TotalBufferedBytes();\n+}\n+\n namespace {\n static mutex* get_tfdataz_metrics_registry_lock() {\n   static mutex tfdataz_metrics_registry_lock(LINKER_INITIALIZED);"
        },
        {
          "filename": "tensorflow/core/data/tfdataz_metrics.h",
          "status": "modified",
          "additions": 8,
          "deletions": 1,
          "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n \n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/time/time.h\"\n+#include \"tensorflow/core/framework/dataset.h\"\n #include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n #include \"tensorflow/core/platform/thread_annotations.h\"\n@@ -95,7 +96,7 @@ class TfDatazMetricsCollector {\n   // We only collect metrics for CPU devices. This is a heuristic to avoid\n   // collecting metrics for device-side iterators created by the multi-device\n   // iterator mechanism.\n-  explicit TfDatazMetricsCollector(const Env& env);\n+  TfDatazMetricsCollector(const Env& env, IteratorBase* iterator);\n \n   // Records `GetNext` call latency.\n   void RecordGetNextLatency(int64_t get_next_latency_usec);\n@@ -109,7 +110,13 @@ class TfDatazMetricsCollector {\n   // Returns the average `GetNext` latency for past 60 minutes.\n   absl::Duration GetAverageLatencyForLastSixtyMinutes();\n \n+  // Returns the total memory (in bytes) used by the iterator.\n+  // Total memory used by the iterator includes the total number of bytes\n+  // buffered in all nodes in the subtree.\n+  int64_t GetIteratorTotalMemoryUsage();\n+\n  private:\n+  IteratorBase* iterator_;  // not owned\n   ApproximateLatencyEstimator latency_estimator_;\n };\n "
        },
        {
          "filename": "tensorflow/core/data/tfdataz_metrics_test.cc",
          "status": "modified",
          "additions": 16,
          "deletions": 12,
          "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/time/time.h\"\n-#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/dataset.h\"\n #include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/test.h\"\n #include \"tensorflow/core/util/fake_clock_env.h\"\n@@ -41,14 +41,16 @@ class TfDatazMetricsTest : public ::testing::Test {\n  protected:\n   void SetUp() override {\n     env_ = std::make_unique<FakeClockEnv>(Env::Default());\n-    tfdataz_metrics_ = std::make_unique<TfDatazMetricsCollector>(*env_);\n+    tfdataz_metrics_ =\n+        std::make_unique<TfDatazMetricsCollector>(*env_, iterator_.get());\n   }\n \n   void TearDown() override {\n     env_.reset();\n     tfdataz_metrics_.reset();\n   }\n \n+  std::unique_ptr<IteratorBase> iterator_;\n   std::unique_ptr<FakeClockEnv> env_;\n   std::unique_ptr<TfDatazMetricsCollector> tfdataz_metrics_;\n };\n@@ -184,10 +186,11 @@ class ScopedTfDataMetricsRegistration {\n };\n \n TEST(TfDatazMetricsRegistryTest, Register) {\n-  auto collector_one =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n-  auto collector_two =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n+  std::unique_ptr<IteratorBase> iterator;\n+  auto collector_one = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n+  auto collector_two = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n \n   ScopedTfDataMetricsRegistration scoped_registration_one(collector_one);\n   ScopedTfDataMetricsRegistration scoped_registration_two(collector_two);\n@@ -196,12 +199,13 @@ TEST(TfDatazMetricsRegistryTest, Register) {\n }\n \n TEST(TfDatazMetricsRegistryTest, Deregister) {\n-  auto collector_one =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n-  auto collector_two =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n-  auto collector_three =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n+  std::unique_ptr<IteratorBase> iterator;\n+  auto collector_one = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n+  auto collector_two = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n+  auto collector_three = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n   ScopedTfDataMetricsRegistration scoped_registration_one(collector_one);\n   ScopedTfDataMetricsRegistration scoped_registration_two(collector_two);\n   ScopedTfDataMetricsRegistration scoped_registration_three(collector_three);"
        },
        {
          "filename": "tensorflow/core/distributed_runtime/eager/remote_mgr.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -33,7 +33,7 @@ Status WithErrorSourcePayload(Status error) {\n   error_source_proto.set_error_source(\n       core::platform::ErrorSourceProto::EAGER_REMOTE_MGR);\n   error.SetPayload(tensorflow::kErrorSource,\n-                   error_source_proto.SerializeAsString());\n+                   absl::Cord(error_source_proto.SerializeAsString()));\n   return error;\n }\n }  // namespace"
        },
        {
          "filename": "tensorflow/core/distributed_runtime/integration_test/coordination_test_opkernel_registration.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -149,7 +149,8 @@ class TestReportErrorToClusterOp : public OpKernel {\n     }\n     tensorflow::Status s(static_cast<tensorflow::error::Code>(error_code),\n                          error_message);\n-    s.SetPayload(tsl::CoordinationErrorPayloadKey(), \"testing error payload\");\n+    s.SetPayload(tsl::CoordinationErrorPayloadKey(),\n+                 absl::Cord(\"testing error payload\"));\n     OP_REQUIRES_OK(ctx, coord_agent->ReportError(s));\n   }\n };"
        },
        {
          "filename": "tensorflow/core/framework/dataset.h",
          "status": "modified",
          "additions": 7,
          "deletions": 0,
          "patch": "@@ -959,6 +959,13 @@ class IteratorBase : public Checkpointable {\n     return OkStatus();\n   }\n \n+  // Returns the total number of bytes buffered by the iterator across all nodes\n+  // in the subtree for which autotuning is enabled.\n+  int64_t TotalBufferedBytes() const {\n+    if (node_) return node_->TotalBufferedBytes();\n+    return 0;\n+  }\n+\n  protected:\n   // Returns a node that models this iterator.\n   virtual std::shared_ptr<model::Node> CreateNode("
        },
        {
          "filename": "tensorflow/core/framework/op_requires.h",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -62,7 +62,7 @@ namespace tensorflow {\n     if (!TF_PREDICT_TRUE(STATUS.ok())) {                                       \\\n       CheckNotInComputeAsync((CTX), \"OP_REQUIRES_OK_ASYNC\");                   \\\n       if (!PAYLOAD_VALUE.empty()) {                                            \\\n-        STATUS.SetPayload(PAYLOAD_KEY, PAYLOAD_VALUE);                         \\\n+        STATUS.SetPayload(PAYLOAD_KEY, absl::Cord(PAYLOAD_VALUE));             \\\n       }                                                                        \\\n       (CTX)->CtxFailureWithWarning(__FILE__, __LINE__, STATUS);                \\\n       return;                                                                  \\"
        },
        {
          "filename": "tensorflow/core/kernels/data/iterator_ops.cc",
          "status": "modified",
          "additions": 3,
          "deletions": 2,
          "patch": "@@ -93,8 +93,6 @@ IteratorResource::IteratorResource(\n                                               /*iterator=*/nullptr)),\n       output_dtypes_(output_dtypes),\n       output_shapes_(output_shapes) {\n-  tf_dataz_metrics_collector_ = std::make_shared<TfDatazMetricsCollector>(*env);\n-  TfDatazMetricsRegistry::Register(tf_dataz_metrics_collector_);\n   VLOG(2) << \"creating iterator resource\";\n }\n \n@@ -274,6 +272,9 @@ Status IteratorResource::SetIteratorFromDataset(OpKernelContext* ctx,\n   new_state->MergeCheckpoint(iter_ctx.checkpoint());\n   mutex_lock l(mu_);\n   std::swap(iterator_state_, new_state);\n+  tf_dataz_metrics_collector_ =\n+      std::make_shared<TfDatazMetricsCollector>(env_, iterator.get());\n+  TfDatazMetricsRegistry::Register(tf_dataz_metrics_collector_);\n   return OkStatus();\n }\n "
        },
        {
          "filename": "tensorflow/core/kernels/function_ops.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -238,7 +238,7 @@ class SymbolicGradientOp : public AsyncOpKernel {\n     OP_REQUIRES_OK_ASYNC(\n         ctx, lib->Instantiate(kGradientOp, AttrSlice(def()), &handle), done);\n \n-    FunctionLibraryRuntime::Options opts;\n+    FunctionLibraryRuntime::Options opts(ctx->step_id());\n     opts.rendezvous = ctx->rendezvous();\n     opts.cancellation_manager = ctx->cancellation_manager();\n     opts.collective_executor = ctx->collective_executor();"
        },
        {
          "filename": "tensorflow/core/kernels/functional_ops.cc",
          "status": "modified",
          "additions": 7,
          "deletions": 3,
          "patch": "@@ -160,7 +160,8 @@ class IfOp : public AsyncOpKernel {\n           then_handle_(then_handle),\n           else_handle_(else_handle),\n           done_(std::move(done)),\n-          lib_(CHECK_NOTNULL(ctx_->function_library())) {\n+          lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()) {\n       SetRunOptions(ctx_, &opts_, true /* always_collect_stats */);\n       for (int i = 1; i < ctx_->num_inputs(); ++i) {\n         args_.push_back(ctx_->input(i));\n@@ -286,7 +287,8 @@ class CaseOp : public AsyncOpKernel {\n           branch_(branch),\n           branch_handles_(branch_handles),\n           done_(std::move(done)),\n-          lib_(CHECK_NOTNULL(ctx_->function_library())) {\n+          lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()) {\n       SetRunOptions(ctx_, &opts_, true /* always_collect_stats */);\n       for (int i = 1; i < ctx_->num_inputs(); ++i) {\n         args_.push_back(ctx_->input(i));\n@@ -507,7 +509,8 @@ class WhileOp : public AsyncOpKernel {\n           cond_handle_(cond_handle),\n           body_handle_(body_handle),\n           done_(std::move(done)),\n-          lib_(CHECK_NOTNULL(ctx_->function_library())) {\n+          lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()) {\n       SetRunOptions(ctx_, &opts_, false /* always_collect_stats */);\n       GetArgsFromContext(ctx, &args_, &loop_var_types_);\n       body_frame_ =\n@@ -751,6 +754,7 @@ class ForOp : public AsyncOpKernel {\n           ctx_(ctx),\n           done_(std::move(done)),\n           lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()),\n           args_(1 + ctx_->num_inputs() - 3) {\n       args_[0] = Tensor(DT_INT32, {});\n       iter_ = &args_[0].scalar<int32>()();"
        },
        {
          "filename": "tensorflow/core/kernels/rnn/gru_ops.cc",
          "status": "modified",
          "additions": 55,
          "deletions": 33,
          "patch": "@@ -49,61 +49,68 @@ class GRUCellBlockOp : public OpKernel {\n     const Tensor* b_c_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"b_c\", &b_c_tensor));\n \n+    // Sanity checks for input shapes.\n+\n+    // Shape of 'x' must be [batch_size, input_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n+                                        \" vs. 2\"));\n     const int64_t batch_size = x_tensor->dim_size(0);\n     const int64_t input_size = x_tensor->dim_size(1);\n-    const int64_t cell_size = h_prev_tensor->dim_size(1);\n-\n-    // Sanity checks for input shapes.\n \n     // Shape of 'h' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(h_prev_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of h_prev must be 2, got \",\n+                                        h_prev_tensor->dims()));\n     OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                         h_prev_tensor->dim_size(0), \" vs. \",\n                                         batch_size));\n-    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n-                errors::InvalidArgument(\n-                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n-                    \" vs. \", cell_size));\n+    const int64_t cell_size = h_prev_tensor->dim_size(1);\n \n     // Shape of 'w_ru' must be [input_size+cell_size, 2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_ru_ must be 2, got \",\n+                                        w_ru_tensor->dims()));\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_ru.dim_size(0) != input_size + cell_size: \",\n                     w_ru_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(1) == cell_size * 2,\n                 errors::InvalidArgument(\"w_ru.dim_size(1) != cell_size * 2: \",\n                                         w_ru_tensor->dim_size(1), \" vs. \",\n                                         cell_size * 2));\n \n     // Shape of 'w_c' must be [input_size+cell_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_c must be 2, got \",\n+                                        w_c_tensor->dims()));\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(0) != input_size + cell_size: \",\n                     w_c_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(1) == cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(1) != cell_size: \", w_c_tensor->dim_size(1),\n                     \" vs. \", cell_size));\n \n     // Shape of 'b_ru' must be [2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_ru must be 1, got \",\n+                                        b_ru_tensor->dims()));\n     OP_REQUIRES(ctx, b_ru_tensor->dim_size(0) == cell_size * 2,\n                 errors::InvalidArgument(\"b_ru.dim_size(0) != cell_size * 2: \",\n                                         b_ru_tensor->dim_size(0), \" vs. \",\n                                         cell_size * 2));\n \n-    OP_REQUIRES(ctx, b_ru_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_ru must be 1\",\n-                                        b_ru_tensor->dims(), \" vs. 1\", 1));\n     // Shape of 'b_c' must be [cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_c must be 1, got \",\n+                                        b_c_tensor->dims()));\n     OP_REQUIRES(ctx, b_c_tensor->dim_size(0) == cell_size,\n                 errors::InvalidArgument(\n                     \"b_c.dim_size(0) != cell_size: \", b_c_tensor->dim_size(0),\n                     \" vs. \", cell_size));\n-    OP_REQUIRES(ctx, b_c_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_c must be 1\",\n-                                        b_c_tensor->dims(), \" vs. 1\"));\n \n     // Create output tensors.\n     Tensor* r_tensor = nullptr;\n@@ -204,65 +211,71 @@ class GRUBlockCellGradOp : public OpKernel {\n     const Tensor* d_h_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"d_h\", &d_h_tensor));\n \n+    // Shape of 'x' must be [batch_size, input_size]\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n+        errors::InvalidArgument(\"Rank of x must be 2, got \", x_tensor->dims()));\n     const int64_t batch_size = x_tensor->dim_size(0);\n     const int64_t input_size = x_tensor->dim_size(1);\n-    const int64_t cell_size = h_prev_tensor->dim_size(1);\n \n-    // Sanity checks for input shapes.\n-\n-    // Shape of 'h_prev' must be [batch_size, cell_size]\n+    // Shape of 'h' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(h_prev_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of h_prev must be 2, got \",\n+                                        h_prev_tensor->dims()));\n     OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                         h_prev_tensor->dim_size(0), \" vs. \",\n                                         batch_size));\n-    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n-                errors::InvalidArgument(\n-                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n-                    \" vs. \", cell_size));\n+    const int64_t cell_size = h_prev_tensor->dim_size(1);\n \n     // Shape of 'w_ru' must be [input_size+cell_size, 2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_ru_ must be 2, got \",\n+                                        w_ru_tensor->dims()));\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_ru.dim_size(0) != input_size + cell_size: \",\n                     w_ru_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(1) == cell_size * 2,\n                 errors::InvalidArgument(\"w_ru.dim_size(1) != cell_size * 2: \",\n                                         w_ru_tensor->dim_size(1), \" vs. \",\n                                         cell_size * 2));\n \n     // Shape of 'w_c' must be [input_size+cell_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_c must be 2, got \",\n+                                        w_c_tensor->dims()));\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(0) != input_size + cell_size: \",\n                     w_c_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(1) == cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(1) != cell_size: \", w_c_tensor->dim_size(1),\n                     \" vs. \", cell_size));\n \n     // Shape of 'b_ru' must be [2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_ru must be 1, got \",\n+                                        b_ru_tensor->dims()));\n     OP_REQUIRES(ctx, b_ru_tensor->dim_size(0) == cell_size * 2,\n                 errors::InvalidArgument(\"b_ru.dim_size(0) != cell_size * 2: \",\n                                         b_ru_tensor->dim_size(0), \" vs. \",\n                                         cell_size * 2));\n \n-    OP_REQUIRES(ctx, b_ru_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_ru must be 1\",\n-                                        b_ru_tensor->dims(), \" vs. 1\"));\n-\n     // Shape of 'b_c' must be [cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_c must be 1, got \",\n+                                        b_c_tensor->dims()));\n     OP_REQUIRES(ctx, b_c_tensor->dim_size(0) == cell_size,\n                 errors::InvalidArgument(\n                     \"b_c.dim_size(0) != cell_size: \", b_c_tensor->dim_size(0),\n                     \" vs. \", cell_size));\n \n-    OP_REQUIRES(ctx, b_c_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_c must be 1 \",\n-                                        b_c_tensor->dims(), \" vs. 1\"));\n-\n     // Shape of 'r' must be [batch_size, cell_size]\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsMatrix(r_tensor->shape()),\n+        errors::InvalidArgument(\"Rank of r must be 2, got \", r_tensor->dims()));\n     OP_REQUIRES(ctx, r_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"r.dims(0) != batch_size: \", r_tensor->dim_size(0), \" vs. \",\n@@ -273,6 +286,9 @@ class GRUBlockCellGradOp : public OpKernel {\n                     cell_size));\n \n     // Shape of 'u' must be [batch_size, cell_size]\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsMatrix(u_tensor->shape()),\n+        errors::InvalidArgument(\"Rank of u must be 2, got \", u_tensor->dims()));\n     OP_REQUIRES(ctx, u_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"u.dims(0) != batch_size: \", u_tensor->dim_size(0), \" vs. \",\n@@ -283,6 +299,9 @@ class GRUBlockCellGradOp : public OpKernel {\n                     cell_size));\n \n     // Shape of 'c' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_c must be 2, got \",\n+                                        c_tensor->dims()));\n     OP_REQUIRES(ctx, c_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"c.dims(0) != batch_size: \", c_tensor->dim_size(0), \" vs. \",\n@@ -293,6 +312,9 @@ class GRUBlockCellGradOp : public OpKernel {\n                     cell_size));\n \n     // Shape of 'd_h' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(d_h_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of d_h must be 2, got \",\n+                                        d_h_tensor->dims()));\n     OP_REQUIRES(ctx, d_h_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"d_h.dims(0) != batch_size: \", d_h_tensor->dim_size(0),"
        },
        {
          "filename": "tensorflow/core/lib/core/status_test.cc",
          "status": "modified",
          "additions": 4,
          "deletions": 4,
          "patch": "@@ -177,23 +177,23 @@ TEST(StatusGroup, AggregateWithMultipleErrorStatus) {\n \n TEST(Status, InvalidPayloadGetsIgnored) {\n   Status s = Status();\n-  s.SetPayload(\"Invalid\", \"Invalid Val\");\n+  s.SetPayload(\"Invalid\", absl::Cord(\"Invalid Val\"));\n   ASSERT_FALSE(s.GetPayload(\"Invalid\").has_value());\n   bool is_err_erased = s.ErasePayload(\"Invalid\");\n   ASSERT_EQ(is_err_erased, false);\n }\n \n TEST(Status, SetPayloadSetsOrUpdatesIt) {\n   Status s(error::INTERNAL, \"Error message\");\n-  s.SetPayload(\"Error key\", \"Original\");\n+  s.SetPayload(\"Error key\", absl::Cord(\"Original\"));\n   ASSERT_EQ(s.GetPayload(\"Error key\"), absl::Cord(\"Original\"));\n-  s.SetPayload(\"Error key\", \"Updated\");\n+  s.SetPayload(\"Error key\", absl::Cord(\"Updated\"));\n   ASSERT_EQ(s.GetPayload(\"Error key\"), absl::Cord(\"Updated\"));\n }\n \n TEST(Status, ErasePayloadRemovesIt) {\n   Status s(error::INTERNAL, \"Error message\");\n-  s.SetPayload(\"Error key\", \"Original\");\n+  s.SetPayload(\"Error key\", absl::Cord(\"Original\"));\n \n   bool is_err_erased = s.ErasePayload(\"Error key\");\n   ASSERT_EQ(is_err_erased, true);"
        },
        {
          "filename": "tensorflow/core/platform/error_payloads.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -27,7 +27,7 @@ void OkOrSetErrorCounterPayload(\n     ErrorSourceProto error_source_proto;\n     error_source_proto.set_error_source(error_source);\n     status.SetPayload(tensorflow::kErrorSource,\n-                      error_source_proto.SerializeAsString());\n+                      absl::Cord(error_source_proto.SerializeAsString()));\n   }\n }\n "
        },
        {
          "filename": "tensorflow/core/tpu/kernels/tpu_compile_op_common.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -408,7 +408,7 @@ Status TpuCompileOpKernelCommon::ComputeInternal(OpKernelContext* ctx) {\n     SerializeToTString(proto, &output.scalar<tstring>()());\n     ctx->set_output(0, output);\n     status.SetPayload(TpuCompileInterface::kTpuCompileErrorPayloadKey,\n-                      output.scalar<tstring>()());\n+                      absl::Cord(output.scalar<tstring>()()));\n   }\n \n   if (status.ok()) {"
        },
        {
          "filename": "tensorflow/core/tpu/kernels/tpu_functional_ops.cc",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -1410,7 +1410,7 @@ Status TPUPartitionedCallOp::InitializeVarOnTPU(\n   TF_RETURN_IF_ERROR(\n       InstantiatePartition(*init_graph, fname, device, &fhandle, nullptr));\n \n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n   opts.step_container = ctx->step_container();\n   opts.cancellation_manager = ctx->cancellation_manager();\n   opts.stats_collector = ctx->stats_collector();\n@@ -1569,7 +1569,7 @@ Status TPUPartitionedCallOp::InitializeShardedVarOnTPU(\n     functions.push_back(DeviceAndFHandle{.device = target, .handle = handle});\n   }\n \n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n \n   // Blocking on threads in the same thread pool is disallowed because\n   // concurrent warm-up requests can exhaust the default thread pool.\n@@ -2702,7 +2702,7 @@ void TPUPartitionedCallOp::ExecuteFunctions(\n     const std::vector<DeviceAndFHandle>& functions, OpKernelContext* ctx,\n     int device_ordinal, int64_t ordinal_selector_req_id, DoneCallback done) {\n   profiler::TraceMe trace_me(\"TPUPartitionedCallOp-ExecuteFunctions\");\n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n   opts.step_container = ctx->step_container();\n   opts.stats_collector = ctx->stats_collector();\n   // TODO(akshayka): Consider selecting a runner on a per-device basis,"
        },
        {
          "filename": "tensorflow/core/tpu/tpu_embedding_errors.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -29,7 +29,8 @@ Status AppendTpuEmbeddingErrorPayload(Status obj) {\n         absl::StrCat(kTpuEmbeddingErrorMessage, \". \", obj.error_message());\n     Status status(obj.code(), error_message);\n     TPUEmbeddingError error_payload;\n-    status.SetPayload(kTpuEmbeddingErrorUrl, error_payload.SerializeAsString());\n+    status.SetPayload(kTpuEmbeddingErrorUrl,\n+                      absl::Cord(error_payload.SerializeAsString()));\n     return status;\n   }\n }"
        },
        {
          "filename": "tensorflow/core/tpu/tpu_embedding_errors.h",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -50,7 +50,8 @@ StatusOr<T> AppendTpuEmbeddingErrorPayload(StatusOr<T> obj) {\n         kTpuEmbeddingErrorMessage, \". \", obj.status().error_message());\n     Status status(obj.status().code(), error_message);\n     TPUEmbeddingError error_payload;\n-    status.SetPayload(kTpuEmbeddingErrorUrl, error_payload.SerializeAsString());\n+    status.SetPayload(kTpuEmbeddingErrorUrl,\n+                      absl::Cord(error_payload.SerializeAsString()));\n     return status;\n   }\n }"
        },
        {
          "filename": "tensorflow/core/util/zen_util.h",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n \n namespace tensorflow {\n \n-int64_t GetMempool() {\n+inline int64_t GetMempool() {\n   static absl::once_flag once;\n   static int64_t mempool = 1;\n   absl::call_once(once, [&] {\n@@ -34,7 +34,7 @@ int64_t GetMempool() {\n   return mempool;\n }\n \n-bool IsBlockedFormatEnabled() {\n+inline bool IsBlockedFormatEnabled() {\n   static absl::once_flag once;\n   static bool blocked_format = false;\n   absl::call_once(once, [&] {"
        },
        {
          "filename": "tensorflow/lite/delegates/xnnpack/README.md",
          "status": "modified",
          "additions": 24,
          "deletions": 173,
          "patch": "@@ -454,23 +454,34 @@ Below is the list of currently supported floating-point operators:\n * Output size, filter and bias (if present) must be static (use\n   `kTfLiteMmapRo` allocation type).\n \n-### Floating-Point (IEEE FP16) Operators (experimental)\n+### Floating-Point (IEEE FP16) Operators\n \n XNNPACK supports half-precision (using IEEE FP16 format) inference for a subset\n of floating-point operators. XNNPACK automatically enables half-precision\n inference when the following conditions are met:\n \n * XNNPACK runs on hardware that natively supports computations in IEEE FP16\n-format. Currently, this hardware is limited to ARM64 devices with ARMv8.2 FP16\n-arithmetics extension, and includes Android phones starting with Pixel 3,\n-Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with A11 or\n-newer SoCs, and all Apple Silicon Macs.\n+format. Currently, this hardware is limited to ARM & ARM64 devices with\n+ARMv8.2 FP16 arithmetics extension, and includes Android phones starting with\n+Pixel 3, Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with\n+A11 or newer SoCs, all Apple Silicon Macs, and Windows ARM64 laptops based with\n+Snapdragon 850 SoC or newer.\n \n * IEEE FP16 inference is supported for every floating-point operator in the\n model.\n \n * The model's \"reduced_precision_support\" metadata indicates that the model\n-is compatible with FP16 inference.\n+is compatible with FP16 inference. The metadata can be added during model\n+conversion using the `_experimental_supported_accumulation_type` attribute\n+of the [tf.lite.TargetSpec](https://www.tensorflow.org/api_docs/python/tf/lite/TargetSpec)\n+object:\n+\n+```python\n+converter.optimizations = [tf.lite.Optimize.DEFAULT]\n+...\n+converter.target_spec.supported_types = [tf.float16]\n+converter.target_spec._experimental_supported_accumulation_type = tf.dtypes.float16\n+```\n \n When the above conditions are met, XNNPACK replace FP32 operators with their\n FP16 equivalents, and insert additional operators to convert model inputs\n@@ -486,7 +497,7 @@ is used. Forcing FP16 inference has several effects:\n * Besides ARM64 devices with ARMv8.2 FP16 arithmetics extension, forced FP16\n inference is supported on x86/x86-64 devices with AVX2 extension in emulation\n mode: all elementary floating-point operations are computed in FP32, then\n-converted to FP16 and back to FP32. Note that such simulation is not exactly\n+converted to FP16 and back to FP32. Note that such simulation is not bit-exact\n equivalent to native FP16 inference, but simulates the effects of restricted\n mantissa precision and exponent range in the native FP16 arithmetics.\n \n@@ -512,171 +523,10 @@ TfLiteDelegate* xnnpack_delegate =\n     TfLiteXNNPackDelegateCreate(&xnnpack_options);\n ```\n \n-Below is the list of operators supported in IEEE FP16 inference:\n-\n-#### `ABS`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `ADD`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `AVERAGE_POOL_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CEIL`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CONV_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CONCATENATION`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `DEPTH_TO_SPACE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `DEPTHWISE_CONV_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `DIV`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `FLOOR`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `FULLY_CONNECTED`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `HARD_SWISH`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `LEAKY_RELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `LOGISTIC`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MAX_POOL_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MAXIMUM`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `MEAN`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MINIMUM`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `MUL`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `NEG`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `PAD`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `PRELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU6`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU_N1_TO_1`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RESHAPE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RESIZE_BILINEAR`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `ROUND`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SLICE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SOFTMAX`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SPACE_TO_DEPTH`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SPLIT`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQRT`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQUARE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQUARED_DIFFERENCE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `STRIDED_SLICE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SUB`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `TRANSPOSE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `TRANSPOSE_CONV`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n+XNNPACK has full feature parity between FP32 and FP16 operators: all operators\n+that are supported for FP32 inference are also supported for FP16 inference,\n+and vice versa. In particular, sparse inference operators are supported for FP16\n+inference on ARM processors.\n \n ### Quantized Operators\n \n@@ -855,7 +705,8 @@ Below is the list of currently supported quantized operators:\n \n XNNPACK backend supports sparse inference for CNN models described in the\n [Fast Sparse ConvNets](https://arxiv.org/abs/1911.09723) paper. Sparse\n-inference is restricted to subgraphs with the following operators:\n+inference is restricted to subgraphs with the following floating-point\n+operators:\n \n * Sparse subgraph must store its weights in sparse representation (using\n   `DENSIFY` operators in the TensorFlow Lite schema)."
        },
        {
          "filename": "tensorflow/python/data/experimental/kernel_tests/service/snapshot_ft_test.py",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -131,14 +131,16 @@ def testSnapshotRecoveryFailsWithOutOfBoundsSourceName(self):\n   def testSnapshotRecoveryFailsWithBadSplitNames(self, bad_split_filename):\n     cluster, _ = self.setup()\n     write_file(os.path.join(self.source_dir(), bad_split_filename))\n-    with self.assertRaisesRegex(ValueError, \"can't parse\"):\n+    with self.assertRaisesRegex(\n+        ValueError, \"Expected split_<local_split_index>_<global_split_index>\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfOrderSplitName(self):\n     cluster, _ = self.setup()\n     write_file(os.path.join(self.source_dir(), \"split_1_0\"))\n-    with self.assertRaisesRegex(ValueError, \"found conflict\"):\n+    with self.assertRaisesRegex(\n+        ValueError, \"The local split index 1 exceeds the global split index 0\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())"
        },
        {
          "filename": "tensorflow/python/framework/BUILD",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "patch": "@@ -1094,6 +1094,8 @@ py_library(\n     name = \"tensor_conversion_registry\",\n     srcs = [\"tensor_conversion_registry.py\"],\n     srcs_version = \"PY3\",\n+    # TODO(b/266747022): remove extra visibility\n+    visibility = visibility + [\"//learning/brain/experimental:__subpackages__\"],\n     deps = [\n         \"//tensorflow/python/eager:context\",\n     ],"
        },
        {
          "filename": "tensorflow/python/framework/errors_test_helper.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -21,8 +21,8 @@ PYBIND11_MODULE(_errors_test_helper, m) {\n   m.def(\"TestRaiseFromStatus\", [](int code) {\n     tensorflow::Status status(static_cast<tensorflow::error::Code>(code),\n                               \"test message\");\n-    status.SetPayload(\"key1\", \"value1\");\n-    status.SetPayload(\"key2\", \"value2\");\n+    status.SetPayload(\"key1\", absl::Cord(\"value1\"));\n+    status.SetPayload(\"key2\", absl::Cord(\"value2\"));\n     MaybeRaiseRegisteredFromStatus(status);\n     return 0;\n   });"
        },
        {
          "filename": "tensorflow/python/tpu/tpu_strategy_util.py",
          "status": "modified",
          "additions": 29,
          "deletions": 3,
          "patch": "@@ -22,6 +22,8 @@\n from tensorflow.python.eager import context\n from tensorflow.python.eager import monitoring\n from tensorflow.python.eager.def_function import function\n+from tensorflow.python.eager.def_function import functions_run_eagerly\n+from tensorflow.python.eager.def_function import run_functions_eagerly\n from tensorflow.python.framework import device\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n@@ -111,6 +113,15 @@ def _tpu_init_fn():\n     # The TPU_SYSTEM device must match the device used in tpu.initialize_system\n     # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n     # devices available.\n+    run_eagerly = functions_run_eagerly()\n+    if run_eagerly:\n+      logging.warning(\n+          \"It looks like tf.function behavior was disabled, perhaps using\"\n+          \" tf.config.run_functions_eagerly.\"\n+          \" tf.tpu.experimental.initialize_tpu_system requires tf.function to\"\n+          \" work. This primitive will override the disable.\"\n+      )\n+    run_functions_eagerly(False)\n     try:\n       with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n         output = _tpu_init_fn()\n@@ -120,7 +131,9 @@ def _tpu_init_fn():\n           None, None,\n           \"TPUs not found in the cluster. Failed in initialization: \"\n           + str(e))\n-\n+    finally:\n+      if run_eagerly is not None:\n+        run_functions_eagerly(run_eagerly)\n     # Clear out the eager context caches since the memory is invalid now.\n     context.context()._initialize_logical_devices()  # pylint: disable=protected-access\n \n@@ -221,8 +234,21 @@ def _tpu_shutdown_fn():\n     # The TPU_SYSTEM device must match the device used in tpu.shutdown_system\n     # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n     # devices available.\n-    with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n-      _tpu_shutdown_fn()\n+    run_eagerly = functions_run_eagerly()\n+    if run_eagerly:\n+      logging.warning(\n+          \"It looks like tf.function behavior was disabled, perhaps using\"\n+          \" tf.config.run_functions_eagerly.\"\n+          \" tf.tpu.experimental.shutdown_tpu_system requires tf.function to\"\n+          \" work. This primitive will override the disable.\"\n+      )\n+    run_functions_eagerly(False)\n+    try:\n+      with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n+        _tpu_shutdown_fn()\n+    finally:\n+      if run_eagerly is not None:\n+        run_functions_eagerly(run_eagerly)\n \n     # Clear out the eager context caches since the memory is invalid now.\n     logging.info(\"Clearing out eager caches\")"
        },
        {
          "filename": "tensorflow/tools/ci_build/release/requirements_common.txt",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -8,14 +8,14 @@ google_pasta ~= 0.2\n h5py ~= 3.8.0  # Earliest version for Python 3.11\n # TODO(b/262592253): Support older versions of NumPy for Python 3.10 and lower\n # to support TFX. Remove when Apache Beam upgrades to newer NumPy.\n-numpy ~= 1.21.4; python_version < '3.11'\n+numpy ~= 1.22.0; python_version < '3.11'\n numpy ~= 1.23.2; python_version >= '3.11' # Earliest version for Python 3.11\n opt_einsum ~= 3.3.0\n protobuf ~= 3.19.3  # NOTE: Earliest version for Python 3.10\n six ~= 1.16.0\n termcolor ~= 2.1.1\n typing_extensions ~= 3.10.0.0\n-wheel ~= 0.36.2\n+wheel ~= 0.38.1\n wrapt ~= 1.14.1\n \n # We need to pin the gast dependency exactly\n@@ -37,4 +37,4 @@ scipy ~= 1.9.2; python_version >= '3.11' # Earliest version for Python 3.11\n \n # This is usually vendored in setuptools but ensure it gets installed in CI anyway\n # No bound here, we prefer the one in setuptools\n-packaging\n\\ No newline at end of file\n+packaging"
        },
        {
          "filename": "tensorflow/tools/ci_build/release/requirements_mac.txt",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -1,7 +1,7 @@\n -r requirements_common.txt\n \n # Dependencies only required for Mac\n-certifi ~= 2020.12.5\n+certifi ~= 2022.12.07\n \n # Install build related dependencies\n twine ~= 3.6.0"
        },
        {
          "filename": "tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -11,15 +11,15 @@ google_pasta ~= 0.2\n h5py ~= 3.8.0 # Earliest version for Python 3.11\n # TODO(b/262592253): Support older versions of NumPy for Python 3.10 and lower\n # to support TFX. Remove when Apache Beam upgrades to newer NumPy.\n-numpy ~= 1.21.4; python_version < '3.11'\n+numpy ~= 1.22.0; python_version < '3.11'\n numpy ~= 1.23.2; python_version >= '3.11' # Earliest version for Python 3.11\n opt_einsum ~= 3.3.0\n packaging ~= 21.3\n protobuf ~= 3.20.1\n six ~= 1.16.0\n termcolor ~= 2.1.1\n typing_extensions ~= 3.10.0.0\n-wheel ~= 0.36.2\n+wheel ~= 0.38.1\n wrapt ~= 1.14.1\n # We need to pin the gast dependency exactly\n gast == 0.4.0\n@@ -48,4 +48,4 @@ twine ~= 3.6.0\n # For user tool scripts\n junitparser ~= 2.2.0\n lxml ~= 4.9.1\n-pylint ~= 2.13.9\n\\ No newline at end of file\n+pylint ~= 2.13.9"
        },
        {
          "filename": "tensorflow/tsl/c/tsl_status.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -36,7 +36,7 @@ void TSL_SetStatus(TSL_Status* s, TSL_Code code, const char* msg) {\n }\n \n void TSL_SetPayload(TSL_Status* s, const char* key, const char* value) {\n-  s->status.SetPayload(key, value);\n+  s->status.SetPayload(key, absl::Cord(absl::string_view(value)));\n }\n \n void TSL_SetStatusFromIOError(TSL_Status* s, int error_code,"
        },
        {
          "filename": "tensorflow/tsl/c/tsl_status_helper_test.cc",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -24,8 +24,8 @@ namespace {\n TEST(StatusHelper, TestStatusHelper) {\n   TSL_Status* s = TSL_NewStatus();\n   Status cc_status(errors::InvalidArgument(\"some error\"));\n-  cc_status.SetPayload(\"key1\", \"value1\");\n-  cc_status.SetPayload(\"key2\", \"value2\");\n+  cc_status.SetPayload(\"key1\", absl::Cord(\"value1\"));\n+  cc_status.SetPayload(\"key2\", absl::Cord(\"value2\"));\n   Set_TSL_Status_from_Status(s, cc_status);\n   ASSERT_EQ(TSL_INVALID_ARGUMENT, TSL_GetCode(s));\n   ASSERT_EQ(std::string(\"some error\"), TSL_Message(s));"
        },
        {
          "filename": "tensorflow/tsl/distributed_runtime/coordination/coordination_service_error_util.h",
          "status": "modified",
          "additions": 5,
          "deletions": 3,
          "patch": "@@ -29,7 +29,7 @@ constexpr absl::string_view CoordinationErrorPayloadKey() {\n // Mark error as a coordination service error (as opposed to RPC\n // errors).\n inline Status MakeCoordinationError(Status s) {\n-  s.SetPayload(CoordinationErrorPayloadKey(), \"\");\n+  s.SetPayload(CoordinationErrorPayloadKey(), absl::Cord(\"\"));\n   return s;\n }\n \n@@ -43,14 +43,16 @@ inline Status MakeCoordinationError(Status s,\n   tensorflow::CoordinationServiceError error;\n   *error.mutable_source_task() = origin;\n   error.set_is_reported_error(is_reported_error);\n-  s.SetPayload(CoordinationErrorPayloadKey(), error.SerializeAsString());\n+  s.SetPayload(CoordinationErrorPayloadKey(),\n+               absl::Cord(error.SerializeAsString()));\n   return s;\n }\n \n // Mark error as a coordination service error with payload.\n inline Status MakeCoordinationError(\n     Status s, const tensorflow::CoordinationServiceError& payload) {\n-  s.SetPayload(CoordinationErrorPayloadKey(), payload.SerializeAsString());\n+  s.SetPayload(CoordinationErrorPayloadKey(),\n+               absl::Cord(payload.SerializeAsString()));\n   return s;\n }\n }  // namespace tsl"
        },
        {
          "filename": "tensorflow/tsl/distributed_runtime/rpc/grpc_util.h",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -71,12 +71,12 @@ inline void InsertSerializedPayloads(Status& s, std::string payloads) {\n   tensorflow::distributed_runtime::GrpcPayloadContainer container;\n   if (container.ParseFromString(payloads)) {\n     for (const auto& key_val : container.payloads()) {\n-      s.SetPayload(key_val.first, key_val.second);\n+      s.SetPayload(key_val.first, absl::Cord(key_val.second));\n     }\n   } else {\n     s.SetPayload(kGrpcPayloadsLost,\n-                 tensorflow::distributed_runtime::GrpcPayloadsLost()\n-                     .SerializeAsString());\n+                 absl::Cord(tensorflow::distributed_runtime::GrpcPayloadsLost()\n+                                .SerializeAsString()));\n   }\n }\n "
        },
        {
          "filename": "tensorflow/tsl/distributed_runtime/rpc/grpc_util_test.cc",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -71,7 +71,7 @@ TestRequest MakeProto(int size) {\n \n TEST(PayloadSerialization, PayloadsAreTransmitted) {\n   Status status = errors::InvalidArgument(\"invalid arg message\");\n-  status.SetPayload(\"a\", \"\\\\xFF\\\\x02\\\\x03\");\n+  status.SetPayload(\"a\", absl::Cord(\"\\\\xFF\\\\x02\\\\x03\"));\n   Status status_recovered = FromGrpcStatus(ToGrpcStatus(status));\n \n   ASSERT_TRUE(status_recovered.GetPayload(\"a\").has_value());"
        },
        {
          "filename": "tensorflow/tsl/platform/errors.h",
          "status": "modified",
          "additions": 3,
          "deletions": 2,
          "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/base/attributes.h\"\n+#include \"absl/strings/cord.h\"\n #include \"absl/strings/str_join.h\"\n #include \"tensorflow/tsl/platform/logging.h\"\n #include \"tensorflow/tsl/platform/macros.h\"\n@@ -102,15 +103,15 @@ inline void InsertPayloads(\n     ::tsl::Status& status,\n     const std::unordered_map<std::string, std::string>& payloads) {\n   for (const auto& payload : payloads) {\n-    status.SetPayload(payload.first, payload.second);\n+    status.SetPayload(payload.first, absl::Cord(payload.second));\n   }\n }\n \n // Copies all payloads from one Status to another. Will overwrite existing\n // payloads in the destination if they exist with the same key.\n inline void CopyPayloads(const ::tsl::Status& from, ::tsl::Status& to) {\n   from.ForEachPayload([&to](tsl::StringPiece key, tsl::StringPiece value) {\n-    to.SetPayload(key, value);\n+    to.SetPayload(key, absl::Cord(value));\n   });\n }\n "
        },
        {
          "filename": "tensorflow/tsl/platform/test.h",
          "status": "modified",
          "additions": 7,
          "deletions": 0,
          "patch": "@@ -85,6 +85,13 @@ int RandomSeed();\n // NOTE: This function is not thread-safe.\n int PickUnusedPortOrDie();\n \n+// Constant which is false internally and true in open source.\n+#ifdef PLATFORM_GOOGLE\n+inline constexpr bool kIsOpenSource = false;\n+#else\n+inline constexpr bool kIsOpenSource = true;\n+#endif  // PLATFORM_GOOGLE\n+\n }  // namespace testing\n }  // namespace tsl\n "
        },
        {
          "filename": "tensorflow/tsl/profiler/lib/BUILD",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "patch": "@@ -213,6 +213,7 @@ cc_library(\n     deps = [\n         \"//tensorflow/tsl/platform:logging\",\n         \"//tensorflow/tsl/platform:macros\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/strings\",\n     ],\n )\n@@ -226,6 +227,7 @@ tsl_cc_test(\n         \"//tensorflow/tsl/platform:test\",\n         \"//tensorflow/tsl/platform:test_main\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n     ],\n )\n "
        },
        {
          "filename": "tensorflow/tsl/profiler/lib/traceme_encode.h",
          "status": "modified",
          "additions": 13,
          "deletions": 5,
          "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <initializer_list>\n #include <string>\n \n+#include \"absl/base/attributes.h\"\n #include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n@@ -31,14 +32,21 @@ namespace profiler {\n \n // An argument passed to TraceMeEncode.\n struct TraceMeArg {\n-  // This constructor is required because absl::AlphaNum is non-copyable.\n-  template <typename Value>\n-  TraceMeArg(absl::string_view k, Value v) : key(k), value(v) {}\n+  // String conversions of value types are supported via AlphaNum. We keep a\n+  // reference to the AlphaNum's internal buffer here, so it must remain valid\n+  // for the lifetime of this object. We cannot store it by value because it is\n+  // not safe to construct an AlphaNum as a member of a class, particularly when\n+  // AbslStringify is being used (it may reference default arguments that are on\n+  // the caller's stack, if we constructed it here those default arguments would\n+  // be destroyed before they are used).\n+  TraceMeArg(absl::string_view k,\n+             const absl::AlphaNum& v ABSL_ATTRIBUTE_LIFETIME_BOUND)\n+      : key(k), value(v.Piece()) {}\n \n   TF_DISALLOW_COPY_AND_ASSIGN(TraceMeArg);\n \n   absl::string_view key;\n-  absl::AlphaNum value;\n+  absl::string_view value;\n };\n \n namespace traceme_internal {\n@@ -74,7 +82,7 @@ TF_ATTRIBUTE_ALWAYS_INLINE inline std::string AppendArgs(\n     for (const auto& arg : args) {\n       out = Append(out, arg.key);\n       *out++ = '=';\n-      out = Append(out, arg.value.Piece());\n+      out = Append(out, arg.value);\n       *out++ = ',';\n     }\n     *(out - 1) = '#';"
        },
        {
          "filename": "tensorflow/tsl/profiler/lib/traceme_encode_test.cc",
          "status": "modified",
          "additions": 22,
          "deletions": 0,
          "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #include <string>\n \n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"tensorflow/tsl/platform/platform.h\"\n #include \"tensorflow/tsl/platform/test.h\"\n \n@@ -53,6 +54,27 @@ TEST(TraceMeEncodeTest, TemporaryStringTest) {\n }\n #endif\n \n+// This can be removed when the absl version has been updated to include\n+// AbslStringify for open source builds.\n+#if defined(PLATFORM_GOOGLE)\n+\n+struct Point {\n+  template <typename Sink>\n+  friend void AbslStringify(Sink& sink, const Point& p) {\n+    absl::Format(&sink, \"(%d, %d)\", p.x, p.y);\n+  }\n+\n+  int x;\n+  int y;\n+};\n+\n+TEST(TraceMeEncodeTest, AbslStringifyTest) {\n+  EXPECT_EQ(TraceMeEncode(\"Plot\", {{\"point\", Point{10, 20}}}),\n+            \"Plot#point=(10, 20)#\");\n+}\n+\n+#endif\n+\n TEST(TraceMeEncodeTest, NoNameTest) {\n   EXPECT_EQ(TraceMeEncode({{\"context\", \"World\"}, {\"request_id\", 42}}),\n             \"#context=World,request_id=42#\");"
        },
        {
          "filename": "third_party/tf_runtime/workspace.bzl",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -6,8 +6,8 @@ def repo():\n     \"\"\"Imports TFRT.\"\"\"\n \n     # Attention: tools parse and update these lines.\n-    TFRT_COMMIT = \"c1248a015d23949afa2471bb21f6f52850aead7d\"\n-    TFRT_SHA256 = \"8cdd8ea905478ac4ffd36ffb39cebe288d3b840d71a02d418bc6a8a760f92af8\"\n+    TFRT_COMMIT = \"c653281a1a23c0c3d41536a983c7d10fcc5b1fbf\"\n+    TFRT_SHA256 = \"3d1edd27c4e36d9cfc9493aef7088489babb370d2a7955bab3545acfbb024ccf\"\n \n     tf_http_archive(\n         name = \"tf_runtime\","
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 1,
        "test_files": 10,
        "unique_directories": 38,
        "max_directory_depth": 6
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "ee156c15e5d1cd7d2bd85885e7fd2bf7e143c2c3",
            "date": "2025-01-14T12:46:22Z",
            "author_login": "pifon2a"
          },
          {
            "sha": "bd43a8255ce9e203b740bcdc09e8f79d3a26f887",
            "date": "2025-01-14T12:33:02Z",
            "author_login": "metaflow"
          },
          {
            "sha": "8003fb40987f176d35364da2af8fcdfab339349e",
            "date": "2025-01-14T11:27:47Z",
            "author_login": "vwbaker"
          },
          {
            "sha": "aeb438b3a08e82ffd347aebf68ea77c5aeb6784f",
            "date": "2025-01-14T10:32:36Z",
            "author_login": "WillFroom"
          },
          {
            "sha": "88136f4028f541553becd7701da1a63610d8079e",
            "date": "2025-01-14T09:58:36Z",
            "author_login": "loislo"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.5,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H",
    "cwe_id": "CWE-125",
    "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, an out of bounds read is in GRUBlockCellGrad. A fix is included in TensorFlow 2.12.0 and 2.11.1.\n",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2023-03-25T00:15:07.077",
    "last_modified": "2024-11-21T07:49:53.210",
    "fix_date": "2023-01-25T23:59:30Z"
  },
  "references": [
    {
      "url": "https://github.com/tensorflow/tensorflow/commit/ff459137c2716a2a60f7d441b855fcb466d778cb",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-68v3-g9cm-rmm6",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://github.com/tensorflow/tensorflow/commit/ff459137c2716a2a60f7d441b855fcb466d778cb",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-68v3-g9cm-rmm6",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:05:09.004992",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "tensorflow",
    "owner": "tensorflow",
    "created_at": "2015-11-07T01:19:20Z",
    "updated_at": "2025-01-14T12:53:26Z",
    "pushed_at": "2025-01-14T12:53:14Z",
    "size": 1120707,
    "stars": 187254,
    "forks": 74432,
    "open_issues": 6569,
    "watchers": 187254,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "C++": 101199988,
      "Python": 45779571,
      "MLIR": 10763008,
      "HTML": 7662661,
      "Starlark": 7430486,
      "Go": 2171370,
      "C": 1288066,
      "Java": 1178817,
      "Jupyter Notebook": 805736,
      "Shell": 701425,
      "Objective-C++": 279654,
      "Objective-C": 169202,
      "CMake": 148610,
      "Smarty": 121630,
      "Swift": 81659,
      "Dockerfile": 37903,
      "C#": 13585,
      "Batchfile": 12126,
      "Ruby": 8898,
      "Perl": 7536,
      "Roff": 5034,
      "Cython": 3899,
      "Makefile": 2845,
      "CSS": 2761,
      "Vim Snippet": 58
    },
    "commit_activity": {
      "total_commits_last_year": 15729,
      "avg_commits_per_week": 302.4807692307692,
      "days_active_last_year": 357
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": false,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-14T12:54:01.412891"
  }
}