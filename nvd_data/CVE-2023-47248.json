{
  "cve_id": "CVE-2023-47248",
  "github_data": {
    "repository": "apache/arrow",
    "fix_commit": "f14170976372436ec1d03a724d8d3f3925484ecf",
    "related_commits": [
      "f14170976372436ec1d03a724d8d3f3925484ecf",
      "f14170976372436ec1d03a724d8d3f3925484ecf"
    ],
    "patch_url": null,
    "fix_commit_details": {
      "sha": "f14170976372436ec1d03a724d8d3f3925484ecf",
      "commit_date": "2023-11-06T22:20:05Z",
      "author": {
        "login": "pitrou",
        "type": "User",
        "stats": {
          "total_commits": 1664,
          "average_weekly_commits": 3.5105485232067513,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 317
        }
      },
      "commit_message": {
        "title": "GH-38607: [Python] Disable PyExtensionType autoload (#38608)",
        "length": 679,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 622,
        "additions": 377,
        "deletions": 245
      },
      "files": [
        {
          "filename": "docs/source/python/extending_types.rst",
          "status": "modified",
          "additions": 58,
          "deletions": 91,
          "patch": "@@ -68,34 +68,43 @@ message).\n See the :ref:`format_metadata_extension_types` section of the metadata\n specification for more details.\n \n-Pyarrow allows you to define such extension types from Python.\n-\n-There are currently two ways:\n-\n-* Subclassing :class:`PyExtensionType`: the (de)serialization is based on pickle.\n-  This is a good option for an extension type that is only used from Python.\n-* Subclassing :class:`ExtensionType`: this allows to give a custom\n-  Python-independent name and serialized metadata, that can potentially be\n-  recognized by other (non-Python) Arrow implementations such as PySpark.\n+Pyarrow allows you to define such extension types from Python by subclassing\n+:class:`ExtensionType` and giving the derived class its own extension name\n+and serialization mechanism. The extension name and serialized metadata\n+can potentially be recognized by other (non-Python) Arrow implementations\n+such as PySpark.\n \n For example, we could define a custom UUID type for 128-bit numbers which can\n-be represented as ``FixedSizeBinary`` type with 16 bytes.\n-Using the first approach, we create a ``UuidType`` subclass, and implement the\n-``__reduce__`` method to ensure the class can be properly pickled::\n+be represented as ``FixedSizeBinary`` type with 16 bytes::\n \n-    class UuidType(pa.PyExtensionType):\n+    class UuidType(pa.ExtensionType):\n \n         def __init__(self):\n-            pa.PyExtensionType.__init__(self, pa.binary(16))\n+            super().__init__(pa.binary(16), \"my_package.uuid\")\n+\n+        def __arrow_ext_serialize__(self):\n+            # Since we don't have a parameterized type, we don't need extra\n+            # metadata to be deserialized\n+            return b''\n \n-        def __reduce__(self):\n-            return UuidType, ()\n+        @classmethod\n+        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+            # Sanity checks, not required but illustrate the method signature.\n+            assert storage_type == pa.binary(16)\n+            assert serialized == b''\n+            # Return an instance of this subclass given the serialized\n+            # metadata.\n+            return UuidType()\n+\n+The special methods ``__arrow_ext_serialize__`` and ``__arrow_ext_deserialize__``\n+define the serialization of an extension type instance. For non-parametric\n+types such as the above, the serialization payload can be left empty.\n \n This can now be used to create arrays and tables holding the extension type::\n \n     >>> uuid_type = UuidType()\n     >>> uuid_type.extension_name\n-    'arrow.py_extension_type'\n+    'my_package.uuid'\n     >>> uuid_type.storage_type\n     FixedSizeBinaryType(fixed_size_binary[16])\n \n@@ -112,8 +121,11 @@ This can now be used to create arrays and tables holding the extension type::\n     ]\n \n This array can be included in RecordBatches, sent over IPC and received in\n-another Python process. The custom UUID type will be preserved there, as long\n-as the definition of the class is available (the type can be unpickled).\n+another Python process. The receiving process must explicitly register the\n+extension type for deserialization, otherwise it will fall back to the\n+storage type::\n+\n+    >>> pa.register_extension_type(UuidType())\n \n For example, creating a RecordBatch and writing it to a stream using the\n IPC protocol::\n@@ -129,43 +141,12 @@ and then reading it back yields the proper type::\n     >>> with pa.ipc.open_stream(buf) as reader:\n     ...    result = reader.read_all()\n     >>> result.column('ext').type\n-    UuidType(extension<arrow.py_extension_type>)\n-\n-We can define the same type using the other option::\n-\n-    class UuidType(pa.ExtensionType):\n-\n-        def __init__(self):\n-            pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n-\n-        def __arrow_ext_serialize__(self):\n-            # since we don't have a parameterized type, we don't need extra\n-            # metadata to be deserialized\n-            return b''\n-\n-        @classmethod\n-        def __arrow_ext_deserialize__(self, storage_type, serialized):\n-            # return an instance of this subclass given the serialized\n-            # metadata.\n-            return UuidType()\n-\n-This is a slightly longer implementation (you need to implement the special\n-methods ``__arrow_ext_serialize__`` and ``__arrow_ext_deserialize__``), and the\n-extension type needs to be registered to be received through IPC (using\n-:func:`register_extension_type`), but it has\n-now a unique name::\n-\n-    >>> uuid_type = UuidType()\n-    >>> uuid_type.extension_name\n-    'my_package.uuid'\n-\n-    >>> pa.register_extension_type(uuid_type)\n+    UuidType(FixedSizeBinaryType(fixed_size_binary[16]))\n \n The receiving application doesn't need to be Python but can still recognize\n-the extension type as a \"uuid\" type, if it has implemented its own extension\n-type to receive it.\n-If the type is not registered in the receiving application, it will fall back\n-to the storage type.\n+the extension type as a \"my_package.uuid\" type, if it has implemented its own\n+extension type to receive it. If the type is not registered in the receiving\n+application, it will fall back to the storage type.\n \n Parameterized extension type\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -187,7 +168,7 @@ of the given frequency since 1970.\n             # attributes need to be set first before calling\n             # super init (as that calls serialize)\n             self._freq = freq\n-            pa.ExtensionType.__init__(self, pa.int64(), 'my_package.period')\n+            super().__init__(pa.int64(), 'my_package.period')\n \n         @property\n         def freq(self):\n@@ -198,7 +179,7 @@ of the given frequency since 1970.\n \n         @classmethod\n         def __arrow_ext_deserialize__(cls, storage_type, serialized):\n-            # return an instance of this subclass given the serialized\n+            # Return an instance of this subclass given the serialized\n             # metadata.\n             serialized = serialized.decode()\n             assert serialized.startswith(\"freq=\")\n@@ -209,31 +190,10 @@ Here, we ensure to store all information in the serialized metadata that is\n needed to reconstruct the instance (in the ``__arrow_ext_deserialize__`` class\n method), in this case the frequency string.\n \n-Note that, once created, the data type instance is considered immutable. If,\n-in the example above, the ``freq`` parameter would change after instantiation,\n-the reconstruction of the type instance after IPC will be incorrect.\n+Note that, once created, the data type instance is considered immutable.\n In the example above, the ``freq`` parameter is therefore stored in a private\n attribute with a public read-only property to access it.\n \n-Parameterized extension types are also possible using the pickle-based type\n-subclassing :class:`PyExtensionType`. The equivalent example for the period\n-data type from above would look like::\n-\n-    class PeriodType(pa.PyExtensionType):\n-\n-        def __init__(self, freq):\n-            self._freq = freq\n-            pa.PyExtensionType.__init__(self, pa.int64())\n-\n-        @property\n-        def freq(self):\n-            return self._freq\n-\n-        def __reduce__(self):\n-            return PeriodType, (self.freq,)\n-\n-Also the storage type does not need to be fixed but can be parameterized.\n-\n Custom extension array class\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n@@ -252,12 +212,16 @@ the data as a 2-D Numpy array ``(N, 3)`` without any copy::\n             return self.storage.flatten().to_numpy().reshape((-1, 3))\n \n \n-    class Point3DType(pa.PyExtensionType):\n+    class Point3DType(pa.ExtensionType):\n         def __init__(self):\n-            pa.PyExtensionType.__init__(self, pa.list_(pa.float32(), 3))\n+            super().__init__(pa.list_(pa.float32(), 3), \"my_package.Point3DType\")\n \n-        def __reduce__(self):\n-            return Point3DType, ()\n+        def __arrow_ext_serialize__(self):\n+            return b''\n+\n+        @classmethod\n+        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+            return Point3DType()\n \n         def __arrow_ext_class__(self):\n             return Point3DArray\n@@ -289,11 +253,8 @@ The additional methods in the extension class are then available to the user::\n \n \n This array can be sent over IPC, received in another Python process, and the custom\n-extension array class will be preserved (as long as the definitions of the classes above\n-are available).\n-\n-The same ``__arrow_ext_class__`` specialization can be used with custom types defined\n-by subclassing :class:`ExtensionType`.\n+extension array class will be preserved (as long as the receiving process registers\n+the extension type using :func:`register_extension_type` before reading the IPC data).\n \n Custom scalar conversion\n ~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -304,18 +265,24 @@ If you want scalars of your custom extension type to convert to a custom type wh\n For example, if we wanted the above example 3D point type to return a custom\n 3D point class instead of a list, we would implement::\n \n+    from collections import namedtuple\n+\n     Point3D = namedtuple(\"Point3D\", [\"x\", \"y\", \"z\"])\n \n     class Point3DScalar(pa.ExtensionScalar):\n         def as_py(self) -> Point3D:\n             return Point3D(*self.value.as_py())\n \n-    class Point3DType(pa.PyExtensionType):\n+    class Point3DType(pa.ExtensionType):\n         def __init__(self):\n-            pa.PyExtensionType.__init__(self, pa.list_(pa.float32(), 3))\n+            super().__init__(pa.list_(pa.float32(), 3), \"my_package.Point3DType\")\n \n-        def __reduce__(self):\n-            return Point3DType, ()\n+        def __arrow_ext_serialize__(self):\n+            return b''\n+\n+        @classmethod\n+        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+            return Point3DType()\n \n         def __arrow_ext_scalar_class__(self):\n             return Point3DScalar"
        },
        {
          "filename": "python/pyarrow/tests/test_cffi.py",
          "status": "modified",
          "additions": 40,
          "deletions": 8,
          "patch": "@@ -16,6 +16,7 @@\n # specific language governing permissions and limitations\n # under the License.\n \n+import contextlib\n import ctypes\n import gc\n \n@@ -51,18 +52,33 @@ def PyCapsule_IsValid(capsule, name):\n     return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1\n \n \n-class ParamExtType(pa.PyExtensionType):\n+@contextlib.contextmanager\n+def registered_extension_type(ext_type):\n+    pa.register_extension_type(ext_type)\n+    try:\n+        yield\n+    finally:\n+        pa.unregister_extension_type(ext_type.extension_name)\n+\n+\n+class ParamExtType(pa.ExtensionType):\n \n     def __init__(self, width):\n         self._width = width\n-        pa.PyExtensionType.__init__(self, pa.binary(width))\n+        super().__init__(pa.binary(width),\n+                         \"pyarrow.tests.test_cffi.ParamExtType\")\n \n     @property\n     def width(self):\n         return self._width\n \n-    def __reduce__(self):\n-        return ParamExtType, (self.width,)\n+    def __arrow_ext_serialize__(self):\n+        return str(self.width).encode()\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        width = int(serialized.decode())\n+        return cls(width)\n \n \n def make_schema():\n@@ -75,6 +91,12 @@ def make_extension_schema():\n                      metadata={b'key1': b'value1'})\n \n \n+def make_extension_storage_schema():\n+    # Should be kept in sync with make_extension_schema\n+    return pa.schema([('ext', ParamExtType(3).storage_type)],\n+                     metadata={b'key1': b'value1'})\n+\n+\n def make_batch():\n     return pa.record_batch([[[1], [2, 42]]], make_schema())\n \n@@ -204,7 +226,10 @@ def test_export_import_array():\n         pa.Array._import_from_c(ptr_array, ptr_schema)\n \n \n-def check_export_import_schema(schema_factory):\n+def check_export_import_schema(schema_factory, expected_schema_factory=None):\n+    if expected_schema_factory is None:\n+        expected_schema_factory = schema_factory\n+\n     c_schema = ffi.new(\"struct ArrowSchema*\")\n     ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n \n@@ -215,7 +240,7 @@ def check_export_import_schema(schema_factory):\n     assert pa.total_allocated_bytes() > old_allocated\n     # Delete and recreate C++ object from exported pointer\n     schema_new = pa.Schema._import_from_c(ptr_schema)\n-    assert schema_new == schema_factory()\n+    assert schema_new == expected_schema_factory()\n     assert pa.total_allocated_bytes() == old_allocated\n     del schema_new\n     assert pa.total_allocated_bytes() == old_allocated\n@@ -240,7 +265,13 @@ def test_export_import_schema():\n \n @needs_cffi\n def test_export_import_schema_with_extension():\n-    check_export_import_schema(make_extension_schema)\n+    # Extension type is unregistered => the storage type is imported\n+    check_export_import_schema(make_extension_schema,\n+                               make_extension_storage_schema)\n+\n+    # Extension type is registered => the extension type is imported\n+    with registered_extension_type(ParamExtType(1)):\n+        check_export_import_schema(make_extension_schema)\n \n \n @needs_cffi\n@@ -319,7 +350,8 @@ def test_export_import_batch():\n \n @needs_cffi\n def test_export_import_batch_with_extension():\n-    check_export_import_batch(make_extension_batch)\n+    with registered_extension_type(ParamExtType(1)):\n+        check_export_import_batch(make_extension_batch)\n \n \n def _export_import_batch_reader(ptr_stream, reader_factory):"
        },
        {
          "filename": "python/pyarrow/tests/test_extension_type.py",
          "status": "modified",
          "additions": 220,
          "deletions": 93,
          "patch": "@@ -15,6 +15,7 @@\n # specific language governing permissions and limitations\n # under the License.\n \n+import contextlib\n import os\n import shutil\n import subprocess\n@@ -29,113 +30,195 @@\n import pytest\n \n \n-class TinyIntType(pa.PyExtensionType):\n+@contextlib.contextmanager\n+def registered_extension_type(ext_type):\n+    pa.register_extension_type(ext_type)\n+    try:\n+        yield\n+    finally:\n+        pa.unregister_extension_type(ext_type.extension_name)\n+\n+\n+@contextlib.contextmanager\n+def enabled_auto_load():\n+    pa.PyExtensionType.set_auto_load(True)\n+    try:\n+        yield\n+    finally:\n+        pa.PyExtensionType.set_auto_load(False)\n+\n+\n+class TinyIntType(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, pa.int8())\n+        super().__init__(pa.int8(), 'pyarrow.tests.TinyIntType')\n \n-    def __reduce__(self):\n-        return TinyIntType, ()\n+    def __arrow_ext_serialize__(self):\n+        return b''\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        assert serialized == b''\n+        assert storage_type == pa.int8()\n+        return cls()\n \n \n-class IntegerType(pa.PyExtensionType):\n+class IntegerType(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, pa.int64())\n+        super().__init__(pa.int64(), 'pyarrow.tests.IntegerType')\n \n-    def __reduce__(self):\n-        return IntegerType, ()\n+    def __arrow_ext_serialize__(self):\n+        return b''\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        assert serialized == b''\n+        assert storage_type == pa.int64()\n+        return cls()\n \n \n-class IntegerEmbeddedType(pa.PyExtensionType):\n+class IntegerEmbeddedType(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, IntegerType())\n+        super().__init__(IntegerType(), 'pyarrow.tests.IntegerType')\n \n-    def __reduce__(self):\n-        return IntegerEmbeddedType, ()\n+    def __arrow_ext_serialize__(self):\n+        # XXX pa.BaseExtensionType should expose C++ serialization method\n+        return self.storage_type.__arrow_ext_serialize__()\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        deserialized_storage_type = storage_type.__arrow_ext_deserialize__(\n+            serialized)\n+        assert deserialized_storage_type == storage_type\n+        return cls()\n \n \n class UuidScalarType(pa.ExtensionScalar):\n     def as_py(self):\n         return None if self.value is None else UUID(bytes=self.value.as_py())\n \n \n-class UuidType(pa.PyExtensionType):\n+class UuidType(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, pa.binary(16))\n-\n-    def __reduce__(self):\n-        return UuidType, ()\n+        super().__init__(pa.binary(16), 'pyarrow.tests.UuidType')\n \n     def __arrow_ext_scalar_class__(self):\n         return UuidScalarType\n \n+    def __arrow_ext_serialize__(self):\n+        return b''\n \n-class UuidType2(pa.PyExtensionType):\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        return cls()\n+\n+\n+class UuidType2(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, pa.binary(16))\n+        super().__init__(pa.binary(16), 'pyarrow.tests.UuidType2')\n \n-    def __reduce__(self):\n-        return UuidType2, ()\n+    def __arrow_ext_serialize__(self):\n+        return b''\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        return cls()\n \n \n-class LabelType(pa.PyExtensionType):\n+class LabelType(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, pa.string())\n+        super().__init__(pa.string(), 'pyarrow.tests.LabelType')\n \n-    def __reduce__(self):\n-        return LabelType, ()\n+    def __arrow_ext_serialize__(self):\n+        return b''\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        return cls()\n \n \n-class ParamExtType(pa.PyExtensionType):\n+class ParamExtType(pa.ExtensionType):\n \n     def __init__(self, width):\n         self._width = width\n-        pa.PyExtensionType.__init__(self, pa.binary(width))\n+        super().__init__(pa.binary(width), 'pyarrow.tests.ParamExtType')\n \n     @property\n     def width(self):\n         return self._width\n \n-    def __reduce__(self):\n-        return ParamExtType, (self.width,)\n+    def __arrow_ext_serialize__(self):\n+        return str(self._width).encode()\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        width = int(serialized.decode())\n+        assert storage_type == pa.binary(width)\n+        return cls(width)\n \n \n-class MyStructType(pa.PyExtensionType):\n+class MyStructType(pa.ExtensionType):\n     storage_type = pa.struct([('left', pa.int64()),\n                               ('right', pa.int64())])\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, self.storage_type)\n+        super().__init__(self.storage_type, 'pyarrow.tests.MyStructType')\n \n-    def __reduce__(self):\n-        return MyStructType, ()\n+    def __arrow_ext_serialize__(self):\n+        return b''\n \n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        assert serialized == b''\n+        assert storage_type == cls.storage_type\n+        return cls()\n \n-class MyListType(pa.PyExtensionType):\n+\n+class MyListType(pa.ExtensionType):\n \n     def __init__(self, storage_type):\n-        pa.PyExtensionType.__init__(self, storage_type)\n+        assert isinstance(storage_type, pa.ListType)\n+        super().__init__(storage_type, 'pyarrow.tests.MyListType')\n \n-    def __reduce__(self):\n-        return MyListType, (self.storage_type,)\n+    def __arrow_ext_serialize__(self):\n+        return b''\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        assert serialized == b''\n+        return cls(storage_type)\n \n \n-class AnnotatedType(pa.PyExtensionType):\n+class AnnotatedType(pa.ExtensionType):\n     \"\"\"\n     Generic extension type that can store any storage type.\n     \"\"\"\n \n     def __init__(self, storage_type, annotation):\n         self.annotation = annotation\n-        super().__init__(storage_type)\n+        super().__init__(storage_type, 'pyarrow.tests.AnnotatedType')\n+\n+    def __arrow_ext_serialize__(self):\n+        return b''\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        assert serialized == b''\n+        return cls(storage_type)\n+\n+\n+class LegacyIntType(pa.PyExtensionType):\n+\n+    def __init__(self):\n+        pa.PyExtensionType.__init__(self, pa.int8())\n \n     def __reduce__(self):\n-        return AnnotatedType, (self.storage_type, self.annotation)\n+        return LegacyIntType, ()\n \n \n def ipc_write_batch(batch):\n@@ -153,12 +236,12 @@ def ipc_read_batch(buf):\n \n def test_ext_type_basics():\n     ty = UuidType()\n-    assert ty.extension_name == \"arrow.py_extension_type\"\n+    assert ty.extension_name == \"pyarrow.tests.UuidType\"\n \n \n def test_ext_type_str():\n     ty = IntegerType()\n-    expected = \"extension<arrow.py_extension_type<IntegerType>>\"\n+    expected = \"extension<pyarrow.tests.IntegerType<IntegerType>>\"\n     assert str(ty) == expected\n     assert pa.DataType.__str__(ty) == expected\n \n@@ -223,7 +306,7 @@ def test_uuid_type_pickle(pickle_module):\n         del ty\n         ty = pickle_module.loads(ser)\n         wr = weakref.ref(ty)\n-        assert ty.extension_name == \"arrow.py_extension_type\"\n+        assert ty.extension_name == \"pyarrow.tests.UuidType\"\n         del ty\n         assert wr() is None\n \n@@ -571,9 +654,9 @@ def test_cast_between_extension_types():\n     assert tiny_int_arr.type == TinyIntType()\n \n     # Casting between extension types w/ different storage types not okay.\n-    msg = (\"Casting from 'extension<arrow.py_extension_type<TinyIntType>>' \"\n+    msg = (\"Casting from 'extension<.*?<TinyIntType>>' \"\n            \"to different extension type \"\n-           \"'extension<arrow.py_extension_type<IntegerType>>' not permitted. \"\n+           \"'extension<.*?<IntegerType>>' not permitted. \"\n            \"One can first cast to the storage type, \"\n            \"then to the extension type.\"\n            )\n@@ -660,53 +743,38 @@ def example_batch():\n     return pa.RecordBatch.from_arrays([arr], [\"exts\"])\n \n \n-def check_example_batch(batch):\n+def check_example_batch(batch, *, expect_extension):\n     arr = batch.column(0)\n-    assert isinstance(arr, pa.ExtensionArray)\n-    assert arr.type.storage_type == pa.binary(3)\n-    assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n+    if expect_extension:\n+        assert isinstance(arr, pa.ExtensionArray)\n+        assert arr.type.storage_type == pa.binary(3)\n+        assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n+    else:\n+        assert arr.type == pa.binary(3)\n+        assert arr.to_pylist() == [b\"foo\", b\"bar\"]\n     return arr\n \n \n-def test_ipc():\n+def test_ipc_unregistered():\n     batch = example_batch()\n     buf = ipc_write_batch(batch)\n     del batch\n \n     batch = ipc_read_batch(buf)\n-    arr = check_example_batch(batch)\n-    assert arr.type == ParamExtType(3)\n+    batch.validate(full=True)\n+    check_example_batch(batch, expect_extension=False)\n \n \n-def test_ipc_unknown_type():\n-    batch = example_batch()\n-    buf = ipc_write_batch(batch)\n-    del batch\n-\n-    orig_type = ParamExtType\n-    try:\n-        # Simulate the original Python type being unavailable.\n-        # Deserialization should not fail but return a placeholder type.\n-        del globals()['ParamExtType']\n+def test_ipc_registered():\n+    with registered_extension_type(ParamExtType(1)):\n+        batch = example_batch()\n+        buf = ipc_write_batch(batch)\n+        del batch\n \n         batch = ipc_read_batch(buf)\n-        arr = check_example_batch(batch)\n-        assert isinstance(arr.type, pa.UnknownExtensionType)\n-\n-        # Can be serialized again\n-        buf2 = ipc_write_batch(batch)\n-        del batch, arr\n-\n-        batch = ipc_read_batch(buf2)\n-        arr = check_example_batch(batch)\n-        assert isinstance(arr.type, pa.UnknownExtensionType)\n-    finally:\n-        globals()['ParamExtType'] = orig_type\n-\n-    # Deserialize again with the type restored\n-    batch = ipc_read_batch(buf2)\n-    arr = check_example_batch(batch)\n-    assert arr.type == ParamExtType(3)\n+        batch.validate(full=True)\n+        arr = check_example_batch(batch, expect_extension=True)\n+        assert arr.type == ParamExtType(3)\n \n \n class PeriodArray(pa.ExtensionArray):\n@@ -930,6 +998,7 @@ def test_parquet_period(tmpdir, registered_period_type):\n \n     # When reading in, properly create extension type if it is registered\n     result = pq.read_table(filename)\n+    result.validate(full=True)\n     assert result.schema.field(\"ext\").type == period_type\n     assert result.schema.field(\"ext\").metadata == {}\n     # Get the exact array class defined by the registered type.\n@@ -939,6 +1008,7 @@ def test_parquet_period(tmpdir, registered_period_type):\n     # When the type is not registered, read in as storage type\n     pa.unregister_extension_type(period_type.extension_name)\n     result = pq.read_table(filename)\n+    result.validate(full=True)\n     assert result.schema.field(\"ext\").type == pa.int64()\n     # The extension metadata is present for roundtripping.\n     assert result.schema.field(\"ext\").metadata == {\n@@ -967,13 +1037,28 @@ def test_parquet_extension_with_nested_storage(tmpdir):\n     filename = tmpdir / 'nested_extension_storage.parquet'\n     pq.write_table(orig_table, filename)\n \n+    # Unregistered\n     table = pq.read_table(filename)\n-    assert table.column('structs').type == mystruct_array.type\n-    assert table.column('lists').type == mylist_array.type\n-    assert table == orig_table\n-\n-    with pytest.raises(pa.ArrowInvalid, match='without all of its fields'):\n-        pq.ParquetFile(filename).read(columns=['structs.left'])\n+    table.validate(full=True)\n+    assert table.column('structs').type == struct_array.type\n+    assert table.column('structs').combine_chunks() == struct_array\n+    assert table.column('lists').type == list_array.type\n+    assert table.column('lists').combine_chunks() == list_array\n+\n+    # Registered\n+    with registered_extension_type(mystruct_array.type):\n+        with registered_extension_type(mylist_array.type):\n+            table = pq.read_table(filename)\n+            table.validate(full=True)\n+            assert table.column('structs').type == mystruct_array.type\n+            assert table.column('lists').type == mylist_array.type\n+            assert table == orig_table\n+\n+            # Cannot select a subfield of an extension type with\n+            # a struct storage type.\n+            with pytest.raises(pa.ArrowInvalid,\n+                               match='without all of its fields'):\n+                pq.ParquetFile(filename).read(columns=['structs.left'])\n \n \n @pytest.mark.parquet\n@@ -995,8 +1080,14 @@ def test_parquet_nested_extension(tmpdir):\n     pq.write_table(orig_table, filename)\n \n     table = pq.read_table(filename)\n-    assert table.column(0).type == struct_array.type\n-    assert table == orig_table\n+    table.validate(full=True)\n+    assert table.column(0).type == pa.struct({'ints': pa.int64(),\n+                                              'exts': pa.int64()})\n+    with registered_extension_type(ext_type):\n+        table = pq.read_table(filename)\n+        table.validate(full=True)\n+        assert table.column(0).type == struct_array.type\n+        assert table == orig_table\n \n     # List of extensions\n     list_array = pa.ListArray.from_arrays([0, 1, None, 3], ext_array)\n@@ -1006,8 +1097,13 @@ def test_parquet_nested_extension(tmpdir):\n     pq.write_table(orig_table, filename)\n \n     table = pq.read_table(filename)\n-    assert table.column(0).type == list_array.type\n-    assert table == orig_table\n+    table.validate(full=True)\n+    assert table.column(0).type == pa.list_(pa.int64())\n+    with registered_extension_type(ext_type):\n+        table = pq.read_table(filename)\n+        table.validate(full=True)\n+        assert table.column(0).type == list_array.type\n+        assert table == orig_table\n \n     # Large list of extensions\n     list_array = pa.LargeListArray.from_arrays([0, 1, None, 3], ext_array)\n@@ -1017,8 +1113,13 @@ def test_parquet_nested_extension(tmpdir):\n     pq.write_table(orig_table, filename)\n \n     table = pq.read_table(filename)\n-    assert table.column(0).type == list_array.type\n-    assert table == orig_table\n+    table.validate(full=True)\n+    assert table.column(0).type == pa.large_list(pa.int64())\n+    with registered_extension_type(ext_type):\n+        table = pq.read_table(filename)\n+        table.validate(full=True)\n+        assert table.column(0).type == list_array.type\n+        assert table == orig_table\n \n \n @pytest.mark.parquet\n@@ -1040,8 +1141,12 @@ def test_parquet_extension_nested_in_extension(tmpdir):\n     pq.write_table(orig_table, filename)\n \n     table = pq.read_table(filename)\n-    assert table.column(0).type == mylist_array.type\n-    assert table == orig_table\n+    assert table.column(0).type == pa.list_(pa.int64())\n+    with registered_extension_type(mylist_array.type):\n+        with registered_extension_type(inner_ext_array.type):\n+            table = pq.read_table(filename)\n+            assert table.column(0).type == mylist_array.type\n+            assert table == orig_table\n \n \n def test_to_numpy():\n@@ -1370,3 +1475,25 @@ def test_tensor_type_is_picklable(pickle_module):\n def test_tensor_type_str(tensor_type, text):\n     tensor_type_str = tensor_type.__str__()\n     assert text in tensor_type_str\n+\n+\n+def test_legacy_int_type():\n+    with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):\n+        ext_ty = LegacyIntType()\n+    arr = pa.array([1, 2, 3], type=ext_ty.storage_type)\n+    ext_arr = pa.ExtensionArray.from_storage(ext_ty, arr)\n+    batch = pa.RecordBatch.from_arrays([ext_arr], names=['ext'])\n+    buf = ipc_write_batch(batch)\n+\n+    with pytest.warns(\n+            RuntimeWarning,\n+            match=\"pickle-based deserialization of pyarrow.PyExtensionType \"\n+                  \"subclasses is disabled by default\"):\n+        batch = ipc_read_batch(buf)\n+        assert isinstance(batch.column(0).type, pa.UnknownExtensionType)\n+\n+    with enabled_auto_load():\n+        with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):\n+            batch = ipc_read_batch(buf)\n+            assert isinstance(batch.column(0).type, LegacyIntType)\n+            assert batch.column(0) == ext_arr"
        },
        {
          "filename": "python/pyarrow/tests/test_pandas.py",
          "status": "modified",
          "additions": 16,
          "deletions": 8,
          "patch": "@@ -4096,13 +4096,20 @@ def test_array_protocol():\n     assert result.equals(expected2)\n \n \n-class DummyExtensionType(pa.PyExtensionType):\n+class DummyExtensionType(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, pa.int64())\n+        super().__init__(pa.int64(),\n+                         'pyarrow.tests.test_pandas.DummyExtensionType')\n \n-    def __reduce__(self):\n-        return DummyExtensionType, ()\n+    def __arrow_ext_serialize__(self):\n+        return b''\n+\n+    @classmethod\n+    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        assert serialized == b''\n+        assert storage_type == pa.int64()\n+        return cls()\n \n \n def PandasArray__arrow_array__(self, type=None):\n@@ -4198,13 +4205,14 @@ def test_convert_to_extension_array(monkeypatch):\n     assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n \n \n-class MyCustomIntegerType(pa.PyExtensionType):\n+class MyCustomIntegerType(pa.ExtensionType):\n \n     def __init__(self):\n-        pa.PyExtensionType.__init__(self, pa.int64())\n+        super().__init__(pa.int64(),\n+                         'pyarrow.tests.test_pandas.MyCustomIntegerType')\n \n-    def __reduce__(self):\n-        return MyCustomIntegerType, ()\n+    def __arrow_ext_serialize__(self):\n+        return b''\n \n     def to_pandas_dtype(self):\n         return pd.Int64Dtype()"
        },
        {
          "filename": "python/pyarrow/types.pxi",
          "status": "modified",
          "additions": 43,
          "deletions": 45,
          "patch": "@@ -1437,7 +1437,10 @@ cdef class ExtensionType(BaseExtensionType):\n     Parameters\n     ----------\n     storage_type : DataType\n+        The underlying storage type for the extension type.\n     extension_name : str\n+        A unique name distinguishing this extension type. The name will be\n+        used when deserializing IPC data.\n \n     Examples\n     --------\n@@ -1671,60 +1674,22 @@ cdef class FixedShapeTensorType(BaseExtensionType):\n                                     self.dim_names, self.permutation)\n \n \n+_py_extension_type_auto_load = False\n+\n+\n cdef class PyExtensionType(ExtensionType):\n     \"\"\"\n     Concrete base class for Python-defined extension types based on pickle\n     for (de)serialization.\n \n+    .. warning::\n+       This class is deprecated and its deserialization is disabled by default.\n+       :class:`ExtensionType` is recommended instead.\n+\n     Parameters\n     ----------\n     storage_type : DataType\n         The storage type for which the extension is built.\n-\n-    Examples\n-    --------\n-    Define a UuidType extension type subclassing PyExtensionType:\n-\n-    >>> import pyarrow as pa\n-    >>> class UuidType(pa.PyExtensionType):\n-    ...     def __init__(self):\n-    ...         pa.PyExtensionType.__init__(self, pa.binary(16))\n-    ...     def __reduce__(self):\n-    ...         return UuidType, ()\n-    ...\n-\n-    Create an instance of UuidType extension type:\n-\n-    >>> uuid_type = UuidType() # doctest: +SKIP\n-    >>> uuid_type # doctest: +SKIP\n-    UuidType(FixedSizeBinaryType(fixed_size_binary[16]))\n-\n-    Inspect the extension type:\n-\n-    >>> uuid_type.extension_name # doctest: +SKIP\n-    'arrow.py_extension_type'\n-    >>> uuid_type.storage_type # doctest: +SKIP\n-    FixedSizeBinaryType(fixed_size_binary[16])\n-\n-    Wrap an array as an extension array:\n-\n-    >>> import uuid\n-    >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)],\n-    ...                          pa.binary(16)) # doctest: +SKIP\n-    >>> uuid_type.wrap_array(storage_array) # doctest: +SKIP\n-    <pyarrow.lib.ExtensionArray object at ...>\n-    [\n-      ...\n-    ]\n-\n-    Or do the same with creating an ExtensionArray:\n-\n-    >>> pa.ExtensionArray.from_storage(uuid_type,\n-    ...                                storage_array) # doctest: +SKIP\n-    <pyarrow.lib.ExtensionArray object at ...>\n-    [\n-      ...\n-    ]\n     \"\"\"\n \n     def __cinit__(self):\n@@ -1733,6 +1698,12 @@ cdef class PyExtensionType(ExtensionType):\n                             \"PyExtensionType\")\n \n     def __init__(self, DataType storage_type):\n+        warnings.warn(\n+            \"pyarrow.PyExtensionType is deprecated \"\n+            \"and will refuse deserialization by default. \"\n+            \"Instead, please derive from pyarrow.ExtensionType and implement \"\n+            \"your own serialization mechanism.\",\n+            FutureWarning)\n         ExtensionType.__init__(self, storage_type, \"arrow.py_extension_type\")\n \n     def __reduce__(self):\n@@ -1744,6 +1715,17 @@ cdef class PyExtensionType(ExtensionType):\n \n     @classmethod\n     def __arrow_ext_deserialize__(cls, storage_type, serialized):\n+        if not _py_extension_type_auto_load:\n+            warnings.warn(\n+                \"pickle-based deserialization of pyarrow.PyExtensionType subclasses \"\n+                \"is disabled by default; if you only ingest \"\n+                \"trusted data files, you may re-enable this using \"\n+                \"`pyarrow.PyExtensionType.set_auto_load(True)`.\\n\"\n+                \"In the future, Python-defined extension subclasses should \"\n+                \"derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) \"\n+                \"and implement their own serialization mechanism.\\n\",\n+                RuntimeWarning)\n+            return UnknownExtensionType(storage_type, serialized)\n         try:\n             ty = pickle.loads(serialized)\n         except Exception:\n@@ -1759,6 +1741,22 @@ cdef class PyExtensionType(ExtensionType):\n                             .format(ty.storage_type, storage_type))\n         return ty\n \n+    # XXX Cython marks extension types as immutable, so cannot expose this\n+    # as a writable class attribute.\n+    @classmethod\n+    def set_auto_load(cls, value):\n+        \"\"\"\n+        Enable or disable auto-loading of serialized PyExtensionType instances.\n+\n+        Parameters\n+        ----------\n+        value : bool\n+            Whether to enable auto-loading.\n+        \"\"\"\n+        global _py_extension_type_auto_load\n+        assert isinstance(value, bool)\n+        _py_extension_type_auto_load = value\n+\n \n cdef class UnknownExtensionType(PyExtensionType):\n     \"\"\""
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 3,
        "unique_directories": 3,
        "max_directory_depth": 3
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "2c90dafa0abeaa5661a4ccac78c16dfd370bca32",
            "date": "2025-01-25T08:33:56Z",
            "author_login": "kou"
          },
          {
            "sha": "c6b6bfb7bd13ba06d43e359bb59f7eb802152fea",
            "date": "2025-01-24T21:26:24Z",
            "author_login": "hiroyuki-sato"
          },
          {
            "sha": "f4a63d41ebbc57566f215c1d1e87fc1647071dae",
            "date": "2025-01-24T09:51:50Z",
            "author_login": "mapleFU"
          },
          {
            "sha": "17a0ff556610c5fc0666f4220a6a0e87c487770c",
            "date": "2025-01-23T19:32:28Z",
            "author_login": "pitrou"
          },
          {
            "sha": "dc581f09f8670ee44e15a801742d574ac93541d1",
            "date": "2025-01-23T05:35:58Z",
            "author_login": "kou"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 9.8,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H",
    "cwe_id": "CWE-502",
    "description": "Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files).\n\nThis vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings.\n\nIt is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. PyPI packages are already available, and we hope that conda-forge packages will be available soon.\n\nIf it is not possible to upgrade, we provide a separate package `pyarrow-hotfix` that disables the vulnerability on older PyArrow versions. See  https://pypi.org/project/pyarrow-hotfix/  for instructions.\n\n",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2023-11-09T09:15:08.223",
    "last_modified": "2024-11-21T08:30:02.447",
    "fix_date": "2023-11-06T22:20:05Z"
  },
  "references": [
    {
      "url": "https://github.com/apache/arrow/commit/f14170976372436ec1d03a724d8d3f3925484ecf",
      "source": "security@apache.org",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://lists.apache.org/thread/yhy7tdfjf9hrl9vfrtzo8p2cyjq87v7n",
      "source": "security@apache.org",
      "tags": [
        "Mailing List",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FR34AIPXVTMB3XPRU5ULV5HHWPMRE33X/",
      "source": "security@apache.org",
      "tags": []
    },
    {
      "url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MAGWEAJDWO2ACYATUQCPXLSYY5C3L3XU/",
      "source": "security@apache.org",
      "tags": []
    },
    {
      "url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MWFYXLVBTBHNKYRXI572RFX7IJDDQGBL/",
      "source": "security@apache.org",
      "tags": []
    },
    {
      "url": "https://pypi.org/project/pyarrow-hotfix/",
      "source": "security@apache.org",
      "tags": [
        "Product"
      ]
    },
    {
      "url": "https://github.com/apache/arrow/commit/f14170976372436ec1d03a724d8d3f3925484ecf",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://lists.apache.org/thread/yhy7tdfjf9hrl9vfrtzo8p2cyjq87v7n",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Mailing List",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FR34AIPXVTMB3XPRU5ULV5HHWPMRE33X/",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MAGWEAJDWO2ACYATUQCPXLSYY5C3L3XU/",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MWFYXLVBTBHNKYRXI572RFX7IJDDQGBL/",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://pypi.org/project/pyarrow-hotfix/",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Product"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:06:39.243645",
    "processing_status": "enhanced"
  },
  "repository_context": {
    "name": "arrow",
    "owner": "apache",
    "created_at": "2016-02-17T08:00:23Z",
    "updated_at": "2025-01-26T04:22:57Z",
    "pushed_at": "2025-01-25T08:33:57Z",
    "size": 203962,
    "stars": 14901,
    "forks": 3603,
    "open_issues": 4464,
    "watchers": 14901,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [
      "main"
    ],
    "languages": {
      "C++": 29757086,
      "Python": 3411059,
      "C#": 1949177,
      "Cython": 1901275,
      "Ruby": 1892967,
      "R": 1736543,
      "TypeScript": 1125239,
      "C": 950249,
      "MATLAB": 932656,
      "Swift": 796404,
      "CMake": 773245,
      "Shell": 449635,
      "Dockerfile": 164529,
      "JavaScript": 143160,
      "Meson": 50695,
      "Thrift": 39093,
      "Batchfile": 34427,
      "Vala": 24760,
      "Jinja": 21621,
      "Objective-C++": 11446,
      "Lua": 8771,
      "Makefile": 8103,
      "Go": 6581,
      "HTML": 5898,
      "Awk": 3709,
      "CSS": 289
    },
    "commit_activity": {
      "total_commits_last_year": 2160,
      "avg_commits_per_week": 41.53846153846154,
      "days_active_last_year": 320
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": false,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-26T07:52:15.188964"
  }
}