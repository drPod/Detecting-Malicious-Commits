{
  "cve_id": "CVE-2023-6730",
  "github_data": {
    "repository": "huggingface/transformers",
    "fix_commit": "1d63b0ec361e7a38f1339385e8a5a855085532ce",
    "related_commits": [
      "1d63b0ec361e7a38f1339385e8a5a855085532ce",
      "1d63b0ec361e7a38f1339385e8a5a855085532ce"
    ],
    "patch_url": "https://github.com/huggingface/transformers/commit/1d63b0ec361e7a38f1339385e8a5a855085532ce.patch",
    "fix_commit_details": {
      "sha": "1d63b0ec361e7a38f1339385e8a5a855085532ce",
      "commit_date": "2023-12-04T15:48:37Z",
      "author": {
        "login": "ydshieh",
        "type": "User",
        "stats": {
          "total_commits": 1074,
          "average_weekly_commits": 3.3046153846153845,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 163
        }
      },
      "commit_message": {
        "title": "Disallow `pickle.load` unless `TRUST_REMOTE_CODE=True` (#27776)",
        "length": 203,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 101,
        "additions": 39,
        "deletions": 62
      },
      "files": [
        {
          "filename": "docs/source/en/model_doc/transfo-xl.md",
          "status": "modified",
          "additions": 8,
          "deletions": 2,
          "patch": "@@ -22,11 +22,17 @@ This model is in maintenance mode only, so we won't accept any new PRs changing\n \n We recommend switching to more recent models for improved security.\n \n-In case you would still like to use `TransfoXL` in your experiments, we recommend using the [Hub checkpoint](https://huggingface.co/transfo-xl-wt103) with a specific revision to ensure you are downloading safe files from the Hub:\n+In case you would still like to use `TransfoXL` in your experiments, we recommend using the [Hub checkpoint](https://huggingface.co/transfo-xl-wt103) with a specific revision to ensure you are downloading safe files from the Hub.\n \n-```\n+You will need to set the environment variable `TRUST_REMOTE_CODE` to `True` in order to allow the\n+usage of `pickle.load()`:\n+\n+```python\n+import os\n from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n \n+os.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n+\n checkpoint = 'transfo-xl-wt103'\n revision = '40a186da79458c9f9de846edfaea79c412137f97'\n "
        },
        {
          "filename": "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py",
          "status": "modified",
          "additions": 16,
          "deletions": 0,
          "patch": "@@ -34,6 +34,7 @@\n     is_torch_available,\n     logging,\n     requires_backends,\n+    strtobool,\n     torch_only_method,\n )\n \n@@ -212,6 +213,14 @@ def __init__(\n             vocab_dict = None\n             if pretrained_vocab_file is not None:\n                 # Priority on pickle files (support PyTorch and TF)\n+                if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+                    raise ValueError(\n+                        \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is \"\n+                        \"potentially malicious. It's recommended to never unpickle data that could have come from an \"\n+                        \"untrusted source, or that could have been tampered with. If you already verified the pickle \"\n+                        \"data and decided to use it, you can set the environment variable \"\n+                        \"`TRUST_REMOTE_CODE` to `True` to allow it.\"\n+                    )\n                 with open(pretrained_vocab_file, \"rb\") as f:\n                     vocab_dict = pickle.load(f)\n \n@@ -790,6 +799,13 @@ def get_lm_corpus(datadir, dataset):\n         corpus = torch.load(fn_pickle)\n     elif os.path.exists(fn):\n         logger.info(\"Loading cached dataset from pickle...\")\n+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+            raise ValueError(\n+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+            )\n         with open(fn, \"rb\") as fp:\n             corpus = pickle.load(fp)\n     else:"
        },
        {
          "filename": "src/transformers/models/rag/retrieval_rag.py",
          "status": "modified",
          "additions": 15,
          "deletions": 1,
          "patch": "@@ -23,7 +23,7 @@\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends\n+from ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends, strtobool\n from .configuration_rag import RagConfig\n from .tokenization_rag import RagTokenizer\n \n@@ -131,6 +131,13 @@ def _resolve_path(self, index_path, filename):\n     def _load_passages(self):\n         logger.info(f\"Loading passages from {self.index_path}\")\n         passages_path = self._resolve_path(self.index_path, self.PASSAGE_FILENAME)\n+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+            raise ValueError(\n+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+            )\n         with open(passages_path, \"rb\") as passages_file:\n             passages = pickle.load(passages_file)\n         return passages\n@@ -140,6 +147,13 @@ def _deserialize_index(self):\n         resolved_index_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index.dpr\")\n         self.index = faiss.read_index(resolved_index_path)\n         resolved_meta_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index_meta.dpr\")\n+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+            raise ValueError(\n+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+            )\n         with open(resolved_meta_path, \"rb\") as metadata_file:\n             self.index_id_to_db_id = pickle.load(metadata_file)\n         assert ("
        },
        {
          "filename": "tests/models/rag/test_retrieval_rag.py",
          "status": "modified",
          "additions": 0,
          "deletions": 59,
          "patch": "@@ -14,7 +14,6 @@\n \n import json\n import os\n-import pickle\n import shutil\n import tempfile\n from unittest import TestCase\n@@ -174,37 +173,6 @@ def get_dummy_custom_hf_index_retriever(self, from_disk: bool):\n             )\n         return retriever\n \n-    def get_dummy_legacy_index_retriever(self):\n-        dataset = Dataset.from_dict(\n-            {\n-                \"id\": [\"0\", \"1\"],\n-                \"text\": [\"foo\", \"bar\"],\n-                \"title\": [\"Foo\", \"Bar\"],\n-                \"embeddings\": [np.ones(self.retrieval_vector_size + 1), 2 * np.ones(self.retrieval_vector_size + 1)],\n-            }\n-        )\n-        dataset.add_faiss_index(\"embeddings\", string_factory=\"Flat\", metric_type=faiss.METRIC_INNER_PRODUCT)\n-\n-        index_file_name = os.path.join(self.tmpdirname, \"hf_bert_base.hnswSQ8_correct_phi_128.c_index\")\n-        dataset.save_faiss_index(\"embeddings\", index_file_name + \".index.dpr\")\n-        pickle.dump(dataset[\"id\"], open(index_file_name + \".index_meta.dpr\", \"wb\"))\n-\n-        passages_file_name = os.path.join(self.tmpdirname, \"psgs_w100.tsv.pkl\")\n-        passages = {sample[\"id\"]: [sample[\"text\"], sample[\"title\"]] for sample in dataset}\n-        pickle.dump(passages, open(passages_file_name, \"wb\"))\n-\n-        config = RagConfig(\n-            retrieval_vector_size=self.retrieval_vector_size,\n-            question_encoder=DPRConfig().to_dict(),\n-            generator=BartConfig().to_dict(),\n-            index_name=\"legacy\",\n-            index_path=self.tmpdirname,\n-        )\n-        retriever = RagRetriever(\n-            config, question_encoder_tokenizer=self.get_dpr_tokenizer(), generator_tokenizer=self.get_bart_tokenizer()\n-        )\n-        return retriever\n-\n     def test_canonical_hf_index_retriever_retrieve(self):\n         n_docs = 1\n         retriever = self.get_dummy_canonical_hf_index_retriever()\n@@ -288,33 +256,6 @@ def test_custom_hf_index_retriever_save_and_from_pretrained_from_disk(self):\n             out = retriever.retrieve(hidden_states, n_docs=1)\n             self.assertTrue(out is not None)\n \n-    def test_legacy_index_retriever_retrieve(self):\n-        n_docs = 1\n-        retriever = self.get_dummy_legacy_index_retriever()\n-        hidden_states = np.array(\n-            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n-        )\n-        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n-        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n-        self.assertEqual(len(doc_dicts), 2)\n-        self.assertEqual(sorted(doc_dicts[0]), [\"text\", \"title\"])\n-        self.assertEqual(len(doc_dicts[0][\"text\"]), n_docs)\n-        self.assertEqual(doc_dicts[0][\"text\"][0], \"bar\")  # max inner product is reached with second doc\n-        self.assertEqual(doc_dicts[1][\"text\"][0], \"foo\")  # max inner product is reached with first doc\n-        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n-\n-    def test_legacy_hf_index_retriever_save_and_from_pretrained(self):\n-        retriever = self.get_dummy_legacy_index_retriever()\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            retriever.save_pretrained(tmp_dirname)\n-            retriever = RagRetriever.from_pretrained(tmp_dirname)\n-            self.assertIsInstance(retriever, RagRetriever)\n-            hidden_states = np.array(\n-                [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n-            )\n-            out = retriever.retrieve(hidden_states, n_docs=1)\n-            self.assertTrue(out is not None)\n-\n     @require_torch\n     @require_tokenizers\n     @require_sentencepiece"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 1,
        "unique_directories": 4,
        "max_directory_depth": 5
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "c61fcde910c34c983ec2c3da2ed41bba3601313f",
            "date": "2025-01-14T17:01:10Z",
            "author_login": "mahdibaghbanzadeh"
          },
          {
            "sha": "b0cdbd911941bdd596ac753fd33849ef83b1a841",
            "date": "2025-01-14T16:05:08Z",
            "author_login": "egojoseph"
          },
          {
            "sha": "a11041ffad285b13d578127cc304b90c2f12ce1f",
            "date": "2025-01-14T10:47:05Z",
            "author_login": "MekkCyber"
          },
          {
            "sha": "df2a812e95bfb7482de66cd0e1ea1f557cde1c79",
            "date": "2025-01-14T10:46:55Z",
            "author_login": "MekkCyber"
          },
          {
            "sha": "050636518a8b19edb36eec76c9b7676571a115a5",
            "date": "2025-01-14T10:37:37Z",
            "author_login": "MekkCyber"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 8.8,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H",
    "cwe_id": "CWE-502",
    "description": "Deserialization of Untrusted Data in GitHub repository huggingface/transformers prior to 4.36.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2023-12-19T13:15:43.380",
    "last_modified": "2024-11-21T08:44:26.450",
    "fix_date": "2023-12-04T15:48:37Z"
  },
  "references": [
    {
      "url": "https://github.com/huggingface/transformers/commit/1d63b0ec361e7a38f1339385e8a5a855085532ce",
      "source": "security@huntr.dev",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://huntr.com/bounties/423611ee-7a2a-442a-babb-3ed2f8385c16",
      "source": "security@huntr.dev",
      "tags": [
        "Exploit"
      ]
    },
    {
      "url": "https://github.com/huggingface/transformers/commit/1d63b0ec361e7a38f1339385e8a5a855085532ce",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://huntr.com/bounties/423611ee-7a2a-442a-babb-3ed2f8385c16",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Exploit"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:06:41.586764",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "transformers",
    "owner": "huggingface",
    "created_at": "2018-10-29T13:56:00Z",
    "updated_at": "2025-01-14T14:50:24Z",
    "pushed_at": "2025-01-14T15:22:48Z",
    "size": 269350,
    "stars": 137574,
    "forks": 27567,
    "open_issues": 1523,
    "watchers": 137574,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [],
    "languages": {
      "Python": 62974709,
      "Cuda": 327808,
      "Dockerfile": 36828,
      "Shell": 30374,
      "C++": 25815,
      "C": 7703,
      "Makefile": 4201,
      "Cython": 3635,
      "Jsonnet": 937
    },
    "commit_activity": {
      "total_commits_last_year": 3739,
      "avg_commits_per_week": 71.90384615384616,
      "days_active_last_year": 290
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-14T15:35:02.372075"
  }
}