{
  "cve_id": "CVE-2024-0964",
  "github_data": {
    "repository": "gradio-app/gradio",
    "fix_commit": "d76bcaaaf0734aaf49a680f94ea9d4d22a602e70",
    "related_commits": [
      "d76bcaaaf0734aaf49a680f94ea9d4d22a602e70",
      "d76bcaaaf0734aaf49a680f94ea9d4d22a602e70"
    ],
    "patch_url": "https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70.patch",
    "fix_commit_details": {
      "sha": "d76bcaaaf0734aaf49a680f94ea9d4d22a602e70",
      "commit_date": "2023-12-12T23:24:46Z",
      "author": {
        "login": "aliabid94",
        "type": "User",
        "stats": {
          "total_commits": 377,
          "average_weekly_commits": 1.1389728096676737,
          "total_additions": 586493,
          "total_deletions": 180686,
          "weeks_active": 134
        }
      },
      "commit_message": {
        "title": "Fix api event drops (#6556)",
        "length": 714,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 1122,
        "additions": 678,
        "deletions": 444
      },
      "files": [
        {
          "filename": ".changeset/ripe-spiders-love.md",
          "status": "added",
          "additions": 7,
          "deletions": 0,
          "patch": "@@ -0,0 +1,7 @@\n+---\n+\"@gradio/client\": patch\n+\"gradio\": patch\n+\"gradio_client\": patch\n+---\n+\n+fix:Fix api event drops"
        },
        {
          "filename": "client/js/src/client.ts",
          "status": "modified",
          "additions": 144,
          "deletions": 2,
          "patch": "@@ -278,6 +278,9 @@ export function api_factory(\n \n \t\t\tconst session_hash = Math.random().toString(36).substring(2);\n \t\t\tconst last_status: Record<string, Status[\"stage\"]> = {};\n+\t\t\tlet stream_open = false;\n+\t\t\tlet event_stream: EventSource | null = null;\n+\t\t\tconst event_callbacks: Record<string, () => Promise<void>> = {};\n \t\t\tlet config: Config;\n \t\t\tlet api_map: Record<string, number> = {};\n \n@@ -437,7 +440,7 @@ export function api_factory(\n \n \t\t\t\tlet websocket: WebSocket;\n \t\t\t\tlet eventSource: EventSource;\n-\t\t\t\tlet protocol = config.protocol ?? \"sse\";\n+\t\t\t\tlet protocol = config.protocol ?? \"ws\";\n \n \t\t\t\tconst _endpoint = typeof endpoint === \"number\" ? \"/predict\" : endpoint;\n \t\t\t\tlet payload: Payload;\n@@ -646,7 +649,7 @@ export function api_factory(\n \t\t\t\t\t\t\t\twebsocket.send(JSON.stringify({ hash: session_hash }))\n \t\t\t\t\t\t\t);\n \t\t\t\t\t\t}\n-\t\t\t\t\t} else {\n+\t\t\t\t\t} else if (protocol == \"sse\") {\n \t\t\t\t\t\tfire_event({\n \t\t\t\t\t\t\ttype: \"status\",\n \t\t\t\t\t\t\tstage: \"pending\",\n@@ -766,6 +769,121 @@ export function api_factory(\n \t\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t}\n \t\t\t\t\t\t};\n+\t\t\t\t\t} else if (protocol == \"sse_v1\") {\n+\t\t\t\t\t\tfire_event({\n+\t\t\t\t\t\t\ttype: \"status\",\n+\t\t\t\t\t\t\tstage: \"pending\",\n+\t\t\t\t\t\t\tqueue: true,\n+\t\t\t\t\t\t\tendpoint: _endpoint,\n+\t\t\t\t\t\t\tfn_index,\n+\t\t\t\t\t\t\ttime: new Date()\n+\t\t\t\t\t\t});\n+\n+\t\t\t\t\t\tpost_data(\n+\t\t\t\t\t\t\t`${http_protocol}//${resolve_root(\n+\t\t\t\t\t\t\t\thost,\n+\t\t\t\t\t\t\t\tconfig.path,\n+\t\t\t\t\t\t\t\ttrue\n+\t\t\t\t\t\t\t)}/queue/join?${url_params}`,\n+\t\t\t\t\t\t\t{\n+\t\t\t\t\t\t\t\t...payload,\n+\t\t\t\t\t\t\t\tsession_hash\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\thf_token\n+\t\t\t\t\t\t).then(([response, status]) => {\n+\t\t\t\t\t\t\tif (status !== 200) {\n+\t\t\t\t\t\t\t\tfire_event({\n+\t\t\t\t\t\t\t\t\ttype: \"status\",\n+\t\t\t\t\t\t\t\t\tstage: \"error\",\n+\t\t\t\t\t\t\t\t\tmessage: BROKEN_CONNECTION_MSG,\n+\t\t\t\t\t\t\t\t\tqueue: true,\n+\t\t\t\t\t\t\t\t\tendpoint: _endpoint,\n+\t\t\t\t\t\t\t\t\tfn_index,\n+\t\t\t\t\t\t\t\t\ttime: new Date()\n+\t\t\t\t\t\t\t\t});\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tevent_id = response.event_id as string;\n+\t\t\t\t\t\t\t\tif (!stream_open) {\n+\t\t\t\t\t\t\t\t\topen_stream();\n+\t\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\t\tlet callback = async function (_data: object): void {\n+\t\t\t\t\t\t\t\t\tconst { type, status, data } = handle_message(\n+\t\t\t\t\t\t\t\t\t\t_data,\n+\t\t\t\t\t\t\t\t\t\tlast_status[fn_index]\n+\t\t\t\t\t\t\t\t\t);\n+\n+\t\t\t\t\t\t\t\t\tif (type === \"update\" && status && !complete) {\n+\t\t\t\t\t\t\t\t\t\t// call 'status' listeners\n+\t\t\t\t\t\t\t\t\t\tfire_event({\n+\t\t\t\t\t\t\t\t\t\t\ttype: \"status\",\n+\t\t\t\t\t\t\t\t\t\t\tendpoint: _endpoint,\n+\t\t\t\t\t\t\t\t\t\t\tfn_index,\n+\t\t\t\t\t\t\t\t\t\t\ttime: new Date(),\n+\t\t\t\t\t\t\t\t\t\t\t...status\n+\t\t\t\t\t\t\t\t\t\t});\n+\t\t\t\t\t\t\t\t\t} else if (type === \"complete\") {\n+\t\t\t\t\t\t\t\t\t\tcomplete = status;\n+\t\t\t\t\t\t\t\t\t} else if (type === \"log\") {\n+\t\t\t\t\t\t\t\t\t\tfire_event({\n+\t\t\t\t\t\t\t\t\t\t\ttype: \"log\",\n+\t\t\t\t\t\t\t\t\t\t\tlog: data.log,\n+\t\t\t\t\t\t\t\t\t\t\tlevel: data.level,\n+\t\t\t\t\t\t\t\t\t\t\tendpoint: _endpoint,\n+\t\t\t\t\t\t\t\t\t\t\tfn_index\n+\t\t\t\t\t\t\t\t\t\t});\n+\t\t\t\t\t\t\t\t\t} else if (type === \"generating\") {\n+\t\t\t\t\t\t\t\t\t\tfire_event({\n+\t\t\t\t\t\t\t\t\t\t\ttype: \"status\",\n+\t\t\t\t\t\t\t\t\t\t\ttime: new Date(),\n+\t\t\t\t\t\t\t\t\t\t\t...status,\n+\t\t\t\t\t\t\t\t\t\t\tstage: status?.stage!,\n+\t\t\t\t\t\t\t\t\t\t\tqueue: true,\n+\t\t\t\t\t\t\t\t\t\t\tendpoint: _endpoint,\n+\t\t\t\t\t\t\t\t\t\t\tfn_index\n+\t\t\t\t\t\t\t\t\t\t});\n+\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t\tif (data) {\n+\t\t\t\t\t\t\t\t\t\tfire_event({\n+\t\t\t\t\t\t\t\t\t\t\ttype: \"data\",\n+\t\t\t\t\t\t\t\t\t\t\ttime: new Date(),\n+\t\t\t\t\t\t\t\t\t\t\tdata: transform_files\n+\t\t\t\t\t\t\t\t\t\t\t\t? transform_output(\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata.data,\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\tapi_info,\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\tconfig.root,\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\tconfig.root_url\n+\t\t\t\t\t\t\t\t\t\t\t\t  )\n+\t\t\t\t\t\t\t\t\t\t\t\t: data.data,\n+\t\t\t\t\t\t\t\t\t\t\tendpoint: _endpoint,\n+\t\t\t\t\t\t\t\t\t\t\tfn_index\n+\t\t\t\t\t\t\t\t\t\t});\n+\n+\t\t\t\t\t\t\t\t\t\tif (complete) {\n+\t\t\t\t\t\t\t\t\t\t\tfire_event({\n+\t\t\t\t\t\t\t\t\t\t\t\ttype: \"status\",\n+\t\t\t\t\t\t\t\t\t\t\t\ttime: new Date(),\n+\t\t\t\t\t\t\t\t\t\t\t\t...complete,\n+\t\t\t\t\t\t\t\t\t\t\t\tstage: status?.stage!,\n+\t\t\t\t\t\t\t\t\t\t\t\tqueue: true,\n+\t\t\t\t\t\t\t\t\t\t\t\tendpoint: _endpoint,\n+\t\t\t\t\t\t\t\t\t\t\t\tfn_index\n+\t\t\t\t\t\t\t\t\t\t\t});\n+\t\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\t\t\tif (status.stage === \"complete\" || status.stage === \"error\") {\n+\t\t\t\t\t\t\t\t\t\tif (event_callbacks[event_id]) {\n+\t\t\t\t\t\t\t\t\t\t\tdelete event_callbacks[event_id];\n+\t\t\t\t\t\t\t\t\t\t\tif (Object.keys(event_callbacks).length === 0) {\n+\t\t\t\t\t\t\t\t\t\t\t\tclose_stream();\n+\t\t\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t};\n+\t\t\t\t\t\t\t\tevent_callbacks[event_id] = callback;\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t});\n \t\t\t\t\t}\n \t\t\t\t});\n \n@@ -864,6 +982,30 @@ export function api_factory(\n \t\t\t\t};\n \t\t\t}\n \n+\t\t\tfunction open_stream(): void {\n+\t\t\t\tstream_open = true;\n+\t\t\t\tlet params = new URLSearchParams({\n+\t\t\t\t\tsession_hash: session_hash\n+\t\t\t\t}).toString();\n+\t\t\t\tlet url = new URL(\n+\t\t\t\t\t`${http_protocol}//${resolve_root(\n+\t\t\t\t\t\thost,\n+\t\t\t\t\t\tconfig.path,\n+\t\t\t\t\t\ttrue\n+\t\t\t\t\t)}/queue/data?${params}`\n+\t\t\t\t);\n+\t\t\t\tevent_stream = new EventSource(url);\n+\t\t\t\tevent_stream.onmessage = async function (event) {\n+\t\t\t\t\tlet _data = JSON.parse(event.data);\n+\t\t\t\t\tawait event_callbacks[_data.event_id](_data);\n+\t\t\t\t};\n+\t\t\t}\n+\n+\t\t\tfunction close_stream(): void {\n+\t\t\t\tstream_open = false;\n+\t\t\t\tevent_stream?.close();\n+\t\t\t}\n+\n \t\t\tasync function component_server(\n \t\t\t\tcomponent_id: number,\n \t\t\t\tfn_name: string,"
        },
        {
          "filename": "client/js/src/types.ts",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -20,7 +20,7 @@ export interface Config {\n \tshow_api: boolean;\n \tstylesheets: string[];\n \tpath: string;\n-\tprotocol?: \"sse\" | \"ws\";\n+\tprotocol?: \"sse_v1\" | \"sse\" | \"ws\";\n }\n \n export interface Payload {"
        },
        {
          "filename": "client/python/gradio_client/client.py",
          "status": "modified",
          "additions": 130,
          "deletions": 19,
          "patch": "@@ -36,6 +36,7 @@\n from gradio_client.utils import (\n     Communicator,\n     JobStatus,\n+    Message,\n     Status,\n     StatusUpdate,\n )\n@@ -124,25 +125,33 @@ def __init__(\n         if self.verbose:\n             print(f\"Loaded as API: {self.src} \u2714\")\n \n+        if auth is not None:\n+            self._login(auth)\n+\n+        self.config = self._get_config()\n+        self.protocol: str = self.config.get(\"protocol\", \"ws\")\n         self.api_url = urllib.parse.urljoin(self.src, utils.API_URL)\n-        self.sse_url = urllib.parse.urljoin(self.src, utils.SSE_URL)\n-        self.sse_data_url = urllib.parse.urljoin(self.src, utils.SSE_DATA_URL)\n+        self.sse_url = urllib.parse.urljoin(\n+            self.src, utils.SSE_URL_V0 if self.protocol == \"sse\" else utils.SSE_URL\n+        )\n+        self.sse_data_url = urllib.parse.urljoin(\n+            self.src,\n+            utils.SSE_DATA_URL_V0 if self.protocol == \"sse\" else utils.SSE_DATA_URL,\n+        )\n         self.ws_url = urllib.parse.urljoin(\n             self.src.replace(\"http\", \"ws\", 1), utils.WS_URL\n         )\n         self.upload_url = urllib.parse.urljoin(self.src, utils.UPLOAD_URL)\n         self.reset_url = urllib.parse.urljoin(self.src, utils.RESET_URL)\n-        if auth is not None:\n-            self._login(auth)\n-        self.config = self._get_config()\n         self.app_version = version.parse(self.config.get(\"version\", \"2.0\"))\n         self._info = self._get_api_info()\n         self.session_hash = str(uuid.uuid4())\n \n-        protocol = self.config.get(\"protocol\")\n-        endpoint_class = Endpoint if protocol == \"sse\" else EndpointV3Compatibility\n+        endpoint_class = (\n+            Endpoint if self.protocol.startswith(\"sse\") else EndpointV3Compatibility\n+        )\n         self.endpoints = [\n-            endpoint_class(self, fn_index, dependency)\n+            endpoint_class(self, fn_index, dependency, self.protocol)\n             for fn_index, dependency in enumerate(self.config[\"dependencies\"])\n         ]\n \n@@ -152,6 +161,84 @@ def __init__(\n         # Disable telemetry by setting the env variable HF_HUB_DISABLE_TELEMETRY=1\n         threading.Thread(target=self._telemetry_thread).start()\n \n+        self.stream_open = False\n+        self.streaming_future: Future | None = None\n+        self.pending_messages_per_event: dict[str, list[Message | None]] = {}\n+        self.pending_event_ids: set[str] = set()\n+\n+    async def stream_messages(self) -> None:\n+        try:\n+            async with httpx.AsyncClient(timeout=httpx.Timeout(timeout=None)) as client:\n+                buffer = \"\"\n+                async with client.stream(\n+                    \"GET\",\n+                    self.sse_url,\n+                    params={\"session_hash\": self.session_hash},\n+                    headers=self.headers,\n+                    cookies=self.cookies,\n+                ) as response:\n+                    async for line in response.aiter_text():\n+                        buffer += line\n+                        while \"\\n\\n\" in buffer:\n+                            message, buffer = buffer.split(\"\\n\\n\", 1)\n+                            if message.startswith(\"data:\"):\n+                                resp = json.loads(message[5:])\n+                                if resp[\"msg\"] == \"heartbeat\":\n+                                    continue\n+                                elif resp[\"msg\"] == \"server_stopped\":\n+                                    for (\n+                                        pending_messages\n+                                    ) in self.pending_messages_per_event.values():\n+                                        pending_messages.append(resp)\n+                                    return\n+                                event_id = resp[\"event_id\"]\n+                                if event_id not in self.pending_messages_per_event:\n+                                    self.pending_messages_per_event[event_id] = []\n+                                self.pending_messages_per_event[event_id].append(resp)\n+                                if resp[\"msg\"] == \"process_completed\":\n+                                    self.pending_event_ids.remove(event_id)\n+                                if len(self.pending_event_ids) == 0:\n+                                    self.stream_open = False\n+                                    return\n+                            elif message == \"\":\n+                                continue\n+                            else:\n+                                raise ValueError(f\"Unexpected SSE line: '{message}'\")\n+        except BaseException as e:\n+            import traceback\n+\n+            traceback.print_exc()\n+            raise e\n+\n+    async def send_data(self, data, hash_data):\n+        async with httpx.AsyncClient() as client:\n+            req = await client.post(\n+                self.sse_data_url,\n+                json={**data, **hash_data},\n+                headers=self.headers,\n+                cookies=self.cookies,\n+            )\n+        req.raise_for_status()\n+        resp = req.json()\n+        event_id = resp[\"event_id\"]\n+\n+        if not self.stream_open:\n+            self.stream_open = True\n+\n+            def open_stream():\n+                return utils.synchronize_async(self.stream_messages)\n+\n+            def close_stream(_):\n+                self.stream_open = False\n+                for _, pending_messages in self.pending_messages_per_event.items():\n+                    pending_messages.append(None)\n+\n+            if self.streaming_future is None or self.streaming_future.done():\n+                self.streaming_future = self.executor.submit(open_stream)\n+                self.streaming_future.add_done_callback(close_stream)\n+\n+        return event_id\n+\n     @classmethod\n     def duplicate(\n         cls,\n@@ -340,7 +427,7 @@ def submit(\n         inferred_fn_index = self._infer_fn_index(api_name, fn_index)\n \n         helper = None\n-        if self.endpoints[inferred_fn_index].protocol in (\"ws\", \"sse\"):\n+        if self.endpoints[inferred_fn_index].protocol in (\"ws\", \"sse\", \"sse_v1\"):\n             helper = self.new_helper(inferred_fn_index)\n         end_to_end_fn = self.endpoints[inferred_fn_index].make_end_to_end_fn(helper)\n         future = self.executor.submit(end_to_end_fn, *args)\n@@ -806,15 +893,17 @@ class ReplaceMe:\n class Endpoint:\n     \"\"\"Helper class for storing all the information about a single API endpoint.\"\"\"\n \n-    def __init__(self, client: Client, fn_index: int, dependency: dict):\n+    def __init__(\n+        self, client: Client, fn_index: int, dependency: dict, protocol: str = \"sse_v1\"\n+    ):\n         self.client: Client = client\n         self.fn_index = fn_index\n         self.dependency = dependency\n         api_name = dependency.get(\"api_name\")\n         self.api_name: str | Literal[False] | None = (\n             \"/\" + api_name if isinstance(api_name, str) else api_name\n         )\n-        self.protocol = \"sse\"\n+        self.protocol = protocol\n         self.input_component_types = [\n             self._get_component_type(id_) for id_ in dependency[\"inputs\"]\n         ]\n@@ -891,7 +980,20 @@ def _predict(*data) -> tuple:\n                 \"session_hash\": self.client.session_hash,\n             }\n \n-            result = utils.synchronize_async(self._sse_fn, data, hash_data, helper)\n+            if self.protocol == \"sse\":\n+                result = utils.synchronize_async(\n+                    self._sse_fn_v0, data, hash_data, helper\n+                )\n+            elif self.protocol == \"sse_v1\":\n+                event_id = utils.synchronize_async(\n+                    self.client.send_data, data, hash_data\n+                )\n+                self.client.pending_event_ids.add(event_id)\n+                self.client.pending_messages_per_event[event_id] = []\n+                result = utils.synchronize_async(self._sse_fn_v1, helper, event_id)\n+            else:\n+                raise ValueError(f\"Unsupported protocol: {self.protocol}\")\n+\n             if \"error\" in result:\n                 raise ValueError(result[\"error\"])\n \n@@ -1068,24 +1170,33 @@ def process_predictions(self, *predictions):\n         predictions = self.reduce_singleton_output(*predictions)\n         return predictions\n \n-    async def _sse_fn(self, data: dict, hash_data: dict, helper: Communicator):\n+    async def _sse_fn_v0(self, data: dict, hash_data: dict, helper: Communicator):\n         async with httpx.AsyncClient(timeout=httpx.Timeout(timeout=None)) as client:\n-            return await utils.get_pred_from_sse(\n+            return await utils.get_pred_from_sse_v0(\n                 client,\n                 data,\n                 hash_data,\n                 helper,\n-                sse_url=self.client.sse_url,\n-                sse_data_url=self.client.sse_data_url,\n-                headers=self.client.headers,\n-                cookies=self.client.cookies,\n+                self.client.sse_url,\n+                self.client.sse_data_url,\n+                self.client.headers,\n+                self.client.cookies,\n             )\n \n+    async def _sse_fn_v1(self, helper: Communicator, event_id: str):\n+        return await utils.get_pred_from_sse_v1(\n+            helper,\n+            self.client.headers,\n+            self.client.cookies,\n+            self.client.pending_messages_per_event,\n+            event_id,\n+        )\n+\n \n class EndpointV3Compatibility:\n     \"\"\"Endpoint class for connecting to v3 endpoints. Backwards compatibility.\"\"\"\n \n-    def __init__(self, client: Client, fn_index: int, dependency: dict):\n+    def __init__(self, client: Client, fn_index: int, dependency: dict, *args):\n         self.client: Client = client\n         self.fn_index = fn_index\n         self.dependency = dependency"
        },
        {
          "filename": "client/python/gradio_client/utils.py",
          "status": "modified",
          "additions": 129,
          "deletions": 15,
          "patch": "@@ -17,7 +17,7 @@\n from enum import Enum\n from pathlib import Path\n from threading import Lock\n-from typing import Any, Callable, Optional\n+from typing import Any, Callable, Optional, TypedDict\n \n import fsspec.asyn\n import httpx\n@@ -26,8 +26,10 @@\n from websockets.legacy.protocol import WebSocketCommonProtocol\n \n API_URL = \"api/predict/\"\n-SSE_URL = \"queue/join\"\n-SSE_DATA_URL = \"queue/data\"\n+SSE_URL_V0 = \"queue/join\"\n+SSE_DATA_URL_V0 = \"queue/data\"\n+SSE_URL = \"queue/data\"\n+SSE_DATA_URL = \"queue/join\"\n WS_URL = \"queue/join\"\n UPLOAD_URL = \"upload\"\n LOGIN_URL = \"login\"\n@@ -48,6 +50,19 @@\n ]\n \n \n+class Message(TypedDict, total=False):\n+    msg: str\n+    output: dict[str, Any]\n+    event_id: str\n+    rank: int\n+    rank_eta: float\n+    queue_size: int\n+    success: bool\n+    progress_data: list[dict]\n+    log: str\n+    level: str\n+\n+\n def get_package_version() -> str:\n     try:\n         package_json_data = (\n@@ -100,6 +115,7 @@ class Status(Enum):\n     PROGRESS = \"PROGRESS\"\n     FINISHED = \"FINISHED\"\n     CANCELLED = \"CANCELLED\"\n+    LOG = \"LOG\"\n \n     @staticmethod\n     def ordering(status: Status) -> int:\n@@ -133,6 +149,7 @@ def msg_to_status(msg: str) -> Status:\n             \"process_generating\": Status.ITERATING,\n             \"process_completed\": Status.FINISHED,\n             \"progress\": Status.PROGRESS,\n+            \"log\": Status.LOG,\n         }[msg]\n \n \n@@ -169,6 +186,7 @@ class StatusUpdate:\n     success: bool | None\n     time: datetime | None\n     progress_data: list[ProgressUnit] | None\n+    log: tuple[str, str] | None = None\n \n \n def create_initial_status_update():\n@@ -307,29 +325,62 @@ async def get_pred_from_ws(\n     return resp[\"output\"]\n \n \n-async def get_pred_from_sse(\n+async def get_pred_from_sse_v0(\n     client: httpx.AsyncClient,\n     data: dict,\n     hash_data: dict,\n     helper: Communicator,\n     sse_url: str,\n     sse_data_url: str,\n     headers: dict[str, str],\n-    cookies: dict[str, str] | None = None,\n+    cookies: dict[str, str] | None,\n ) -> dict[str, Any] | None:\n     done, pending = await asyncio.wait(\n         [\n-            asyncio.create_task(check_for_cancel(helper, cookies)),\n+            asyncio.create_task(check_for_cancel(helper, headers, cookies)),\n             asyncio.create_task(\n-                stream_sse(\n+                stream_sse_v0(\n                     client,\n                     data,\n                     hash_data,\n                     helper,\n                     sse_url,\n                     sse_data_url,\n-                    headers=headers,\n-                    cookies=cookies,\n+                    headers,\n+                    cookies,\n+                )\n+            ),\n+        ],\n+        return_when=asyncio.FIRST_COMPLETED,\n+    )\n+\n+    for task in pending:\n+        task.cancel()\n+        try:\n+            await task\n+        except asyncio.CancelledError:\n+            pass\n+\n+    assert len(done) == 1\n+    for task in done:\n+        return task.result()\n+\n+\n+async def get_pred_from_sse_v1(\n+    helper: Communicator,\n+    headers: dict[str, str],\n+    cookies: dict[str, str] | None,\n+    pending_messages_per_event: dict[str, list[Message | None]],\n+    event_id: str,\n+) -> dict[str, Any] | None:\n+    done, pending = await asyncio.wait(\n+        [\n+            asyncio.create_task(check_for_cancel(helper, headers, cookies)),\n+            asyncio.create_task(\n+                stream_sse_v1(\n+                    helper,\n+                    pending_messages_per_event,\n+                    event_id,\n                 )\n             ),\n         ],\n@@ -348,7 +399,9 @@ async def get_pred_from_sse(\n         return task.result()\n \n \n-async def check_for_cancel(helper: Communicator, cookies: dict[str, str] | None):\n+async def check_for_cancel(\n+    helper: Communicator, headers: dict[str, str], cookies: dict[str, str] | None\n+):\n     while True:\n         await asyncio.sleep(0.05)\n         with helper.lock:\n@@ -357,28 +410,31 @@ async def check_for_cancel(helper: Communicator, cookies: dict[str, str] | None)\n     if helper.event_id:\n         async with httpx.AsyncClient() as http:\n             await http.post(\n-                helper.reset_url, json={\"event_id\": helper.event_id}, cookies=cookies\n+                helper.reset_url,\n+                json={\"event_id\": helper.event_id},\n+                headers=headers,\n+                cookies=cookies,\n             )\n     raise CancelledError()\n \n \n-async def stream_sse(\n+async def stream_sse_v0(\n     client: httpx.AsyncClient,\n     data: dict,\n     hash_data: dict,\n     helper: Communicator,\n     sse_url: str,\n     sse_data_url: str,\n     headers: dict[str, str],\n-    cookies: dict[str, str] | None = None,\n+    cookies: dict[str, str] | None,\n ) -> dict[str, Any]:\n     try:\n         async with client.stream(\n             \"GET\",\n             sse_url,\n             params=hash_data,\n-            cookies=cookies,\n             headers=headers,\n+            cookies=cookies,\n         ) as response:\n             async for line in response.aiter_text():\n                 if line.startswith(\"data:\"):\n@@ -413,8 +469,8 @@ async def stream_sse(\n                         req = await client.post(\n                             sse_data_url,\n                             json={\"event_id\": event_id, **data, **hash_data},\n-                            cookies=cookies,\n                             headers=headers,\n+                            cookies=cookies,\n                         )\n                         req.raise_for_status()\n                     elif resp[\"msg\"] == \"process_completed\":\n@@ -426,6 +482,64 @@ async def stream_sse(\n         raise\n \n \n+async def stream_sse_v1(\n+    helper: Communicator,\n+    pending_messages_per_event: dict[str, list[Message | None]],\n+    event_id: str,\n+) -> dict[str, Any]:\n+    try:\n+        pending_messages = pending_messages_per_event[event_id]\n+\n+        while True:\n+            if len(pending_messages) > 0:\n+                msg = pending_messages.pop(0)\n+            else:\n+                await asyncio.sleep(0.05)\n+                continue\n+\n+            if msg is None:\n+                raise CancelledError()\n+\n+            with helper.lock:\n+                log_message = None\n+                if msg[\"msg\"] == \"log\":\n+                    log = msg.get(\"log\")\n+                    level = msg.get(\"level\")\n+                    if log and level:\n+                        log_message = (log, level)\n+                status_update = StatusUpdate(\n+                    code=Status.msg_to_status(msg[\"msg\"]),\n+                    queue_size=msg.get(\"queue_size\"),\n+                    rank=msg.get(\"rank\", None),\n+                    success=msg.get(\"success\"),\n+                    time=datetime.now(),\n+                    eta=msg.get(\"rank_eta\"),\n+                    progress_data=ProgressUnit.from_msg(msg[\"progress_data\"])\n+                    if \"progress_data\" in msg\n+                    else None,\n+                    log=log_message,\n+                )\n+                output = msg.get(\"output\", {}).get(\"data\", [])\n+                if output and status_update.code != Status.FINISHED:\n+                    try:\n+                        result = helper.prediction_processor(*output)\n+                    except Exception as e:\n+                        result = [e]\n+                    helper.job.outputs.append(result)\n+                helper.job.latest_status = status_update\n+\n+            if msg[\"msg\"] == \"queue_full\":\n+                raise QueueError(\"Queue is full! Please try again.\")\n+            elif msg[\"msg\"] == \"process_completed\":\n+                del pending_messages_per_event[event_id]\n+                return msg[\"output\"]\n+            elif msg[\"msg\"] == \"server_stopped\":\n+                raise ValueError(\"Server stopped.\")\n+\n+    except asyncio.CancelledError:\n+        raise\n+\n+\n ########################\n # Data processing utils\n ########################"
        },
        {
          "filename": "client/python/test/test_client.py",
          "status": "modified",
          "additions": 11,
          "deletions": 0,
          "patch": "@@ -95,6 +95,17 @@ def test_private_space_v4(self):\n         output = client.predict(\"abc\", api_name=\"/predict\")\n         assert output == \"abc\"\n \n+    @pytest.mark.flaky\n+    def test_private_space_v4_sse_v1(self):\n+        space_id = \"gradio-tests/not-actually-private-spacev4-sse-v1\"\n+        api = huggingface_hub.HfApi()\n+        assert api.space_info(space_id).private\n+        client = Client(\n+            space_id,\n+        )\n+        output = client.predict(\"abc\", api_name=\"/predict\")\n+        assert output == \"abc\"\n+\n     def test_state(self, increment_demo):\n         with connect(increment_demo) as client:\n             output = client.predict(api_name=\"/increment_without_queue\")"
        },
        {
          "filename": "gradio/blocks.py",
          "status": "modified",
          "additions": 9,
          "deletions": 2,
          "patch": "@@ -893,6 +893,13 @@ def set_event_trigger(\n             fn = get_continuous_fn(fn, every)\n         elif every:\n             raise ValueError(\"Cannot set a value for `every` without a `fn`.\")\n+        if every and concurrency_limit is not None:\n+            if concurrency_limit == \"default\":\n+                concurrency_limit = None\n+            else:\n+                raise ValueError(\n+                    \"Cannot set a value for `concurrency_limit` with `every`.\"\n+                )\n \n         if _targets[0][1] == \"change\" and trigger_mode is None:\n             trigger_mode = \"always_last\"\n@@ -1581,7 +1588,7 @@ def get_config_file(self):\n             \"is_colab\": utils.colab_check(),\n             \"stylesheets\": self.stylesheets,\n             \"theme\": self.theme.name,\n-            \"protocol\": \"sse\",\n+            \"protocol\": \"sse_v1\",\n         }\n \n         def get_layout(block):\n@@ -2169,7 +2176,7 @@ def close(self, verbose: bool = True) -> None:\n         try:\n             if wasm_utils.IS_WASM:\n                 # NOTE:\n-                # Normally, queue-related async tasks (e.g. continuous events created by `gr.Blocks.load(..., every=interval)`, whose async tasks are started at the `/queue/join` endpoint function)\n+                # Normally, queue-related async tasks (e.g. continuous events created by `gr.Blocks.load(..., every=interval)`, whose async tasks are started at the `/queue/data` endpoint function)\n                 # are running in an event loop in the server thread,\n                 # so they will be cancelled by `self.server.close()` below.\n                 # However, in the Wasm env, we don't have the `server` and"
        },
        {
          "filename": "gradio/queueing.py",
          "status": "modified",
          "additions": 69,
          "deletions": 76,
          "patch": "@@ -23,7 +23,7 @@\n )\n from gradio.exceptions import Error\n from gradio.helpers import TrackedIterable\n-from gradio.utils import run_coro_in_background, safe_get_lock, set_task_name\n+from gradio.utils import LRUCache, run_coro_in_background, safe_get_lock, set_task_name\n \n if TYPE_CHECKING:\n     from gradio.blocks import BlockFunction\n@@ -37,7 +37,6 @@ def __init__(\n         request: fastapi.Request,\n         username: str | None,\n     ):\n-        self.message_queue = ThreadQueue()\n         self.session_hash = session_hash\n         self.fn_index = fn_index\n         self.request = request\n@@ -48,28 +47,6 @@ def __init__(\n         self.progress_pending: bool = False\n         self.alive = True\n \n-    def send_message(\n-        self,\n-        message_type: str,\n-        data: dict | None = None,\n-        final: bool = False,\n-    ):\n-        data = {} if data is None else data\n-        self.message_queue.put_nowait({\"msg\": message_type, **data})\n-        if final:\n-            self.message_queue.put_nowait(None)\n-\n-    async def get_data(self, timeout=5) -> bool:\n-        self.send_message(\"send_data\", {\"event_id\": self._id})\n-        sleep_interval = 0.05\n-        wait_time = 0\n-        while wait_time < timeout and self.alive:\n-            if self.data is not None:\n-                break\n-            await asyncio.sleep(sleep_interval)\n-            wait_time += sleep_interval\n-        return self.data is not None\n-\n \n class Queue:\n     def __init__(\n@@ -81,6 +58,9 @@ def __init__(\n         block_fns: list[BlockFunction],\n         default_concurrency_limit: int | None | Literal[\"not_set\"] = \"not_set\",\n     ):\n+        self.pending_messages_per_session: LRUCache[str, ThreadQueue] = LRUCache(2000)\n+        self.pending_event_ids_session: dict[str, set[str]] = {}\n+        self.pending_message_lock = safe_get_lock()\n         self.event_queue: list[Event] = []\n         self.awaiting_data_events: dict[str, Event] = {}\n         self.stopped = False\n@@ -132,6 +112,16 @@ def start(self):\n     def close(self):\n         self.stopped = True\n \n+    def send_message(\n+        self,\n+        event: Event,\n+        message_type: str,\n+        data: dict | None = None,\n+    ):\n+        data = {} if data is None else data\n+        messages = self.pending_messages_per_session[event.session_hash]\n+        messages.put_nowait({\"msg\": message_type, \"event_id\": event._id, **data})\n+\n     def _resolve_concurrency_limit(self, default_concurrency_limit):\n         \"\"\"\n         Handles the logic of resolving the default_concurrency_limit as this can be specified via a combination\n@@ -152,13 +142,33 @@ def _resolve_concurrency_limit(self, default_concurrency_limit):\n         else:\n             return 1\n \n-    def attach_data(self, body: PredictBody):\n-        event_id = body.event_id\n-        if event_id in self.awaiting_data_events:\n-            event = self.awaiting_data_events[event_id]\n-            event.data = body\n-        else:\n-            raise ValueError(\"Event not found\", event_id)\n+    async def push(\n+        self, body: PredictBody, request: fastapi.Request, username: str | None\n+    ):\n+        if body.session_hash is None:\n+            raise ValueError(\"No session hash provided.\")\n+        if body.fn_index is None:\n+            raise ValueError(\"No function index provided.\")\n+        queue_len = len(self.event_queue)\n+        if self.max_size is not None and queue_len >= self.max_size:\n+            raise ValueError(\n+                f\"Queue is full. Max size is {self.max_size} and current size is {queue_len}.\"\n+            )\n+\n+        event = Event(body.session_hash, body.fn_index, request, username)\n+        event.data = body\n+        async with self.pending_message_lock:\n+            if body.session_hash not in self.pending_messages_per_session:\n+                self.pending_messages_per_session[body.session_hash] = ThreadQueue()\n+            if body.session_hash not in self.pending_event_ids_session:\n+                self.pending_event_ids_session[body.session_hash] = set()\n+        self.pending_event_ids_session[body.session_hash].add(event._id)\n+        self.event_queue.append(event)\n+\n+        estimation = self.get_estimation()\n+        await self.send_estimation(event, estimation, queue_len)\n+\n+        return event._id\n \n     def _cancel_asyncio_tasks(self):\n         for task in self._asyncio_tasks:\n@@ -276,7 +286,7 @@ async def start_progress_updates(self) -> None:\n             for event in events:\n                 if event.progress_pending and event.progress:\n                     event.progress_pending = False\n-                    event.send_message(\"progress\", event.progress.model_dump())\n+                    self.send_message(event, \"progress\", event.progress.model_dump())\n \n             await asyncio.sleep(self.progress_update_sleep_when_free)\n \n@@ -320,34 +330,23 @@ def log_message(\n                     log=log,\n                     level=level,\n                 )\n-                event.send_message(\"log\", log_message.model_dump())\n-\n-    def push(self, event: Event) -> int | None:\n-        \"\"\"\n-        Add event to queue, or return None if Queue is full\n-        Parameters:\n-            event: Event to add to Queue\n-        Returns:\n-            rank of submitted Event\n-        \"\"\"\n-        queue_len = len(self.event_queue)\n-        if self.max_size is not None and queue_len >= self.max_size:\n-            return None\n-        self.event_queue.append(event)\n-        return queue_len\n-\n-    async def clean_event(self, event: Event | str) -> None:\n-        if isinstance(event, str):\n-            for job_set in self.active_jobs:\n-                if job_set:\n-                    for job in job_set:\n-                        if job._id == event:\n-                            event = job\n-                            break\n-        if isinstance(event, str):\n-            raise ValueError(\"Event not found\", event)\n-        event.alive = False\n-        if event in self.event_queue:\n+                self.send_message(event, \"log\", log_message.model_dump())\n+\n+    async def clean_events(\n+        self, *, session_hash: str | None = None, event_id: str | None = None\n+    ) -> None:\n+        for job_set in self.active_jobs:\n+            if job_set:\n+                for job in job_set:\n+                    if job.session_hash == session_hash or job._id == event_id:\n+                        job.alive = False\n+\n+        events_to_remove = []\n+        for event in self.event_queue:\n+            if event.session_hash == session_hash or event._id == event_id:\n+                events_to_remove.append(event)\n+\n+        for event in events_to_remove:\n             async with self.delete_lock:\n                 self.event_queue.remove(event)\n \n@@ -391,7 +390,7 @@ async def send_estimation(\n             if None not in self.active_jobs:\n                 # Add estimated amount of time for a thread to get empty\n                 estimation.rank_eta += self.avg_concurrent_process_time\n-        event.send_message(\"estimation\", estimation.model_dump())\n+        self.send_message(event, \"estimation\", estimation.model_dump())\n         return estimation\n \n     def update_estimation(self, duration: float) -> None:\n@@ -485,14 +484,7 @@ async def process_events(self, events: list[Event], batch: bool) -> None:\n         awake_events: list[Event] = []\n         try:\n             for event in events:\n-                if not event.data:\n-                    self.awaiting_data_events[event._id] = event\n-                    client_awake = await event.get_data()\n-                    del self.awaiting_data_events[event._id]\n-                    if not client_awake:\n-                        await self.clean_event(event)\n-                        continue\n-                event.send_message(\"process_starts\")\n+                self.send_message(event, \"process_starts\")\n                 awake_events.append(event)\n             if not awake_events:\n                 return\n@@ -505,7 +497,8 @@ async def process_events(self, events: list[Event], batch: bool) -> None:\n                 response = None\n                 err = e\n                 for event in awake_events:\n-                    event.send_message(\n+                    self.send_message(\n+                        event,\n                         \"process_completed\",\n                         {\n                             \"output\": {\n@@ -515,7 +508,6 @@ async def process_events(self, events: list[Event], batch: bool) -> None:\n                             },\n                             \"success\": False,\n                         },\n-                        final=True,\n                     )\n             if response and response.get(\"is_generating\", False):\n                 old_response = response\n@@ -524,7 +516,8 @@ async def process_events(self, events: list[Event], batch: bool) -> None:\n                     old_response = response\n                     old_err = err\n                     for event in awake_events:\n-                        event.send_message(\n+                        self.send_message(\n+                            event,\n                             \"process_generating\",\n                             {\n                                 \"output\": old_response,\n@@ -545,7 +538,8 @@ async def process_events(self, events: list[Event], batch: bool) -> None:\n                         relevant_response = err\n                     else:\n                         relevant_response = old_response or old_err\n-                    event.send_message(\n+                    self.send_message(\n+                        event,\n                         \"process_completed\",\n                         {\n                             \"output\": {\"error\": str(relevant_response)}\n@@ -554,20 +548,19 @@ async def process_events(self, events: list[Event], batch: bool) -> None:\n                             \"success\": relevant_response\n                             and not isinstance(relevant_response, Exception),\n                         },\n-                        final=True,\n                     )\n             elif response:\n                 output = copy.deepcopy(response)\n                 for e, event in enumerate(awake_events):\n                     if batch and \"data\" in output:\n                         output[\"data\"] = list(zip(*response.get(\"data\")))[e]\n-                    event.send_message(\n+                    self.send_message(\n+                        event,\n                         \"process_completed\",\n                         {\n                             \"output\": output,\n                             \"success\": response is not None,\n                         },\n-                        final=True,\n                     )\n             end_time = time.time()\n             if response is not None:"
        },
        {
          "filename": "gradio/routes.py",
          "status": "modified",
          "additions": 46,
          "deletions": 52,
          "patch": "@@ -55,7 +55,7 @@\n from gradio.exceptions import Error\n from gradio.helpers import CACHED_FOLDER\n from gradio.oauth import attach_oauth\n-from gradio.queueing import Estimation, Event\n+from gradio.queueing import Estimation\n from gradio.route_utils import (  # noqa: F401\n     FileUploadProgress,\n     GradioMultiPartParser,\n@@ -65,10 +65,7 @@\n )\n from gradio.state_holder import StateHolder\n from gradio.utils import (\n-    cancel_tasks,\n     get_package_version,\n-    run_coro_in_background,\n-    set_task_name,\n )\n \n if TYPE_CHECKING:\n@@ -532,7 +529,7 @@ async def reset_iterator(body: ResetBody):\n             async with app.lock:\n                 del app.iterators[body.event_id]\n                 app.iterators_to_reset.add(body.event_id)\n-                await app.get_blocks()._queue.clean_event(body.event_id)\n+                await app.get_blocks()._queue.clean_events(event_id=body.event_id)\n             return {\"success\": True}\n \n         # had to use '/run' endpoint for Colab compatibility, '/api' supported for backwards compatibility\n@@ -582,63 +579,38 @@ async def predict(\n                 )\n             return output\n \n-        @app.get(\"/queue/join\", dependencies=[Depends(login_check)])\n-        async def queue_join(\n-            fn_index: int,\n-            session_hash: str,\n+        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])\n+        async def queue_data(\n             request: fastapi.Request,\n-            username: str = Depends(get_current_user),\n-            data: Optional[str] = None,\n+            session_hash: str,\n         ):\n             blocks = app.get_blocks()\n-            if blocks._queue.server_app is None:\n-                blocks._queue.set_server_app(app)\n-\n-            event = Event(session_hash, fn_index, request, username)\n-            if data is not None:\n-                input_data = json.loads(data)\n-                event.data = PredictBody(\n-                    session_hash=session_hash,\n-                    fn_index=fn_index,\n-                    data=input_data,\n-                    request=request,\n-                )\n-\n-            # Continuous events are not put in the queue so that they do not\n-            # occupy the queue's resource as they are expected to run forever\n-            if blocks.dependencies[event.fn_index].get(\"every\", 0):\n-                await cancel_tasks({f\"{event.session_hash}_{event.fn_index}\"})\n-                await blocks._queue.reset_iterators(event._id)\n-                blocks._queue.continuous_tasks.append(event)\n-                task = run_coro_in_background(\n-                    blocks._queue.process_events, [event], False\n-                )\n-                set_task_name(task, event.session_hash, event.fn_index, batch=False)\n-                app._asyncio_tasks.append(task)\n-            else:\n-                rank = blocks._queue.push(event)\n-                if rank is None:\n-                    event.send_message(\"queue_full\", final=True)\n-                else:\n-                    estimation = blocks._queue.get_estimation()\n-                    await blocks._queue.send_estimation(event, estimation, rank)\n \n             async def sse_stream(request: fastapi.Request):\n                 try:\n                     last_heartbeat = time.perf_counter()\n                     while True:\n                         if await request.is_disconnected():\n-                            await blocks._queue.clean_event(event)\n-                        if not event.alive:\n+                            await blocks._queue.clean_events(session_hash=session_hash)\n                             return\n \n+                        if (\n+                            session_hash\n+                            not in blocks._queue.pending_messages_per_session\n+                        ):\n+                            raise HTTPException(\n+                                status_code=status.HTTP_404_NOT_FOUND,\n+                                detail=\"Session not found.\",\n+                            )\n+\n                         heartbeat_rate = 15\n                         check_rate = 0.05\n                         message = None\n                         try:\n-                            message = event.message_queue.get_nowait()\n-                            if message is None:  # end of stream marker\n-                                return\n+                            messages = blocks._queue.pending_messages_per_session[\n+                                session_hash\n+                            ]\n+                            message = messages.get_nowait()\n                         except EmptyQueue:\n                             await asyncio.sleep(check_rate)\n                             if time.perf_counter() - last_heartbeat > heartbeat_rate:\n@@ -648,25 +620,47 @@ async def sse_stream(request: fastapi.Request):\n                                 # and then the stream will retry leading to infinite queue \ud83d\ude2c\n                                 last_heartbeat = time.perf_counter()\n \n+                        if blocks._queue.stopped:\n+                            message = {\"msg\": \"server_stopped\", \"success\": False}\n                         if message:\n                             yield f\"data: {json.dumps(message)}\\n\\n\"\n+                            if message[\"msg\"] == \"process_completed\":\n+                                blocks._queue.pending_event_ids_session[\n+                                    session_hash\n+                                ].remove(message[\"event_id\"])\n+                                if message[\"msg\"] == \"server_stopped\" or (\n+                                    message[\"msg\"] == \"process_completed\"\n+                                    and (\n+                                        len(\n+                                            blocks._queue.pending_event_ids_session[\n+                                                session_hash\n+                                            ]\n+                                        )\n+                                        == 0\n+                                    )\n+                                ):\n+                                    return\n                 except asyncio.CancelledError as e:\n-                    await blocks._queue.clean_event(event)\n+                    del blocks._queue.pending_messages_per_session[session_hash]\n+                    await blocks._queue.clean_events(session_hash=session_hash)\n                     raise e\n \n             return StreamingResponse(\n                 sse_stream(request),\n                 media_type=\"text/event-stream\",\n             )\n \n-        @app.post(\"/queue/data\", dependencies=[Depends(login_check)])\n-        async def queue_data(\n+        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])\n+        async def queue_join(\n             body: PredictBody,\n             request: fastapi.Request,\n             username: str = Depends(get_current_user),\n         ):\n-            blocks = app.get_blocks()\n-            blocks._queue.attach_data(body)\n+            if blocks._queue.server_app is None:\n+                blocks._queue.set_server_app(app)\n+\n+            event_id = await blocks._queue.push(body, request, username)\n+            return {\"event_id\": event_id}\n \n         @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n         @app.post(\"/component_server/\", dependencies=[Depends(login_check)])"
        },
        {
          "filename": "gradio/utils.py",
          "status": "modified",
          "additions": 19,
          "deletions": 0,
          "patch": "@@ -19,6 +19,7 @@\n import urllib.parse\n import warnings\n from abc import ABC, abstractmethod\n+from collections import OrderedDict\n from contextlib import contextmanager\n from io import BytesIO\n from numbers import Number\n@@ -28,6 +29,7 @@\n     TYPE_CHECKING,\n     Any,\n     Callable,\n+    Generic,\n     Iterable,\n     Iterator,\n     Optional,\n@@ -997,3 +999,20 @@ def convert_to_dict_if_dataclass(value):\n     if dataclasses.is_dataclass(value):\n         return dataclasses.asdict(value)\n     return value\n+\n+\n+K = TypeVar(\"K\")\n+V = TypeVar(\"V\")\n+\n+\n+class LRUCache(OrderedDict, Generic[K, V]):\n+    def __init__(self, max_size: int = 100):\n+        super().__init__()\n+        self.max_size: int = max_size\n+\n+    def __setitem__(self, key: K, value: V) -> None:\n+        if key in self:\n+            self.move_to_end(key)\n+        elif len(self) >= self.max_size:\n+            self.popitem(last=False)\n+        super().__setitem__(key, value)"
        },
        {
          "filename": "js/app/test/blocks_chained_events.spec.ts",
          "status": "modified",
          "additions": 0,
          "deletions": 30,
          "patch": "@@ -1,20 +1,10 @@\n import { test, expect } from \"@gradio/tootils\";\n \n test(\".success should not run if function fails\", async ({ page }) => {\n-\tlet last_iteration;\n \tconst textbox = page.getByLabel(\"Result\");\n \tawait expect(textbox).toHaveValue(\"\");\n \n-\tpage.on(\"websocket\", (ws) => {\n-\t\tlast_iteration = ws.waitForEvent(\"framereceived\", {\n-\t\t\tpredicate: (event) => {\n-\t\t\t\treturn JSON.parse(event.payload as string).msg === \"process_completed\";\n-\t\t\t}\n-\t\t});\n-\t});\n-\n \tawait page.click(\"text=Trigger Failure\");\n-\tawait last_iteration;\n \texpect(textbox).toHaveValue(\"\");\n });\n \n@@ -38,17 +28,7 @@ test(\"Consecutive .success event is triggered successfully\", async ({\n });\n \n test(\"gr.Error makes the toast show up\", async ({ page }) => {\n-\tlet complete;\n-\tpage.on(\"websocket\", (ws) => {\n-\t\tcomplete = ws.waitForEvent(\"framereceived\", {\n-\t\t\tpredicate: (event) => {\n-\t\t\t\treturn JSON.parse(event.payload as string).msg === \"process_completed\";\n-\t\t\t}\n-\t\t});\n-\t});\n-\n \tawait page.click(\"text=Trigger Failure\");\n-\tawait complete;\n \n \tconst toast = page.getByTestId(\"toast-body\");\n \texpect(toast).toContainText(\"error\");\n@@ -60,17 +40,7 @@ test(\"gr.Error makes the toast show up\", async ({ page }) => {\n test(\"ValueError makes the toast show up when show_error=True\", async ({\n \tpage\n }) => {\n-\tlet complete;\n-\tpage.on(\"websocket\", (ws) => {\n-\t\tcomplete = ws.waitForEvent(\"framereceived\", {\n-\t\t\tpredicate: (event) => {\n-\t\t\t\treturn JSON.parse(event.payload as string).msg === \"process_completed\";\n-\t\t\t}\n-\t\t});\n-\t});\n-\n \tawait page.click(\"text=Trigger Failure With ValueError\");\n-\tawait complete;\n \n \tconst toast = page.getByTestId(\"toast-body\");\n \texpect(toast).toContainText(\"error\");"
        },
        {
          "filename": "scripts/benchmark_queue.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -107,7 +107,7 @@ async def main(host, n_results=100):\n     parser.add_argument(\"-o\", \"--output\", type=str, help=\"path to write output to\", required=False)     \n     args = parser.parse_args()\n \n-    host = f\"{demo.local_url.replace('http', 'ws')}queue/join\"\n+    host = f\"{demo.local_url.replace('http', 'ws')}queue/data\"\n     data = asyncio.run(main(host, n_results=args.n_jobs))\n     data = dict(zip(data[\"fn_to_hit\"], data[\"duration\"]))\n     "
        },
        {
          "filename": "test/test_helpers.py",
          "status": "modified",
          "additions": 109,
          "deletions": 240,
          "patch": "@@ -8,7 +8,7 @@\n from pathlib import Path\n from unittest.mock import patch\n \n-import httpx\n+import gradio_client as grc\n import pytest\n from gradio_client import media_data, utils\n from pydub import AudioSegment\n@@ -660,50 +660,29 @@ def greet(s, prog=gr.Progress()):\n             button.click(greet, name, greeting)\n         demo.queue(max_size=1).launch(prevent_thread_lock=True)\n \n-        progress_updates = []\n-        async with httpx.AsyncClient() as client:\n-            async with client.stream(\n-                \"GET\",\n-                f\"http://localhost:{demo.server_port}/queue/join\",\n-                params={\"fn_index\": 0, \"session_hash\": \"shdce\"},\n-            ) as response:\n-                async for line in response.aiter_text():\n-                    if line.startswith(\"data:\"):\n-                        msg = json.loads(line[5:])\n-                    if msg[\"msg\"] == \"send_data\":\n-                        event_id = msg[\"event_id\"]\n-                        req = await client.post(\n-                            f\"http://localhost:{demo.server_port}/queue/data\",\n-                            json={\n-                                \"event_id\": event_id,\n-                                \"data\": [0],\n-                                \"fn_index\": 0,\n-                            },\n-                        )\n-                        if not req.is_success:\n-                            raise ValueError(\n-                                f\"Could not send payload to endpoint: {req.text}\"\n-                            )\n-                    if msg[\"msg\"] == \"progress\":\n-                        progress_updates.append(msg[\"progress_data\"])\n-                    if msg[\"msg\"] == \"process_completed\":\n-                        break\n-\n-        assert progress_updates == [\n-            [\n-                {\n-                    \"index\": None,\n-                    \"length\": None,\n-                    \"unit\": \"steps\",\n-                    \"progress\": 0.0,\n-                    \"desc\": \"start\",\n-                }\n-            ],\n-            [{\"index\": 0, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 1, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 2, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 3, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 4, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n+        client = grc.Client(demo.local_url)\n+        job = client.submit(\"Gradio\")\n+\n+        status_updates = []\n+        while not job.done():\n+            status = job.status()\n+            update = (\n+                status.progress_data[0].index if status.progress_data else None,\n+                status.progress_data[0].desc if status.progress_data else None,\n+            )\n+            if update != (None, None) and (\n+                len(status_updates) == 0 or status_updates[-1] != update\n+            ):\n+                status_updates.append(update)\n+            time.sleep(0.05)\n+\n+        assert status_updates == [\n+            (None, \"start\"),\n+            (0, None),\n+            (1, None),\n+            (2, None),\n+            (3, None),\n+            (4, None),\n         ]\n \n     @pytest.mark.asyncio\n@@ -726,77 +705,32 @@ def greet(s, prog=gr.Progress(track_tqdm=True)):\n             button.click(greet, name, greeting)\n         demo.queue(max_size=1).launch(prevent_thread_lock=True)\n \n-        progress_updates = []\n-        async with httpx.AsyncClient() as client:\n-            async with client.stream(\n-                \"GET\",\n-                f\"http://localhost:{demo.server_port}/queue/join\",\n-                params={\"fn_index\": 0, \"session_hash\": \"shdce\"},\n-            ) as response:\n-                async for line in response.aiter_text():\n-                    if line.startswith(\"data:\"):\n-                        msg = json.loads(line[5:])\n-                    if msg[\"msg\"] == \"send_data\":\n-                        event_id = msg[\"event_id\"]\n-                        req = await client.post(\n-                            f\"http://localhost:{demo.server_port}/queue/data\",\n-                            json={\n-                                \"event_id\": event_id,\n-                                \"data\": [0],\n-                                \"fn_index\": 0,\n-                            },\n-                        )\n-                        if not req.is_success:\n-                            raise ValueError(\n-                                f\"Could not send payload to endpoint: {req.text}\"\n-                            )\n-                    if msg[\"msg\"] == \"progress\":\n-                        progress_updates.append(msg[\"progress_data\"])\n-                    if msg[\"msg\"] == \"process_completed\":\n-                        break\n-\n-        assert progress_updates == [\n-            [\n-                {\n-                    \"index\": None,\n-                    \"length\": None,\n-                    \"unit\": \"steps\",\n-                    \"progress\": 0.0,\n-                    \"desc\": \"start\",\n-                }\n-            ],\n-            [{\"index\": 0, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 1, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 2, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 3, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [{\"index\": 4, \"length\": 4, \"unit\": \"iter\", \"progress\": None, \"desc\": None}],\n-            [\n-                {\n-                    \"index\": 0,\n-                    \"length\": 3,\n-                    \"unit\": \"steps\",\n-                    \"progress\": None,\n-                    \"desc\": \"alphabet\",\n-                }\n-            ],\n-            [\n-                {\n-                    \"index\": 1,\n-                    \"length\": 3,\n-                    \"unit\": \"steps\",\n-                    \"progress\": None,\n-                    \"desc\": \"alphabet\",\n-                }\n-            ],\n-            [\n-                {\n-                    \"index\": 2,\n-                    \"length\": 3,\n-                    \"unit\": \"steps\",\n-                    \"progress\": None,\n-                    \"desc\": \"alphabet\",\n-                }\n-            ],\n+        client = grc.Client(demo.local_url)\n+        job = client.submit(\"Gradio\")\n+\n+        status_updates = []\n+        while not job.done():\n+            status = job.status()\n+            update = (\n+                status.progress_data[0].index if status.progress_data else None,\n+                status.progress_data[0].desc if status.progress_data else None,\n+            )\n+            if update != (None, None) and (\n+                len(status_updates) == 0 or status_updates[-1] != update\n+            ):\n+                status_updates.append(update)\n+            time.sleep(0.05)\n+\n+        assert status_updates == [\n+            (None, \"start\"),\n+            (0, None),\n+            (1, None),\n+            (2, None),\n+            (3, None),\n+            (4, None),\n+            (0, \"alphabet\"),\n+            (1, \"alphabet\"),\n+            (2, \"alphabet\"),\n         ]\n \n     @pytest.mark.asyncio\n@@ -811,63 +745,29 @@ def greet(s, _=gr.Progress(track_tqdm=True)):\n         demo = gr.Interface(greet, \"text\", \"text\")\n         demo.queue().launch(prevent_thread_lock=True)\n \n-        progress_updates = []\n-        async with httpx.AsyncClient() as client:\n-            async with client.stream(\n-                \"GET\",\n-                f\"http://localhost:{demo.server_port}/queue/join\",\n-                params={\"fn_index\": 0, \"session_hash\": \"shdce\"},\n-            ) as response:\n-                async for line in response.aiter_text():\n-                    if line.startswith(\"data:\"):\n-                        msg = json.loads(line[5:])\n-                    if msg[\"msg\"] == \"send_data\":\n-                        event_id = msg[\"event_id\"]\n-                        req = await client.post(\n-                            f\"http://localhost:{demo.server_port}/queue/data\",\n-                            json={\n-                                \"event_id\": event_id,\n-                                \"data\": [\"abc\"],\n-                                \"fn_index\": 0,\n-                            },\n-                        )\n-                        if not req.is_success:\n-                            raise ValueError(\n-                                f\"Could not send payload to endpoint: {req.text}\"\n-                            )\n-                    if msg[\"msg\"] == \"progress\":\n-                        progress_updates.append(msg[\"progress_data\"])\n-                    if msg[\"msg\"] == \"process_completed\":\n-                        break\n-\n-        assert progress_updates == [\n-            [\n-                {\n-                    \"index\": 1,\n-                    \"length\": 3,\n-                    \"unit\": \"steps\",\n-                    \"progress\": None,\n-                    \"desc\": None,\n-                }\n-            ],\n-            [\n-                {\n-                    \"index\": 2,\n-                    \"length\": 3,\n-                    \"unit\": \"steps\",\n-                    \"progress\": None,\n-                    \"desc\": None,\n-                }\n-            ],\n-            [\n-                {\n-                    \"index\": 3,\n-                    \"length\": 3,\n-                    \"unit\": \"steps\",\n-                    \"progress\": None,\n-                    \"desc\": None,\n-                }\n-            ],\n+        client = grc.Client(demo.local_url)\n+        job = client.submit(\"Gradio\")\n+\n+        status_updates = []\n+        while not job.done():\n+            status = job.status()\n+            update = (\n+                status.progress_data[0].index if status.progress_data else None,\n+                status.progress_data[0].unit if status.progress_data else None,\n+            )\n+            if update != (None, None) and (\n+                len(status_updates) == 0 or status_updates[-1] != update\n+            ):\n+                status_updates.append(update)\n+            time.sleep(0.05)\n+\n+        assert status_updates == [\n+            (1, \"steps\"),\n+            (2, \"steps\"),\n+            (3, \"steps\"),\n+            (4, \"steps\"),\n+            (5, \"steps\"),\n+            (6, \"steps\"),\n         ]\n \n     @pytest.mark.asyncio\n@@ -878,45 +778,30 @@ def greet(s):\n                 time.sleep(0.15)\n             if len(s) < 5:\n                 gr.Warning(\"Too short!\")\n+                time.sleep(0.15)\n             return f\"Hello, {s}!\"\n \n         demo = gr.Interface(greet, \"text\", \"text\")\n         demo.queue().launch(prevent_thread_lock=True)\n \n-        log_messages = []\n-        async with httpx.AsyncClient() as client:\n-            async with client.stream(\n-                \"GET\",\n-                f\"http://localhost:{demo.server_port}/queue/join\",\n-                params={\"fn_index\": 0, \"session_hash\": \"shdce\"},\n-            ) as response:\n-                async for line in response.aiter_text():\n-                    if line.startswith(\"data:\"):\n-                        msg = json.loads(line[5:])\n-                    if msg[\"msg\"] == \"send_data\":\n-                        event_id = msg[\"event_id\"]\n-                        req = await client.post(\n-                            f\"http://localhost:{demo.server_port}/queue/data\",\n-                            json={\n-                                \"event_id\": event_id,\n-                                \"data\": [\"abc\"],\n-                                \"fn_index\": 0,\n-                            },\n-                        )\n-                        if not req.is_success:\n-                            raise ValueError(\n-                                f\"Could not send payload to endpoint: {req.text}\"\n-                            )\n-                    if msg[\"msg\"] == \"log\":\n-                        log_messages.append([msg[\"log\"], msg[\"level\"]])\n-                    if msg[\"msg\"] == \"process_completed\":\n-                        break\n-\n-        assert log_messages == [\n-            [\"Letter a\", \"info\"],\n-            [\"Letter b\", \"info\"],\n-            [\"Letter c\", \"info\"],\n-            [\"Too short!\", \"warning\"],\n+        client = grc.Client(demo.local_url)\n+        job = client.submit(\"Jon\")\n+\n+        status_updates = []\n+        while not job.done():\n+            status = job.status()\n+            update = status.log\n+            if update is not None and (\n+                len(status_updates) == 0 or status_updates[-1] != update\n+            ):\n+                status_updates.append(update)\n+            time.sleep(0.05)\n+\n+        assert status_updates == [\n+            (\"Letter J\", \"info\"),\n+            (\"Letter o\", \"info\"),\n+            (\"Letter n\", \"info\"),\n+            (\"Too short!\", \"warning\"),\n         ]\n \n \n@@ -926,11 +811,13 @@ async def test_info_isolation(async_handler: bool):\n     async def greet_async(name):\n         await asyncio.sleep(2)\n         gr.Info(f\"Hello {name}\")\n+        await asyncio.sleep(1)\n         return name\n \n     def greet_sync(name):\n         time.sleep(2)\n         gr.Info(f\"Hello {name}\")\n+        time.sleep(1)\n         return name\n \n     demo = gr.Interface(\n@@ -942,42 +829,24 @@ def greet_sync(name):\n     demo.launch(prevent_thread_lock=True)\n \n     async def session_interaction(name, delay=0):\n-        await asyncio.sleep(delay)\n-\n-        log_messages = []\n-        async with httpx.AsyncClient() as client:\n-            async with client.stream(\n-                \"GET\",\n-                f\"http://localhost:{demo.server_port}/queue/join\",\n-                params={\"fn_index\": 0, \"session_hash\": name},\n-            ) as response:\n-                async for line in response.aiter_text():\n-                    if line.startswith(\"data:\"):\n-                        msg = json.loads(line[5:])\n-                    if msg[\"msg\"] == \"send_data\":\n-                        event_id = msg[\"event_id\"]\n-                        req = await client.post(\n-                            f\"http://localhost:{demo.server_port}/queue/data\",\n-                            json={\n-                                \"event_id\": event_id,\n-                                \"data\": [name],\n-                                \"fn_index\": 0,\n-                            },\n-                        )\n-                        if not req.is_success:\n-                            raise ValueError(\n-                                f\"Could not send payload to endpoint: {req.text}\"\n-                            )\n-                    if msg[\"msg\"] == \"log\":\n-                        log_messages.append(msg[\"log\"])\n-                    if msg[\"msg\"] == \"process_completed\":\n-                        break\n-        return log_messages\n+        client = grc.Client(demo.local_url)\n+        job = client.submit(name)\n+\n+        status_updates = []\n+        while not job.done():\n+            status = job.status()\n+            update = status.log\n+            if update is not None and (\n+                len(status_updates) == 0 or status_updates[-1] != update\n+            ):\n+                status_updates.append(update)\n+            time.sleep(0.05)\n+        return status_updates[-1][0] if status_updates else None\n \n     alice_logs, bob_logs = await asyncio.gather(\n         session_interaction(\"Alice\"),\n         session_interaction(\"Bob\", delay=1),\n     )\n \n-    assert alice_logs == [\"Hello Alice\"]\n-    assert bob_logs == [\"Hello Bob\"]\n+    assert alice_logs == \"Hello Alice\"\n+    assert bob_logs == \"Hello Bob\""
        },
        {
          "filename": "test/test_queueing.py",
          "status": "modified",
          "additions": 3,
          "deletions": 6,
          "patch": "@@ -18,8 +18,6 @@ def greet(x):\n \n             name.submit(greet, name, output)\n \n-        demo.launch(prevent_thread_lock=True)\n-\n         with connect(demo) as client:\n             job = client.submit(\"x\", fn_index=0)\n             assert job.result() == \"Hello, x!\"\n@@ -92,7 +90,7 @@ def test_default_concurrency_limits(self, default_concurrency_limit, statuses):\n \n             @add_btn.click(inputs=[a, b], outputs=output)\n             def add(x, y):\n-                time.sleep(2)\n+                time.sleep(4)\n                 return x + y\n \n         demo.queue(default_concurrency_limit=default_concurrency_limit)\n@@ -105,7 +103,7 @@ def add(x, y):\n         add_job_2 = client.submit(1, 1, fn_index=0)\n         add_job_3 = client.submit(1, 1, fn_index=0)\n \n-        time.sleep(1)\n+        time.sleep(2)\n \n         add_job_statuses = [add_job_1.status(), add_job_2.status(), add_job_3.status()]\n         assert sorted([s.code.value for s in add_job_statuses]) == statuses\n@@ -161,12 +159,11 @@ def div(x, y):\n             sub_job_1 = client.submit(1, 1, fn_index=1)\n             sub_job_2 = client.submit(1, 1, fn_index=1)\n             sub_job_3 = client.submit(1, 1, fn_index=1)\n-            sub_job_3 = client.submit(1, 1, fn_index=1)\n             mul_job_1 = client.submit(1, 1, fn_index=2)\n             div_job_1 = client.submit(1, 1, fn_index=3)\n             mul_job_2 = client.submit(1, 1, fn_index=2)\n \n-            time.sleep(1)\n+            time.sleep(2)\n \n             add_job_statuses = [\n                 add_job_1.status(),"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 4,
        "unique_directories": 8,
        "max_directory_depth": 3
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "62cd4ef302cd7529d1f8842b416f9d8fcf6b5750",
            "date": "2025-01-14T15:52:06Z",
            "author_login": "hannahblair"
          },
          {
            "sha": "a91cb9c42c02b70273b4cd55ab18148a7436fe57",
            "date": "2025-01-13T19:14:27Z",
            "author_login": "amanchauhan11"
          },
          {
            "sha": "7fa9b6fc97b90a4c0d07cbf066b810247fc84724",
            "date": "2025-01-10T20:39:10Z",
            "author_login": "gradio-pr-bot"
          },
          {
            "sha": "e742dcccb376692c9ddd5a6c251080e7c5936574",
            "date": "2025-01-10T19:46:57Z",
            "author_login": "aliabid94"
          },
          {
            "sha": "decb5944552e951525c283e069e116c1ff6b6807",
            "date": "2025-01-10T18:28:16Z",
            "author_login": "abidlabs"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 9.4,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:L",
    "cwe_id": "CWE-22",
    "description": "A local file include could be remotely triggered in Gradio due to a vulnerable user-supplied JSON value in an API request.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-02-05T23:15:08.190",
    "last_modified": "2024-11-21T08:47:54.250",
    "fix_date": "2023-12-12T23:24:46Z"
  },
  "references": [
    {
      "url": "https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70",
      "source": "security@huntr.dev",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://huntr.com/bounties/25e25501-5918-429c-8541-88832dfd3741",
      "source": "security@huntr.dev",
      "tags": [
        "Exploit",
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://huntr.com/bounties/25e25501-5918-429c-8541-88832dfd3741",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Exploit",
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:07:05.587959",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "gradio",
    "owner": "gradio-app",
    "created_at": "2018-12-19T08:24:04Z",
    "updated_at": "2025-01-14T14:42:12Z",
    "pushed_at": "2025-01-14T12:42:39Z",
    "size": 280474,
    "stars": 35207,
    "forks": 2655,
    "open_issues": 470,
    "watchers": 35207,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [],
    "languages": {
      "Python": 3114195,
      "Svelte": 1181173,
      "TypeScript": 956766,
      "JavaScript": 60749,
      "CSS": 51082,
      "Jupyter Notebook": 32113,
      "HTML": 22988,
      "Batchfile": 6463,
      "Shell": 6049,
      "MDX": 1670
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "apache-2.0"
    },
    "collected_at": "2025-01-14T15:07:33.405924"
  }
}