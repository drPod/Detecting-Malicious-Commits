{
  "cve_id": "CVE-2024-34359",
  "github_data": {
    "repository": "abetlen/llama-cpp-python",
    "fix_commit": "b454f40a9a1787b2b5659cd2cb00819d983185df",
    "related_commits": [
      "b454f40a9a1787b2b5659cd2cb00819d983185df",
      "b454f40a9a1787b2b5659cd2cb00819d983185df"
    ],
    "patch_url": "https://github.com/abetlen/llama-cpp-python/commit/b454f40a9a1787b2b5659cd2cb00819d983185df.patch",
    "fix_commit_details": {
      "sha": "b454f40a9a1787b2b5659cd2cb00819d983185df",
      "commit_date": "2024-05-10T04:47:56Z",
      "author": {
        "login": "retr0reg",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Merge pull request from GHSA-56xg-wfcc-g829",
        "length": 87,
        "has_description": true,
        "references_issue": false
      },
      "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
      },
      "files": [
        {
          "filename": "llama_cpp/llama_chat_format.py",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -11,6 +11,7 @@\n from typing import Any, Dict, Iterator, List, Literal, Optional, Tuple, Union, Protocol, cast\n \n import jinja2\n+from jinja2.sandbox import ImmutableSandboxedEnvironment\n \n import numpy as np\n import numpy.typing as npt\n@@ -191,7 +192,7 @@ def __init__(\n         self.add_generation_prompt = add_generation_prompt\n         self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n \n-        self._environment = jinja2.Environment(\n+        self._environment = ImmutableSandboxedEnvironment(\n             loader=jinja2.BaseLoader(),\n             trim_blocks=True,\n             lstrip_blocks=True,"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 1,
        "max_directory_depth": 1
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "0580cf273debf4a7f2efcdfd5ef092ff5cedf9b0",
            "date": "2025-01-08T21:53:54Z",
            "author_login": "abetlen"
          },
          {
            "sha": "e8f14ceff9b3279c67368d85f9457bb27168d126",
            "date": "2025-01-08T21:46:43Z",
            "author_login": "gjpower"
          },
          {
            "sha": "1d5f5340e49d90bc097c1f75605a4dd7408c2378",
            "date": "2025-01-08T21:33:53Z",
            "author_login": "abetlen"
          },
          {
            "sha": "c9dfad4b6fdebc85f8f7914e8e46c6d9299ff4b4",
            "date": "2024-12-30T18:43:46Z",
            "author_login": "abetlen"
          },
          {
            "sha": "2bc1d97c9672320828e70dc8293d5f8754682109",
            "date": "2024-12-19T06:55:12Z",
            "author_login": "abetlen"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 9.6,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H",
    "cwe_id": "CWE-76",
    "description": "llama-cpp-python is the Python bindings for llama.cpp. `llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to remote code execution by a carefully constructed payload.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-05-14T15:38:45.093",
    "last_modified": "2024-11-21T09:18:30.130",
    "fix_date": "2024-05-10T04:47:56Z"
  },
  "references": [
    {
      "url": "https://github.com/abetlen/llama-cpp-python/commit/b454f40a9a1787b2b5659cd2cb00819d983185df",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/abetlen/llama-cpp-python/security/advisories/GHSA-56xg-wfcc-g829",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/abetlen/llama-cpp-python/commit/b454f40a9a1787b2b5659cd2cb00819d983185df",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://github.com/abetlen/llama-cpp-python/security/advisories/GHSA-56xg-wfcc-g829",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:20.896407",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "llama-cpp-python",
    "owner": "abetlen",
    "created_at": "2023-03-23T09:30:33Z",
    "updated_at": "2025-01-14T19:37:23Z",
    "pushed_at": "2025-01-08T21:54:38Z",
    "size": 2344,
    "stars": 8407,
    "forks": 1001,
    "open_issues": 550,
    "watchers": 8407,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [],
    "languages": {
      "Python": 554102,
      "CMake": 6738,
      "Shell": 5703,
      "Dockerfile": 3987,
      "Makefile": 2355
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "mit"
    },
    "collected_at": "2025-01-14T22:22:28.045673"
  }
}