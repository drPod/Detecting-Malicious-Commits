{
  "cve_id": "CVE-2024-3572",
  "github_data": {
    "repository": "scrapy/scrapy",
    "fix_commit": "809bfac4890f75fc73607318a04d2ccba71b3d9f",
    "related_commits": [
      "809bfac4890f75fc73607318a04d2ccba71b3d9f",
      "809bfac4890f75fc73607318a04d2ccba71b3d9f"
    ],
    "patch_url": "https://github.com/scrapy/scrapy/commit/809bfac4890f75fc73607318a04d2ccba71b3d9f.patch",
    "fix_commit_details": {
      "sha": "809bfac4890f75fc73607318a04d2ccba71b3d9f",
      "commit_date": "2024-02-14T16:16:25Z",
      "author": {
        "login": "Gallaecio",
        "type": "User",
        "stats": {
          "total_commits": 664,
          "average_weekly_commits": 0.7676300578034682,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 166
        }
      },
      "commit_message": {
        "title": "Merge branch '2.11-compression-bomb' into 2.11",
        "length": 46,
        "has_description": false,
        "references_issue": false
      },
      "stats": {
        "total": 839,
        "additions": 613,
        "deletions": 226
      },
      "files": [
        {
          "filename": "docs/news.rst",
          "status": "modified",
          "additions": 19,
          "deletions": 0,
          "patch": "@@ -19,6 +19,16 @@ Highlights:\n Security bug fixes\n ~~~~~~~~~~~~~~~~~~\n \n+-   :setting:`DOWNLOAD_MAXSIZE` and :setting:`DOWNLOAD_WARNSIZE` now also apply\n+    to the decompressed response body. Please, see the `7j7m-v7m3-jqm7 security\n+    advisory`_ for more information.\n+\n+    .. _7j7m-v7m3-jqm7 security advisory: https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7\n+\n+-   Also in relation with the `7j7m-v7m3-jqm7 security advisory`_, the\n+    deprecated ``scrapy.downloadermiddlewares.decompression`` module has been\n+    removed.\n+\n -   The ``Authorization`` header is now dropped on redirects to a different\n     domain. Please, see the `cw9j-q3vf-hrrv security advisory`_ for more\n     information.\n@@ -2941,13 +2951,22 @@ affect subclasses:\n \n (:issue:`3884`)\n \n+\n .. _release-1.8.4:\n \n Scrapy 1.8.4 (unreleased)\n -------------------------\n \n **Security bug fixes:**\n \n+-   :setting:`DOWNLOAD_MAXSIZE` and :setting:`DOWNLOAD_WARNSIZE` now also apply\n+    to the decompressed response body. Please, see the `7j7m-v7m3-jqm7 security\n+    advisory`_ for more information.\n+\n+-   Also in relation with the `7j7m-v7m3-jqm7 security advisory`_, use of the\n+    ``scrapy.downloadermiddlewares.decompression`` module is discouraged and\n+    will trigger a warning.\n+\n -   The ``Authorization`` header is now dropped on redirects to a different\n     domain. Please, see the `cw9j-q3vf-hrrv security advisory`_ for more\n     information."
        },
        {
          "filename": "docs/topics/request-response.rst",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -731,6 +731,7 @@ Those are:\n * :reqmeta:`download_fail_on_dataloss`\n * :reqmeta:`download_latency`\n * :reqmeta:`download_maxsize`\n+* :reqmeta:`download_warnsize`\n * :reqmeta:`download_timeout`\n * ``ftp_password`` (See :setting:`FTP_PASSWORD` for more info)\n * ``ftp_user`` (See :setting:`FTP_USER` for more info)"
        },
        {
          "filename": "docs/topics/settings.rst",
          "status": "modified",
          "additions": 19,
          "deletions": 17,
          "patch": "@@ -873,40 +873,42 @@ The amount of time (in secs) that the downloader will wait before timing out.\n     Request.meta key.\n \n .. setting:: DOWNLOAD_MAXSIZE\n+.. reqmeta:: download_maxsize\n \n DOWNLOAD_MAXSIZE\n ----------------\n \n-Default: ``1073741824`` (1024MB)\n-\n-The maximum response size (in bytes) that downloader will download.\n+Default: ``1073741824`` (1 GiB)\n \n-If you want to disable it set to 0.\n+The maximum response body size (in bytes) allowed. Bigger responses are\n+aborted and ignored.\n \n-.. reqmeta:: download_maxsize\n+This applies both before and after compression. If decompressing a response\n+body would exceed this limit, decompression is aborted and the response is\n+ignored.\n \n-.. note::\n+Use ``0`` to disable this limit.\n \n-    This size can be set per spider using :attr:`download_maxsize`\n-    spider attribute and per-request using :reqmeta:`download_maxsize`\n-    Request.meta key.\n+This limit can be set per spider using the :attr:`download_maxsize` spider\n+attribute and per request using the :reqmeta:`download_maxsize` Request.meta\n+key.\n \n .. setting:: DOWNLOAD_WARNSIZE\n+.. reqmeta:: download_warnsize\n \n DOWNLOAD_WARNSIZE\n -----------------\n \n-Default: ``33554432`` (32MB)\n-\n-The response size (in bytes) that downloader will start to warn.\n+Default: ``33554432`` (32 MiB)\n \n-If you want to disable it set to 0.\n+If the size of a response exceeds this value, before or after compression, a\n+warning will be logged about it.\n \n-.. note::\n+Use ``0`` to disable this limit.\n \n-    This size can be set per spider using :attr:`download_warnsize`\n-    spider attribute and per-request using :reqmeta:`download_warnsize`\n-    Request.meta key.\n+This limit can be set per spider using the :attr:`download_warnsize` spider\n+attribute and per request using the :reqmeta:`download_warnsize` Request.meta\n+key.\n \n .. setting:: DOWNLOAD_FAIL_ON_DATALOSS\n "
        },
        {
          "filename": "scrapy/downloadermiddlewares/decompression.py",
          "status": "removed",
          "additions": 0,
          "deletions": 94,
          "patch": "@@ -1,94 +0,0 @@\n-\"\"\" This module implements the DecompressionMiddleware which tries to recognise\n-and extract the potentially compressed responses that may arrive.\n-\"\"\"\n-\n-import bz2\n-import gzip\n-import logging\n-import tarfile\n-import zipfile\n-from io import BytesIO\n-from tempfile import mktemp\n-from warnings import warn\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.responsetypes import responsetypes\n-\n-warn(\n-    \"scrapy.downloadermiddlewares.decompression is deprecated\",\n-    ScrapyDeprecationWarning,\n-    stacklevel=2,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class DecompressionMiddleware:\n-    \"\"\"This middleware tries to recognise and extract the possibly compressed\n-    responses that may arrive.\"\"\"\n-\n-    def __init__(self):\n-        self._formats = {\n-            \"tar\": self._is_tar,\n-            \"zip\": self._is_zip,\n-            \"gz\": self._is_gzip,\n-            \"bz2\": self._is_bzip2,\n-        }\n-\n-    def _is_tar(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n-        except tarfile.ReadError:\n-            return\n-\n-        body = tar_file.extractfile(tar_file.members[0]).read()\n-        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_zip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            zip_file = zipfile.ZipFile(archive)\n-        except zipfile.BadZipFile:\n-            return\n-\n-        namelist = zip_file.namelist()\n-        body = zip_file.read(namelist[0])\n-        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_gzip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            body = gzip.GzipFile(fileobj=archive).read()\n-        except OSError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_bzip2(self, response):\n-        try:\n-            body = bz2.decompress(response.body)\n-        except OSError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def process_response(self, request, response, spider):\n-        if not response.body:\n-            return response\n-\n-        for fmt, func in self._formats.items():\n-            new_response = func(response)\n-            if new_response:\n-                logger.debug(\n-                    \"Decompressed response with format: %(responsefmt)s\",\n-                    {\"responsefmt\": fmt},\n-                    extra={\"spider\": spider},\n-                )\n-                return new_response\n-        return response"
        },
        {
          "filename": "scrapy/downloadermiddlewares/httpcompression.py",
          "status": "modified",
          "additions": 65,
          "deletions": 35,
          "patch": "@@ -1,53 +1,78 @@\n-import io\n import warnings\n-import zlib\n+from logging import getLogger\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy import signals\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils._compression import (\n+    _DecompressionMaxSizeExceeded,\n+    _inflate,\n+    _unbrotli,\n+    _unzstd,\n+)\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n+logger = getLogger(__name__)\n+\n ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\n \n try:\n-    import brotli\n-\n-    ACCEPTED_ENCODINGS.append(b\"br\")\n+    import brotli  # noqa: F401\n except ImportError:\n     pass\n+else:\n+    ACCEPTED_ENCODINGS.append(b\"br\")\n \n try:\n-    import zstandard\n-\n-    ACCEPTED_ENCODINGS.append(b\"zstd\")\n+    import zstandard  # noqa: F401\n except ImportError:\n     pass\n+else:\n+    ACCEPTED_ENCODINGS.append(b\"zstd\")\n \n \n class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n \n-    def __init__(self, stats=None):\n-        self.stats = stats\n+    def __init__(self, stats=None, *, crawler=None):\n+        if not crawler:\n+            self.stats = stats\n+            self._max_size = 1073741824\n+            self._warn_size = 33554432\n+            return\n+        self.stats = crawler.stats\n+        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n+        crawler.signals.connect(self.open_spider, signals.spider_opened)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n             raise NotConfigured\n         try:\n-            return cls(stats=crawler.stats)\n+            return cls(crawler=crawler)\n         except TypeError:\n             warnings.warn(\n                 \"HttpCompressionMiddleware subclasses must either modify \"\n-                \"their '__init__' method to support a 'stats' parameter or \"\n-                \"reimplement the 'from_crawler' method.\",\n+                \"their '__init__' method to support a 'crawler' parameter or \"\n+                \"reimplement their 'from_crawler' method.\",\n                 ScrapyDeprecationWarning,\n             )\n-            result = cls()\n-            result.stats = crawler.stats\n-            return result\n+            mw = cls()\n+            mw.stats = crawler.stats\n+            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n+            crawler.signals.connect(mw.open_spider, signals.spider_opened)\n+            return mw\n+\n+    def open_spider(self, spider):\n+        if hasattr(spider, \"download_maxsize\"):\n+            self._max_size = spider.download_maxsize\n+        if hasattr(spider, \"download_warnsize\"):\n+            self._warn_size = spider.download_warnsize\n \n     def process_request(self, request, spider):\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n@@ -59,7 +84,24 @@ def process_response(self, request, response, spider):\n             content_encoding = response.headers.getlist(\"Content-Encoding\")\n             if content_encoding:\n                 encoding = content_encoding.pop()\n-                decoded_body = self._decode(response.body, encoding.lower())\n+                max_size = request.meta.get(\"download_maxsize\", self._max_size)\n+                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\n+                try:\n+                    decoded_body = self._decode(\n+                        response.body, encoding.lower(), max_size\n+                    )\n+                except _DecompressionMaxSizeExceeded:\n+                    raise IgnoreRequest(\n+                        f\"Ignored response {response} because its body \"\n+                        f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"\n+                        f\"({max_size} B) during decompression.\"\n+                    )\n+                if len(response.body) < warn_size <= len(decoded_body):\n+                    logger.warning(\n+                        f\"{response} body size after decompression \"\n+                        f\"({len(decoded_body)} B) is larger than the \"\n+                        f\"download warning size ({warn_size} B).\"\n+                    )\n                 if self.stats:\n                     self.stats.inc_value(\n                         \"httpcompression/response_bytes\",\n@@ -83,25 +125,13 @@ def process_response(self, request, response, spider):\n \n         return response\n \n-    def _decode(self, body, encoding):\n+    def _decode(self, body, encoding, max_size):\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\n-            body = gunzip(body)\n-\n+            return gunzip(body, max_size=max_size)\n         if encoding == b\"deflate\":\n-            try:\n-                body = zlib.decompress(body)\n-            except zlib.error:\n-                # ugly hack to work with raw deflate content that may\n-                # be sent by microsoft servers. For more information, see:\n-                # http://carsten.codimi.de/gzip.yaws/\n-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n-                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n-                body = zlib.decompress(body, -15)\n+            return _inflate(body, max_size=max_size)\n         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n-            body = brotli.decompress(body)\n+            return _unbrotli(body, max_size=max_size)\n         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n-            # Using its streaming API since its simple API could handle only cases\n-            # where there is content size data embedded in the frame\n-            reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\n-            body = reader.read()\n+            return _unzstd(body, max_size=max_size)\n         return body"
        },
        {
          "filename": "scrapy/spiders/sitemap.py",
          "status": "modified",
          "additions": 34,
          "deletions": 1,
          "patch": "@@ -1,11 +1,19 @@\n import logging\n import re\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.http import Request, XmlResponse\n from scrapy.spiders import Spider\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded\n from scrapy.utils.gz import gunzip, gzip_magic_number\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -14,6 +22,19 @@ class SitemapSpider(Spider):\n     sitemap_rules = [(\"\", \"parse\")]\n     sitemap_follow = [\"\"]\n     sitemap_alternate_links = False\n+    _max_size: int\n+    _warn_size: int\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n+        spider._max_size = getattr(\n+            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+        )\n+        spider._warn_size = getattr(\n+            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\n+        )\n+        return spider\n \n     def __init__(self, *a, **kw):\n         super().__init__(*a, **kw)\n@@ -71,7 +92,19 @@ def _get_sitemap_body(self, response):\n         if isinstance(response, XmlResponse):\n             return response.body\n         if gzip_magic_number(response):\n-            return gunzip(response.body)\n+            uncompressed_size = len(response.body)\n+            max_size = response.meta.get(\"download_maxsize\", self._max_size)\n+            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\n+            try:\n+                body = gunzip(response.body, max_size=max_size)\n+            except _DecompressionMaxSizeExceeded:\n+                return None\n+            if uncompressed_size < warn_size <= len(body):\n+                logger.warning(\n+                    f\"{response} body size after decompression ({len(body)} B) \"\n+                    f\"is larger than the download warning size ({warn_size} B).\"\n+                )\n+            return body\n         # actual gzipped sitemap files are decompressed above ;\n         # if we are here (response body is not gzipped)\n         # and have a response for .xml.gz,"
        },
        {
          "filename": "scrapy/utils/_compression.py",
          "status": "added",
          "additions": 94,
          "deletions": 0,
          "patch": "@@ -0,0 +1,94 @@\n+import zlib\n+from io import BytesIO\n+\n+try:\n+    import brotli\n+except ImportError:\n+    pass\n+\n+try:\n+    import zstandard\n+except ImportError:\n+    pass\n+\n+\n+_CHUNK_SIZE = 65536  # 64 KiB\n+\n+\n+class _DecompressionMaxSizeExceeded(ValueError):\n+    pass\n+\n+\n+def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n+    decompressor = zlib.decompressobj()\n+    raw_decompressor = zlib.decompressobj(wbits=-15)\n+    input_stream = BytesIO(data)\n+    output_stream = BytesIO()\n+    output_chunk = b\".\"\n+    decompressed_size = 0\n+    while output_chunk:\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\n+        try:\n+            output_chunk = decompressor.decompress(input_chunk)\n+        except zlib.error:\n+            if decompressor != raw_decompressor:\n+                # ugly hack to work with raw deflate content that may\n+                # be sent by microsoft servers. For more information, see:\n+                # http://carsten.codimi.de/gzip.yaws/\n+                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n+                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n+                decompressor = raw_decompressor\n+                output_chunk = decompressor.decompress(input_chunk)\n+            else:\n+                raise\n+        decompressed_size += len(output_chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size} B) exceed the specified maximum \"\n+                f\"({max_size} B).\"\n+            )\n+        output_stream.write(output_chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()\n+\n+\n+def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n+    decompressor = brotli.Decompressor()\n+    input_stream = BytesIO(data)\n+    output_stream = BytesIO()\n+    output_chunk = b\".\"\n+    decompressed_size = 0\n+    while output_chunk:\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\n+        output_chunk = decompressor.process(input_chunk)\n+        decompressed_size += len(output_chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size} B) exceed the specified maximum \"\n+                f\"({max_size} B).\"\n+            )\n+        output_stream.write(output_chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()\n+\n+\n+def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n+    decompressor = zstandard.ZstdDecompressor()\n+    stream_reader = decompressor.stream_reader(BytesIO(data))\n+    output_stream = BytesIO()\n+    output_chunk = b\".\"\n+    decompressed_size = 0\n+    while output_chunk:\n+        output_chunk = stream_reader.read(_CHUNK_SIZE)\n+        decompressed_size += len(output_chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size} B) exceed the specified maximum \"\n+                f\"({max_size} B).\"\n+            )\n+        output_stream.write(output_chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()"
        },
        {
          "filename": "scrapy/utils/gz.py",
          "status": "modified",
          "additions": 18,
          "deletions": 8,
          "patch": "@@ -1,31 +1,41 @@\n import struct\n from gzip import GzipFile\n from io import BytesIO\n-from typing import List\n \n from scrapy.http import Response\n \n+from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\n \n-def gunzip(data: bytes) -> bytes:\n+\n+def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n     \"\"\"Gunzip the given data and return as much data as possible.\n \n     This is resilient to CRC checksum errors.\n     \"\"\"\n     f = GzipFile(fileobj=BytesIO(data))\n-    output_list: List[bytes] = []\n+    output_stream = BytesIO()\n     chunk = b\".\"\n+    decompressed_size = 0\n     while chunk:\n         try:\n-            chunk = f.read1(8196)\n-            output_list.append(chunk)\n+            chunk = f.read1(_CHUNK_SIZE)\n         except (OSError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n-            # some pages are quite small so output_list is empty\n-            if output_list:\n+            # some pages are quite small so output_stream is empty\n+            if output_stream.getbuffer().nbytes > 0:\n                 break\n             raise\n-    return b\"\".join(output_list)\n+        decompressed_size += len(chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size} B) exceed the specified maximum \"\n+                f\"({max_size} B).\"\n+            )\n+        output_stream.write(chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()\n \n \n def gzip_magic_number(response: Response) -> bool:"
        },
        {
          "filename": "tests/sample_data/compressed/bomb-br.bin",
          "status": "added",
          "additions": 2,
          "deletions": 0,
          "patch": "@@ -0,0 +1,2 @@\n+\ufffd;\ufffd\ufffd\ufffd\ufffd\ufffdn\u07aaVp\t\u0013SmoY\b2\ufffd\ufffd\n+\ufffd(\ufffd)-\u0434=_o\u0001\n\\ No newline at end of file"
        },
        {
          "filename": "tests/sample_data/compressed/bomb-deflate.bin",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "tests/sample_data/compressed/bomb-gzip.bin",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "tests/sample_data/compressed/bomb-zstd.bin",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "tests/test_downloadermiddleware_decompression.py",
          "status": "removed",
          "additions": 0,
          "deletions": 53,
          "patch": "@@ -1,53 +0,0 @@\n-from unittest import TestCase, main\n-\n-from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n-from scrapy.http import Response, XmlResponse\n-from scrapy.spiders import Spider\n-from scrapy.utils.test import assert_samelines\n-from tests import get_testdata\n-\n-\n-def _test_data(formats):\n-    uncompressed_body = get_testdata(\"compressed\", \"feed-sample1.xml\")\n-    test_responses = {}\n-    for format in formats:\n-        body = get_testdata(\"compressed\", \"feed-sample1.\" + format)\n-        test_responses[format] = Response(\"http://foo.com/bar\", body=body)\n-    return uncompressed_body, test_responses\n-\n-\n-class DecompressionMiddlewareTest(TestCase):\n-    test_formats = [\"tar\", \"xml.bz2\", \"xml.gz\", \"zip\"]\n-    uncompressed_body, test_responses = _test_data(test_formats)\n-\n-    def setUp(self):\n-        self.mw = DecompressionMiddleware()\n-        self.spider = Spider(\"foo\")\n-\n-    def test_known_compression_formats(self):\n-        for fmt in self.test_formats:\n-            rsp = self.test_responses[fmt]\n-            new = self.mw.process_response(None, rsp, self.spider)\n-            error_msg = f\"Failed {fmt}, response type {type(new).__name__}\"\n-            assert isinstance(new, XmlResponse), error_msg\n-            assert_samelines(self, new.body, self.uncompressed_body, fmt)\n-\n-    def test_plain_response(self):\n-        rsp = Response(url=\"http://test.com\", body=self.uncompressed_body)\n-        new = self.mw.process_response(None, rsp, self.spider)\n-        assert new is rsp\n-        assert_samelines(self, new.body, rsp.body)\n-\n-    def test_empty_response(self):\n-        rsp = Response(url=\"http://test.com\", body=b\"\")\n-        new = self.mw.process_response(None, rsp, self.spider)\n-        assert new is rsp\n-        assert not rsp.body\n-        assert not new.body\n-\n-    def tearDown(self):\n-        del self.mw\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
          "filename": "tests/test_downloadermiddleware_httpcompression.py",
          "status": "modified",
          "additions": 240,
          "deletions": 15,
          "patch": "@@ -1,16 +1,18 @@\n from gzip import GzipFile\n from io import BytesIO\n+from logging import WARNING\n from pathlib import Path\n from unittest import SkipTest, TestCase\n from warnings import catch_warnings\n \n+from testfixtures import LogCapture\n from w3lib.encoding import resolve_encoding\n \n from scrapy.downloadermiddlewares.httpcompression import (\n     ACCEPTED_ENCODINGS,\n     HttpCompressionMiddleware,\n )\n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n@@ -35,6 +37,15 @@\n         \"html-zstd-streaming-no-content-size.bin\",\n         \"zstd\",\n     ),\n+    **{\n+        f\"bomb-{format_id}\": (f\"bomb-{format_id}.bin\", format_id)\n+        for format_id in (\n+            \"br\",  # 34 \u2192 11 511 612\n+            \"deflate\",  # 27 968 \u2192 11 511 612\n+            \"gzip\",  # 27 988 \u2192 11 511 612\n+            \"zstd\",  # 1 096 \u2192 11 511 612\n+        )\n+    },\n }\n \n \n@@ -115,18 +126,6 @@ def test_process_response_gzip(self):\n         self.assertStatsEqual(\"httpcompression/response_count\", 1)\n         self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n-    def test_process_response_gzip_no_stats(self):\n-        mw = HttpCompressionMiddleware()\n-        response = self._getresponse(\"gzip\")\n-        request = response.request\n-\n-        self.assertEqual(response.headers[\"Content-Encoding\"], b\"gzip\")\n-        newresponse = mw.process_response(request, response, self.spider)\n-        self.assertEqual(mw.stats, None)\n-        assert newresponse is not response\n-        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n-        assert \"Content-Encoding\" not in newresponse.headers\n-\n     def test_process_response_br(self):\n         try:\n             import brotli  # noqa: F401\n@@ -373,6 +372,232 @@ def test_process_response_head_request_no_decode_required(self):\n         self.assertStatsEqual(\"httpcompression/response_count\", None)\n         self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n \n+    def _test_compression_bomb_setting(self, compression_id):\n+        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n+        crawler = get_crawler(Spider, settings_dict=settings)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        self.assertRaises(\n+            IgnoreRequest,\n+            mw.process_response,\n+            response.request,\n+            response,\n+            spider,\n+        )\n+\n+    def test_compression_bomb_setting_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_compression_bomb_setting(\"br\")\n+\n+    def test_compression_bomb_setting_deflate(self):\n+        self._test_compression_bomb_setting(\"deflate\")\n+\n+    def test_compression_bomb_setting_gzip(self):\n+        self._test_compression_bomb_setting(\"gzip\")\n+\n+    def test_compression_bomb_setting_zstd(self):\n+        self._test_compression_bomb_setting(\"zstd\")\n+\n+    def _test_compression_bomb_spider_attr(self, compression_id):\n+        class DownloadMaxSizeSpider(Spider):\n+            download_maxsize = 10_000_000\n+\n+        crawler = get_crawler(DownloadMaxSizeSpider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        self.assertRaises(\n+            IgnoreRequest,\n+            mw.process_response,\n+            response.request,\n+            response,\n+            spider,\n+        )\n+\n+    def test_compression_bomb_spider_attr_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_compression_bomb_spider_attr(\"br\")\n+\n+    def test_compression_bomb_spider_attr_deflate(self):\n+        self._test_compression_bomb_spider_attr(\"deflate\")\n+\n+    def test_compression_bomb_spider_attr_gzip(self):\n+        self._test_compression_bomb_spider_attr(\"gzip\")\n+\n+    def test_compression_bomb_spider_attr_zstd(self):\n+        self._test_compression_bomb_spider_attr(\"zstd\")\n+\n+    def _test_compression_bomb_request_meta(self, compression_id):\n+        crawler = get_crawler(Spider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        response.meta[\"download_maxsize\"] = 10_000_000\n+        self.assertRaises(\n+            IgnoreRequest,\n+            mw.process_response,\n+            response.request,\n+            response,\n+            spider,\n+        )\n+\n+    def test_compression_bomb_request_meta_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_compression_bomb_request_meta(\"br\")\n+\n+    def test_compression_bomb_request_meta_deflate(self):\n+        self._test_compression_bomb_request_meta(\"deflate\")\n+\n+    def test_compression_bomb_request_meta_gzip(self):\n+        self._test_compression_bomb_request_meta(\"gzip\")\n+\n+    def test_compression_bomb_request_meta_zstd(self):\n+        self._test_compression_bomb_request_meta(\"zstd\")\n+\n+    def _test_download_warnsize_setting(self, compression_id):\n+        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n+        crawler = get_crawler(Spider, settings_dict=settings)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+\n+        with LogCapture(\n+            \"scrapy.downloadermiddlewares.httpcompression\",\n+            propagate=False,\n+            level=WARNING,\n+        ) as log:\n+            mw.process_response(response.request, response, spider)\n+        log.check(\n+            (\n+                \"scrapy.downloadermiddlewares.httpcompression\",\n+                \"WARNING\",\n+                (\n+                    \"<200 http://scrapytest.org/> body size after \"\n+                    \"decompression (11511612 B) is larger than the download \"\n+                    \"warning size (10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_setting_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_download_warnsize_setting(\"br\")\n+\n+    def test_download_warnsize_setting_deflate(self):\n+        self._test_download_warnsize_setting(\"deflate\")\n+\n+    def test_download_warnsize_setting_gzip(self):\n+        self._test_download_warnsize_setting(\"gzip\")\n+\n+    def test_download_warnsize_setting_zstd(self):\n+        self._test_download_warnsize_setting(\"zstd\")\n+\n+    def _test_download_warnsize_spider_attr(self, compression_id):\n+        class DownloadWarnSizeSpider(Spider):\n+            download_warnsize = 10_000_000\n+\n+        crawler = get_crawler(DownloadWarnSizeSpider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+\n+        with LogCapture(\n+            \"scrapy.downloadermiddlewares.httpcompression\",\n+            propagate=False,\n+            level=WARNING,\n+        ) as log:\n+            mw.process_response(response.request, response, spider)\n+        log.check(\n+            (\n+                \"scrapy.downloadermiddlewares.httpcompression\",\n+                \"WARNING\",\n+                (\n+                    \"<200 http://scrapytest.org/> body size after \"\n+                    \"decompression (11511612 B) is larger than the download \"\n+                    \"warning size (10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_spider_attr_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_download_warnsize_spider_attr(\"br\")\n+\n+    def test_download_warnsize_spider_attr_deflate(self):\n+        self._test_download_warnsize_spider_attr(\"deflate\")\n+\n+    def test_download_warnsize_spider_attr_gzip(self):\n+        self._test_download_warnsize_spider_attr(\"gzip\")\n+\n+    def test_download_warnsize_spider_attr_zstd(self):\n+        self._test_download_warnsize_spider_attr(\"zstd\")\n+\n+    def _test_download_warnsize_request_meta(self, compression_id):\n+        crawler = get_crawler(Spider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        response.meta[\"download_warnsize\"] = 10_000_000\n+\n+        with LogCapture(\n+            \"scrapy.downloadermiddlewares.httpcompression\",\n+            propagate=False,\n+            level=WARNING,\n+        ) as log:\n+            mw.process_response(response.request, response, spider)\n+        log.check(\n+            (\n+                \"scrapy.downloadermiddlewares.httpcompression\",\n+                \"WARNING\",\n+                (\n+                    \"<200 http://scrapytest.org/> body size after \"\n+                    \"decompression (11511612 B) is larger than the download \"\n+                    \"warning size (10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_request_meta_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_download_warnsize_request_meta(\"br\")\n+\n+    def test_download_warnsize_request_meta_deflate(self):\n+        self._test_download_warnsize_request_meta(\"deflate\")\n+\n+    def test_download_warnsize_request_meta_gzip(self):\n+        self._test_download_warnsize_request_meta(\"gzip\")\n+\n+    def test_download_warnsize_request_meta_zstd(self):\n+        self._test_download_warnsize_request_meta(\"zstd\")\n+\n \n class HttpCompressionSubclassTest(TestCase):\n     def test_init_missing_stats(self):\n@@ -393,8 +618,8 @@ def __init__(self):\n             (\n                 (\n                     \"HttpCompressionMiddleware subclasses must either modify \"\n-                    \"their '__init__' method to support a 'stats' parameter \"\n-                    \"or reimplement the 'from_crawler' method.\"\n+                    \"their '__init__' method to support a 'crawler' parameter \"\n+                    \"or reimplement their 'from_crawler' method.\"\n                 ),\n             ),\n         )"
        },
        {
          "filename": "tests/test_spider.py",
          "status": "modified",
          "additions": 121,
          "deletions": 3,
          "patch": "@@ -2,6 +2,8 @@\n import inspect\n import warnings\n from io import BytesIO\n+from logging import WARNING\n+from pathlib import Path\n from typing import Any\n from unittest import mock\n \n@@ -25,7 +27,7 @@\n )\n from scrapy.spiders.init import InitSpider\n from scrapy.utils.test import get_crawler\n-from tests import get_testdata\n+from tests import get_testdata, tests_datadir\n \n \n class SpiderTest(unittest.TestCase):\n@@ -489,7 +491,8 @@ class SitemapSpiderTest(SpiderTest):\n     GZBODY = f.getvalue()\n \n     def assertSitemapBody(self, response, body):\n-        spider = self.spider_class(\"example.com\")\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n         self.assertEqual(spider._get_sitemap_body(response), body)\n \n     def test_get_sitemap_body(self):\n@@ -507,6 +510,7 @@ def test_get_sitemap_body_gzip_headers(self):\n             url=\"http://www.example.com/sitemap\",\n             body=self.GZBODY,\n             headers={\"content-type\": \"application/gzip\"},\n+            request=Request(\"http://www.example.com/sitemap\"),\n         )\n         self.assertSitemapBody(r, self.BODY)\n \n@@ -515,7 +519,11 @@ def test_get_sitemap_body_xml_url(self):\n         self.assertSitemapBody(r, self.BODY)\n \n     def test_get_sitemap_body_xml_url_compressed(self):\n-        r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)\n+        r = Response(\n+            url=\"http://www.example.com/sitemap.xml.gz\",\n+            body=self.GZBODY,\n+            request=Request(\"http://www.example.com/sitemap\"),\n+        )\n         self.assertSitemapBody(r, self.BODY)\n \n         # .xml.gz but body decoded by HttpCompression middleware already\n@@ -692,6 +700,116 @@ def sitemap_filter(self, entries):\n             [\"http://www.example.com/sitemap2.xml\"],\n         )\n \n+    def test_compression_bomb_setting(self):\n+        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n+        crawler = get_crawler(settings_dict=settings)\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(url=\"https://example.com\")\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        self.assertIsNone(spider._get_sitemap_body(response))\n+\n+    def test_compression_bomb_spider_attr(self):\n+        class DownloadMaxSizeSpider(self.spider_class):\n+            download_maxsize = 10_000_000\n+\n+        crawler = get_crawler()\n+        spider = DownloadMaxSizeSpider.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(url=\"https://example.com\")\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        self.assertIsNone(spider._get_sitemap_body(response))\n+\n+    def test_compression_bomb_request_meta(self):\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(\n+            url=\"https://example.com\", meta={\"download_maxsize\": 10_000_000}\n+        )\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        self.assertIsNone(spider._get_sitemap_body(response))\n+\n+    def test_download_warnsize_setting(self):\n+        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n+        crawler = get_crawler(settings_dict=settings)\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(url=\"https://example.com\")\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        with LogCapture(\n+            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n+        ) as log:\n+            spider._get_sitemap_body(response)\n+        log.check(\n+            (\n+                \"scrapy.spiders.sitemap\",\n+                \"WARNING\",\n+                (\n+                    \"<200 https://example.com> body size after decompression \"\n+                    \"(11511612 B) is larger than the download warning size \"\n+                    \"(10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_spider_attr(self):\n+        class DownloadWarnSizeSpider(self.spider_class):\n+            download_warnsize = 10_000_000\n+\n+        crawler = get_crawler()\n+        spider = DownloadWarnSizeSpider.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(\n+            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n+        )\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        with LogCapture(\n+            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n+        ) as log:\n+            spider._get_sitemap_body(response)\n+        log.check(\n+            (\n+                \"scrapy.spiders.sitemap\",\n+                \"WARNING\",\n+                (\n+                    \"<200 https://example.com> body size after decompression \"\n+                    \"(11511612 B) is larger than the download warning size \"\n+                    \"(10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_request_meta(self):\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(\n+            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n+        )\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        with LogCapture(\n+            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n+        ) as log:\n+            spider._get_sitemap_body(response)\n+        log.check(\n+            (\n+                \"scrapy.spiders.sitemap\",\n+                \"WARNING\",\n+                (\n+                    \"<200 https://example.com> body size after decompression \"\n+                    \"(11511612 B) is larger than the download warning size \"\n+                    \"(10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n \n class DeprecationTest(unittest.TestCase):\n     def test_crawl_spider(self):"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 1,
        "dependency_files": 0,
        "test_files": 7,
        "unique_directories": 7,
        "max_directory_depth": 3
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "402500b164efc01257679247d3dd1628a5f90f5e",
            "date": "2025-01-10T18:08:27Z",
            "author_login": "ionut-ciubotariu"
          },
          {
            "sha": "1fc91bb46262118c9ff7aa2b4719d880f727699f",
            "date": "2025-01-08T16:28:51Z",
            "author_login": "BurnzZ"
          },
          {
            "sha": "b6d69e389576c137a33d14c9e9319891dce68442",
            "date": "2025-01-07T17:29:31Z",
            "author_login": "Gallaecio"
          },
          {
            "sha": "3154b08e90d9777dfe2879b8686b6fc63a793c84",
            "date": "2025-01-07T14:40:25Z",
            "author_login": "wRAR"
          },
          {
            "sha": "7dfbecd3924a0d7a9e555e9cc3618cdb06b5415d",
            "date": "2025-01-07T14:11:10Z",
            "author_login": "wRAR"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": null,
    "cvss_vector": null,
    "cwe_id": "CWE-409",
    "description": "The scrapy/scrapy project is vulnerable to XML External Entity (XXE) attacks due to the use of lxml.etree.fromstring for parsing untrusted XML data without proper validation. This vulnerability allows attackers to perform denial of service attacks, access local files, generate network connections, or circumvent firewalls by submitting specially crafted XML data. ",
    "attack_vector": null,
    "attack_complexity": null
  },
  "temporal_data": {
    "published_date": "2024-04-16T00:15:12.387",
    "last_modified": "2024-11-21T09:29:54.250",
    "fix_date": "2024-02-14T16:16:25Z"
  },
  "references": [
    {
      "url": "https://github.com/scrapy/scrapy/commit/809bfac4890f75fc73607318a04d2ccba71b3d9f",
      "source": "security@huntr.dev",
      "tags": []
    },
    {
      "url": "https://huntr.com/bounties/c4a0fac9-0c5a-4718-9ee4-2d06d58adabb",
      "source": "security@huntr.dev",
      "tags": []
    },
    {
      "url": "https://github.com/scrapy/scrapy/commit/809bfac4890f75fc73607318a04d2ccba71b3d9f",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://huntr.com/bounties/c4a0fac9-0c5a-4718-9ee4-2d06d58adabb",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:04.492916",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "scrapy",
    "owner": "scrapy",
    "created_at": "2010-02-22T02:01:14Z",
    "updated_at": "2025-01-14T12:03:42Z",
    "pushed_at": "2025-01-10T18:08:27Z",
    "size": 26971,
    "stars": 53789,
    "forks": 10611,
    "open_issues": 611,
    "watchers": 53789,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "Python": 2367474,
      "HTML": 3290,
      "Roff": 2010,
      "Shell": 252
    },
    "commit_activity": {
      "total_commits_last_year": 389,
      "avg_commits_per_week": 7.480769230769231,
      "days_active_last_year": 122
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "bsd-3-clause"
    },
    "collected_at": "2025-01-14T13:18:23.407520"
  }
}