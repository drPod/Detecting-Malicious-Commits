{
  "cve_id": "CVE-2024-37306",
  "github_data": {
    "repository": "cvat-ai/cvat",
    "fix_commit": "5d36d10e493d92e893d7eae595544bcbe9cce1ce",
    "related_commits": [
      "5d36d10e493d92e893d7eae595544bcbe9cce1ce",
      "5d36d10e493d92e893d7eae595544bcbe9cce1ce"
    ],
    "patch_url": "https://github.com/cvat-ai/cvat/commit/5d36d10e493d92e893d7eae595544bcbe9cce1ce.patch",
    "fix_commit_details": {
      "sha": "5d36d10e493d92e893d7eae595544bcbe9cce1ce",
      "commit_date": "2024-06-13T12:17:13Z",
      "author": {
        "login": "SpecLad",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Merge pull request from GHSA-jpf9-646h-4px7",
        "length": 1702,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 438,
        "additions": 280,
        "deletions": 158
      },
      "files": [
        {
          "filename": "changelog.d/20240523_192101_roman_export_backup_csrf.md",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -0,0 +1,4 @@\n+### Security\n+\n+- Mitigated a CSRF vulnerability in backup and export-related endpoints\n+  (<https://github.com/cvat-ai/cvat/security/advisories/GHSA-jpf9-646h-4px7>)"
        },
        {
          "filename": "cvat/apps/dataset_manager/tests/test_rest_api_formats.py",
          "status": "modified",
          "additions": 1,
          "deletions": 15,
          "patch": "@@ -317,7 +317,7 @@ def _create_annotations_in_job(self, task, job_id,  name_ann, key_get_values):\n     def _download_file(self, url, data, user, file_name):\n         response = self._get_request_with_data(url, data, user)\n         self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)\n-        response = self._get_request_with_data(url, data, user)\n+        response = self._get_request_with_data(url, {**data, \"action\": \"download\"}, user)\n         self.assertEqual(response.status_code, status.HTTP_200_OK)\n \n         content = BytesIO(b\"\".join(response.streaming_content))\n@@ -659,7 +659,6 @@ def test_api_v2_dump_and_upload_annotations_with_objects_are_different_images(se\n                     file_zip_name = osp.join(test_dir, f'{test_name}_{upload_type}.zip')\n                     data = {\n                         \"format\": dump_format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_zip_name)\n                     self.assertEqual(osp.exists(file_zip_name), True)\n@@ -700,7 +699,6 @@ def test_api_v2_dump_and_upload_annotations_with_objects_are_different_video(sel\n \n                     data = {\n                         \"format\": dump_format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_zip_name)\n                     self.assertEqual(osp.exists(file_zip_name), True)\n@@ -732,7 +730,6 @@ def test_api_v2_dump_and_upload_with_objects_type_is_track_and_outside_property(\n             file_zip_name = osp.join(test_dir, f'{test_name}.zip')\n             data = {\n                 \"format\": dump_format_name,\n-                \"action\": \"download\",\n             }\n             self._download_file(url, data, self.admin, file_zip_name)\n             self.assertEqual(osp.exists(file_zip_name), True)\n@@ -756,7 +753,6 @@ def test_api_v2_dump_and_upload_with_objects_type_is_track_and_keyframe_property\n \n             data = {\n                 \"format\": dump_format_name,\n-                \"action\": \"download\",\n             }\n             self._download_file(url, data, self.admin, file_zip_name)\n             self.assertEqual(osp.exists(file_zip_name), True)\n@@ -781,7 +777,6 @@ def test_api_v2_dump_upload_annotations_from_several_jobs(self):\n             file_zip_name = osp.join(test_dir, f'{test_name}.zip')\n             data = {\n                 \"format\": dump_format_name,\n-                \"action\": \"download\",\n             }\n             self._download_file(url, data, self.admin, file_zip_name)\n             self.assertEqual(osp.exists(file_zip_name), True)\n@@ -817,7 +812,6 @@ def test_api_v2_dump_annotations_from_several_jobs(self):\n                     file_zip_name = osp.join(test_dir, f'{test_name}.zip')\n                     data = {\n                         \"format\": dump_format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_zip_name)\n                     self.assertEqual(osp.exists(file_zip_name), True)\n@@ -905,7 +899,6 @@ def test_api_v2_dump_empty_frames(self):\n                     file_zip_name = osp.join(test_dir, f'empty_{dump_format_name}.zip')\n                     data = {\n                         \"format\": dump_format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_zip_name)\n                     self.assertEqual(osp.exists(file_zip_name), True)\n@@ -983,7 +976,6 @@ def test_api_v2_rewriting_annotations(self):\n                     file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')\n                     data = {\n                         \"format\": dump_format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_zip_name)\n                     self.assertEqual(osp.exists(file_zip_name), True)\n@@ -1027,7 +1019,6 @@ def test_api_v2_tasks_annotations_dump_and_upload_many_jobs_with_datumaro(self):\n \n                     data = {\n                         \"format\": dump_format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_zip_name)\n                     self._check_downloaded_file(file_zip_name)\n@@ -1100,7 +1091,6 @@ def test_api_v2_tasks_annotations_dump_and_upload_with_datumaro(self):\n                         file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')\n                         data = {\n                             \"format\": dump_format_name,\n-                            \"action\": \"download\",\n                         }\n                         self._download_file(url, data, self.admin, file_zip_name)\n                         self._check_downloaded_file(file_zip_name)\n@@ -1128,7 +1118,6 @@ def test_api_v2_check_duplicated_polygon_points(self):\n         task_id = task[\"id\"]\n         data = {\n             \"format\": \"CVAT for video 1.1\",\n-            \"action\": \"download\",\n         }\n         annotation_name = \"CVAT for video 1.1 polygon\"\n         self._create_annotations(task, annotation_name, \"default\")\n@@ -1170,7 +1159,6 @@ def test_api_v2_check_widerface_with_all_attributes(self):\n                 url = self._generate_url_dump_tasks_annotations(task_id)\n                 data = {\n                     \"format\": dump_format_name,\n-                    \"action\": \"download\",\n                 }\n                 with TestDir() as test_dir:\n                     file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')\n@@ -1207,7 +1195,6 @@ def test_api_v2_check_mot_with_shapes_only(self):\n                 url = self._generate_url_dump_tasks_annotations(task_id)\n                 data = {\n                     \"format\": format_name,\n-                    \"action\": \"download\",\n                 }\n                 with TestDir() as test_dir:\n                     file_zip_name = osp.join(test_dir, f'{test_name}_{format_name}.zip')\n@@ -1245,7 +1232,6 @@ def test_api_v2_check_attribute_import_in_tracks(self):\n                 url = self._generate_url_dump_tasks_annotations(task_id)\n                 data = {\n                     \"format\": dump_format_name,\n-                    \"action\": \"download\",\n                 }\n                 with TestDir() as test_dir:\n                     file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')"
        },
        {
          "filename": "cvat/apps/engine/backup.py",
          "status": "modified",
          "additions": 48,
          "deletions": 39,
          "patch": "@@ -952,14 +952,12 @@ def export(db_instance, request, queue_name):\n         field_name=StorageType.TARGET\n     )\n \n+    last_instance_update_time = timezone.localtime(db_instance.updated_date)\n+\n     queue = django_rq.get_queue(queue_name)\n     rq_id = f\"export:{obj_type}.id{db_instance.pk}-by-{request.user}\"\n     rq_job = queue.fetch_job(rq_id)\n \n-    last_instance_update_time = timezone.localtime(db_instance.updated_date)\n-    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n-    location = location_conf.get('location')\n-\n     if rq_job:\n         rq_request = rq_job.meta.get('request', None)\n         request_time = rq_request.get(\"timestamp\", None) if rq_request else None\n@@ -968,43 +966,54 @@ def export(db_instance, request, queue_name):\n             # we have to enqueue dependent jobs after canceling one\n             rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n             rq_job.delete()\n-        else:\n-            if rq_job.is_finished:\n-                if location == Location.LOCAL:\n-                    file_path = rq_job.return_value()\n-\n-                    if not file_path:\n-                        return Response('A result for exporting job was not found for finished RQ job', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n-\n-                    elif not os.path.exists(file_path):\n-                        return Response('The result file does not exist in export cache', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n-\n-                    filename = filename or build_backup_file_name(\n-                        class_name=obj_type,\n-                        identifier=db_instance.name,\n-                        timestamp=timestamp,\n-                        extension=os.path.splitext(file_path)[1]\n-                    )\n-\n-                    if action == \"download\":\n-                        rq_job.delete()\n-                        return sendfile(request, file_path, attachment=True,\n-                            attachment_filename=filename)\n-\n-                    return Response(status=status.HTTP_201_CREATED)\n-\n-                elif location == Location.CLOUD_STORAGE:\n-                    rq_job.delete()\n-                    return Response(status=status.HTTP_200_OK)\n-                else:\n-                    raise NotImplementedError()\n-            elif rq_job.is_failed:\n-                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n+            rq_job = None\n+\n+    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n+    location = location_conf.get('location')\n+\n+    if action == \"download\":\n+        if location != Location.LOCAL:\n+            return Response('Action \"download\" is only supported for a local backup location', status=status.HTTP_400_BAD_REQUEST)\n+\n+        if not rq_job or not rq_job.is_finished:\n+            return Response('Backup has not finished', status=status.HTTP_400_BAD_REQUEST)\n+\n+        file_path = rq_job.return_value()\n+\n+        if not file_path:\n+            return Response('A result for exporting job was not found for finished RQ job', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n+\n+        elif not os.path.exists(file_path):\n+            return Response('The result file does not exist in export cache', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n+\n+        filename = filename or build_backup_file_name(\n+            class_name=obj_type,\n+            identifier=db_instance.name,\n+            timestamp=timestamp,\n+            extension=os.path.splitext(file_path)[1]\n+        )\n+\n+        rq_job.delete()\n+        return sendfile(request, file_path, attachment=True,\n+            attachment_filename=filename)\n+\n+    if rq_job:\n+        if rq_job.is_finished:\n+            if location == Location.LOCAL:\n+                return Response(status=status.HTTP_201_CREATED)\n+\n+            elif location == Location.CLOUD_STORAGE:\n                 rq_job.delete()\n-                return Response(exc_info,\n-                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n+                return Response(status=status.HTTP_200_OK)\n             else:\n-                return Response(status=status.HTTP_202_ACCEPTED)\n+                raise NotImplementedError()\n+        elif rq_job.is_failed:\n+            exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n+            rq_job.delete()\n+            return Response(exc_info,\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n+        else:\n+            return Response(status=status.HTTP_202_ACCEPTED)\n \n     ttl = dm.views.PROJECT_CACHE_TTL.total_seconds()\n     user_id = request.user.id"
        },
        {
          "filename": "cvat/apps/engine/mixins.py",
          "status": "modified",
          "additions": 36,
          "deletions": 1,
          "patch": "@@ -12,13 +12,15 @@\n from pathlib import Path\n from tempfile import NamedTemporaryFile\n from unittest import mock\n-from typing import Optional, Callable, Dict, Any\n+from typing import Optional, Callable, Dict, Any, Mapping\n \n import django_rq\n from attr.converters import to_bool\n from django.conf import settings\n from rest_framework import mixins, status\n+from rest_framework.authentication import SessionAuthentication\n from rest_framework.response import Response\n+from rest_framework.views import APIView\n \n from cvat.apps.engine.location import StorageType, get_location_configuration\n from cvat.apps.engine.log import ServerLogManager\n@@ -498,3 +500,36 @@ def perform_update(self, serializer):\n     def partial_update(self, request, *args, **kwargs):\n         with mock.patch.object(self, 'update', new=self._update, create=True):\n             return mixins.UpdateModelMixin.partial_update(self, request=request, *args, **kwargs)\n+\n+class CsrfWorkaroundMixin(APIView):\n+    \"\"\"\n+    Disables session authentication for GET/HEAD requests\n+    for which csrf_workaround_is_needed returns True.\n+\n+    csrf_workaround_is_needed is supposed to be overridden by each view.\n+\n+    This only exists to mitigate CSRF attacks on several known endpoints that\n+    perform side effects in response to GET requests. Do not use this in\n+    new code: instead, make sure that all endpoints with side effects use\n+    a method other than GET/HEAD. Then Django's built-in CSRF protection\n+    will cover them.\n+    \"\"\"\n+\n+    @staticmethod\n+    def csrf_workaround_is_needed(query_params: Mapping[str, str]) -> bool:\n+        return False\n+\n+    def get_authenticators(self):\n+        authenticators = super().get_authenticators()\n+\n+        if (\n+            self.request and\n+            # Don't apply the workaround for requests from unit tests, since\n+            # they can only use session authentication.\n+            not getattr(self.request, \"_dont_enforce_csrf_checks\", False) and\n+            self.request.method in (\"GET\", \"HEAD\") and\n+            self.csrf_workaround_is_needed(self.request.GET)\n+        ):\n+            authenticators = [a for a in authenticators if not isinstance(a, SessionAuthentication)]\n+\n+        return authenticators"
        },
        {
          "filename": "cvat/apps/engine/tests/test_rest_api_3D.py",
          "status": "modified",
          "additions": 1,
          "deletions": 5,
          "patch": "@@ -153,7 +153,7 @@ def _delete_request(self, path, user):\n     def _download_file(self, url, data, user, file_name):\n         response = self._get_request_with_data(url, data, user)\n         self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)\n-        response = self._get_request_with_data(url, data, user)\n+        response = self._get_request_with_data(url, {**data, \"action\": \"download\"}, user)\n         self.assertEqual(response.status_code, status.HTTP_200_OK)\n \n         content = BytesIO(b\"\".join(response.streaming_content))\n@@ -583,7 +583,6 @@ def test_api_v2_rewrite_annotation(self):\n                     file_name = osp.join(test_dir, f\"{format_name}.zip\")\n                     data = {\n                         \"format\": format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_name)\n                     self.assertTrue(osp.exists(file_name))\n@@ -622,7 +621,6 @@ def test_api_v2_dump_and_upload_empty_annotation(self):\n                     file_name = osp.join(test_dir, f\"{format_name}.zip\")\n                     data = {\n                         \"format\": format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_name)\n                     self.assertTrue(osp.exists(file_name))\n@@ -663,7 +661,6 @@ def test_api_v2_dump_and_upload_several_jobs(self):\n                     file_name = osp.join(test_dir, f\"{format_name}.zip\")\n                     data = {\n                         \"format\": format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_name)\n \n@@ -687,7 +684,6 @@ def test_api_v2_upload_annotation_with_attributes(self):\n                     file_name = osp.join(test_dir, f\"{format_name}.zip\")\n                     data = {\n                         \"format\": format_name,\n-                        \"action\": \"download\",\n                     }\n                     self._download_file(url, data, self.admin, file_name)\n                     self.assertTrue(osp.exists(file_name))"
        },
        {
          "filename": "cvat/apps/engine/views.py",
          "status": "modified",
          "additions": 131,
          "deletions": 96,
          "patch": "@@ -7,7 +7,7 @@\n import os.path as osp\n from PIL import Image\n from types import SimpleNamespace\n-from typing import Optional, Any, Dict, List, cast, Callable\n+from typing import Optional, Any, Dict, List, cast, Callable, Mapping\n import traceback\n import textwrap\n from copy import copy\n@@ -76,7 +76,9 @@\n     build_annotations_file_name,\n )\n from cvat.apps.engine import backup\n-from cvat.apps.engine.mixins import PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\n+from cvat.apps.engine.mixins import (\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\n+)\n from cvat.apps.engine.location import get_location_configuration, StorageType\n \n from . import models, task\n@@ -206,6 +208,12 @@ def plugins(request):\n         }\n         return Response(PluginsSerializer(data).data)\n \n+def csrf_workaround_is_needed_for_backup(query_params: Mapping[str, str]) -> bool:\n+    return query_params.get('action') != 'download'\n+\n+def csrf_workaround_is_needed_for_export(query_params: Mapping[str, str]) -> bool:\n+    return 'format' in query_params and query_params.get('action') != 'download'\n+\n @extend_schema(tags=['projects'])\n @extend_schema_view(\n     list=extend_schema(\n@@ -239,7 +247,7 @@ def plugins(request):\n )\n class ProjectViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,\n     mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,\n-    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\n ):\n     queryset = models.Project.objects.select_related(\n         'assignee', 'owner', 'target_storage', 'source_storage', 'annotation_guide',\n@@ -347,7 +355,9 @@ def perform_create(self, serializer, **kwargs):\n             '405': OpenApiResponse(description='Format is not available'),\n         })\n     @action(detail=True, methods=['GET', 'POST', 'OPTIONS'], serializer_class=None,\n-        url_path=r'dataset/?$', parser_classes=_UPLOAD_PARSER_CLASSES)\n+        url_path=r'dataset/?$', parser_classes=_UPLOAD_PARSER_CLASSES,\n+        csrf_workaround_is_needed=lambda qp:\n+            csrf_workaround_is_needed_for_export(qp) and qp.get(\"action\") != \"import_status\")\n     def dataset(self, request, pk):\n         self._object = self.get_object() # force call of check_object_permissions()\n \n@@ -479,7 +489,8 @@ def upload_finished(self, request):\n             '405': OpenApiResponse(description='Format is not available'),\n         })\n     @action(detail=True, methods=['GET'],\n-        serializer_class=LabeledDataSerializer)\n+        serializer_class=LabeledDataSerializer,\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\n     def annotations(self, request, pk):\n         self._object = self.get_object() # force call of check_object_permissions()\n         return self.export_annotations(\n@@ -511,7 +522,8 @@ def annotations(self, request, pk):\n             '201': OpenApiResponse(description='Output backup file is ready for downloading'),\n             '202': OpenApiResponse(description='Creating a backup file has been started'),\n         })\n-    @action(methods=['GET'], detail=True, url_path='backup')\n+    @action(methods=['GET'], detail=True, url_path='backup',\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_backup)\n     def export_backup(self, request, pk=None):\n         return self.serialize(request, backup.export)\n \n@@ -765,7 +777,7 @@ def __call__(self, request, start, stop, db_data):\n \n class TaskViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,\n     mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,\n-    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\n ):\n     queryset = Task.objects.select_related(\n         'data', 'assignee', 'owner',\n@@ -880,7 +892,8 @@ def append_backup_chunk(self, request, file_id):\n             '202': OpenApiResponse(description='Creating a backup file has been started'),\n             '400': OpenApiResponse(description='Backup of a task without data is not allowed'),\n         })\n-    @action(methods=['GET'], detail=True, url_path='backup')\n+    @action(methods=['GET'], detail=True, url_path='backup',\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_backup)\n     def export_backup(self, request, pk=None):\n         if self.get_object().data is None:\n             return Response(\n@@ -1330,7 +1343,8 @@ def append_data_chunk(self, request, pk, file_id):\n             '204': OpenApiResponse(description='The annotation has been deleted'),\n         })\n     @action(detail=True, methods=['GET', 'DELETE', 'PUT', 'PATCH', 'POST', 'OPTIONS'], url_path=r'annotations/?$',\n-        serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES)\n+        serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES,\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\n     def annotations(self, request, pk):\n         self._object = self.get_object() # force call of check_object_permissions()\n         if request.method == 'GET':\n@@ -1509,7 +1523,7 @@ def metadata(self, request, pk):\n             '405': OpenApiResponse(description='Format is not available'),\n         })\n     @action(detail=True, methods=['GET'], serializer_class=None,\n-        url_path='dataset')\n+        url_path='dataset', csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\n     def dataset_export(self, request, pk):\n         self._object = self.get_object() # force call of check_object_permissions()\n \n@@ -1584,7 +1598,7 @@ def preview(self, request, pk):\n )\n class JobViewSet(viewsets.GenericViewSet, mixins.ListModelMixin, mixins.CreateModelMixin,\n     mixins.RetrieveModelMixin, PartialUpdateModelMixin, mixins.DestroyModelMixin,\n-    UploadMixin, AnnotationMixin\n+    UploadMixin, AnnotationMixin, CsrfWorkaroundMixin\n ):\n     queryset = Job.objects.select_related('assignee', 'segment__task__data',\n         'segment__task__project', 'segment__task__annotation_guide', 'segment__task__project__annotation_guide',\n@@ -1776,7 +1790,8 @@ def upload_finished(self, request):\n             '204': OpenApiResponse(description='The annotation has been deleted'),\n         })\n     @action(detail=True, methods=['GET', 'DELETE', 'PUT', 'PATCH', 'POST', 'OPTIONS'], url_path=r'annotations/?$',\n-        serializer_class=LabeledDataSerializer, parser_classes=_UPLOAD_PARSER_CLASSES)\n+        serializer_class=LabeledDataSerializer, parser_classes=_UPLOAD_PARSER_CLASSES,\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\n     def annotations(self, request, pk):\n         self._object = self.get_object() # force call of check_object_permissions()\n         if request.method == 'GET':\n@@ -1873,7 +1888,7 @@ def append_annotations_chunk(self, request, pk, file_id):\n             '405': OpenApiResponse(description='Format is not available'),\n         })\n     @action(detail=True, methods=['GET'], serializer_class=None,\n-        url_path='dataset')\n+        url_path='dataset', csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\n     def dataset_export(self, request, pk):\n         self._object = self.get_object() # force call of check_object_permissions()\n \n@@ -2951,110 +2966,130 @@ def _export_annotations(\n     elif not format_desc.ENABLED:\n         return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n \n+    instance_update_time = timezone.localtime(db_instance.updated_date)\n+    if isinstance(db_instance, Project):\n+        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\n+        instance_update_time = max(tasks_update + [instance_update_time])\n+\n     queue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)\n     rq_job = queue.fetch_job(rq_id)\n \n+    if rq_job:\n+        rq_request = rq_job.meta.get('request', None)\n+        request_time = rq_request.get('timestamp', None) if rq_request else None\n+        if request_time is None or request_time < instance_update_time:\n+            # The result is outdated, need to restart the export.\n+            # Cancel the current job.\n+            # The new attempt will be made after the last existing job.\n+            # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\n+            # we have to enqueue dependent jobs after canceling one.\n+            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n+            rq_job.delete()\n+            rq_job = None\n+\n     location = location_conf.get('location')\n     if location not in Location.list():\n         raise serializers.ValidationError(\n             f\"Unexpected location {location} specified for the request\"\n         )\n \n     cache_ttl = dm.views.get_export_cache_ttl(db_instance)\n-    instance_update_time = timezone.localtime(db_instance.updated_date)\n-    if isinstance(db_instance, Project):\n-        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\n-        instance_update_time = max(tasks_update + [instance_update_time])\n \n     instance_timestamp = datetime.strftime(instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n     is_annotation_file = rq_id.startswith('export:annotations')\n \n+    REQUEST_TIMEOUT = 60\n+\n+    if action == \"download\":\n+        if location != Location.LOCAL:\n+            return Response('Action \"download\" is only supported for a local export location',\n+                status=status.HTTP_400_BAD_REQUEST)\n+\n+        if not rq_job or not rq_job.is_finished:\n+            return Response('Export has not finished', status=status.HTTP_400_BAD_REQUEST)\n+\n+        file_path = rq_job.return_value()\n+\n+        if not file_path:\n+            return Response(\n+                'A result for exporting job was not found for finished RQ job',\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR\n+            )\n+\n+        with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):\n+            if not osp.exists(file_path):\n+                return Response(\n+                    \"The exported file has expired, please retry exporting\",\n+                    status=status.HTTP_404_NOT_FOUND\n+                )\n+\n+            filename = filename or \\\n+                build_annotations_file_name(\n+                    class_name=db_instance.__class__.__name__,\n+                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n+                    timestamp=instance_timestamp,\n+                    format_name=format_name,\n+                    is_annotation_file=is_annotation_file,\n+                    extension=osp.splitext(file_path)[1]\n+                )\n+\n+            rq_job.delete()\n+            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\n+\n+\n     if rq_job:\n-        rq_request = rq_job.meta.get('request', None)\n-        request_time = rq_request.get('timestamp', None) if rq_request else None\n-        if request_time is None or request_time < instance_update_time:\n-            # The result is outdated, need to restart the export.\n-            # Cancel the current job.\n+        if rq_job.is_finished:\n+            if location == Location.CLOUD_STORAGE:\n+                rq_job.delete()\n+                return Response(status=status.HTTP_200_OK)\n+\n+            elif location == Location.LOCAL:\n+                file_path = rq_job.return_value()\n+\n+                if not file_path:\n+                    return Response(\n+                        'A result for exporting job was not found for finished RQ job',\n+                        status=status.HTTP_500_INTERNAL_SERVER_ERROR\n+                    )\n+\n+                with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):\n+                    if osp.exists(file_path):\n+                        # Update last update time to prolong the export lifetime\n+                        # as the last access time is not available on every filesystem\n+                        os.utime(file_path, None)\n+\n+                        return Response(status=status.HTTP_201_CREATED)\n+                    else:\n+                        # Cancel and reenqueue the job.\n+                        # The new attempt will be made after the last existing job.\n+                        # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\n+                        # we have to enqueue dependent jobs after canceling one.\n+                        rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n+                        rq_job.delete()\n+            else:\n+                raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\n+        elif rq_job.is_failed:\n+            exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n+            rq_job.delete()\n+            return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n+        elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\n+            # Sometimes jobs can depend on outdated jobs in the deferred jobs registry.\n+            # They can be fetched by their specific ids, but are not listed by get_job_ids().\n+            # Supposedly, this can happen because of the server restarts\n+            # (potentially, because the redis used for the queue is inmemory).\n+            # Another potential reason is canceling without enqueueing dependents.\n+            # Such dependencies are never removed or finished,\n+            # as there is no TTL for deferred jobs,\n+            # so the current job can be blocked indefinitely.\n+\n+            # Cancel the current job and then reenqueue it, considering the current situation.\n             # The new attempt will be made after the last existing job.\n             # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\n             # we have to enqueue dependent jobs after canceling one.\n             rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n             rq_job.delete()\n         else:\n-            if rq_job.is_finished:\n-                if location == Location.CLOUD_STORAGE:\n-                    rq_job.delete()\n-                    return Response(status=status.HTTP_200_OK)\n-\n-                elif location == Location.LOCAL:\n-                    file_path = rq_job.return_value()\n-\n-                    if not file_path:\n-                        return Response(\n-                            'A result for exporting job was not found for finished RQ job',\n-                            status=status.HTTP_500_INTERNAL_SERVER_ERROR\n-                        )\n-\n-                    with dm.util.get_export_cache_lock(\n-                        file_path, ttl=60, # request timeout\n-                    ):\n-                        if action == \"download\":\n-                            if not osp.exists(file_path):\n-                                return Response(\n-                                    \"The exported file has expired, please retry exporting\",\n-                                    status=status.HTTP_404_NOT_FOUND\n-                                )\n-\n-                            filename = filename or \\\n-                                build_annotations_file_name(\n-                                    class_name=db_instance.__class__.__name__,\n-                                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n-                                    timestamp=instance_timestamp,\n-                                    format_name=format_name,\n-                                    is_annotation_file=is_annotation_file,\n-                                    extension=osp.splitext(file_path)[1]\n-                                )\n-\n-                            rq_job.delete()\n-                            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\n-                        else:\n-                            if osp.exists(file_path):\n-                                # Update last update time to prolong the export lifetime\n-                                # as the last access time is not available on every filesystem\n-                                os.utime(file_path, None)\n-\n-                                return Response(status=status.HTTP_201_CREATED)\n-                            else:\n-                                # Cancel and reenqueue the job.\n-                                # The new attempt will be made after the last existing job.\n-                                # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\n-                                # we have to enqueue dependent jobs after canceling one.\n-                                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n-                                rq_job.delete()\n-                else:\n-                    raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\n-            elif rq_job.is_failed:\n-                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n-                rq_job.delete()\n-                return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n-            elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\n-                # Sometimes jobs can depend on outdated jobs in the deferred jobs registry.\n-                # They can be fetched by their specific ids, but are not listed by get_job_ids().\n-                # Supposedly, this can happen because of the server restarts\n-                # (potentially, because the redis used for the queue is inmemory).\n-                # Another potential reason is canceling without enqueueing dependents.\n-                # Such dependencies are never removed or finished,\n-                # as there is no TTL for deferred jobs,\n-                # so the current job can be blocked indefinitely.\n-\n-                # Cancel the current job and then reenqueue it, considering the current situation.\n-                # The new attempt will be made after the last existing job.\n-                # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\n-                # we have to enqueue dependent jobs after canceling one.\n-                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n-                rq_job.delete()\n-            else:\n-                return Response(status=status.HTTP_202_ACCEPTED)\n+            return Response(status=status.HTTP_202_ACCEPTED)\n     try:\n         if request.scheme:\n             server_address = request.scheme + '://'"
        },
        {
          "filename": "tests/python/rest_api/test_csrf_workaround.py",
          "status": "added",
          "additions": 57,
          "deletions": 0,
          "patch": "@@ -0,0 +1,57 @@\n+# Copyright (C) 2024 CVAT.ai Corporation\n+#\n+# SPDX-License-Identifier: MIT\n+\n+import pytest\n+from cvat_sdk.api_client import ApiClient, Configuration, models\n+from cvat_sdk.api_client.exceptions import UnauthorizedException\n+\n+from shared.utils.config import BASE_URL, USER_PASS\n+\n+\n+class TestCsrfWorkaround:\n+    \"\"\"\n+    Test that session authentication does not work with endpoints\n+    that respond to GET requests and produce side effects.\n+    \"\"\"\n+\n+    @pytest.fixture(autouse=True)\n+    def setup(self, admin_user):\n+        self.client = ApiClient(Configuration(host=BASE_URL))\n+\n+        # Don't store the result - we only want the session cookie.\n+        self.client.auth_api.create_login(\n+            models.LoginSerializerExRequest(username=admin_user, password=USER_PASS)\n+        )\n+\n+        # Test that session authentication works in general.\n+        user, _ = self.client.users_api.retrieve_self()\n+        assert user.username == admin_user\n+\n+    def test_project(self, projects):\n+        actual_project, _ = self.client.projects_api.retrieve(next(iter(projects))[\"id\"])\n+\n+        with pytest.raises(UnauthorizedException):\n+            self.client.projects_api.retrieve_backup(actual_project.id)\n+        with pytest.raises(UnauthorizedException):\n+            self.client.projects_api.retrieve_annotations(id=actual_project.id, format=\"COCO 1.0\")\n+        with pytest.raises(UnauthorizedException):\n+            self.client.projects_api.retrieve_dataset(id=actual_project.id, format=\"COCO 1.0\")\n+\n+    def test_task(self, tasks):\n+        actual_task, _ = self.client.tasks_api.retrieve(next(iter(tasks))[\"id\"])\n+\n+        with pytest.raises(UnauthorizedException):\n+            self.client.tasks_api.retrieve_backup(actual_task.id)\n+        with pytest.raises(UnauthorizedException):\n+            self.client.tasks_api.retrieve_annotations(id=actual_task.id, format=\"COCO 1.0\")\n+        with pytest.raises(UnauthorizedException):\n+            self.client.tasks_api.retrieve_dataset(id=actual_task.id, format=\"COCO 1.0\")\n+\n+    def test_job(self, jobs):\n+        actual_job, _ = self.client.jobs_api.retrieve(next(iter(jobs))[\"id\"])\n+\n+        with pytest.raises(UnauthorizedException):\n+            self.client.jobs_api.retrieve_annotations(id=actual_job.id, format=\"COCO 1.0\")\n+        with pytest.raises(UnauthorizedException):\n+            self.client.jobs_api.retrieve_dataset(id=actual_job.id, format=\"COCO 1.0\")"
        },
        {
          "filename": "tests/python/shared/utils/resource_import_export.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -97,8 +97,8 @@ def _export_resource_to_cloud_storage(\n         status = response.status_code\n \n         while status != _expect_status:\n-            assert status in (HTTPStatus.CREATED, HTTPStatus.ACCEPTED)\n-            response = get_method(user, f\"{obj}/{obj_id}/{resource}\", action=\"download\", **kwargs)\n+            assert status == HTTPStatus.ACCEPTED\n+            response = get_method(user, f\"{obj}/{obj_id}/{resource}\", **kwargs)\n             status = response.status_code\n \n     def _import_annotations_from_cloud_storage("
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 4,
        "unique_directories": 6,
        "max_directory_depth": 4
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "e276295dec449ad965e34808e4bad9ce5ba249d9",
            "date": "2025-01-14T16:47:01Z",
            "author_login": "Marishka17"
          },
          {
            "sha": "a7018633fff9468c2e1823d41c02f3f37053023f",
            "date": "2025-01-14T16:12:08Z",
            "author_login": "SpecLad"
          },
          {
            "sha": "b63f68a88647963b3be460f14c196e901ea1c13e",
            "date": "2025-01-14T13:32:05Z",
            "author_login": "Eldies"
          },
          {
            "sha": "13fd5a7c590dfac1bf7b0e5c3ec291940ebc6380",
            "date": "2025-01-13T18:23:02Z",
            "author_login": "klakhov"
          },
          {
            "sha": "42bc3452bb6b77008f49d306efd8d32cfa2b5b2a",
            "date": "2025-01-13T15:55:58Z",
            "author_login": "SpecLad"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.1,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:L/A:N",
    "cwe_id": "CWE-352",
    "description": "Computer Vision Annotation Tool (CVAT) is an interactive video and image annotation tool for computer vision. Starting in version 2.2.0 and prior to version 2.14.3, if an attacker can trick a logged-in CVAT user into visiting a malicious URL, they can initiate a dataset export or a backup from a project, task or job that the victim user has permission to export into a cloud storage that the victim user has access to. The name of the resulting file can be chosen by the attacker. This implies that the attacker can overwrite arbitrary files in any cloud storage that the victim can access and, if the attacker has read access to the cloud storage used in the attack, they can obtain media files, annotations, settings and other information from any projects, tasks or jobs that the victim has permission to export. Version 2.14.3 contains a fix for the issue. No known workarounds are available.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-06-13T15:15:53.333",
    "last_modified": "2024-11-21T09:23:34.460",
    "fix_date": "2024-06-13T12:17:13Z"
  },
  "references": [
    {
      "url": "https://github.com/cvat-ai/cvat/commit/5d36d10e493d92e893d7eae595544bcbe9cce1ce",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/cvat-ai/cvat/security/advisories/GHSA-jpf9-646h-4px7",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/cvat-ai/cvat/commit/5d36d10e493d92e893d7eae595544bcbe9cce1ce",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://github.com/cvat-ai/cvat/security/advisories/GHSA-jpf9-646h-4px7",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:26.370836",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "cvat",
    "owner": "cvat-ai",
    "created_at": "2018-06-29T14:02:45Z",
    "updated_at": "2025-01-14T13:32:13Z",
    "pushed_at": "2025-01-14T15:11:25Z",
    "size": 328727,
    "stars": 12953,
    "forks": 3081,
    "open_issues": 570,
    "watchers": 12953,
    "has_security_policy": false,
    "default_branch": "develop",
    "protected_branches": [
      "develop",
      "hotfix-2.4.1",
      "hotfix-2.4.7"
    ],
    "languages": {
      "Python": 3863013,
      "TypeScript": 3665803,
      "JavaScript": 1195931,
      "Mustache": 205368,
      "SCSS": 144217,
      "Open Policy Agent": 75701,
      "HTML": 39212,
      "Dockerfile": 11104,
      "Shell": 8504,
      "Smarty": 4858,
      "Jinja": 138
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "mit"
    },
    "collected_at": "2025-01-14T15:26:05.726176"
  }
}