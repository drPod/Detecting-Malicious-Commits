{
  "cve_id": "CVE-2024-38459",
  "github_data": {
    "repository": "langchain-ai/langchain",
    "fix_commit": "ce0b0f22a175139df8f41cdcfb4d2af411112009",
    "related_commits": [
      "ce0b0f22a175139df8f41cdcfb4d2af411112009",
      "ce0b0f22a175139df8f41cdcfb4d2af411112009"
    ],
    "patch_url": "https://github.com/langchain-ai/langchain/commit/ce0b0f22a175139df8f41cdcfb4d2af411112009.patch",
    "fix_commit_details": {
      "sha": "ce0b0f22a175139df8f41cdcfb4d2af411112009",
      "commit_date": "2024-06-13T19:41:24Z",
      "author": {
        "login": "eyurtsev",
        "type": "User",
        "stats": {
          "total_commits": 822,
          "average_weekly_commits": 7.0256410256410255,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 81
        }
      },
      "commit_message": {
        "title": "experimental[major]: Force users to opt-in into code that relies on the python repl (#22860)",
        "length": 256,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 181,
        "additions": 168,
        "deletions": 13
      },
      "files": [
        {
          "filename": "libs/experimental/langchain_experimental/agents/agent_toolkits/pandas/base.py",
          "status": "modified",
          "additions": 33,
          "deletions": 0,
          "patch": "@@ -168,10 +168,23 @@ def create_pandas_dataframe_agent(\n     number_of_head_rows: int = 5,\n     extra_tools: Sequence[BaseTool] = (),\n     engine: Literal[\"pandas\", \"modin\"] = \"pandas\",\n+    allow_dangerous_code: bool = False,\n     **kwargs: Any,\n ) -> AgentExecutor:\n     \"\"\"Construct a Pandas agent from an LLM and dataframe(s).\n \n+    Security Notice:\n+        This agent relies on access to a python repl tool which can execute\n+        arbitrary code. This can be dangerous and requires a specially sandboxed\n+        environment to be safely used. Failure to run this code in a properly\n+        sandboxed environment can lead to arbitrary code execution vulnerabilities,\n+        which can lead to data breaches, data loss, or other security incidents.\n+\n+        Do not use this code with untrusted inputs, with elevated permissions,\n+        or without consulting your security team about proper sandboxing!\n+\n+        You must opt-in to use this functionality by setting allow_dangerous_code=True.\n+\n     Args:\n         llm: Language model to use for the agent. If agent_type is \"tool-calling\" then\n             llm is expected to support tool calling.\n@@ -198,6 +211,16 @@ def create_pandas_dataframe_agent(\n             include_df_in_prompt is True.\n         extra_tools: Additional tools to give to agent on top of a PythonAstREPLTool.\n         engine: One of \"modin\" or \"pandas\". Defaults to \"pandas\".\n+        allow_dangerous_code: bool, default False\n+            This agent relies on access to a python repl tool which can execute\n+            arbitrary code. This can be dangerous and requires a specially sandboxed\n+            environment to be safely used.\n+            Failure to properly sandbox this class can lead to arbitrary code execution\n+            vulnerabilities, which can lead to data breaches, data loss, or\n+            other security incidents.\n+            You must opt in to use this functionality by setting\n+            allow_dangerous_code=True.\n+\n         **kwargs: DEPRECATED. Not used, kept for backwards compatibility.\n \n     Returns:\n@@ -221,6 +244,16 @@ def create_pandas_dataframe_agent(\n             )\n \n     \"\"\"\n+    if not allow_dangerous_code:\n+        raise ValueError(\n+            \"This agent relies on access to a python repl tool which can execute \"\n+            \"arbitrary code. This can be dangerous and requires a specially sandboxed \"\n+            \"environment to be safely used. Please read the security notice in the \"\n+            \"doc-string of this function. You must opt-in to use this functionality \"\n+            \"by setting allow_dangerous_code=True.\"\n+            \"For general security guidelines, please see: \"\n+            \"https://python.langchain.com/v0.1/docs/security/\"\n+        )\n     try:\n         if engine == \"modin\":\n             import modin.pandas as pd"
        },
        {
          "filename": "libs/experimental/langchain_experimental/agents/agent_toolkits/spark/base.py",
          "status": "modified",
          "additions": 36,
          "deletions": 1,
          "patch": "@@ -42,9 +42,44 @@ def create_spark_dataframe_agent(\n     max_execution_time: Optional[float] = None,\n     early_stopping_method: str = \"force\",\n     agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n+    allow_dangerous_code: bool = False,\n     **kwargs: Any,\n ) -> AgentExecutor:\n-    \"\"\"Construct a Spark agent from an LLM and dataframe.\"\"\"\n+    \"\"\"Construct a Spark agent from an LLM and dataframe.\n+\n+    Security Notice:\n+        This agent relies on access to a python repl tool which can execute\n+        arbitrary code. This can be dangerous and requires a specially sandboxed\n+        environment to be safely used. Failure to run this code in a properly\n+        sandboxed environment can lead to arbitrary code execution vulnerabilities,\n+        which can lead to data breaches, data loss, or other security incidents.\n+\n+        Do not use this code with untrusted inputs, with elevated permissions,\n+        or without consulting your security team about proper sandboxing!\n+\n+        You must opt in to use this functionality by setting allow_dangerous_code=True.\n+\n+    Args:\n+        allow_dangerous_code: bool, default False\n+            This agent relies on access to a python repl tool which can execute\n+            arbitrary code. This can be dangerous and requires a specially sandboxed\n+            environment to be safely used.\n+            Failure to properly sandbox this class can lead to arbitrary code execution\n+            vulnerabilities, which can lead to data breaches, data loss, or\n+            other security incidents.\n+            You must opt in to use this functionality by setting\n+            allow_dangerous_code=True.\n+    \"\"\"\n+    if not allow_dangerous_code:\n+        raise ValueError(\n+            \"This agent relies on access to a python repl tool which can execute \"\n+            \"arbitrary code. This can be dangerous and requires a specially sandboxed \"\n+            \"environment to be safely used. Please read the security notice in the \"\n+            \"doc-string of this function. You must opt-in to use this functionality \"\n+            \"by setting allow_dangerous_code=True.\"\n+            \"For general security guidelines, please see: \"\n+            \"https://python.langchain.com/v0.1/docs/security/\"\n+        )\n \n     if not _validate_spark_df(df) and not _validate_spark_connect_df(df):\n         raise ImportError(\"Spark is not installed. run `pip install pyspark`.\")"
        },
        {
          "filename": "libs/experimental/langchain_experimental/agents/agent_toolkits/xorbits/base.py",
          "status": "modified",
          "additions": 37,
          "deletions": 1,
          "patch": "@@ -29,9 +29,45 @@ def create_xorbits_agent(\n     max_execution_time: Optional[float] = None,\n     early_stopping_method: str = \"force\",\n     agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n+    allow_dangerous_code: bool = False,\n     **kwargs: Dict[str, Any],\n ) -> AgentExecutor:\n-    \"\"\"Construct a xorbits agent from an LLM and dataframe.\"\"\"\n+    \"\"\"Construct a xorbits agent from an LLM and dataframe.\n+\n+    Security Notice:\n+        This agent relies on access to a python repl tool which can execute\n+        arbitrary code. This can be dangerous and requires a specially sandboxed\n+        environment to be safely used. Failure to run this code in a properly\n+        sandboxed environment can lead to arbitrary code execution vulnerabilities,\n+        which can lead to data breaches, data loss, or other security incidents.\n+\n+        Do not use this code with untrusted inputs, with elevated permissions,\n+        or without consulting your security team about proper sandboxing!\n+\n+        You must opt in to use this functionality by setting allow_dangerous_code=True.\n+\n+    Args:\n+        allow_dangerous_code: bool, default False\n+            This agent relies on access to a python repl tool which can execute\n+            arbitrary code. This can be dangerous and requires a specially sandboxed\n+            environment to be safely used.\n+            Failure to properly sandbox this class can lead to arbitrary code execution\n+            vulnerabilities, which can lead to data breaches, data loss, or\n+            other security incidents.\n+            You must opt in to use this functionality by setting\n+            allow_dangerous_code=True.\n+    \"\"\"\n+    if not allow_dangerous_code:\n+        raise ValueError(\n+            \"This agent relies on access to a python repl tool which can execute \"\n+            \"arbitrary code. This can be dangerous and requires a specially sandboxed \"\n+            \"environment to be safely used. Please read the security notice in the \"\n+            \"doc-string of this function. You must opt-in to use this functionality \"\n+            \"by setting allow_dangerous_code=True.\"\n+            \"For general security guidelines, please see: \"\n+            \"https://python.langchain.com/v0.1/docs/security/\"\n+        )\n+\n     try:\n         from xorbits import numpy as np\n         from xorbits import pandas as pd"
        },
        {
          "filename": "libs/experimental/langchain_experimental/pal_chain/base.py",
          "status": "modified",
          "additions": 31,
          "deletions": 1,
          "patch": "@@ -18,7 +18,7 @@\n \n from langchain_experimental.pal_chain.colored_object_prompt import COLORED_OBJECT_PROMPT\n from langchain_experimental.pal_chain.math_prompt import MATH_PROMPT\n-from langchain_experimental.pydantic_v1 import Extra, Field\n+from langchain_experimental.pydantic_v1 import Extra, Field, root_validator\n \n COMMAND_EXECUTION_FUNCTIONS = [\"system\", \"exec\", \"execfile\", \"eval\", \"__import__\"]\n COMMAND_EXECUTION_ATTRIBUTES = [\n@@ -129,6 +129,36 @@ class PALChain(Chain):\n     \"\"\"Validations to perform on the generated code.\"\"\"\n     timeout: Optional[int] = 10\n     \"\"\"Timeout in seconds for the generated code to execute.\"\"\"\n+    allow_dangerous_code: bool = False\n+    \"\"\"This chain relies on the execution of generated code, which can be dangerous.\n+    \n+    This class implements an AI technique that generates and evaluates\n+    Python code, which can be dangerous and requires a specially sandboxed\n+    environment to be safely used. While this class implements some basic guardrails\n+    by limiting available locals/globals and by parsing and inspecting\n+    the generated Python AST using `PALValidation`, those guardrails will not\n+    deter sophisticated attackers and are not a replacement for a proper sandbox.\n+    Do not use this class on untrusted inputs, with elevated permissions,\n+    or without consulting your security team about proper sandboxing!\n+    \n+    Failure to properly sandbox this class can lead to arbitrary code execution\n+    vulnerabilities, which can lead to data breaches, data loss, or other security\n+    incidents.\n+    \"\"\"\n+\n+    @root_validator(pre=False, skip_on_failure=True)\n+    def post_init(cls, values: Dict) -> Dict:\n+        if not values[\"allow_dangerous_code\"]:\n+            raise ValueError(\n+                \"This chain relies on the execution of generated code, \"\n+                \"which can be dangerous. \"\n+                \"Please read the security notice for this class, and only \"\n+                \"use it if you understand the security implications. \"\n+                \"If you want to proceed, you will need to opt-in, by setting \"\n+                \"`allow_dangerous_code` to `True`.\"\n+            )\n+\n+        return values\n \n     class Config:\n         \"\"\"Configuration for this pydantic object.\"\"\""
        },
        {
          "filename": "libs/experimental/tests/integration_tests/chains/test_pal.py",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -8,7 +8,7 @@\n def test_math_prompt() -> None:\n     \"\"\"Test math prompt.\"\"\"\n     llm = OpenAI(temperature=0, max_tokens=512)\n-    pal_chain = PALChain.from_math_prompt(llm, timeout=None)\n+    pal_chain = PALChain.from_math_prompt(llm, timeout=None, allow_dangerous_code=False)\n     question = (\n         \"Jan has three times the number of pets as Marcia. \"\n         \"Marcia has two more pets than Cindy. \"\n@@ -21,7 +21,9 @@ def test_math_prompt() -> None:\n def test_colored_object_prompt() -> None:\n     \"\"\"Test colored object prompt.\"\"\"\n     llm = OpenAI(temperature=0, max_tokens=512)\n-    pal_chain = PALChain.from_colored_object_prompt(llm, timeout=None)\n+    pal_chain = PALChain.from_colored_object_prompt(\n+        llm, timeout=None, allow_dangerous_code=False\n+    )\n     question = (\n         \"On the desk, you see two blue booklets, \"\n         \"two purple booklets, and two yellow pairs of sunglasses. \""
        },
        {
          "filename": "libs/experimental/tests/unit_tests/agents/agent_toolkits/pandas/test_base.py",
          "status": "modified",
          "additions": 9,
          "deletions": 2,
          "patch": "@@ -11,5 +11,12 @@\n def test_create_pandas_dataframe_agent() -> None:\n     import pandas as pd\n \n-    create_pandas_dataframe_agent(FakeLLM(), pd.DataFrame())\n-    create_pandas_dataframe_agent(FakeLLM(), [pd.DataFrame(), pd.DataFrame()])\n+    with pytest.raises(ValueError):\n+        create_pandas_dataframe_agent(\n+            FakeLLM(), pd.DataFrame(), allow_dangerous_code=False\n+        )\n+\n+    create_pandas_dataframe_agent(FakeLLM(), pd.DataFrame(), allow_dangerous_code=True)\n+    create_pandas_dataframe_agent(\n+        FakeLLM(), [pd.DataFrame(), pd.DataFrame()], allow_dangerous_code=True\n+    )"
        },
        {
          "filename": "libs/experimental/tests/unit_tests/test_pal.py",
          "status": "modified",
          "additions": 18,
          "deletions": 6,
          "patch": "@@ -189,7 +189,9 @@ def test_math_question_1() -> None:\n     prompt = MATH_PROMPT.format(question=question)\n     queries = {prompt: _MATH_SOLUTION_1}\n     fake_llm = FakeLLM(queries=queries)\n-    fake_pal_chain = PALChain.from_math_prompt(fake_llm, timeout=None)\n+    fake_pal_chain = PALChain.from_math_prompt(\n+        fake_llm, timeout=None, allow_dangerous_code=True\n+    )\n     output = fake_pal_chain.run(question)\n     assert output == \"8\"\n \n@@ -202,7 +204,9 @@ def test_math_question_2() -> None:\n     prompt = MATH_PROMPT.format(question=question)\n     queries = {prompt: _MATH_SOLUTION_2}\n     fake_llm = FakeLLM(queries=queries)\n-    fake_pal_chain = PALChain.from_math_prompt(fake_llm, timeout=None)\n+    fake_pal_chain = PALChain.from_math_prompt(\n+        fake_llm, timeout=None, allow_dangerous_code=True\n+    )\n     output = fake_pal_chain.run(question)\n     assert output == \"33\"\n \n@@ -214,7 +218,9 @@ def test_math_question_3() -> None:\n     prompt = MATH_PROMPT.format(question=question)\n     queries = {prompt: _MATH_SOLUTION_3}\n     fake_llm = FakeLLM(queries=queries)\n-    fake_pal_chain = PALChain.from_math_prompt(fake_llm, timeout=None)\n+    fake_pal_chain = PALChain.from_math_prompt(\n+        fake_llm, timeout=None, allow_dangerous_code=True\n+    )\n     with pytest.raises(ValueError) as exc_info:\n         fake_pal_chain.run(question)\n     assert (\n@@ -231,7 +237,9 @@ def test_math_question_infinite_loop() -> None:\n     prompt = MATH_PROMPT.format(question=question)\n     queries = {prompt: _MATH_SOLUTION_INFINITE_LOOP}\n     fake_llm = FakeLLM(queries=queries)\n-    fake_pal_chain = PALChain.from_math_prompt(fake_llm, timeout=1)\n+    fake_pal_chain = PALChain.from_math_prompt(\n+        fake_llm, timeout=1, allow_dangerous_code=True\n+    )\n     output = fake_pal_chain.run(question)\n     assert output == \"Execution timed out\"\n \n@@ -245,7 +253,9 @@ def test_color_question_1() -> None:\n     prompt = COLORED_OBJECT_PROMPT.format(question=question)\n     queries = {prompt: _COLORED_OBJECT_SOLUTION_1}\n     fake_llm = FakeLLM(queries=queries)\n-    fake_pal_chain = PALChain.from_colored_object_prompt(fake_llm, timeout=None)\n+    fake_pal_chain = PALChain.from_colored_object_prompt(\n+        fake_llm, timeout=None, allow_dangerous_code=True\n+    )\n     output = fake_pal_chain.run(question)\n     assert output == \"0\"\n \n@@ -260,7 +270,9 @@ def test_color_question_2() -> None:\n     prompt = COLORED_OBJECT_PROMPT.format(question=question)\n     queries = {prompt: _COLORED_OBJECT_SOLUTION_2}\n     fake_llm = FakeLLM(queries=queries)\n-    fake_pal_chain = PALChain.from_colored_object_prompt(fake_llm, timeout=None)\n+    fake_pal_chain = PALChain.from_colored_object_prompt(\n+        fake_llm, timeout=None, allow_dangerous_code=True\n+    )\n     output = fake_pal_chain.run(question)\n     assert output == \"brown\"\n "
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 3,
        "unique_directories": 7,
        "max_directory_depth": 7
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "efadad6067095a2a9afd4c3063bf777ca076584a",
            "date": "2025-01-14T18:27:52Z",
            "author_login": "gkorland"
          },
          {
            "sha": "4ab04ad6be0f8f51d22a7df759d7719781fa22f5",
            "date": "2025-01-14T17:55:16Z",
            "author_login": "baskaryan"
          },
          {
            "sha": "d9b856abadef0e7e7338a82f0b2e1239ce3fbd61",
            "date": "2025-01-14T15:23:34Z",
            "author_login": "michaelnchin"
          },
          {
            "sha": "c55af44711ba9180ce8a51a55a385f31023341b5",
            "date": "2025-01-13T23:32:40Z",
            "author_login": "efriis"
          },
          {
            "sha": "cdf3a17e55bd594341c390051dc20c5e5a74b966",
            "date": "2025-01-13T21:25:00Z",
            "author_login": "efriis"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.8,
    "cvss_vector": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H",
    "cwe_id": "CWE-276",
    "description": "langchain_experimental (aka LangChain Experimental) before 0.0.61 for LangChain provides Python REPL access without an opt-in step. NOTE; this issue exists because of an incomplete fix for CVE-2024-27444.",
    "attack_vector": "LOCAL",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-06-16T15:15:51.840",
    "last_modified": "2024-11-21T09:25:57.203",
    "fix_date": "2024-06-13T19:41:24Z"
  },
  "references": [
    {
      "url": "https://github.com/langchain-ai/langchain/commit/ce0b0f22a175139df8f41cdcfb4d2af411112009",
      "source": "cve@mitre.org",
      "tags": []
    },
    {
      "url": "https://github.com/langchain-ai/langchain/compare/langchain-experimental==0.0.60...langchain-experimental==0.0.61",
      "source": "cve@mitre.org",
      "tags": []
    },
    {
      "url": "https://github.com/langchain-ai/langchain/pull/22860",
      "source": "cve@mitre.org",
      "tags": []
    },
    {
      "url": "https://github.com/langchain-ai/langchain/commit/ce0b0f22a175139df8f41cdcfb4d2af411112009",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://github.com/langchain-ai/langchain/compare/langchain-experimental==0.0.60...langchain-experimental==0.0.61",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://github.com/langchain-ai/langchain/pull/22860",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:28.830432",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "langchain",
    "owner": "langchain-ai",
    "created_at": "2022-10-17T02:58:36Z",
    "updated_at": "2025-01-14T13:08:38Z",
    "pushed_at": "2025-01-13T23:32:41Z",
    "size": 376391,
    "stars": 98220,
    "forks": 15956,
    "open_issues": 435,
    "watchers": 98220,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "Jupyter Notebook": 31945259,
      "Python": 19475935,
      "MDX": 56545,
      "Makefile": 55385,
      "Shell": 19562,
      "XSLT": 19446,
      "HTML": 9026,
      "TeX": 2242,
      "Dockerfile": 1311,
      "JavaScript": 471
    },
    "commit_activity": {
      "total_commits_last_year": 7057,
      "avg_commits_per_week": 135.71153846153845,
      "days_active_last_year": 327
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": false,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "mit"
    },
    "collected_at": "2025-01-14T13:11:47.805811"
  }
}