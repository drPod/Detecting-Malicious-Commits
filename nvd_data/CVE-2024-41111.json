{
  "cve_id": "CVE-2024-41111",
  "github_data": {
    "repository": "BishopFox/sliver",
    "fix_commit": "5016fb8d7cdff38c79e22e8293e58300f8d3bd57",
    "related_commits": [
      "5016fb8d7cdff38c79e22e8293e58300f8d3bd57",
      "5016fb8d7cdff38c79e22e8293e58300f8d3bd57"
    ],
    "patch_url": "https://github.com/BishopFox/sliver/commit/5016fb8d7cdff38c79e22e8293e58300f8d3bd57.patch",
    "fix_commit_details": {
      "sha": "5016fb8d7cdff38c79e22e8293e58300f8d3bd57",
      "commit_date": "2024-04-30T21:34:59Z",
      "author": {
        "login": "moloch--",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Update wazero",
        "length": 13,
        "has_description": false,
        "references_issue": false
      },
      "stats": {
        "total": 41341,
        "additions": 41341,
        "deletions": 0
      },
      "files": [
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/checkpoint.go",
          "status": "added",
          "additions": 48,
          "deletions": 0,
          "patch": "@@ -0,0 +1,48 @@\n+package experimental\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/expctxkeys\"\n+)\n+\n+// Snapshot holds the execution state at the time of a Snapshotter.Snapshot call.\n+type Snapshot interface {\n+\t// Restore sets the Wasm execution state to the capture. Because a host function\n+\t// calling this is resetting the pointer to the executation stack, the host function\n+\t// will not be able to return values in the normal way. ret is a slice of values the\n+\t// host function intends to return from the restored function.\n+\tRestore(ret []uint64)\n+}\n+\n+// Snapshotter allows host functions to snapshot the WebAssembly execution environment.\n+type Snapshotter interface {\n+\t// Snapshot captures the current execution state.\n+\tSnapshot() Snapshot\n+}\n+\n+// EnableSnapshotterKey is a context key to indicate that snapshotting should be enabled.\n+// The context.Context passed to a exported function invocation should have this key set\n+// to a non-nil value, and host functions will be able to retrieve it using SnapshotterKey.\n+//\n+// Deprecated: use WithSnapshotter to enable snapshots.\n+type EnableSnapshotterKey = expctxkeys.EnableSnapshotterKey\n+\n+// WithSnapshotter enables snapshots.\n+// Passing the returned context to a exported function invocation enables snapshots,\n+// and allows host functions to retrieve the Snapshotter using GetSnapshotter.\n+func WithSnapshotter(ctx context.Context) context.Context {\n+\treturn context.WithValue(ctx, expctxkeys.EnableSnapshotterKey{}, struct{}{})\n+}\n+\n+// SnapshotterKey is a context key to access a Snapshotter from a host function.\n+// It is only present if EnableSnapshotter was set in the function invocation context.\n+//\n+// Deprecated: use GetSnapshotter to get the snapshotter.\n+type SnapshotterKey = expctxkeys.SnapshotterKey\n+\n+// GetSnapshotter gets the Snapshotter from a host function.\n+// It is only present if WithSnapshotter was called with the function invocation context.\n+func GetSnapshotter(ctx context.Context) Snapshotter {\n+\treturn ctx.Value(expctxkeys.SnapshotterKey{}).(Snapshotter)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/features.go",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": "@@ -0,0 +1,15 @@\n+package experimental\n+\n+import \"github.com/tetratelabs/wazero/api\"\n+\n+// CoreFeaturesThreads enables threads instructions (\"threads\").\n+//\n+// # Notes\n+//\n+//   - The instruction list is too long to enumerate in godoc.\n+//     See https://github.com/WebAssembly/threads/blob/main/proposals/threads/Overview.md\n+//   - Atomic operations are guest-only until api.Memory or otherwise expose them to host functions.\n+//   - On systems without mmap available, the memory will pre-allocate to the maximum size. Many\n+//     binaries will use a theroetical maximum like 4GB, so if using such a binary on a system\n+//     without mmap, consider editing the binary to reduce the max size setting of memory.\n+const CoreFeaturesThreads = api.CoreFeatureSIMD << 1"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/memory.go",
          "status": "added",
          "additions": 50,
          "deletions": 0,
          "patch": "@@ -0,0 +1,50 @@\n+package experimental\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/expctxkeys\"\n+)\n+\n+// MemoryAllocator is a memory allocation hook,\n+// invoked to create a LinearMemory.\n+type MemoryAllocator interface {\n+\t// Allocate should create a new LinearMemory with the given specification:\n+\t// cap is the suggested initial capacity for the backing []byte,\n+\t// and max the maximum length that will ever be requested.\n+\t//\n+\t// Notes:\n+\t//   - To back a shared memory, the address of the backing []byte cannot\n+\t//     change. This is checked at runtime. Implementations should document\n+\t//     if the returned LinearMemory meets this requirement.\n+\tAllocate(cap, max uint64) LinearMemory\n+}\n+\n+// MemoryAllocatorFunc is a convenience for defining inlining a MemoryAllocator.\n+type MemoryAllocatorFunc func(cap, max uint64) LinearMemory\n+\n+// Allocate implements MemoryAllocator.Allocate.\n+func (f MemoryAllocatorFunc) Allocate(cap, max uint64) LinearMemory {\n+\treturn f(cap, max)\n+}\n+\n+// LinearMemory is an expandable []byte that backs a Wasm linear memory.\n+type LinearMemory interface {\n+\t// Reallocates the linear memory to size bytes in length.\n+\t//\n+\t// Notes:\n+\t//   - To back a shared memory, Reallocate can't change the address of the\n+\t//     backing []byte (only its length/capacity may change).\n+\tReallocate(size uint64) []byte\n+\t// Free the backing memory buffer.\n+\tFree()\n+}\n+\n+// WithMemoryAllocator registers the given MemoryAllocator into the given\n+// context.Context.\n+func WithMemoryAllocator(ctx context.Context, allocator MemoryAllocator) context.Context {\n+\tif allocator != nil {\n+\t\treturn context.WithValue(ctx, expctxkeys.MemoryAllocatorKey{}, allocator)\n+\t}\n+\treturn ctx\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/sys/dir.go",
          "status": "added",
          "additions": 92,
          "deletions": 0,
          "patch": "@@ -0,0 +1,92 @@\n+package sys\n+\n+import (\n+\t\"fmt\"\n+\t\"io/fs\"\n+\n+\t\"github.com/tetratelabs/wazero/sys\"\n+)\n+\n+// FileType is fs.FileMode masked on fs.ModeType. For example, zero is a\n+// regular file, fs.ModeDir is a directory and fs.ModeIrregular is unknown.\n+//\n+// Note: This is defined by Linux, not POSIX.\n+type FileType = fs.FileMode\n+\n+// Dirent is an entry read from a directory via File.Readdir.\n+//\n+// # Notes\n+//\n+//   - This extends `dirent` defined in POSIX with some fields defined by\n+//     Linux. See https://man7.org/linux/man-pages/man3/readdir.3.html and\n+//     https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/dirent.h.html\n+//   - This has a subset of fields defined in sys.Stat_t. Notably, there is no\n+//     field corresponding to Stat_t.Dev because that value will be constant\n+//     for all files in a directory. To get the Dev value, call File.Stat on\n+//     the directory File.Readdir was called on.\n+type Dirent struct {\n+\t// Ino is the file serial number, or zero if not available. See Ino for\n+\t// more details including impact returning a zero value.\n+\tIno sys.Inode\n+\n+\t// Name is the base name of the directory entry. Empty is invalid.\n+\tName string\n+\n+\t// Type is fs.FileMode masked on fs.ModeType. For example, zero is a\n+\t// regular file, fs.ModeDir is a directory and fs.ModeIrregular is unknown.\n+\t//\n+\t// Note: This is defined by Linux, not POSIX.\n+\tType fs.FileMode\n+}\n+\n+func (d *Dirent) String() string {\n+\treturn fmt.Sprintf(\"name=%s, type=%v, ino=%d\", d.Name, d.Type, d.Ino)\n+}\n+\n+// IsDir returns true if the Type is fs.ModeDir.\n+func (d *Dirent) IsDir() bool {\n+\treturn d.Type == fs.ModeDir\n+}\n+\n+// DirFile is embeddable to reduce the amount of functions to implement a file.\n+type DirFile struct{}\n+\n+// IsAppend implements File.IsAppend\n+func (DirFile) IsAppend() bool {\n+\treturn false\n+}\n+\n+// SetAppend implements File.SetAppend\n+func (DirFile) SetAppend(bool) Errno {\n+\treturn EISDIR\n+}\n+\n+// IsDir implements File.IsDir\n+func (DirFile) IsDir() (bool, Errno) {\n+\treturn true, 0\n+}\n+\n+// Read implements File.Read\n+func (DirFile) Read([]byte) (int, Errno) {\n+\treturn 0, EISDIR\n+}\n+\n+// Pread implements File.Pread\n+func (DirFile) Pread([]byte, int64) (int, Errno) {\n+\treturn 0, EISDIR\n+}\n+\n+// Write implements File.Write\n+func (DirFile) Write([]byte) (int, Errno) {\n+\treturn 0, EISDIR\n+}\n+\n+// Pwrite implements File.Pwrite\n+func (DirFile) Pwrite([]byte, int64) (int, Errno) {\n+\treturn 0, EISDIR\n+}\n+\n+// Truncate implements File.Truncate\n+func (DirFile) Truncate(int64) Errno {\n+\treturn EISDIR\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/sys/file.go",
          "status": "added",
          "additions": 317,
          "deletions": 0,
          "patch": "@@ -0,0 +1,317 @@\n+package sys\n+\n+import \"github.com/tetratelabs/wazero/sys\"\n+\n+// File is a writeable fs.File bridge backed by syscall functions needed for ABI\n+// including WASI and runtime.GOOS=js.\n+//\n+// Implementations should embed UnimplementedFile for forward compatibility. Any\n+// unsupported method or parameter should return ENOSYS.\n+//\n+// # Errors\n+//\n+// All methods that can return an error return a Errno, which is zero\n+// on success.\n+//\n+// Restricting to Errno matches current WebAssembly host functions,\n+// which are constrained to well-known error codes. For example, `GOOS=js` maps\n+// hard coded values and panics otherwise. More commonly, WASI maps syscall\n+// errors to u32 numeric values.\n+//\n+// # Notes\n+//\n+//   - You must call Close to avoid file resource conflicts. For example,\n+//     Windows cannot delete the underlying directory while a handle to it\n+//     remains open.\n+//   - A writable filesystem abstraction is not yet implemented as of Go 1.20.\n+//     See https://github.com/golang/go/issues/45757\n+type File interface {\n+\t// Dev returns the device ID (Stat_t.Dev) of this file, zero if unknown or\n+\t// an error retrieving it.\n+\t//\n+\t// # Errors\n+\t//\n+\t// Possible errors are those from Stat, except ENOSYS should not\n+\t// be returned. Zero should be returned if there is no implementation.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - Implementations should cache this result.\n+\t//   - This combined with Ino can implement os.SameFile.\n+\tDev() (uint64, Errno)\n+\n+\t// Ino returns the serial number (Stat_t.Ino) of this file, zero if unknown\n+\t// or an error retrieving it.\n+\t//\n+\t// # Errors\n+\t//\n+\t// Possible errors are those from Stat, except ENOSYS should not\n+\t// be returned. Zero should be returned if there is no implementation.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - Implementations should cache this result.\n+\t//   - This combined with Dev can implement os.SameFile.\n+\tIno() (sys.Inode, Errno)\n+\n+\t// IsDir returns true if this file is a directory or an error there was an\n+\t// error retrieving this information.\n+\t//\n+\t// # Errors\n+\t//\n+\t// Possible errors are those from Stat, except ENOSYS should not\n+\t// be returned. false should be returned if there is no implementation.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - Implementations should cache this result.\n+\tIsDir() (bool, Errno)\n+\n+\t// IsAppend returns true if the file was opened with O_APPEND, or\n+\t// SetAppend was successfully enabled on this file.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This might not match the underlying state of the file descriptor if\n+\t//     the file was not opened via OpenFile.\n+\tIsAppend() bool\n+\n+\t// SetAppend toggles the append mode (O_APPEND) of this file.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - There is no `O_APPEND` for `fcntl` in POSIX, so implementations may\n+\t//     have to re-open the underlying file to apply this. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/open.html\n+\tSetAppend(enable bool) Errno\n+\n+\t// Stat is similar to syscall.Fstat.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Fstat and `fstatat` with `AT_FDCWD` in POSIX.\n+\t//     See https://pubs.opengroup.org/onlinepubs/9699919799/functions/stat.html\n+\t//   - A fs.FileInfo backed implementation sets atim, mtim and ctim to the\n+\t//     same value.\n+\t//   - Windows allows you to stat a closed directory.\n+\tStat() (sys.Stat_t, Errno)\n+\n+\t// Read attempts to read all bytes in the file into `buf`, and returns the\n+\t// count read even on error.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed or not readable.\n+\t//   - EISDIR: the file was a directory.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like io.Reader and `read` in POSIX, preferring semantics of\n+\t//     io.Reader. See https://pubs.opengroup.org/onlinepubs/9699919799/functions/read.html\n+\t//   - Unlike io.Reader, there is no io.EOF returned on end-of-file. To\n+\t//     read the file completely, the caller must repeat until `n` is zero.\n+\tRead(buf []byte) (n int, errno Errno)\n+\n+\t// Pread attempts to read all bytes in the file into `p`, starting at the\n+\t// offset `off`, and returns the count read even on error.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed or not readable.\n+\t//   - EINVAL: the offset was negative.\n+\t//   - EISDIR: the file was a directory.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like io.ReaderAt and `pread` in POSIX, preferring semantics\n+\t//     of io.ReaderAt. See https://pubs.opengroup.org/onlinepubs/9699919799/functions/pread.html\n+\t//   - Unlike io.ReaderAt, there is no io.EOF returned on end-of-file. To\n+\t//     read the file completely, the caller must repeat until `n` is zero.\n+\tPread(buf []byte, off int64) (n int, errno Errno)\n+\n+\t// Seek attempts to set the next offset for Read or Write and returns the\n+\t// resulting absolute offset or an error.\n+\t//\n+\t// # Parameters\n+\t//\n+\t// The `offset` parameters is interpreted in terms of `whence`:\n+\t//   - io.SeekStart: relative to the start of the file, e.g. offset=0 sets\n+\t//     the next Read or Write to the beginning of the file.\n+\t//   - io.SeekCurrent: relative to the current offset, e.g. offset=16 sets\n+\t//     the next Read or Write 16 bytes past the prior.\n+\t//   - io.SeekEnd: relative to the end of the file, e.g. offset=-1 sets the\n+\t//     next Read or Write to the last byte in the file.\n+\t//\n+\t// # Behavior when a directory\n+\t//\n+\t// The only supported use case for a directory is seeking to `offset` zero\n+\t// (`whence` = io.SeekStart). This should have the same behavior as\n+\t// os.File, which resets any internal state used by Readdir.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed or not readable.\n+\t//   - EINVAL: the offset was negative.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like io.Seeker and `fseek` in POSIX, preferring semantics\n+\t//     of io.Seeker. See https://pubs.opengroup.org/onlinepubs/9699919799/functions/fseek.html\n+\tSeek(offset int64, whence int) (newOffset int64, errno Errno)\n+\n+\t// Readdir reads the contents of the directory associated with file and\n+\t// returns a slice of up to n Dirent values in an arbitrary order. This is\n+\t// a stateful function, so subsequent calls return any next values.\n+\t//\n+\t// If n > 0, Readdir returns at most n entries or an error.\n+\t// If n <= 0, Readdir returns all remaining entries or an error.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file was closed or not a directory.\n+\t//   - ENOENT: the directory could not be read (e.g. deleted).\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like `Readdir` on os.File, but unlike `readdir` in POSIX.\n+\t//     See https://pubs.opengroup.org/onlinepubs/9699919799/functions/readdir.html\n+\t//   - Unlike os.File, there is no io.EOF returned on end-of-directory. To\n+\t//     read the directory completely, the caller must repeat until the\n+\t//     count read (`len(dirents)`) is less than `n`.\n+\t//   - See /RATIONALE.md for design notes.\n+\tReaddir(n int) (dirents []Dirent, errno Errno)\n+\n+\t// Write attempts to write all bytes in `p` to the file, and returns the\n+\t// count written even on error.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file was closed, not writeable, or a directory.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like io.Writer and `write` in POSIX, preferring semantics of\n+\t//     io.Writer. See https://pubs.opengroup.org/onlinepubs/9699919799/functions/write.html\n+\tWrite(buf []byte) (n int, errno Errno)\n+\n+\t// Pwrite attempts to write all bytes in `p` to the file at the given\n+\t// offset `off`, and returns the count written even on error.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed or not writeable.\n+\t//   - EINVAL: the offset was negative.\n+\t//   - EISDIR: the file was a directory.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like io.WriterAt and `pwrite` in POSIX, preferring semantics\n+\t//     of io.WriterAt. See https://pubs.opengroup.org/onlinepubs/9699919799/functions/pwrite.html\n+\tPwrite(buf []byte, off int64) (n int, errno Errno)\n+\n+\t// Truncate truncates a file to a specified length.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed.\n+\t//   - EINVAL: the `size` is negative.\n+\t//   - EISDIR: the file was a directory.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Ftruncate and `ftruncate` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/ftruncate.html\n+\t//   - Windows does not error when calling Truncate on a closed file.\n+\tTruncate(size int64) Errno\n+\n+\t// Sync synchronizes changes to the file.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - EBADF: the file or directory was closed.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Fsync and `fsync` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html\n+\t//   - This returns with no error instead of ENOSYS when\n+\t//     unimplemented. This prevents fake filesystems from erring.\n+\t//   - Windows does not error when calling Sync on a closed file.\n+\tSync() Errno\n+\n+\t// Datasync synchronizes the data of a file.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - EBADF: the file or directory was closed.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Fdatasync and `fdatasync` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/fdatasync.html\n+\t//   - This returns with no error instead of ENOSYS when\n+\t//     unimplemented. This prevents fake filesystems from erring.\n+\t//   - As this is commonly missing, some implementations dispatch to Sync.\n+\tDatasync() Errno\n+\n+\t// Utimens set file access and modification times of this file, at\n+\t// nanosecond precision.\n+\t//\n+\t// # Parameters\n+\t//\n+\t// The `atim` and `mtim` parameters refer to access and modification time\n+\t// stamps as defined in sys.Stat_t. To retain one or the other, substitute\n+\t// it with the pseudo-timestamp UTIME_OMIT.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EBADF: the file or directory was closed.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.UtimesNano and `futimens` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/futimens.html\n+\t//   - Windows requires files to be open with O_RDWR, which means you\n+\t//     cannot use this to update timestamps on a directory (EPERM).\n+\tUtimens(atim, mtim int64) Errno\n+\n+\t// Close closes the underlying file.\n+\t//\n+\t// A zero Errno is returned if unimplemented or success.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Close and `close` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/close.html\n+\tClose() Errno\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/sys/fs.go",
          "status": "added",
          "additions": 293,
          "deletions": 0,
          "patch": "@@ -0,0 +1,293 @@\n+package sys\n+\n+import (\n+\t\"io/fs\"\n+\n+\t\"github.com/tetratelabs/wazero/sys\"\n+)\n+\n+// FS is a writeable fs.FS bridge backed by syscall functions needed for ABI\n+// including WASI and runtime.GOOS=js.\n+//\n+// Implementations should embed UnimplementedFS for forward compatibility. Any\n+// unsupported method or parameter should return ENO\n+//\n+// # Errors\n+//\n+// All methods that can return an error return a Errno, which is zero\n+// on success.\n+//\n+// Restricting to Errno matches current WebAssembly host functions,\n+// which are constrained to well-known error codes. For example, `GOOS=js` maps\n+// hard coded values and panics otherwise. More commonly, WASI maps syscall\n+// errors to u32 numeric values.\n+//\n+// # Notes\n+//\n+// A writable filesystem abstraction is not yet implemented as of Go 1.20. See\n+// https://github.com/golang/go/issues/45757\n+type FS interface {\n+\t// OpenFile opens a file. It should be closed via Close on File.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `path` or `flag` is invalid.\n+\t//   - EISDIR: the path was a directory, but flag included O_RDWR or\n+\t//     O_WRONLY\n+\t//   - ENOENT: `path` doesn't exist and `flag` doesn't contain O_CREAT.\n+\t//\n+\t// # Constraints on the returned file\n+\t//\n+\t// Implementations that can read flags should enforce them regardless of\n+\t// the type returned. For example, while os.File implements io.Writer,\n+\t// attempts to write to a directory or a file opened with O_RDONLY fail\n+\t// with a EBADF.\n+\t//\n+\t// Some implementations choose whether to enforce read-only opens, namely\n+\t// fs.FS. While fs.FS is supported (Adapt), wazero cannot runtime enforce\n+\t// open flags. Instead, we encourage good behavior and test our built-in\n+\t// implementations.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like os.OpenFile, except the path is relative to this file\n+\t//     system, and Errno is returned instead of os.PathError.\n+\t//   - Implications of permissions when O_CREAT are described in Chmod notes.\n+\t//   - This is like `open` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/open.html\n+\tOpenFile(path string, flag Oflag, perm fs.FileMode) (File, Errno)\n+\n+\t// Lstat gets file status without following symbolic links.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - ENOENT: `path` doesn't exist.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Lstat, except the `path` is relative to this\n+\t//     file system.\n+\t//   - This is like `lstat` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/lstat.html\n+\t//   - An fs.FileInfo backed implementation sets atim, mtim and ctim to the\n+\t//     same value.\n+\t//   - When the path is a symbolic link, the stat returned is for the link,\n+\t//     not the file it refers to.\n+\tLstat(path string) (sys.Stat_t, Errno)\n+\n+\t// Stat gets file status.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - ENOENT: `path` doesn't exist.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Stat, except the `path` is relative to this\n+\t//     file system.\n+\t//   - This is like `stat` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/stat.html\n+\t//   - An fs.FileInfo backed implementation sets atim, mtim and ctim to the\n+\t//     same value.\n+\t//   - When the path is a symbolic link, the stat returned is for the file\n+\t//     it refers to.\n+\tStat(path string) (sys.Stat_t, Errno)\n+\n+\t// Mkdir makes a directory.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `path` is invalid.\n+\t//   - EEXIST: `path` exists and is a directory.\n+\t//   - ENOTDIR: `path` exists and is a file.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Mkdir, except the `path` is relative to this\n+\t//     file system.\n+\t//   - This is like `mkdir` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/mkdir.html\n+\t//   - Implications of permissions are described in Chmod notes.\n+\tMkdir(path string, perm fs.FileMode) Errno\n+\n+\t// Chmod changes the mode of the file.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `path` is invalid.\n+\t//   - ENOENT: `path` does not exist.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Chmod, except the `path` is relative to this\n+\t//     file system.\n+\t//   - This is like `chmod` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/chmod.html\n+\t//   - Windows ignores the execute bit, and any permissions come back as\n+\t//     group and world. For example, chmod of 0400 reads back as 0444, and\n+\t//     0700 0666. Also, permissions on directories aren't supported at all.\n+\tChmod(path string, perm fs.FileMode) Errno\n+\n+\t// Rename renames file or directory.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `from` or `to` is invalid.\n+\t//   - ENOENT: `from` or `to` don't exist.\n+\t//   - ENOTDIR: `from` is a directory and `to` exists as a file.\n+\t//   - EISDIR: `from` is a file and `to` exists as a directory.\n+\t//   - ENOTEMPTY: `both from` and `to` are existing directory, but\n+\t//    `to` is not empty.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Rename, except the paths are relative to this\n+\t//     file system.\n+\t//   - This is like `rename` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/rename.html\n+\t//   -  Windows doesn't let you overwrite an existing directory.\n+\tRename(from, to string) Errno\n+\n+\t// Rmdir removes a directory.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `path` is invalid.\n+\t//   - ENOENT: `path` doesn't exist.\n+\t//   - ENOTDIR: `path` exists, but isn't a directory.\n+\t//   - ENOTEMPTY: `path` exists, but isn't empty.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Rmdir, except the `path` is relative to this\n+\t//     file system.\n+\t//   - This is like `rmdir` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/rmdir.html\n+\t//   - As of Go 1.19, Windows maps ENOTDIR to ENOENT.\n+\tRmdir(path string) Errno\n+\n+\t// Unlink removes a directory entry.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `path` is invalid.\n+\t//   - ENOENT: `path` doesn't exist.\n+\t//   - EISDIR: `path` exists, but is a directory.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Unlink, except the `path` is relative to this\n+\t//     file system.\n+\t//   - This is like `unlink` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/unlink.html\n+\t//   - On Windows, syscall.Unlink doesn't delete symlink to directory unlike other platforms. Implementations might\n+\t//     want to combine syscall.RemoveDirectory with syscall.Unlink in order to delete such links on Windows.\n+\t//     See https://learn.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-removedirectorya\n+\tUnlink(path string) Errno\n+\n+\t// Link creates a \"hard\" link from oldPath to newPath, in contrast to a\n+\t// soft link (via Symlink).\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EPERM: `oldPath` is invalid.\n+\t//   - ENOENT: `oldPath` doesn't exist.\n+\t//   - EISDIR: `newPath` exists, but is a directory.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Link, except the `oldPath` is relative to this\n+\t//     file system.\n+\t//   - This is like `link` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/link.html\n+\tLink(oldPath, newPath string) Errno\n+\n+\t// Symlink creates a \"soft\" link from oldPath to newPath, in contrast to a\n+\t// hard link (via Link).\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EPERM: `oldPath` or `newPath` is invalid.\n+\t//   - EEXIST: `newPath` exists.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Symlink, except the `oldPath` is relative to\n+\t//     this file system.\n+\t//   - This is like `symlink` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/symlink.html\n+\t//   - Only `newPath` is relative to this file system and `oldPath` is kept\n+\t//     as-is. That is because the link is only resolved relative to the\n+\t//     directory when dereferencing it (e.g. ReadLink).\n+\t//     See https://github.com/bytecodealliance/cap-std/blob/v1.0.4/cap-std/src/fs/dir.rs#L404-L409\n+\t//     for how others implement this.\n+\t//   - Symlinks in Windows requires `SeCreateSymbolicLinkPrivilege`.\n+\t//     Otherwise, EPERM results.\n+\t//     See https://learn.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links\n+\tSymlink(oldPath, linkName string) Errno\n+\n+\t// Readlink reads the contents of a symbolic link.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `path` is invalid.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.Readlink, except the path is relative to this\n+\t//     filesystem.\n+\t//   - This is like `readlink` in POSIX. See\n+\t//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/readlink.html\n+\t//   - On Windows, the path separator is different from other platforms,\n+\t//     but to provide consistent results to Wasm, this normalizes to a \"/\"\n+\t//     separator.\n+\tReadlink(path string) (string, Errno)\n+\n+\t// Utimens set file access and modification times on a path relative to\n+\t// this file system, at nanosecond precision.\n+\t//\n+\t// # Parameters\n+\t//\n+\t// If the path is a symbolic link, the target of expanding that link is\n+\t// updated.\n+\t//\n+\t// The `atim` and `mtim` parameters refer to access and modification time\n+\t// stamps as defined in sys.Stat_t. To retain one or the other, substitute\n+\t// it with the pseudo-timestamp UTIME_OMIT.\n+\t//\n+\t// # Errors\n+\t//\n+\t// A zero Errno is success. The below are expected otherwise:\n+\t//   - ENOSYS: the implementation does not support this function.\n+\t//   - EINVAL: `path` is invalid.\n+\t//   - EEXIST: `path` exists and is a directory.\n+\t//   - ENOTDIR: `path` exists and is a file.\n+\t//\n+\t// # Notes\n+\t//\n+\t//   - This is like syscall.UtimesNano and `utimensat` with `AT_FDCWD` in\n+\t//     POSIX. See https://pubs.opengroup.org/onlinepubs/9699919799/functions/futimens.html\n+\tUtimens(path string, atim, mtim int64) Errno\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/sys/oflag.go",
          "status": "added",
          "additions": 70,
          "deletions": 0,
          "patch": "@@ -0,0 +1,70 @@\n+package sys\n+\n+// Oflag are flags used for FS.OpenFile. Values, including zero, should not be\n+// interpreted numerically. Instead, use by constants prefixed with 'O_' with\n+// special casing noted below.\n+//\n+// # Notes\n+//\n+//   - O_RDONLY, O_RDWR and O_WRONLY are mutually exclusive, while the other\n+//     flags can coexist bitwise.\n+//   - This is like `flag` in os.OpenFile and `oflag` in POSIX. See\n+//     https://pubs.opengroup.org/onlinepubs/9699919799/functions/open.html\n+type Oflag uint32\n+\n+// This is a subset of oflags to reduce implementation burden. `wasip1` splits\n+// these across `oflags` and `fdflags`. We can't rely on the Go `os` package,\n+// as it is missing some values. Any flags added will be defined in POSIX\n+// order, as needed by functions that explicitly document accepting them.\n+//\n+// https://github.com/WebAssembly/WASI/blob/snapshot-01/phases/snapshot/docs.md#-oflags-flagsu16\n+// https://github.com/WebAssembly/WASI/blob/snapshot-01/phases/snapshot/docs.md#-fdflags-flagsu16\n+const (\n+\t// O_RDONLY is like os.O_RDONLY\n+\tO_RDONLY Oflag = iota\n+\n+\t// O_RDWR is like os.O_RDWR\n+\tO_RDWR\n+\n+\t// O_WRONLY is like os.O_WRONLY\n+\tO_WRONLY\n+\n+\t// Define bitflags as they are in POSIX `open`: alphabetically\n+\n+\t// O_APPEND is like os.O_APPEND\n+\tO_APPEND Oflag = 1 << iota\n+\n+\t// O_CREAT is link os.O_CREATE\n+\tO_CREAT\n+\n+\t// O_DIRECTORY is defined on some platforms as syscall.O_DIRECTORY.\n+\t//\n+\t// Note: This ensures that the opened file is a directory. Those emulating\n+\t// on platforms that don't support the O_DIRECTORY, can double-check the\n+\t// result with File.IsDir (or stat) and err if not a directory.\n+\tO_DIRECTORY\n+\n+\t// O_DSYNC is defined on some platforms as syscall.O_DSYNC.\n+\tO_DSYNC\n+\n+\t// O_EXCL is defined on some platforms as syscall.O_EXCL.\n+\tO_EXCL\n+\n+\t// O_NOFOLLOW is defined on some platforms as syscall.O_NOFOLLOW.\n+\t//\n+\t// Note: This allows programs to ensure that if the opened file is a\n+\t// symbolic link, the link itself is opened instead of its target.\n+\tO_NOFOLLOW\n+\n+\t// O_NONBLOCK is defined on some platforms as syscall.O_NONBLOCK.\n+\tO_NONBLOCK\n+\n+\t// O_RSYNC is defined on some platforms as syscall.O_RSYNC.\n+\tO_RSYNC\n+\n+\t// O_SYNC is defined on some platforms as syscall.O_SYNC.\n+\tO_SYNC\n+\n+\t// O_TRUNC is defined on some platforms as syscall.O_TRUNC.\n+\tO_TRUNC\n+)"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/sys/syscall_errno_unsupported.go",
          "status": "added",
          "additions": 7,
          "deletions": 0,
          "patch": "@@ -0,0 +1,7 @@\n+//go:build plan9 || aix\n+\n+package sys\n+\n+func syscallToErrno(err error) (Errno, bool) {\n+\treturn 0, false\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/sys/time.go",
          "status": "added",
          "additions": 10,
          "deletions": 0,
          "patch": "@@ -0,0 +1,10 @@\n+package sys\n+\n+import \"math\"\n+\n+// UTIME_OMIT is a special constant for use in updating times via FS.Utimens\n+// or File.Utimens. When used for atim or mtim, the value is retained.\n+//\n+// Note: This may be implemented via a stat when the underlying filesystem\n+// does not support this value.\n+const UTIME_OMIT int64 = math.MinInt64"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/experimental/sys/unimplemented.go",
          "status": "added",
          "additions": 160,
          "deletions": 0,
          "patch": "@@ -0,0 +1,160 @@\n+package sys\n+\n+import (\n+\t\"io/fs\"\n+\n+\t\"github.com/tetratelabs/wazero/sys\"\n+)\n+\n+// UnimplementedFS is an FS that returns ENOSYS for all functions,\n+// This should be embedded to have forward compatible implementations.\n+type UnimplementedFS struct{}\n+\n+// OpenFile implements FS.OpenFile\n+func (UnimplementedFS) OpenFile(path string, flag Oflag, perm fs.FileMode) (File, Errno) {\n+\treturn nil, ENOSYS\n+}\n+\n+// Lstat implements FS.Lstat\n+func (UnimplementedFS) Lstat(path string) (sys.Stat_t, Errno) {\n+\treturn sys.Stat_t{}, ENOSYS\n+}\n+\n+// Stat implements FS.Stat\n+func (UnimplementedFS) Stat(path string) (sys.Stat_t, Errno) {\n+\treturn sys.Stat_t{}, ENOSYS\n+}\n+\n+// Readlink implements FS.Readlink\n+func (UnimplementedFS) Readlink(path string) (string, Errno) {\n+\treturn \"\", ENOSYS\n+}\n+\n+// Mkdir implements FS.Mkdir\n+func (UnimplementedFS) Mkdir(path string, perm fs.FileMode) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Chmod implements FS.Chmod\n+func (UnimplementedFS) Chmod(path string, perm fs.FileMode) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Rename implements FS.Rename\n+func (UnimplementedFS) Rename(from, to string) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Rmdir implements FS.Rmdir\n+func (UnimplementedFS) Rmdir(path string) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Link implements FS.Link\n+func (UnimplementedFS) Link(_, _ string) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Symlink implements FS.Symlink\n+func (UnimplementedFS) Symlink(_, _ string) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Unlink implements FS.Unlink\n+func (UnimplementedFS) Unlink(path string) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Utimens implements FS.Utimens\n+func (UnimplementedFS) Utimens(path string, atim, mtim int64) Errno {\n+\treturn ENOSYS\n+}\n+\n+// UnimplementedFile is a File that returns ENOSYS for all functions,\n+// except where no-op are otherwise documented.\n+//\n+// This should be embedded to have forward compatible implementations.\n+type UnimplementedFile struct{}\n+\n+// Dev implements File.Dev\n+func (UnimplementedFile) Dev() (uint64, Errno) {\n+\treturn 0, 0\n+}\n+\n+// Ino implements File.Ino\n+func (UnimplementedFile) Ino() (sys.Inode, Errno) {\n+\treturn 0, 0\n+}\n+\n+// IsDir implements File.IsDir\n+func (UnimplementedFile) IsDir() (bool, Errno) {\n+\treturn false, 0\n+}\n+\n+// IsAppend implements File.IsAppend\n+func (UnimplementedFile) IsAppend() bool {\n+\treturn false\n+}\n+\n+// SetAppend implements File.SetAppend\n+func (UnimplementedFile) SetAppend(bool) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Stat implements File.Stat\n+func (UnimplementedFile) Stat() (sys.Stat_t, Errno) {\n+\treturn sys.Stat_t{}, ENOSYS\n+}\n+\n+// Read implements File.Read\n+func (UnimplementedFile) Read([]byte) (int, Errno) {\n+\treturn 0, ENOSYS\n+}\n+\n+// Pread implements File.Pread\n+func (UnimplementedFile) Pread([]byte, int64) (int, Errno) {\n+\treturn 0, ENOSYS\n+}\n+\n+// Seek implements File.Seek\n+func (UnimplementedFile) Seek(int64, int) (int64, Errno) {\n+\treturn 0, ENOSYS\n+}\n+\n+// Readdir implements File.Readdir\n+func (UnimplementedFile) Readdir(int) (dirents []Dirent, errno Errno) {\n+\treturn nil, ENOSYS\n+}\n+\n+// Write implements File.Write\n+func (UnimplementedFile) Write([]byte) (int, Errno) {\n+\treturn 0, ENOSYS\n+}\n+\n+// Pwrite implements File.Pwrite\n+func (UnimplementedFile) Pwrite([]byte, int64) (int, Errno) {\n+\treturn 0, ENOSYS\n+}\n+\n+// Truncate implements File.Truncate\n+func (UnimplementedFile) Truncate(int64) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Sync implements File.Sync\n+func (UnimplementedFile) Sync() Errno {\n+\treturn 0 // not ENOSYS\n+}\n+\n+// Datasync implements File.Datasync\n+func (UnimplementedFile) Datasync() Errno {\n+\treturn 0 // not ENOSYS\n+}\n+\n+// Utimens implements File.Utimens\n+func (UnimplementedFile) Utimens(int64, int64) Errno {\n+\treturn ENOSYS\n+}\n+\n+// Close implements File.Close\n+func (UnimplementedFile) Close() (errno Errno) { return }"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/README.md",
          "status": "added",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -0,0 +1 @@\n+wazevo is the new optimizing compiler engine which only supports arm64. AMD64 is under active development."
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/abi.go",
          "status": "added",
          "additions": 170,
          "deletions": 0,
          "patch": "@@ -0,0 +1,170 @@\n+package backend\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+type (\n+\t// FunctionABI represents the ABI information for a function which corresponds to a ssa.Signature.\n+\tFunctionABI struct {\n+\t\tInitialized bool\n+\n+\t\tArgs, Rets                 []ABIArg\n+\t\tArgStackSize, RetStackSize int64\n+\n+\t\tArgIntRealRegs   byte\n+\t\tArgFloatRealRegs byte\n+\t\tRetIntRealRegs   byte\n+\t\tRetFloatRealRegs byte\n+\t}\n+\n+\t// ABIArg represents either argument or return value's location.\n+\tABIArg struct {\n+\t\t// Index is the index of the argument.\n+\t\tIndex int\n+\t\t// Kind is the kind of the argument.\n+\t\tKind ABIArgKind\n+\t\t// Reg is valid if Kind == ABIArgKindReg.\n+\t\t// This VReg must be based on RealReg.\n+\t\tReg regalloc.VReg\n+\t\t// Offset is valid if Kind == ABIArgKindStack.\n+\t\t// This is the offset from the beginning of either arg or ret stack slot.\n+\t\tOffset int64\n+\t\t// Type is the type of the argument.\n+\t\tType ssa.Type\n+\t}\n+\n+\t// ABIArgKind is the kind of ABI argument.\n+\tABIArgKind byte\n+)\n+\n+const (\n+\t// ABIArgKindReg represents an argument passed in a register.\n+\tABIArgKindReg = iota\n+\t// ABIArgKindStack represents an argument passed in the stack.\n+\tABIArgKindStack\n+)\n+\n+// String implements fmt.Stringer.\n+func (a *ABIArg) String() string {\n+\treturn fmt.Sprintf(\"args[%d]: %s\", a.Index, a.Kind)\n+}\n+\n+// String implements fmt.Stringer.\n+func (a ABIArgKind) String() string {\n+\tswitch a {\n+\tcase ABIArgKindReg:\n+\t\treturn \"reg\"\n+\tcase ABIArgKindStack:\n+\t\treturn \"stack\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+// Init initializes the abiImpl for the given signature.\n+func (a *FunctionABI) Init(sig *ssa.Signature, argResultInts, argResultFloats []regalloc.RealReg) {\n+\tif len(a.Rets) < len(sig.Results) {\n+\t\ta.Rets = make([]ABIArg, len(sig.Results))\n+\t}\n+\ta.Rets = a.Rets[:len(sig.Results)]\n+\ta.RetStackSize = a.setABIArgs(a.Rets, sig.Results, argResultInts, argResultFloats)\n+\tif argsNum := len(sig.Params); len(a.Args) < argsNum {\n+\t\ta.Args = make([]ABIArg, argsNum)\n+\t}\n+\ta.Args = a.Args[:len(sig.Params)]\n+\ta.ArgStackSize = a.setABIArgs(a.Args, sig.Params, argResultInts, argResultFloats)\n+\n+\t// Gather the real registers usages in arg/return.\n+\ta.ArgIntRealRegs, a.ArgFloatRealRegs = 0, 0\n+\ta.RetIntRealRegs, a.RetFloatRealRegs = 0, 0\n+\tfor i := range a.Rets {\n+\t\tr := &a.Rets[i]\n+\t\tif r.Kind == ABIArgKindReg {\n+\t\t\tif r.Type.IsInt() {\n+\t\t\t\ta.RetIntRealRegs++\n+\t\t\t} else {\n+\t\t\t\ta.RetFloatRealRegs++\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor i := range a.Args {\n+\t\targ := &a.Args[i]\n+\t\tif arg.Kind == ABIArgKindReg {\n+\t\t\tif arg.Type.IsInt() {\n+\t\t\t\ta.ArgIntRealRegs++\n+\t\t\t} else {\n+\t\t\t\ta.ArgFloatRealRegs++\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\ta.Initialized = true\n+}\n+\n+// setABIArgs sets the ABI arguments in the given slice. This assumes that len(s) >= len(types)\n+// where if len(s) > len(types), the last elements of s is for the multi-return slot.\n+func (a *FunctionABI) setABIArgs(s []ABIArg, types []ssa.Type, ints, floats []regalloc.RealReg) (stackSize int64) {\n+\til, fl := len(ints), len(floats)\n+\n+\tvar stackOffset int64\n+\tintParamIndex, floatParamIndex := 0, 0\n+\tfor i, typ := range types {\n+\t\targ := &s[i]\n+\t\targ.Index = i\n+\t\targ.Type = typ\n+\t\tif typ.IsInt() {\n+\t\t\tif intParamIndex >= il {\n+\t\t\t\targ.Kind = ABIArgKindStack\n+\t\t\t\tconst slotSize = 8 // Align 8 bytes.\n+\t\t\t\targ.Offset = stackOffset\n+\t\t\t\tstackOffset += slotSize\n+\t\t\t} else {\n+\t\t\t\targ.Kind = ABIArgKindReg\n+\t\t\t\targ.Reg = regalloc.FromRealReg(ints[intParamIndex], regalloc.RegTypeInt)\n+\t\t\t\tintParamIndex++\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif floatParamIndex >= fl {\n+\t\t\t\targ.Kind = ABIArgKindStack\n+\t\t\t\tslotSize := int64(8)   // Align at least 8 bytes.\n+\t\t\t\tif typ.Bits() == 128 { // Vector.\n+\t\t\t\t\tslotSize = 16\n+\t\t\t\t}\n+\t\t\t\targ.Offset = stackOffset\n+\t\t\t\tstackOffset += slotSize\n+\t\t\t} else {\n+\t\t\t\targ.Kind = ABIArgKindReg\n+\t\t\t\targ.Reg = regalloc.FromRealReg(floats[floatParamIndex], regalloc.RegTypeFloat)\n+\t\t\t\tfloatParamIndex++\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn stackOffset\n+}\n+\n+func (a *FunctionABI) AlignedArgResultStackSlotSize() uint32 {\n+\tstackSlotSize := a.RetStackSize + a.ArgStackSize\n+\t// Align stackSlotSize to 16 bytes.\n+\tstackSlotSize = (stackSlotSize + 15) &^ 15\n+\t// Check overflow 32-bit.\n+\tif stackSlotSize > 0xFFFFFFFF {\n+\t\tpanic(\"ABI stack slot size overflow\")\n+\t}\n+\treturn uint32(stackSlotSize)\n+}\n+\n+func (a *FunctionABI) ABIInfoAsUint64() uint64 {\n+\treturn uint64(a.ArgIntRealRegs)<<56 |\n+\t\tuint64(a.ArgFloatRealRegs)<<48 |\n+\t\tuint64(a.RetIntRealRegs)<<40 |\n+\t\tuint64(a.RetFloatRealRegs)<<32 |\n+\t\tuint64(a.AlignedArgResultStackSlotSize())\n+}\n+\n+func ABIInfoFromUint64(info uint64) (argIntRealRegs, argFloatRealRegs, retIntRealRegs, retFloatRealRegs byte, stackSlotSize uint32) {\n+\treturn byte(info >> 56), byte(info >> 48), byte(info >> 40), byte(info >> 32), uint32(info)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/backend.go",
          "status": "added",
          "additions": 3,
          "deletions": 0,
          "patch": "@@ -0,0 +1,3 @@\n+// Package backend must be free of Wasm-specific concept. In other words,\n+// this package must not import internal/wasm package.\n+package backend"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/compiler.go",
          "status": "added",
          "additions": 417,
          "deletions": 0,
          "patch": "@@ -0,0 +1,417 @@\n+package backend\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+// NewCompiler returns a new Compiler that can generate a machine code.\n+func NewCompiler(ctx context.Context, mach Machine, builder ssa.Builder) Compiler {\n+\treturn newCompiler(ctx, mach, builder)\n+}\n+\n+func newCompiler(_ context.Context, mach Machine, builder ssa.Builder) *compiler {\n+\targResultInts, argResultFloats := mach.ArgsResultsRegs()\n+\tc := &compiler{\n+\t\tmach: mach, ssaBuilder: builder,\n+\t\tnextVRegID:      regalloc.VRegIDNonReservedBegin,\n+\t\targResultInts:   argResultInts,\n+\t\targResultFloats: argResultFloats,\n+\t}\n+\tmach.SetCompiler(c)\n+\treturn c\n+}\n+\n+// Compiler is the backend of wazevo which takes ssa.Builder and Machine,\n+// use the information there to emit the final machine code.\n+type Compiler interface {\n+\t// SSABuilder returns the ssa.Builder used by this compiler.\n+\tSSABuilder() ssa.Builder\n+\n+\t// Compile executes the following steps:\n+\t// \t1. Lower()\n+\t// \t2. RegAlloc()\n+\t// \t3. Finalize()\n+\t// \t4. Encode()\n+\t//\n+\t// Each step can be called individually for testing purpose, therefore they are exposed in this interface too.\n+\t//\n+\t// The returned byte slices are the machine code and the relocation information for the machine code.\n+\t// The caller is responsible for copying them immediately since the compiler may reuse the buffer.\n+\tCompile(ctx context.Context) (_ []byte, _ []RelocationInfo, _ error)\n+\n+\t// Lower lowers the given ssa.Instruction to the machine-specific instructions.\n+\tLower()\n+\n+\t// RegAlloc performs the register allocation after Lower is called.\n+\tRegAlloc()\n+\n+\t// Finalize performs the finalization of the compilation, including machine code emission.\n+\t// This must be called after RegAlloc.\n+\tFinalize(ctx context.Context) error\n+\n+\t// Buf returns the buffer of the encoded machine code. This is only used for testing purpose.\n+\tBuf() []byte\n+\n+\tBufPtr() *[]byte\n+\n+\t// Format returns the debug string of the current state of the compiler.\n+\tFormat() string\n+\n+\t// Init initializes the internal state of the compiler for the next compilation.\n+\tInit()\n+\n+\t// AllocateVReg allocates a new virtual register of the given type.\n+\tAllocateVReg(typ ssa.Type) regalloc.VReg\n+\n+\t// ValueDefinition returns the definition of the given value.\n+\tValueDefinition(ssa.Value) *SSAValueDefinition\n+\n+\t// VRegOf returns the virtual register of the given ssa.Value.\n+\tVRegOf(value ssa.Value) regalloc.VReg\n+\n+\t// TypeOf returns the ssa.Type of the given virtual register.\n+\tTypeOf(regalloc.VReg) ssa.Type\n+\n+\t// MatchInstr returns true if the given definition is from an instruction with the given opcode, the current group ID,\n+\t// and a refcount of 1. That means, the instruction can be merged/swapped within the current instruction group.\n+\tMatchInstr(def *SSAValueDefinition, opcode ssa.Opcode) bool\n+\n+\t// MatchInstrOneOf is the same as MatchInstr but for multiple opcodes. If it matches one of ssa.Opcode,\n+\t// this returns the opcode. Otherwise, this returns ssa.OpcodeInvalid.\n+\t//\n+\t// Note: caller should be careful to avoid excessive allocation on opcodes slice.\n+\tMatchInstrOneOf(def *SSAValueDefinition, opcodes []ssa.Opcode) ssa.Opcode\n+\n+\t// AddRelocationInfo appends the relocation information for the function reference at the current buffer offset.\n+\tAddRelocationInfo(funcRef ssa.FuncRef)\n+\n+\t// AddSourceOffsetInfo appends the source offset information for the given offset.\n+\tAddSourceOffsetInfo(executableOffset int64, sourceOffset ssa.SourceOffset)\n+\n+\t// SourceOffsetInfo returns the source offset information for the current buffer offset.\n+\tSourceOffsetInfo() []SourceOffsetInfo\n+\n+\t// EmitByte appends a byte to the buffer. Used during the code emission.\n+\tEmitByte(b byte)\n+\n+\t// Emit4Bytes appends 4 bytes to the buffer. Used during the code emission.\n+\tEmit4Bytes(b uint32)\n+\n+\t// Emit8Bytes appends 8 bytes to the buffer. Used during the code emission.\n+\tEmit8Bytes(b uint64)\n+\n+\t// GetFunctionABI returns the ABI information for the given signature.\n+\tGetFunctionABI(sig *ssa.Signature) *FunctionABI\n+}\n+\n+// RelocationInfo represents the relocation information for a call instruction.\n+type RelocationInfo struct {\n+\t// Offset represents the offset from the beginning of the machine code of either a function or the entire module.\n+\tOffset int64\n+\t// Target is the target function of the call instruction.\n+\tFuncRef ssa.FuncRef\n+}\n+\n+// compiler implements Compiler.\n+type compiler struct {\n+\tmach       Machine\n+\tcurrentGID ssa.InstructionGroupID\n+\tssaBuilder ssa.Builder\n+\t// nextVRegID is the next virtual register ID to be allocated.\n+\tnextVRegID regalloc.VRegID\n+\t// ssaValueToVRegs maps ssa.ValueID to regalloc.VReg.\n+\tssaValueToVRegs [] /* VRegID to */ regalloc.VReg\n+\t// ssaValueDefinitions maps ssa.ValueID to its definition.\n+\tssaValueDefinitions []SSAValueDefinition\n+\t// ssaValueRefCounts is a cached list obtained by ssa.Builder.ValueRefCounts().\n+\tssaValueRefCounts []int\n+\t// returnVRegs is the list of virtual registers that store the return values.\n+\treturnVRegs  []regalloc.VReg\n+\tvarEdges     [][2]regalloc.VReg\n+\tvarEdgeTypes []ssa.Type\n+\tconstEdges   []struct {\n+\t\tcInst *ssa.Instruction\n+\t\tdst   regalloc.VReg\n+\t}\n+\tvRegSet         []bool\n+\tvRegIDs         []regalloc.VRegID\n+\ttempRegs        []regalloc.VReg\n+\ttmpVals         []ssa.Value\n+\tssaTypeOfVRegID [] /* VRegID to */ ssa.Type\n+\tbuf             []byte\n+\trelocations     []RelocationInfo\n+\tsourceOffsets   []SourceOffsetInfo\n+\t// abis maps ssa.SignatureID to the ABI implementation.\n+\tabis                           []FunctionABI\n+\targResultInts, argResultFloats []regalloc.RealReg\n+}\n+\n+// SourceOffsetInfo is a data to associate the source offset with the executable offset.\n+type SourceOffsetInfo struct {\n+\t// SourceOffset is the source offset in the original source code.\n+\tSourceOffset ssa.SourceOffset\n+\t// ExecutableOffset is the offset in the compiled executable.\n+\tExecutableOffset int64\n+}\n+\n+// Compile implements Compiler.Compile.\n+func (c *compiler) Compile(ctx context.Context) ([]byte, []RelocationInfo, error) {\n+\tc.Lower()\n+\tif wazevoapi.PrintSSAToBackendIRLowering && wazevoapi.PrintEnabledIndex(ctx) {\n+\t\tfmt.Printf(\"[[[after lowering for %s ]]]%s\\n\", wazevoapi.GetCurrentFunctionName(ctx), c.Format())\n+\t}\n+\tif wazevoapi.DeterministicCompilationVerifierEnabled {\n+\t\twazevoapi.VerifyOrSetDeterministicCompilationContextValue(ctx, \"After lowering to ISA specific IR\", c.Format())\n+\t}\n+\tc.RegAlloc()\n+\tif wazevoapi.PrintRegisterAllocated && wazevoapi.PrintEnabledIndex(ctx) {\n+\t\tfmt.Printf(\"[[[after regalloc for %s]]]%s\\n\", wazevoapi.GetCurrentFunctionName(ctx), c.Format())\n+\t}\n+\tif wazevoapi.DeterministicCompilationVerifierEnabled {\n+\t\twazevoapi.VerifyOrSetDeterministicCompilationContextValue(ctx, \"After Register Allocation\", c.Format())\n+\t}\n+\tif err := c.Finalize(ctx); err != nil {\n+\t\treturn nil, nil, err\n+\t}\n+\tif wazevoapi.PrintFinalizedMachineCode && wazevoapi.PrintEnabledIndex(ctx) {\n+\t\tfmt.Printf(\"[[[after finalize for %s]]]%s\\n\", wazevoapi.GetCurrentFunctionName(ctx), c.Format())\n+\t}\n+\tif wazevoapi.DeterministicCompilationVerifierEnabled {\n+\t\twazevoapi.VerifyOrSetDeterministicCompilationContextValue(ctx, \"After Finalization\", c.Format())\n+\t}\n+\treturn c.buf, c.relocations, nil\n+}\n+\n+// RegAlloc implements Compiler.RegAlloc.\n+func (c *compiler) RegAlloc() {\n+\tc.mach.RegAlloc()\n+}\n+\n+// Finalize implements Compiler.Finalize.\n+func (c *compiler) Finalize(ctx context.Context) error {\n+\tc.mach.PostRegAlloc()\n+\treturn c.mach.Encode(ctx)\n+}\n+\n+// setCurrentGroupID sets the current instruction group ID.\n+func (c *compiler) setCurrentGroupID(gid ssa.InstructionGroupID) {\n+\tc.currentGID = gid\n+}\n+\n+// assignVirtualRegisters assigns a virtual register to each ssa.ValueID Valid in the ssa.Builder.\n+func (c *compiler) assignVirtualRegisters() {\n+\tbuilder := c.ssaBuilder\n+\trefCounts := builder.ValueRefCounts()\n+\tc.ssaValueRefCounts = refCounts\n+\n+\tneed := len(refCounts)\n+\tif need >= len(c.ssaValueToVRegs) {\n+\t\tc.ssaValueToVRegs = append(c.ssaValueToVRegs, make([]regalloc.VReg, need+1)...)\n+\t}\n+\tif need >= len(c.ssaValueDefinitions) {\n+\t\tc.ssaValueDefinitions = append(c.ssaValueDefinitions, make([]SSAValueDefinition, need+1)...)\n+\t}\n+\n+\tfor blk := builder.BlockIteratorReversePostOrderBegin(); blk != nil; blk = builder.BlockIteratorReversePostOrderNext() {\n+\t\t// First we assign a virtual register to each parameter.\n+\t\tfor i := 0; i < blk.Params(); i++ {\n+\t\t\tp := blk.Param(i)\n+\t\t\tpid := p.ID()\n+\t\t\ttyp := p.Type()\n+\t\t\tvreg := c.AllocateVReg(typ)\n+\t\t\tc.ssaValueToVRegs[pid] = vreg\n+\t\t\tc.ssaValueDefinitions[pid] = SSAValueDefinition{BlockParamValue: p, BlkParamVReg: vreg}\n+\t\t\tc.ssaTypeOfVRegID[vreg.ID()] = p.Type()\n+\t\t}\n+\n+\t\t// Assigns each value to a virtual register produced by instructions.\n+\t\tfor cur := blk.Root(); cur != nil; cur = cur.Next() {\n+\t\t\tr, rs := cur.Returns()\n+\t\t\tvar N int\n+\t\t\tif r.Valid() {\n+\t\t\t\tid := r.ID()\n+\t\t\t\tssaTyp := r.Type()\n+\t\t\t\ttyp := r.Type()\n+\t\t\t\tvReg := c.AllocateVReg(typ)\n+\t\t\t\tc.ssaValueToVRegs[id] = vReg\n+\t\t\t\tc.ssaValueDefinitions[id] = SSAValueDefinition{\n+\t\t\t\t\tInstr:    cur,\n+\t\t\t\t\tN:        0,\n+\t\t\t\t\tRefCount: refCounts[id],\n+\t\t\t\t}\n+\t\t\t\tc.ssaTypeOfVRegID[vReg.ID()] = ssaTyp\n+\t\t\t\tN++\n+\t\t\t}\n+\t\t\tfor _, r := range rs {\n+\t\t\t\tid := r.ID()\n+\t\t\t\tssaTyp := r.Type()\n+\t\t\t\tvReg := c.AllocateVReg(ssaTyp)\n+\t\t\t\tc.ssaValueToVRegs[id] = vReg\n+\t\t\t\tc.ssaValueDefinitions[id] = SSAValueDefinition{\n+\t\t\t\t\tInstr:    cur,\n+\t\t\t\t\tN:        N,\n+\t\t\t\t\tRefCount: refCounts[id],\n+\t\t\t\t}\n+\t\t\t\tc.ssaTypeOfVRegID[vReg.ID()] = ssaTyp\n+\t\t\t\tN++\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tfor i, retBlk := 0, builder.ReturnBlock(); i < retBlk.Params(); i++ {\n+\t\ttyp := retBlk.Param(i).Type()\n+\t\tvReg := c.AllocateVReg(typ)\n+\t\tc.returnVRegs = append(c.returnVRegs, vReg)\n+\t\tc.ssaTypeOfVRegID[vReg.ID()] = typ\n+\t}\n+}\n+\n+// AllocateVReg implements Compiler.AllocateVReg.\n+func (c *compiler) AllocateVReg(typ ssa.Type) regalloc.VReg {\n+\tregType := regalloc.RegTypeOf(typ)\n+\tr := regalloc.VReg(c.nextVRegID).SetRegType(regType)\n+\n+\tid := r.ID()\n+\tif int(id) >= len(c.ssaTypeOfVRegID) {\n+\t\tc.ssaTypeOfVRegID = append(c.ssaTypeOfVRegID, make([]ssa.Type, id+1)...)\n+\t}\n+\tc.ssaTypeOfVRegID[id] = typ\n+\tc.nextVRegID++\n+\treturn r\n+}\n+\n+// Init implements Compiler.Init.\n+func (c *compiler) Init() {\n+\tc.currentGID = 0\n+\tc.nextVRegID = regalloc.VRegIDNonReservedBegin\n+\tc.returnVRegs = c.returnVRegs[:0]\n+\tc.mach.Reset()\n+\tc.varEdges = c.varEdges[:0]\n+\tc.constEdges = c.constEdges[:0]\n+\tc.buf = c.buf[:0]\n+\tc.sourceOffsets = c.sourceOffsets[:0]\n+\tc.relocations = c.relocations[:0]\n+}\n+\n+// ValueDefinition implements Compiler.ValueDefinition.\n+func (c *compiler) ValueDefinition(value ssa.Value) *SSAValueDefinition {\n+\treturn &c.ssaValueDefinitions[value.ID()]\n+}\n+\n+// VRegOf implements Compiler.VRegOf.\n+func (c *compiler) VRegOf(value ssa.Value) regalloc.VReg {\n+\treturn c.ssaValueToVRegs[value.ID()]\n+}\n+\n+// Format implements Compiler.Format.\n+func (c *compiler) Format() string {\n+\treturn c.mach.Format()\n+}\n+\n+// TypeOf implements Compiler.Format.\n+func (c *compiler) TypeOf(v regalloc.VReg) ssa.Type {\n+\treturn c.ssaTypeOfVRegID[v.ID()]\n+}\n+\n+// MatchInstr implements Compiler.MatchInstr.\n+func (c *compiler) MatchInstr(def *SSAValueDefinition, opcode ssa.Opcode) bool {\n+\tinstr := def.Instr\n+\treturn def.IsFromInstr() &&\n+\t\tinstr.Opcode() == opcode &&\n+\t\tinstr.GroupID() == c.currentGID &&\n+\t\tdef.RefCount < 2\n+}\n+\n+// MatchInstrOneOf implements Compiler.MatchInstrOneOf.\n+func (c *compiler) MatchInstrOneOf(def *SSAValueDefinition, opcodes []ssa.Opcode) ssa.Opcode {\n+\tinstr := def.Instr\n+\tif !def.IsFromInstr() {\n+\t\treturn ssa.OpcodeInvalid\n+\t}\n+\n+\tif instr.GroupID() != c.currentGID {\n+\t\treturn ssa.OpcodeInvalid\n+\t}\n+\n+\tif def.RefCount >= 2 {\n+\t\treturn ssa.OpcodeInvalid\n+\t}\n+\n+\topcode := instr.Opcode()\n+\tfor _, op := range opcodes {\n+\t\tif opcode == op {\n+\t\t\treturn opcode\n+\t\t}\n+\t}\n+\treturn ssa.OpcodeInvalid\n+}\n+\n+// SSABuilder implements Compiler .SSABuilder.\n+func (c *compiler) SSABuilder() ssa.Builder {\n+\treturn c.ssaBuilder\n+}\n+\n+// AddSourceOffsetInfo implements Compiler.AddSourceOffsetInfo.\n+func (c *compiler) AddSourceOffsetInfo(executableOffset int64, sourceOffset ssa.SourceOffset) {\n+\tc.sourceOffsets = append(c.sourceOffsets, SourceOffsetInfo{\n+\t\tSourceOffset:     sourceOffset,\n+\t\tExecutableOffset: executableOffset,\n+\t})\n+}\n+\n+// SourceOffsetInfo implements Compiler.SourceOffsetInfo.\n+func (c *compiler) SourceOffsetInfo() []SourceOffsetInfo {\n+\treturn c.sourceOffsets\n+}\n+\n+// AddRelocationInfo implements Compiler.AddRelocationInfo.\n+func (c *compiler) AddRelocationInfo(funcRef ssa.FuncRef) {\n+\tc.relocations = append(c.relocations, RelocationInfo{\n+\t\tOffset:  int64(len(c.buf)),\n+\t\tFuncRef: funcRef,\n+\t})\n+}\n+\n+// Emit8Bytes implements Compiler.Emit8Bytes.\n+func (c *compiler) Emit8Bytes(b uint64) {\n+\tc.buf = append(c.buf, byte(b), byte(b>>8), byte(b>>16), byte(b>>24), byte(b>>32), byte(b>>40), byte(b>>48), byte(b>>56))\n+}\n+\n+// Emit4Bytes implements Compiler.Emit4Bytes.\n+func (c *compiler) Emit4Bytes(b uint32) {\n+\tc.buf = append(c.buf, byte(b), byte(b>>8), byte(b>>16), byte(b>>24))\n+}\n+\n+// EmitByte implements Compiler.EmitByte.\n+func (c *compiler) EmitByte(b byte) {\n+\tc.buf = append(c.buf, b)\n+}\n+\n+// Buf implements Compiler.Buf.\n+func (c *compiler) Buf() []byte {\n+\treturn c.buf\n+}\n+\n+// BufPtr implements Compiler.BufPtr.\n+func (c *compiler) BufPtr() *[]byte {\n+\treturn &c.buf\n+}\n+\n+func (c *compiler) GetFunctionABI(sig *ssa.Signature) *FunctionABI {\n+\tif int(sig.ID) >= len(c.abis) {\n+\t\tc.abis = append(c.abis, make([]FunctionABI, int(sig.ID)+1)...)\n+\t}\n+\n+\tabi := &c.abis[sig.ID]\n+\tif abi.Initialized {\n+\t\treturn abi\n+\t}\n+\n+\tabi.Init(sig, c.argResultInts, c.argResultFloats)\n+\treturn abi\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/compiler_lower.go",
          "status": "added",
          "additions": 226,
          "deletions": 0,
          "patch": "@@ -0,0 +1,226 @@\n+package backend\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+// Lower implements Compiler.Lower.\n+func (c *compiler) Lower() {\n+\tc.assignVirtualRegisters()\n+\tc.mach.SetCurrentABI(c.GetFunctionABI(c.ssaBuilder.Signature()))\n+\tc.mach.ExecutableContext().StartLoweringFunction(c.ssaBuilder.BlockIDMax())\n+\tc.lowerBlocks()\n+}\n+\n+// lowerBlocks lowers each block in the ssa.Builder.\n+func (c *compiler) lowerBlocks() {\n+\tbuilder := c.ssaBuilder\n+\tfor blk := builder.BlockIteratorReversePostOrderBegin(); blk != nil; blk = builder.BlockIteratorReversePostOrderNext() {\n+\t\tc.lowerBlock(blk)\n+\t}\n+\n+\tectx := c.mach.ExecutableContext()\n+\t// After lowering all blocks, we need to link adjacent blocks to layout one single instruction list.\n+\tvar prev ssa.BasicBlock\n+\tfor next := builder.BlockIteratorReversePostOrderBegin(); next != nil; next = builder.BlockIteratorReversePostOrderNext() {\n+\t\tif prev != nil {\n+\t\t\tectx.LinkAdjacentBlocks(prev, next)\n+\t\t}\n+\t\tprev = next\n+\t}\n+}\n+\n+func (c *compiler) lowerBlock(blk ssa.BasicBlock) {\n+\tmach := c.mach\n+\tectx := mach.ExecutableContext()\n+\tectx.StartBlock(blk)\n+\n+\t// We traverse the instructions in reverse order because we might want to lower multiple\n+\t// instructions together.\n+\tcur := blk.Tail()\n+\n+\t// First gather the branching instructions at the end of the blocks.\n+\tvar br0, br1 *ssa.Instruction\n+\tif cur.IsBranching() {\n+\t\tbr0 = cur\n+\t\tcur = cur.Prev()\n+\t\tif cur != nil && cur.IsBranching() {\n+\t\t\tbr1 = cur\n+\t\t\tcur = cur.Prev()\n+\t\t}\n+\t}\n+\n+\tif br0 != nil {\n+\t\tc.lowerBranches(br0, br1)\n+\t}\n+\n+\tif br1 != nil && br0 == nil {\n+\t\tpanic(\"BUG? when a block has conditional branch but doesn't end with an unconditional branch?\")\n+\t}\n+\n+\t// Now start lowering the non-branching instructions.\n+\tfor ; cur != nil; cur = cur.Prev() {\n+\t\tc.setCurrentGroupID(cur.GroupID())\n+\t\tif cur.Lowered() {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tswitch cur.Opcode() {\n+\t\tcase ssa.OpcodeReturn:\n+\t\t\trets := cur.ReturnVals()\n+\t\t\tif len(rets) > 0 {\n+\t\t\t\tc.mach.LowerReturns(rets)\n+\t\t\t}\n+\t\t\tc.mach.InsertReturn()\n+\t\tdefault:\n+\t\t\tmach.LowerInstr(cur)\n+\t\t}\n+\t\tectx.FlushPendingInstructions()\n+\t}\n+\n+\t// Finally, if this is the entry block, we have to insert copies of arguments from the real location to the VReg.\n+\tif blk.EntryBlock() {\n+\t\tc.lowerFunctionArguments(blk)\n+\t}\n+\n+\tectx.EndBlock()\n+}\n+\n+// lowerBranches is called right after StartBlock and before any LowerInstr call if\n+// there are branches to the given block. br0 is the very end of the block and b1 is the before the br0 if it exists.\n+// At least br0 is not nil, but br1 can be nil if there's no branching before br0.\n+//\n+// See ssa.Instruction IsBranching, and the comment on ssa.BasicBlock.\n+func (c *compiler) lowerBranches(br0, br1 *ssa.Instruction) {\n+\tectx := c.mach.ExecutableContext()\n+\n+\tc.setCurrentGroupID(br0.GroupID())\n+\tc.mach.LowerSingleBranch(br0)\n+\tectx.FlushPendingInstructions()\n+\tif br1 != nil {\n+\t\tc.setCurrentGroupID(br1.GroupID())\n+\t\tc.mach.LowerConditionalBranch(br1)\n+\t\tectx.FlushPendingInstructions()\n+\t}\n+\n+\tif br0.Opcode() == ssa.OpcodeJump {\n+\t\t_, args, target := br0.BranchData()\n+\t\targExists := len(args) != 0\n+\t\tif argExists && br1 != nil {\n+\t\t\tpanic(\"BUG: critical edge split failed\")\n+\t\t}\n+\t\tif argExists && target.ReturnBlock() {\n+\t\t\tif len(args) > 0 {\n+\t\t\t\tc.mach.LowerReturns(args)\n+\t\t\t}\n+\t\t} else if argExists {\n+\t\t\tc.lowerBlockArguments(args, target)\n+\t\t}\n+\t}\n+\tectx.FlushPendingInstructions()\n+}\n+\n+func (c *compiler) lowerFunctionArguments(entry ssa.BasicBlock) {\n+\tectx := c.mach.ExecutableContext()\n+\n+\tc.tmpVals = c.tmpVals[:0]\n+\tfor i := 0; i < entry.Params(); i++ {\n+\t\tp := entry.Param(i)\n+\t\tif c.ssaValueRefCounts[p.ID()] > 0 {\n+\t\t\tc.tmpVals = append(c.tmpVals, p)\n+\t\t} else {\n+\t\t\t// If the argument is not used, we can just pass an invalid value.\n+\t\t\tc.tmpVals = append(c.tmpVals, ssa.ValueInvalid)\n+\t\t}\n+\t}\n+\tc.mach.LowerParams(c.tmpVals)\n+\tectx.FlushPendingInstructions()\n+}\n+\n+// lowerBlockArguments lowers how to pass arguments to the given successor block.\n+func (c *compiler) lowerBlockArguments(args []ssa.Value, succ ssa.BasicBlock) {\n+\tif len(args) != succ.Params() {\n+\t\tpanic(\"BUG: mismatched number of arguments\")\n+\t}\n+\n+\tc.varEdges = c.varEdges[:0]\n+\tc.varEdgeTypes = c.varEdgeTypes[:0]\n+\tc.constEdges = c.constEdges[:0]\n+\tfor i := 0; i < len(args); i++ {\n+\t\tdst := succ.Param(i)\n+\t\tsrc := args[i]\n+\n+\t\tdstReg := c.VRegOf(dst)\n+\t\tsrcDef := c.ssaValueDefinitions[src.ID()]\n+\t\tif srcDef.IsFromInstr() && srcDef.Instr.Constant() {\n+\t\t\tc.constEdges = append(c.constEdges, struct {\n+\t\t\t\tcInst *ssa.Instruction\n+\t\t\t\tdst   regalloc.VReg\n+\t\t\t}{cInst: srcDef.Instr, dst: dstReg})\n+\t\t} else {\n+\t\t\tsrcReg := c.VRegOf(src)\n+\t\t\t// Even when the src=dst, insert the move so that we can keep such registers keep-alive.\n+\t\t\tc.varEdges = append(c.varEdges, [2]regalloc.VReg{srcReg, dstReg})\n+\t\t\tc.varEdgeTypes = append(c.varEdgeTypes, src.Type())\n+\t\t}\n+\t}\n+\n+\t// Check if there's an overlap among the dsts and srcs in varEdges.\n+\tc.vRegIDs = c.vRegIDs[:0]\n+\tfor _, edge := range c.varEdges {\n+\t\tsrc := edge[0].ID()\n+\t\tif int(src) >= len(c.vRegSet) {\n+\t\t\tc.vRegSet = append(c.vRegSet, make([]bool, src+1)...)\n+\t\t}\n+\t\tc.vRegSet[src] = true\n+\t\tc.vRegIDs = append(c.vRegIDs, src)\n+\t}\n+\tseparated := true\n+\tfor _, edge := range c.varEdges {\n+\t\tdst := edge[1].ID()\n+\t\tif int(dst) >= len(c.vRegSet) {\n+\t\t\tc.vRegSet = append(c.vRegSet, make([]bool, dst+1)...)\n+\t\t} else {\n+\t\t\tif c.vRegSet[dst] {\n+\t\t\t\tseparated = false\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, id := range c.vRegIDs {\n+\t\tc.vRegSet[id] = false // reset for the next use.\n+\t}\n+\n+\tif separated {\n+\t\t// If there's no overlap, we can simply move the source to destination.\n+\t\tfor i, edge := range c.varEdges {\n+\t\t\tsrc, dst := edge[0], edge[1]\n+\t\t\tc.mach.InsertMove(dst, src, c.varEdgeTypes[i])\n+\t\t}\n+\t} else {\n+\t\t// Otherwise, we allocate a temporary registers and move the source to the temporary register,\n+\t\t//\n+\t\t// First move all of them to temporary registers.\n+\t\tc.tempRegs = c.tempRegs[:0]\n+\t\tfor i, edge := range c.varEdges {\n+\t\t\tsrc := edge[0]\n+\t\t\ttyp := c.varEdgeTypes[i]\n+\t\t\ttemp := c.AllocateVReg(typ)\n+\t\t\tc.tempRegs = append(c.tempRegs, temp)\n+\t\t\tc.mach.InsertMove(temp, src, typ)\n+\t\t}\n+\t\t// Then move the temporary registers to the destination.\n+\t\tfor i, edge := range c.varEdges {\n+\t\t\ttemp := c.tempRegs[i]\n+\t\t\tdst := edge[1]\n+\t\t\tc.mach.InsertMove(dst, temp, c.varEdgeTypes[i])\n+\t\t}\n+\t}\n+\n+\t// Finally, move the constants.\n+\tfor _, edge := range c.constEdges {\n+\t\tcInst, dst := edge.cInst, edge.dst\n+\t\tc.mach.InsertLoadConstantBlockArg(cInst, dst)\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/executable_context.go",
          "status": "added",
          "additions": 219,
          "deletions": 0,
          "patch": "@@ -0,0 +1,219 @@\n+package backend\n+\n+import (\n+\t\"fmt\"\n+\t\"math\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+type ExecutableContext interface {\n+\t// StartLoweringFunction is called when the lowering of the given function is started.\n+\t// maximumBlockID is the maximum value of ssa.BasicBlockID existing in the function.\n+\tStartLoweringFunction(maximumBlockID ssa.BasicBlockID)\n+\n+\t// LinkAdjacentBlocks is called after finished lowering all blocks in order to create one single instruction list.\n+\tLinkAdjacentBlocks(prev, next ssa.BasicBlock)\n+\n+\t// StartBlock is called when the compilation of the given block is started.\n+\t// The order of this being called is the reverse post order of the ssa.BasicBlock(s) as we iterate with\n+\t// ssa.Builder BlockIteratorReversePostOrderBegin and BlockIteratorReversePostOrderEnd.\n+\tStartBlock(ssa.BasicBlock)\n+\n+\t// EndBlock is called when the compilation of the current block is finished.\n+\tEndBlock()\n+\n+\t// FlushPendingInstructions flushes the pending instructions to the buffer.\n+\t// This will be called after the lowering of each SSA Instruction.\n+\tFlushPendingInstructions()\n+}\n+\n+type ExecutableContextT[Instr any] struct {\n+\tCurrentSSABlk ssa.BasicBlock\n+\n+\t// InstrPool is the InstructionPool of instructions.\n+\tInstructionPool wazevoapi.Pool[Instr]\n+\tasNop           func(*Instr)\n+\tsetNext         func(*Instr, *Instr)\n+\tsetPrev         func(*Instr, *Instr)\n+\n+\t// RootInstr is the root instruction of the executable.\n+\tRootInstr         *Instr\n+\tlabelPositionPool wazevoapi.Pool[LabelPosition[Instr]]\n+\tNextLabel         Label\n+\t// LabelPositions maps a label to the instructions of the region which the label represents.\n+\tLabelPositions     map[Label]*LabelPosition[Instr]\n+\tOrderedBlockLabels []*LabelPosition[Instr]\n+\n+\t// PerBlockHead and PerBlockEnd are the head and tail of the instruction list per currently-compiled ssa.BasicBlock.\n+\tPerBlockHead, PerBlockEnd *Instr\n+\t// PendingInstructions are the instructions which are not yet emitted into the instruction list.\n+\tPendingInstructions []*Instr\n+\n+\t// SsaBlockIDToLabels maps an SSA block ID to the label.\n+\tSsaBlockIDToLabels []Label\n+}\n+\n+func NewExecutableContextT[Instr any](\n+\tresetInstruction func(*Instr),\n+\tsetNext func(*Instr, *Instr),\n+\tsetPrev func(*Instr, *Instr),\n+\tasNop func(*Instr),\n+) *ExecutableContextT[Instr] {\n+\treturn &ExecutableContextT[Instr]{\n+\t\tInstructionPool:   wazevoapi.NewPool[Instr](resetInstruction),\n+\t\tasNop:             asNop,\n+\t\tsetNext:           setNext,\n+\t\tsetPrev:           setPrev,\n+\t\tlabelPositionPool: wazevoapi.NewPool[LabelPosition[Instr]](resetLabelPosition[Instr]),\n+\t\tLabelPositions:    make(map[Label]*LabelPosition[Instr]),\n+\t\tNextLabel:         LabelInvalid,\n+\t}\n+}\n+\n+func resetLabelPosition[T any](l *LabelPosition[T]) {\n+\t*l = LabelPosition[T]{}\n+}\n+\n+// StartLoweringFunction implements ExecutableContext.\n+func (e *ExecutableContextT[Instr]) StartLoweringFunction(max ssa.BasicBlockID) {\n+\timax := int(max)\n+\tif len(e.SsaBlockIDToLabels) <= imax {\n+\t\t// Eagerly allocate labels for the blocks since the underlying slice will be used for the next iteration.\n+\t\te.SsaBlockIDToLabels = append(e.SsaBlockIDToLabels, make([]Label, imax+1)...)\n+\t}\n+}\n+\n+func (e *ExecutableContextT[Instr]) StartBlock(blk ssa.BasicBlock) {\n+\te.CurrentSSABlk = blk\n+\n+\tl := e.SsaBlockIDToLabels[e.CurrentSSABlk.ID()]\n+\tif l == LabelInvalid {\n+\t\tl = e.AllocateLabel()\n+\t\te.SsaBlockIDToLabels[blk.ID()] = l\n+\t}\n+\n+\tend := e.allocateNop0()\n+\te.PerBlockHead, e.PerBlockEnd = end, end\n+\n+\tlabelPos, ok := e.LabelPositions[l]\n+\tif !ok {\n+\t\tlabelPos = e.AllocateLabelPosition(l)\n+\t\te.LabelPositions[l] = labelPos\n+\t}\n+\te.OrderedBlockLabels = append(e.OrderedBlockLabels, labelPos)\n+\tlabelPos.Begin, labelPos.End = end, end\n+\tlabelPos.SB = blk\n+}\n+\n+// EndBlock implements ExecutableContext.\n+func (e *ExecutableContextT[T]) EndBlock() {\n+\t// Insert nop0 as the head of the block for convenience to simplify the logic of inserting instructions.\n+\te.insertAtPerBlockHead(e.allocateNop0())\n+\n+\tl := e.SsaBlockIDToLabels[e.CurrentSSABlk.ID()]\n+\te.LabelPositions[l].Begin = e.PerBlockHead\n+\n+\tif e.CurrentSSABlk.EntryBlock() {\n+\t\te.RootInstr = e.PerBlockHead\n+\t}\n+}\n+\n+func (e *ExecutableContextT[T]) insertAtPerBlockHead(i *T) {\n+\tif e.PerBlockHead == nil {\n+\t\te.PerBlockHead = i\n+\t\te.PerBlockEnd = i\n+\t\treturn\n+\t}\n+\te.setNext(i, e.PerBlockHead)\n+\te.setPrev(e.PerBlockHead, i)\n+\te.PerBlockHead = i\n+}\n+\n+// FlushPendingInstructions implements ExecutableContext.\n+func (e *ExecutableContextT[T]) FlushPendingInstructions() {\n+\tl := len(e.PendingInstructions)\n+\tif l == 0 {\n+\t\treturn\n+\t}\n+\tfor i := l - 1; i >= 0; i-- { // reverse because we lower instructions in reverse order.\n+\t\te.insertAtPerBlockHead(e.PendingInstructions[i])\n+\t}\n+\te.PendingInstructions = e.PendingInstructions[:0]\n+}\n+\n+func (e *ExecutableContextT[T]) Reset() {\n+\te.labelPositionPool.Reset()\n+\te.InstructionPool.Reset()\n+\tfor l := Label(0); l <= e.NextLabel; l++ {\n+\t\tdelete(e.LabelPositions, l)\n+\t}\n+\te.PendingInstructions = e.PendingInstructions[:0]\n+\te.OrderedBlockLabels = e.OrderedBlockLabels[:0]\n+\te.RootInstr = nil\n+\te.SsaBlockIDToLabels = e.SsaBlockIDToLabels[:0]\n+\te.PerBlockHead, e.PerBlockEnd = nil, nil\n+\te.NextLabel = LabelInvalid\n+}\n+\n+// AllocateLabel allocates an unused label.\n+func (e *ExecutableContextT[T]) AllocateLabel() Label {\n+\te.NextLabel++\n+\treturn e.NextLabel\n+}\n+\n+func (e *ExecutableContextT[T]) AllocateLabelPosition(la Label) *LabelPosition[T] {\n+\tl := e.labelPositionPool.Allocate()\n+\tl.L = la\n+\treturn l\n+}\n+\n+func (e *ExecutableContextT[T]) GetOrAllocateSSABlockLabel(blk ssa.BasicBlock) Label {\n+\tif blk.ReturnBlock() {\n+\t\treturn LabelReturn\n+\t}\n+\tl := e.SsaBlockIDToLabels[blk.ID()]\n+\tif l == LabelInvalid {\n+\t\tl = e.AllocateLabel()\n+\t\te.SsaBlockIDToLabels[blk.ID()] = l\n+\t}\n+\treturn l\n+}\n+\n+func (e *ExecutableContextT[T]) allocateNop0() *T {\n+\ti := e.InstructionPool.Allocate()\n+\te.asNop(i)\n+\treturn i\n+}\n+\n+// LinkAdjacentBlocks implements backend.Machine.\n+func (e *ExecutableContextT[T]) LinkAdjacentBlocks(prev, next ssa.BasicBlock) {\n+\tprevLabelPos := e.LabelPositions[e.GetOrAllocateSSABlockLabel(prev)]\n+\tnextLabelPos := e.LabelPositions[e.GetOrAllocateSSABlockLabel(next)]\n+\te.setNext(prevLabelPos.End, nextLabelPos.Begin)\n+}\n+\n+// LabelPosition represents the regions of the generated code which the label represents.\n+type LabelPosition[Instr any] struct {\n+\tSB           ssa.BasicBlock\n+\tL            Label\n+\tBegin, End   *Instr\n+\tBinaryOffset int64\n+}\n+\n+// Label represents a position in the generated code which is either\n+// a real instruction or the constant InstructionPool (e.g. jump tables).\n+//\n+// This is exactly the same as the traditional \"label\" in assembly code.\n+type Label uint32\n+\n+const (\n+\tLabelInvalid Label = 0\n+\tLabelReturn  Label = math.MaxUint32\n+)\n+\n+// String implements backend.Machine.\n+func (l Label) String() string {\n+\treturn fmt.Sprintf(\"L%d\", l)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/go_call.go",
          "status": "added",
          "additions": 33,
          "deletions": 0,
          "patch": "@@ -0,0 +1,33 @@\n+package backend\n+\n+import \"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\n+// GoFunctionCallRequiredStackSize returns the size of the stack required for the Go function call.\n+// argBegin is the index of the first argument in the signature which is not either execution context or module context.\n+func GoFunctionCallRequiredStackSize(sig *ssa.Signature, argBegin int) (ret, retUnaligned int64) {\n+\tvar paramNeededInBytes, resultNeededInBytes int64\n+\tfor _, p := range sig.Params[argBegin:] {\n+\t\ts := int64(p.Size())\n+\t\tif s < 8 {\n+\t\t\ts = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t}\n+\t\tparamNeededInBytes += s\n+\t}\n+\tfor _, r := range sig.Results {\n+\t\ts := int64(r.Size())\n+\t\tif s < 8 {\n+\t\t\ts = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t}\n+\t\tresultNeededInBytes += s\n+\t}\n+\n+\tif paramNeededInBytes > resultNeededInBytes {\n+\t\tret = paramNeededInBytes\n+\t} else {\n+\t\tret = resultNeededInBytes\n+\t}\n+\tretUnaligned = ret\n+\t// Align to 16 bytes.\n+\tret = (ret + 15) &^ 15\n+\treturn\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/abi.go",
          "status": "added",
          "additions": 186,
          "deletions": 0,
          "patch": "@@ -0,0 +1,186 @@\n+package amd64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+// For the details of the ABI, see:\n+// https://github.com/golang/go/blob/49d42128fd8594c172162961ead19ac95e247d24/src/cmd/compile/abi-internal.md#amd64-architecture\n+\n+var (\n+\tintArgResultRegs   = []regalloc.RealReg{rax, rbx, rcx, rdi, rsi, r8, r9, r10, r11}\n+\tfloatArgResultRegs = []regalloc.RealReg{xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7}\n+)\n+\n+var regInfo = &regalloc.RegisterInfo{\n+\tAllocatableRegisters: [regalloc.NumRegType][]regalloc.RealReg{\n+\t\tregalloc.RegTypeInt: {\n+\t\t\trax, rcx, rdx, rbx, rsi, rdi, r8, r9, r10, r11, r12, r13, r14, r15,\n+\t\t},\n+\t\tregalloc.RegTypeFloat: {\n+\t\t\txmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7, xmm8, xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15,\n+\t\t},\n+\t},\n+\tCalleeSavedRegisters: regalloc.NewRegSet(\n+\t\trdx, r12, r13, r14, r15,\n+\t\txmm8, xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15,\n+\t),\n+\tCallerSavedRegisters: regalloc.NewRegSet(\n+\t\trax, rcx, rbx, rsi, rdi, r8, r9, r10, r11,\n+\t\txmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+\t),\n+\tRealRegToVReg: []regalloc.VReg{\n+\t\trax: raxVReg, rcx: rcxVReg, rdx: rdxVReg, rbx: rbxVReg, rsp: rspVReg, rbp: rbpVReg, rsi: rsiVReg, rdi: rdiVReg,\n+\t\tr8: r8VReg, r9: r9VReg, r10: r10VReg, r11: r11VReg, r12: r12VReg, r13: r13VReg, r14: r14VReg, r15: r15VReg,\n+\t\txmm0: xmm0VReg, xmm1: xmm1VReg, xmm2: xmm2VReg, xmm3: xmm3VReg, xmm4: xmm4VReg, xmm5: xmm5VReg, xmm6: xmm6VReg,\n+\t\txmm7: xmm7VReg, xmm8: xmm8VReg, xmm9: xmm9VReg, xmm10: xmm10VReg, xmm11: xmm11VReg, xmm12: xmm12VReg,\n+\t\txmm13: xmm13VReg, xmm14: xmm14VReg, xmm15: xmm15VReg,\n+\t},\n+\tRealRegName: func(r regalloc.RealReg) string { return regNames[r] },\n+\tRealRegType: func(r regalloc.RealReg) regalloc.RegType {\n+\t\tif r < xmm0 {\n+\t\t\treturn regalloc.RegTypeInt\n+\t\t}\n+\t\treturn regalloc.RegTypeFloat\n+\t},\n+}\n+\n+// ArgsResultsRegs implements backend.Machine.\n+func (m *machine) ArgsResultsRegs() (argResultInts, argResultFloats []regalloc.RealReg) {\n+\treturn intArgResultRegs, floatArgResultRegs\n+}\n+\n+// LowerParams implements backend.Machine.\n+func (m *machine) LowerParams(args []ssa.Value) {\n+\ta := m.currentABI\n+\n+\tfor i, ssaArg := range args {\n+\t\tif !ssaArg.Valid() {\n+\t\t\tcontinue\n+\t\t}\n+\t\treg := m.c.VRegOf(ssaArg)\n+\t\targ := &a.Args[i]\n+\t\tif arg.Kind == backend.ABIArgKindReg {\n+\t\t\tm.InsertMove(reg, arg.Reg, arg.Type)\n+\t\t} else {\n+\t\t\t//\n+\t\t\t//            (high address)\n+\t\t\t//          +-----------------+\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      ret Y      |\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      ret 0      |\n+\t\t\t//          |      arg X      |\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      arg 1      |\n+\t\t\t//          |      arg 0      |\n+\t\t\t//          |   ReturnAddress |\n+\t\t\t//          |    Caller_RBP   |\n+\t\t\t//          +-----------------+ <-- RBP\n+\t\t\t//          |   ...........   |\n+\t\t\t//          |   clobbered  M  |\n+\t\t\t//          |   ............  |\n+\t\t\t//          |   clobbered  0  |\n+\t\t\t//          |   spill slot N  |\n+\t\t\t//          |   ...........   |\n+\t\t\t//          |   spill slot 0  |\n+\t\t\t//   RSP--> +-----------------+\n+\t\t\t//             (low address)\n+\n+\t\t\t// Load the value from the arg stack slot above the current RBP.\n+\t\t\tload := m.allocateInstr()\n+\t\t\tmem := newOperandMem(m.newAmodeImmRBPReg(uint32(arg.Offset + 16)))\n+\t\t\tswitch arg.Type {\n+\t\t\tcase ssa.TypeI32:\n+\t\t\t\tload.asMovzxRmR(extModeLQ, mem, reg)\n+\t\t\tcase ssa.TypeI64:\n+\t\t\t\tload.asMov64MR(mem, reg)\n+\t\t\tcase ssa.TypeF32:\n+\t\t\t\tload.asXmmUnaryRmR(sseOpcodeMovss, mem, reg)\n+\t\t\tcase ssa.TypeF64:\n+\t\t\t\tload.asXmmUnaryRmR(sseOpcodeMovsd, mem, reg)\n+\t\t\tcase ssa.TypeV128:\n+\t\t\t\tload.asXmmUnaryRmR(sseOpcodeMovdqu, mem, reg)\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t\tm.insert(load)\n+\t\t}\n+\t}\n+}\n+\n+// LowerReturns implements backend.Machine.\n+func (m *machine) LowerReturns(rets []ssa.Value) {\n+\t// Load the XMM registers first as it might need a temporary register to inline\n+\t// constant return.\n+\ta := m.currentABI\n+\tfor i, ret := range rets {\n+\t\tr := &a.Rets[i]\n+\t\tif !r.Type.IsInt() {\n+\t\t\tm.LowerReturn(ret, r)\n+\t\t}\n+\t}\n+\t// Then load the GPR registers.\n+\tfor i, ret := range rets {\n+\t\tr := &a.Rets[i]\n+\t\tif r.Type.IsInt() {\n+\t\t\tm.LowerReturn(ret, r)\n+\t\t}\n+\t}\n+}\n+\n+func (m *machine) LowerReturn(ret ssa.Value, r *backend.ABIArg) {\n+\treg := m.c.VRegOf(ret)\n+\tif def := m.c.ValueDefinition(ret); def.IsFromInstr() {\n+\t\t// Constant instructions are inlined.\n+\t\tif inst := def.Instr; inst.Constant() {\n+\t\t\tm.insertLoadConstant(inst, reg)\n+\t\t}\n+\t}\n+\tif r.Kind == backend.ABIArgKindReg {\n+\t\tm.InsertMove(r.Reg, reg, ret.Type())\n+\t} else {\n+\t\t//\n+\t\t//            (high address)\n+\t\t//          +-----------------+\n+\t\t//          |     .......     |\n+\t\t//          |      ret Y      |\n+\t\t//          |     .......     |\n+\t\t//          |      ret 0      |\n+\t\t//          |      arg X      |\n+\t\t//          |     .......     |\n+\t\t//          |      arg 1      |\n+\t\t//          |      arg 0      |\n+\t\t//          |   ReturnAddress |\n+\t\t//          |    Caller_RBP   |\n+\t\t//          +-----------------+ <-- RBP\n+\t\t//          |   ...........   |\n+\t\t//          |   clobbered  M  |\n+\t\t//          |   ............  |\n+\t\t//          |   clobbered  0  |\n+\t\t//          |   spill slot N  |\n+\t\t//          |   ...........   |\n+\t\t//          |   spill slot 0  |\n+\t\t//   RSP--> +-----------------+\n+\t\t//             (low address)\n+\n+\t\t// Store the value to the return stack slot above the current RBP.\n+\t\tstore := m.allocateInstr()\n+\t\tmem := newOperandMem(m.newAmodeImmRBPReg(uint32(m.currentABI.ArgStackSize + 16 + r.Offset)))\n+\t\tswitch r.Type {\n+\t\tcase ssa.TypeI32:\n+\t\t\tstore.asMovRM(reg, mem, 4)\n+\t\tcase ssa.TypeI64:\n+\t\t\tstore.asMovRM(reg, mem, 8)\n+\t\tcase ssa.TypeF32:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovss, reg, mem)\n+\t\tcase ssa.TypeF64:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovsd, reg, mem)\n+\t\tcase ssa.TypeV128:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovdqu, reg, mem)\n+\t\t}\n+\t\tm.insert(store)\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/abi_entry_amd64.go",
          "status": "added",
          "additions": 9,
          "deletions": 0,
          "patch": "@@ -0,0 +1,9 @@\n+package amd64\n+\n+// entrypoint enters the machine code generated by this backend which begins with the preamble generated by functionABI.EmitGoEntryPreamble below.\n+// This implements wazevo.entrypoint, and see the comments there for detail.\n+func entrypoint(preambleExecutable, functionExecutable *byte, executionContextPtr uintptr, moduleContextPtr *byte, paramResultPtr *uint64, goAllocatedStackSlicePtr uintptr)\n+\n+// afterGoFunctionCallEntrypoint enters the machine code after growing the stack.\n+// This implements wazevo.afterGoFunctionCallEntrypoint, and see the comments there for detail.\n+func afterGoFunctionCallEntrypoint(executable *byte, executionContextPtr uintptr, stackPointer, framePointer uintptr)"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/abi_entry_amd64.s",
          "status": "added",
          "additions": 29,
          "deletions": 0,
          "patch": "@@ -0,0 +1,29 @@\n+#include \"funcdata.h\"\n+#include \"textflag.h\"\n+\n+// entrypoint(preambleExecutable, functionExecutable *byte, executionContextPtr uintptr, moduleContextPtr *byte, paramResultPtr *uint64, goAllocatedStackSlicePtr uintptr\n+TEXT \u00b7entrypoint(SB), NOSPLIT|NOFRAME, $0-48\n+\tMOVQ preambleExecutable+0(FP), R11\n+\tMOVQ functionExectuable+8(FP), R14\n+\tMOVQ executionContextPtr+16(FP), AX       // First argument is passed in AX.\n+\tMOVQ moduleContextPtr+24(FP), BX          // Second argument is passed in BX.\n+\tMOVQ paramResultSlicePtr+32(FP), R12\n+\tMOVQ goAllocatedStackSlicePtr+40(FP), R13\n+\tJMP  R11\n+\n+// afterGoFunctionCallEntrypoint(executable *byte, executionContextPtr uintptr, stackPointer, framePointer uintptr)\n+TEXT \u00b7afterGoFunctionCallEntrypoint(SB), NOSPLIT|NOFRAME, $0-32\n+\tMOVQ executable+0(FP), CX\n+\tMOVQ executionContextPtr+8(FP), AX // First argument is passed in AX.\n+\n+\t// Save the stack pointer and frame pointer.\n+\tMOVQ BP, 16(AX) // 16 == ExecutionContextOffsetOriginalFramePointer\n+\tMOVQ SP, 24(AX) // 24 == ExecutionContextOffsetOriginalStackPointer\n+\n+\t// Then set the stack pointer and frame pointer to the values we got from the Go runtime.\n+\tMOVQ framePointer+24(FP), BP\n+\n+\t// WARNING: do not update SP before BP, because the Go translates (FP) as (SP) + 8.\n+\tMOVQ stackPointer+16(FP), SP\n+\n+\tJMP CX"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/abi_entry_preamble.go",
          "status": "added",
          "additions": 248,
          "deletions": 0,
          "patch": "@@ -0,0 +1,248 @@\n+package amd64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+var (\n+\texecutionContextPtrReg = raxVReg\n+\n+\t// Followings are callee saved registers. They can be used freely in the entry preamble\n+\t// since the preamble is called via Go assembly function which has stack-based ABI.\n+\n+\t// savedExecutionContextPtr also must be a callee-saved reg so that they can be used in the prologue and epilogue.\n+\tsavedExecutionContextPtr = rdxVReg\n+\t// paramResultSlicePtr must match with entrypoint function in abi_entry_amd64.s.\n+\tparamResultSlicePtr = r12VReg\n+\t// goAllocatedStackPtr must match with entrypoint function in abi_entry_amd64.s.\n+\tgoAllocatedStackPtr = r13VReg\n+\t// functionExecutable must match with entrypoint function in abi_entry_amd64.s.\n+\tfunctionExecutable = r14VReg\n+\ttmpIntReg          = r15VReg\n+\ttmpXmmReg          = xmm15VReg\n+)\n+\n+// CompileEntryPreamble implements backend.Machine.\n+func (m *machine) CompileEntryPreamble(sig *ssa.Signature) []byte {\n+\troot := m.compileEntryPreamble(sig)\n+\tm.encodeWithoutSSA(root)\n+\tbuf := m.c.Buf()\n+\treturn buf\n+}\n+\n+func (m *machine) compileEntryPreamble(sig *ssa.Signature) *instruction {\n+\tabi := backend.FunctionABI{}\n+\tabi.Init(sig, intArgResultRegs, floatArgResultRegs)\n+\n+\troot := m.allocateNop()\n+\n+\t//// ----------------------------------- prologue ----------------------------------- ////\n+\n+\t// First, we save executionContextPtrReg into a callee-saved register so that it can be used in epilogue as well.\n+\t// \t\tmov %executionContextPtrReg, %savedExecutionContextPtr\n+\tcur := m.move64(executionContextPtrReg, savedExecutionContextPtr, root)\n+\n+\t// Next is to save the original RBP and RSP into the execution context.\n+\tcur = m.saveOriginalRSPRBP(cur)\n+\n+\t// Now set the RSP to the Go-allocated stack pointer.\n+\t// \t\tmov %goAllocatedStackPtr, %rsp\n+\tcur = m.move64(goAllocatedStackPtr, rspVReg, cur)\n+\n+\tif stackSlotSize := abi.AlignedArgResultStackSlotSize(); stackSlotSize > 0 {\n+\t\t// Allocate stack slots for the arguments and return values.\n+\t\t// \t\tsub $stackSlotSize, %rsp\n+\t\tspDec := m.allocateInstr().asAluRmiR(aluRmiROpcodeSub, newOperandImm32(uint32(stackSlotSize)), rspVReg, true)\n+\t\tcur = linkInstr(cur, spDec)\n+\t}\n+\n+\tvar offset uint32\n+\tfor i := range abi.Args {\n+\t\tif i < 2 {\n+\t\t\t// module context ptr and execution context ptr are passed in rax and rbx by the Go assembly function.\n+\t\t\tcontinue\n+\t\t}\n+\t\targ := &abi.Args[i]\n+\t\tcur = m.goEntryPreamblePassArg(cur, paramResultSlicePtr, offset, arg)\n+\t\tif arg.Type == ssa.TypeV128 {\n+\t\t\toffset += 16\n+\t\t} else {\n+\t\t\toffset += 8\n+\t\t}\n+\t}\n+\n+\t// Zero out RBP so that the unwind/stack growth code can correctly detect the end of the stack.\n+\tzerosRbp := m.allocateInstr().asAluRmiR(aluRmiROpcodeXor, newOperandReg(rbpVReg), rbpVReg, true)\n+\tcur = linkInstr(cur, zerosRbp)\n+\n+\t// Now ready to call the real function. Note that at this point stack pointer is already set to the Go-allocated,\n+\t// which is aligned to 16 bytes.\n+\tcall := m.allocateInstr().asCallIndirect(newOperandReg(functionExecutable), &abi)\n+\tcur = linkInstr(cur, call)\n+\n+\t//// ----------------------------------- epilogue ----------------------------------- ////\n+\n+\t// Read the results from regs and the stack, and set them correctly into the paramResultSlicePtr.\n+\toffset = 0\n+\tfor i := range abi.Rets {\n+\t\tr := &abi.Rets[i]\n+\t\tcur = m.goEntryPreamblePassResult(cur, paramResultSlicePtr, offset, r, uint32(abi.ArgStackSize))\n+\t\tif r.Type == ssa.TypeV128 {\n+\t\t\toffset += 16\n+\t\t} else {\n+\t\t\toffset += 8\n+\t\t}\n+\t}\n+\n+\t// Finally, restore the original RBP and RSP.\n+\tcur = m.restoreOriginalRSPRBP(cur)\n+\n+\tret := m.allocateInstr().asRet()\n+\tlinkInstr(cur, ret)\n+\treturn root\n+}\n+\n+// saveOriginalRSPRBP saves the original RSP and RBP into the execution context.\n+func (m *machine) saveOriginalRSPRBP(cur *instruction) *instruction {\n+\t// \t\tmov %rbp, wazevoapi.ExecutionContextOffsetOriginalFramePointer(%executionContextPtrReg)\n+\t// \t\tmov %rsp, wazevoapi.ExecutionContextOffsetOriginalStackPointer(%executionContextPtrReg)\n+\tcur = m.loadOrStore64AtExecutionCtx(executionContextPtrReg, wazevoapi.ExecutionContextOffsetOriginalFramePointer, rbpVReg, true, cur)\n+\tcur = m.loadOrStore64AtExecutionCtx(executionContextPtrReg, wazevoapi.ExecutionContextOffsetOriginalStackPointer, rspVReg, true, cur)\n+\treturn cur\n+}\n+\n+// restoreOriginalRSPRBP restores the original RSP and RBP from the execution context.\n+func (m *machine) restoreOriginalRSPRBP(cur *instruction) *instruction {\n+\t// \t\tmov wazevoapi.ExecutionContextOffsetOriginalFramePointer(%executionContextPtrReg), %rbp\n+\t// \t\tmov wazevoapi.ExecutionContextOffsetOriginalStackPointer(%executionContextPtrReg), %rsp\n+\tcur = m.loadOrStore64AtExecutionCtx(savedExecutionContextPtr, wazevoapi.ExecutionContextOffsetOriginalFramePointer, rbpVReg, false, cur)\n+\tcur = m.loadOrStore64AtExecutionCtx(savedExecutionContextPtr, wazevoapi.ExecutionContextOffsetOriginalStackPointer, rspVReg, false, cur)\n+\treturn cur\n+}\n+\n+func (m *machine) move64(src, dst regalloc.VReg, prev *instruction) *instruction {\n+\tmov := m.allocateInstr().asMovRR(src, dst, true)\n+\treturn linkInstr(prev, mov)\n+}\n+\n+func (m *machine) loadOrStore64AtExecutionCtx(execCtx regalloc.VReg, offset wazevoapi.Offset, r regalloc.VReg, store bool, prev *instruction) *instruction {\n+\tmem := newOperandMem(m.newAmodeImmReg(offset.U32(), execCtx))\n+\tinstr := m.allocateInstr()\n+\tif store {\n+\t\tinstr.asMovRM(r, mem, 8)\n+\t} else {\n+\t\tinstr.asMov64MR(mem, r)\n+\t}\n+\treturn linkInstr(prev, instr)\n+}\n+\n+// This is for debugging.\n+func (m *machine) linkUD2(cur *instruction) *instruction { //nolint\n+\treturn linkInstr(cur, m.allocateInstr().asUD2())\n+}\n+\n+func (m *machine) goEntryPreamblePassArg(cur *instruction, paramSlicePtr regalloc.VReg, offsetInParamSlice uint32, arg *backend.ABIArg) *instruction {\n+\tvar dst regalloc.VReg\n+\targTyp := arg.Type\n+\tif arg.Kind == backend.ABIArgKindStack {\n+\t\t// Caller saved registers ca\n+\t\tswitch argTyp {\n+\t\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\t\tdst = tmpIntReg\n+\t\tcase ssa.TypeF32, ssa.TypeF64, ssa.TypeV128:\n+\t\t\tdst = tmpXmmReg\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t} else {\n+\t\tdst = arg.Reg\n+\t}\n+\n+\tload := m.allocateInstr()\n+\ta := newOperandMem(m.newAmodeImmReg(offsetInParamSlice, paramSlicePtr))\n+\tswitch arg.Type {\n+\tcase ssa.TypeI32:\n+\t\tload.asMovzxRmR(extModeLQ, a, dst)\n+\tcase ssa.TypeI64:\n+\t\tload.asMov64MR(a, dst)\n+\tcase ssa.TypeF32:\n+\t\tload.asXmmUnaryRmR(sseOpcodeMovss, a, dst)\n+\tcase ssa.TypeF64:\n+\t\tload.asXmmUnaryRmR(sseOpcodeMovsd, a, dst)\n+\tcase ssa.TypeV128:\n+\t\tload.asXmmUnaryRmR(sseOpcodeMovdqu, a, dst)\n+\t}\n+\n+\tcur = linkInstr(cur, load)\n+\tif arg.Kind == backend.ABIArgKindStack {\n+\t\t// Store back to the stack.\n+\t\tstore := m.allocateInstr()\n+\t\ta := newOperandMem(m.newAmodeImmReg(uint32(arg.Offset), rspVReg))\n+\t\tswitch arg.Type {\n+\t\tcase ssa.TypeI32:\n+\t\t\tstore.asMovRM(dst, a, 4)\n+\t\tcase ssa.TypeI64:\n+\t\t\tstore.asMovRM(dst, a, 8)\n+\t\tcase ssa.TypeF32:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovss, dst, a)\n+\t\tcase ssa.TypeF64:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovsd, dst, a)\n+\t\tcase ssa.TypeV128:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovdqu, dst, a)\n+\t\t}\n+\t\tcur = linkInstr(cur, store)\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) goEntryPreamblePassResult(cur *instruction, resultSlicePtr regalloc.VReg, offsetInResultSlice uint32, result *backend.ABIArg, resultStackSlotBeginOffset uint32) *instruction {\n+\tvar r regalloc.VReg\n+\tif result.Kind == backend.ABIArgKindStack {\n+\t\t// Load the value to the temporary.\n+\t\tload := m.allocateInstr()\n+\t\toffset := resultStackSlotBeginOffset + uint32(result.Offset)\n+\t\ta := newOperandMem(m.newAmodeImmReg(offset, rspVReg))\n+\t\tswitch result.Type {\n+\t\tcase ssa.TypeI32:\n+\t\t\tr = tmpIntReg\n+\t\t\tload.asMovzxRmR(extModeLQ, a, r)\n+\t\tcase ssa.TypeI64:\n+\t\t\tr = tmpIntReg\n+\t\t\tload.asMov64MR(a, r)\n+\t\tcase ssa.TypeF32:\n+\t\t\tr = tmpXmmReg\n+\t\t\tload.asXmmUnaryRmR(sseOpcodeMovss, a, r)\n+\t\tcase ssa.TypeF64:\n+\t\t\tr = tmpXmmReg\n+\t\t\tload.asXmmUnaryRmR(sseOpcodeMovsd, a, r)\n+\t\tcase ssa.TypeV128:\n+\t\t\tr = tmpXmmReg\n+\t\t\tload.asXmmUnaryRmR(sseOpcodeMovdqu, a, r)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tcur = linkInstr(cur, load)\n+\t} else {\n+\t\tr = result.Reg\n+\t}\n+\n+\tstore := m.allocateInstr()\n+\ta := newOperandMem(m.newAmodeImmReg(offsetInResultSlice, resultSlicePtr))\n+\tswitch result.Type {\n+\tcase ssa.TypeI32:\n+\t\tstore.asMovRM(r, a, 4)\n+\tcase ssa.TypeI64:\n+\t\tstore.asMovRM(r, a, 8)\n+\tcase ssa.TypeF32:\n+\t\tstore.asXmmMovRM(sseOpcodeMovss, r, a)\n+\tcase ssa.TypeF64:\n+\t\tstore.asXmmMovRM(sseOpcodeMovsd, r, a)\n+\tcase ssa.TypeV128:\n+\t\tstore.asXmmMovRM(sseOpcodeMovdqu, r, a)\n+\t}\n+\n+\treturn linkInstr(cur, store)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/abi_go_call.go",
          "status": "added",
          "additions": 443,
          "deletions": 0,
          "patch": "@@ -0,0 +1,443 @@\n+package amd64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+var calleeSavedVRegs = []regalloc.VReg{\n+\trdxVReg, r12VReg, r13VReg, r14VReg, r15VReg,\n+\txmm8VReg, xmm9VReg, xmm10VReg, xmm11VReg, xmm12VReg, xmm13VReg, xmm14VReg, xmm15VReg,\n+}\n+\n+// CompileGoFunctionTrampoline implements backend.Machine.\n+func (m *machine) CompileGoFunctionTrampoline(exitCode wazevoapi.ExitCode, sig *ssa.Signature, needModuleContextPtr bool) []byte {\n+\tectx := m.ectx\n+\targBegin := 1 // Skips exec context by default.\n+\tif needModuleContextPtr {\n+\t\targBegin++\n+\t}\n+\n+\tabi := &backend.FunctionABI{}\n+\tabi.Init(sig, intArgResultRegs, floatArgResultRegs)\n+\tm.currentABI = abi\n+\n+\tcur := m.allocateNop()\n+\tectx.RootInstr = cur\n+\n+\t// Execution context is always the first argument.\n+\texecCtrPtr := raxVReg\n+\n+\t// First we update RBP and RSP just like the normal prologue.\n+\t//\n+\t//                   (high address)                     (high address)\n+\t//       RBP ----> +-----------------+                +-----------------+\n+\t//                 |     .......     |                |     .......     |\n+\t//                 |      ret Y      |                |      ret Y      |\n+\t//                 |     .......     |                |     .......     |\n+\t//                 |      ret 0      |                |      ret 0      |\n+\t//                 |      arg X      |                |      arg X      |\n+\t//                 |     .......     |     ====>      |     .......     |\n+\t//                 |      arg 1      |                |      arg 1      |\n+\t//                 |      arg 0      |                |      arg 0      |\n+\t//                 |   Return Addr   |                |   Return Addr   |\n+\t//       RSP ----> +-----------------+                |    Caller_RBP   |\n+\t//                    (low address)                   +-----------------+ <----- RSP, RBP\n+\t//\n+\tcur = m.setupRBPRSP(cur)\n+\n+\tgoSliceSizeAligned, goSliceSizeAlignedUnaligned := backend.GoFunctionCallRequiredStackSize(sig, argBegin)\n+\tcur = m.insertStackBoundsCheck(goSliceSizeAligned+8 /* size of the Go slice */, cur)\n+\n+\t// Save the callee saved registers.\n+\tcur = m.saveRegistersInExecutionContext(cur, execCtrPtr, calleeSavedVRegs)\n+\n+\tif needModuleContextPtr {\n+\t\tmoduleCtrPtr := rbxVReg // Module context is always the second argument.\n+\t\tmem := m.newAmodeImmReg(\n+\t\t\twazevoapi.ExecutionContextOffsetGoFunctionCallCalleeModuleContextOpaque.U32(),\n+\t\t\texecCtrPtr)\n+\t\tstore := m.allocateInstr().asMovRM(moduleCtrPtr, newOperandMem(mem), 8)\n+\t\tcur = linkInstr(cur, store)\n+\t}\n+\n+\t// Now let's advance the RSP to the stack slot for the arguments.\n+\t//\n+\t//                (high address)                     (high address)\n+\t//              +-----------------+               +-----------------+\n+\t//              |     .......     |               |     .......     |\n+\t//              |      ret Y      |               |      ret Y      |\n+\t//              |     .......     |               |     .......     |\n+\t//              |      ret 0      |               |      ret 0      |\n+\t//              |      arg X      |               |      arg X      |\n+\t//              |     .......     |   =======>    |     .......     |\n+\t//              |      arg 1      |               |      arg 1      |\n+\t//              |      arg 0      |               |      arg 0      |\n+\t//              |   Return Addr   |               |   Return Addr   |\n+\t//              |    Caller_RBP   |               |    Caller_RBP   |\n+\t//  RBP,RSP --> +-----------------+               +-----------------+ <----- RBP\n+\t//                 (low address)                  |  arg[N]/ret[M]  |\n+\t//                                                |    ..........   |\n+\t//                                                |  arg[1]/ret[1]  |\n+\t//                                                |  arg[0]/ret[0]  |\n+\t//                                                +-----------------+ <----- RSP\n+\t//                                                   (low address)\n+\t//\n+\t// where the region of \"arg[0]/ret[0] ... arg[N]/ret[M]\" is the stack used by the Go functions,\n+\t// therefore will be accessed as the usual []uint64. So that's where we need to pass/receive\n+\t// the arguments/return values to/from Go function.\n+\tcur = m.addRSP(-int32(goSliceSizeAligned), cur)\n+\n+\t// Next, we need to store all the arguments to the stack in the typical Wasm stack style.\n+\tvar offsetInGoSlice int32\n+\tfor i := range abi.Args[argBegin:] {\n+\t\targ := &abi.Args[argBegin+i]\n+\t\tvar v regalloc.VReg\n+\t\tif arg.Kind == backend.ABIArgKindReg {\n+\t\t\tv = arg.Reg\n+\t\t} else {\n+\t\t\t// We have saved callee saved registers, so we can use them.\n+\t\t\tif arg.Type.IsInt() {\n+\t\t\t\tv = r15VReg\n+\t\t\t} else {\n+\t\t\t\tv = xmm15VReg\n+\t\t\t}\n+\t\t\tmem := newOperandMem(m.newAmodeImmReg(uint32(arg.Offset+16 /* to skip caller_rbp and ret_addr */), rbpVReg))\n+\t\t\tload := m.allocateInstr()\n+\t\t\tswitch arg.Type {\n+\t\t\tcase ssa.TypeI32:\n+\t\t\t\tload.asMovzxRmR(extModeLQ, mem, v)\n+\t\t\tcase ssa.TypeI64:\n+\t\t\t\tload.asMov64MR(mem, v)\n+\t\t\tcase ssa.TypeF32:\n+\t\t\t\tload.asXmmUnaryRmR(sseOpcodeMovss, mem, v)\n+\t\t\tcase ssa.TypeF64:\n+\t\t\t\tload.asXmmUnaryRmR(sseOpcodeMovsd, mem, v)\n+\t\t\tcase ssa.TypeV128:\n+\t\t\t\tload.asXmmUnaryRmR(sseOpcodeMovdqu, mem, v)\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t\tcur = linkInstr(cur, load)\n+\t\t}\n+\n+\t\tstore := m.allocateInstr()\n+\t\tmem := newOperandMem(m.newAmodeImmReg(uint32(offsetInGoSlice), rspVReg))\n+\t\tswitch arg.Type {\n+\t\tcase ssa.TypeI32:\n+\t\t\tstore.asMovRM(v, mem, 4)\n+\t\t\toffsetInGoSlice += 8 // always uint64 rep.\n+\t\tcase ssa.TypeI64:\n+\t\t\tstore.asMovRM(v, mem, 8)\n+\t\t\toffsetInGoSlice += 8\n+\t\tcase ssa.TypeF32:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovss, v, mem)\n+\t\t\toffsetInGoSlice += 8 // always uint64 rep.\n+\t\tcase ssa.TypeF64:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovsd, v, mem)\n+\t\t\toffsetInGoSlice += 8\n+\t\tcase ssa.TypeV128:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovdqu, v, mem)\n+\t\t\toffsetInGoSlice += 16\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tcur = linkInstr(cur, store)\n+\t}\n+\n+\t// Finally we push the size of the slice to the stack so the stack looks like:\n+\t//\n+\t//          (high address)\n+\t//       +-----------------+\n+\t//       |     .......     |\n+\t//       |      ret Y      |\n+\t//       |     .......     |\n+\t//       |      ret 0      |\n+\t//       |      arg X      |\n+\t//       |     .......     |\n+\t//       |      arg 1      |\n+\t//       |      arg 0      |\n+\t//       |   Return Addr   |\n+\t//       |    Caller_RBP   |\n+\t//       +-----------------+ <----- RBP\n+\t//       |  arg[N]/ret[M]  |\n+\t//       |    ..........   |\n+\t//       |  arg[1]/ret[1]  |\n+\t//       |  arg[0]/ret[0]  |\n+\t//       |    slice size   |\n+\t//       +-----------------+ <----- RSP\n+\t//         (low address)\n+\t//\n+\t// \t\tpush $sliceSize\n+\tcur = linkInstr(cur, m.allocateInstr().asPush64(newOperandImm32(uint32(goSliceSizeAlignedUnaligned))))\n+\n+\t// Load the exitCode to the register.\n+\texitCodeReg := r12VReg // Callee saved which is already saved.\n+\tcur = linkInstr(cur, m.allocateInstr().asImm(exitCodeReg, uint64(exitCode), false))\n+\n+\tsaveRsp, saveRbp, setExitCode := m.allocateExitInstructions(execCtrPtr, exitCodeReg)\n+\tcur = linkInstr(cur, setExitCode)\n+\tcur = linkInstr(cur, saveRsp)\n+\tcur = linkInstr(cur, saveRbp)\n+\n+\t// Ready to exit the execution.\n+\tcur = m.storeReturnAddressAndExit(cur, execCtrPtr)\n+\n+\t// We don't need the slice size anymore, so pop it.\n+\tcur = m.addRSP(8, cur)\n+\n+\t// Ready to set up the results.\n+\toffsetInGoSlice = 0\n+\t// To avoid overwriting with the execution context pointer by the result, we need to track the offset,\n+\t// and defer the restoration of the result to the end of this function.\n+\tvar argOverlapWithExecCtxOffset int32 = -1\n+\tfor i := range abi.Rets {\n+\t\tr := &abi.Rets[i]\n+\t\tvar v regalloc.VReg\n+\t\tisRegResult := r.Kind == backend.ABIArgKindReg\n+\t\tif isRegResult {\n+\t\t\tv = r.Reg\n+\t\t\tif v.RealReg() == execCtrPtr.RealReg() {\n+\t\t\t\targOverlapWithExecCtxOffset = offsetInGoSlice\n+\t\t\t\toffsetInGoSlice += 8 // always uint64 rep.\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif r.Type.IsInt() {\n+\t\t\t\tv = r15VReg\n+\t\t\t} else {\n+\t\t\t\tv = xmm15VReg\n+\t\t\t}\n+\t\t}\n+\n+\t\tload := m.allocateInstr()\n+\t\tmem := newOperandMem(m.newAmodeImmReg(uint32(offsetInGoSlice), rspVReg))\n+\t\tswitch r.Type {\n+\t\tcase ssa.TypeI32:\n+\t\t\tload.asMovzxRmR(extModeLQ, mem, v)\n+\t\t\toffsetInGoSlice += 8 // always uint64 rep.\n+\t\tcase ssa.TypeI64:\n+\t\t\tload.asMov64MR(mem, v)\n+\t\t\toffsetInGoSlice += 8\n+\t\tcase ssa.TypeF32:\n+\t\t\tload.asXmmUnaryRmR(sseOpcodeMovss, mem, v)\n+\t\t\toffsetInGoSlice += 8 // always uint64 rep.\n+\t\tcase ssa.TypeF64:\n+\t\t\tload.asXmmUnaryRmR(sseOpcodeMovsd, mem, v)\n+\t\t\toffsetInGoSlice += 8\n+\t\tcase ssa.TypeV128:\n+\t\t\tload.asXmmUnaryRmR(sseOpcodeMovdqu, mem, v)\n+\t\t\toffsetInGoSlice += 16\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tcur = linkInstr(cur, load)\n+\n+\t\tif !isRegResult {\n+\t\t\t// We need to store it back to the result slot above rbp.\n+\t\t\tstore := m.allocateInstr()\n+\t\t\tmem := newOperandMem(m.newAmodeImmReg(uint32(abi.ArgStackSize+r.Offset+16 /* to skip caller_rbp and ret_addr */), rbpVReg))\n+\t\t\tswitch r.Type {\n+\t\t\tcase ssa.TypeI32:\n+\t\t\t\tstore.asMovRM(v, mem, 4)\n+\t\t\tcase ssa.TypeI64:\n+\t\t\t\tstore.asMovRM(v, mem, 8)\n+\t\t\tcase ssa.TypeF32:\n+\t\t\t\tstore.asXmmMovRM(sseOpcodeMovss, v, mem)\n+\t\t\tcase ssa.TypeF64:\n+\t\t\t\tstore.asXmmMovRM(sseOpcodeMovsd, v, mem)\n+\t\t\tcase ssa.TypeV128:\n+\t\t\t\tstore.asXmmMovRM(sseOpcodeMovdqu, v, mem)\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t\tcur = linkInstr(cur, store)\n+\t\t}\n+\t}\n+\n+\t// Before return, we need to restore the callee saved registers.\n+\tcur = m.restoreRegistersInExecutionContext(cur, execCtrPtr, calleeSavedVRegs)\n+\n+\tif argOverlapWithExecCtxOffset >= 0 {\n+\t\t// At this point execCtt is not used anymore, so we can finally store the\n+\t\t// result to the register which overlaps with the execution context pointer.\n+\t\tmem := newOperandMem(m.newAmodeImmReg(uint32(argOverlapWithExecCtxOffset), rspVReg))\n+\t\tload := m.allocateInstr().asMov64MR(mem, execCtrPtr)\n+\t\tcur = linkInstr(cur, load)\n+\t}\n+\n+\t// Finally ready to return.\n+\tcur = m.revertRBPRSP(cur)\n+\tlinkInstr(cur, m.allocateInstr().asRet())\n+\n+\tm.encodeWithoutSSA(ectx.RootInstr)\n+\treturn m.c.Buf()\n+}\n+\n+func (m *machine) saveRegistersInExecutionContext(cur *instruction, execCtx regalloc.VReg, regs []regalloc.VReg) *instruction {\n+\toffset := wazevoapi.ExecutionContextOffsetSavedRegistersBegin.I64()\n+\tfor _, v := range regs {\n+\t\tstore := m.allocateInstr()\n+\t\tmem := newOperandMem(m.newAmodeImmReg(uint32(offset), execCtx))\n+\t\tswitch v.RegType() {\n+\t\tcase regalloc.RegTypeInt:\n+\t\t\tstore.asMovRM(v, mem, 8)\n+\t\tcase regalloc.RegTypeFloat:\n+\t\t\tstore.asXmmMovRM(sseOpcodeMovdqu, v, mem)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tcur = linkInstr(cur, store)\n+\t\toffset += 16 // See execution context struct. Each register is 16 bytes-aligned unconditionally.\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) restoreRegistersInExecutionContext(cur *instruction, execCtx regalloc.VReg, regs []regalloc.VReg) *instruction {\n+\toffset := wazevoapi.ExecutionContextOffsetSavedRegistersBegin.I64()\n+\tfor _, v := range regs {\n+\t\tload := m.allocateInstr()\n+\t\tmem := newOperandMem(m.newAmodeImmReg(uint32(offset), execCtx))\n+\t\tswitch v.RegType() {\n+\t\tcase regalloc.RegTypeInt:\n+\t\t\tload.asMov64MR(mem, v)\n+\t\tcase regalloc.RegTypeFloat:\n+\t\t\tload.asXmmUnaryRmR(sseOpcodeMovdqu, mem, v)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tcur = linkInstr(cur, load)\n+\t\toffset += 16 // See execution context struct. Each register is 16 bytes-aligned unconditionally.\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) storeReturnAddressAndExit(cur *instruction, execCtx regalloc.VReg) *instruction {\n+\treadRip := m.allocateInstr()\n+\tcur = linkInstr(cur, readRip)\n+\n+\tripReg := r12VReg // Callee saved which is already saved.\n+\tsaveRip := m.allocateInstr().asMovRM(\n+\t\tripReg,\n+\t\tnewOperandMem(m.newAmodeImmReg(wazevoapi.ExecutionContextOffsetGoCallReturnAddress.U32(), execCtx)),\n+\t\t8,\n+\t)\n+\tcur = linkInstr(cur, saveRip)\n+\n+\texit := m.allocateExitSeq(execCtx)\n+\tcur = linkInstr(cur, exit)\n+\n+\tnop, l := m.allocateBrTarget()\n+\tcur = linkInstr(cur, nop)\n+\treadRip.asLEA(newOperandLabel(l), ripReg)\n+\treturn cur\n+}\n+\n+// saveRequiredRegs is the set of registers that must be saved/restored during growing stack when there's insufficient\n+// stack space left. Basically this is the all allocatable registers except for RSP and RBP, and RAX which contains the\n+// execution context pointer. ExecCtx pointer is always the first argument so we don't need to save it.\n+var stackGrowSaveVRegs = []regalloc.VReg{\n+\trdxVReg, r12VReg, r13VReg, r14VReg, r15VReg,\n+\trcxVReg, rbxVReg, rsiVReg, rdiVReg, r8VReg, r9VReg, r10VReg, r11VReg,\n+\txmm8VReg, xmm9VReg, xmm10VReg, xmm11VReg, xmm12VReg, xmm13VReg, xmm14VReg, xmm15VReg,\n+\txmm0VReg, xmm1VReg, xmm2VReg, xmm3VReg, xmm4VReg, xmm5VReg, xmm6VReg, xmm7VReg,\n+}\n+\n+// CompileStackGrowCallSequence implements backend.Machine.\n+func (m *machine) CompileStackGrowCallSequence() []byte {\n+\tectx := m.ectx\n+\n+\tcur := m.allocateNop()\n+\tectx.RootInstr = cur\n+\n+\tcur = m.setupRBPRSP(cur)\n+\n+\t// Execution context is always the first argument.\n+\texecCtrPtr := raxVReg\n+\n+\t// Save the callee saved and argument registers.\n+\tcur = m.saveRegistersInExecutionContext(cur, execCtrPtr, stackGrowSaveVRegs)\n+\n+\t// Load the exitCode to the register.\n+\texitCodeReg := r12VReg // Already saved.\n+\tcur = linkInstr(cur, m.allocateInstr().asImm(exitCodeReg, uint64(wazevoapi.ExitCodeGrowStack), false))\n+\n+\tsaveRsp, saveRbp, setExitCode := m.allocateExitInstructions(execCtrPtr, exitCodeReg)\n+\tcur = linkInstr(cur, setExitCode)\n+\tcur = linkInstr(cur, saveRsp)\n+\tcur = linkInstr(cur, saveRbp)\n+\n+\t// Ready to exit the execution.\n+\tcur = m.storeReturnAddressAndExit(cur, execCtrPtr)\n+\n+\t// After the exit, restore the saved registers.\n+\tcur = m.restoreRegistersInExecutionContext(cur, execCtrPtr, stackGrowSaveVRegs)\n+\n+\t// Finally ready to return.\n+\tcur = m.revertRBPRSP(cur)\n+\tlinkInstr(cur, m.allocateInstr().asRet())\n+\n+\tm.encodeWithoutSSA(ectx.RootInstr)\n+\treturn m.c.Buf()\n+}\n+\n+// insertStackBoundsCheck will insert the instructions after `cur` to check the\n+// stack bounds, and if there's no sufficient spaces required for the function,\n+// exit the execution and try growing it in Go world.\n+func (m *machine) insertStackBoundsCheck(requiredStackSize int64, cur *instruction) *instruction {\n+\t//\t\tadd $requiredStackSize, %rsp ;; Temporarily update the sp.\n+\t// \t\tcmp ExecutionContextOffsetStackBottomPtr(%rax), %rsp ;; Compare the stack bottom and the sp.\n+\t// \t\tja .ok\n+\t//\t\tsub $requiredStackSize, %rsp ;; Reverse the temporary update.\n+\t//      pushq r15 ;; save the temporary.\n+\t//\t\tmov $requiredStackSize, %r15\n+\t//\t\tmov %15, ExecutionContextOffsetStackGrowRequiredSize(%rax) ;; Set the required size in the execution context.\n+\t//      popq r15 ;; restore the temporary.\n+\t//\t\tcallq *ExecutionContextOffsetStackGrowCallTrampolineAddress(%rax) ;; Call the Go function to grow the stack.\n+\t//\t\tjmp .cont\n+\t// .ok:\n+\t//\t\tsub $requiredStackSize, %rsp ;; Reverse the temporary update.\n+\t// .cont:\n+\tcur = m.addRSP(-int32(requiredStackSize), cur)\n+\tcur = linkInstr(cur, m.allocateInstr().asCmpRmiR(true,\n+\t\tnewOperandMem(m.newAmodeImmReg(wazevoapi.ExecutionContextOffsetStackBottomPtr.U32(), raxVReg)),\n+\t\trspVReg, true))\n+\n+\tja := m.allocateInstr()\n+\tcur = linkInstr(cur, ja)\n+\n+\tcur = m.addRSP(int32(requiredStackSize), cur)\n+\n+\t// Save the temporary.\n+\n+\tcur = linkInstr(cur, m.allocateInstr().asPush64(newOperandReg(r15VReg)))\n+\t// Load the required size to the temporary.\n+\tcur = linkInstr(cur, m.allocateInstr().asImm(r15VReg, uint64(requiredStackSize), true))\n+\t// Set the required size in the execution context.\n+\tcur = linkInstr(cur, m.allocateInstr().asMovRM(r15VReg,\n+\t\tnewOperandMem(m.newAmodeImmReg(wazevoapi.ExecutionContextOffsetStackGrowRequiredSize.U32(), raxVReg)), 8))\n+\t// Restore the temporary.\n+\tcur = linkInstr(cur, m.allocateInstr().asPop64(r15VReg))\n+\t// Call the Go function to grow the stack.\n+\tcur = linkInstr(cur, m.allocateInstr().asCallIndirect(newOperandMem(m.newAmodeImmReg(\n+\t\twazevoapi.ExecutionContextOffsetStackGrowCallTrampolineAddress.U32(), raxVReg)), nil))\n+\t// Jump to the continuation.\n+\tjmpToCont := m.allocateInstr()\n+\tcur = linkInstr(cur, jmpToCont)\n+\n+\t// .ok:\n+\tokInstr, ok := m.allocateBrTarget()\n+\tcur = linkInstr(cur, okInstr)\n+\tja.asJmpIf(condNBE, newOperandLabel(ok))\n+\t// On the ok path, we only need to reverse the temporary update.\n+\tcur = m.addRSP(int32(requiredStackSize), cur)\n+\n+\t// .cont:\n+\tcontInstr, cont := m.allocateBrTarget()\n+\tcur = linkInstr(cur, contInstr)\n+\tjmpToCont.asJmp(newOperandLabel(cont))\n+\n+\treturn cur\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/cond.go",
          "status": "added",
          "additions": 168,
          "deletions": 0,
          "patch": "@@ -0,0 +1,168 @@\n+package amd64\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+type cond byte\n+\n+const (\n+\t// condO represents (overflow) condition.\n+\tcondO cond = iota\n+\t// condNO represents (no overflow) condition.\n+\tcondNO\n+\t// condB represents (< unsigned) condition.\n+\tcondB\n+\t// condNB represents (>= unsigned) condition.\n+\tcondNB\n+\t// condZ represents (zero) condition.\n+\tcondZ\n+\t// condNZ represents (not-zero) condition.\n+\tcondNZ\n+\t// condBE represents (<= unsigned) condition.\n+\tcondBE\n+\t// condNBE represents (> unsigned) condition.\n+\tcondNBE\n+\t// condS represents (negative) condition.\n+\tcondS\n+\t// condNS represents (not-negative) condition.\n+\tcondNS\n+\t// condP represents (parity) condition.\n+\tcondP\n+\t// condNP represents (not parity) condition.\n+\tcondNP\n+\t// condL represents (< signed) condition.\n+\tcondL\n+\t// condNL represents (>= signed) condition.\n+\tcondNL\n+\t// condLE represents (<= signed) condition.\n+\tcondLE\n+\t// condNLE represents (> signed) condition.\n+\tcondNLE\n+\n+\tcondInvalid\n+)\n+\n+func (c cond) String() string {\n+\tswitch c {\n+\tcase condO:\n+\t\treturn \"o\"\n+\tcase condNO:\n+\t\treturn \"no\"\n+\tcase condB:\n+\t\treturn \"b\"\n+\tcase condNB:\n+\t\treturn \"nb\"\n+\tcase condZ:\n+\t\treturn \"z\"\n+\tcase condNZ:\n+\t\treturn \"nz\"\n+\tcase condBE:\n+\t\treturn \"be\"\n+\tcase condNBE:\n+\t\treturn \"nbe\"\n+\tcase condS:\n+\t\treturn \"s\"\n+\tcase condNS:\n+\t\treturn \"ns\"\n+\tcase condL:\n+\t\treturn \"l\"\n+\tcase condNL:\n+\t\treturn \"nl\"\n+\tcase condLE:\n+\t\treturn \"le\"\n+\tcase condNLE:\n+\t\treturn \"nle\"\n+\tcase condP:\n+\t\treturn \"p\"\n+\tcase condNP:\n+\t\treturn \"np\"\n+\tdefault:\n+\t\tpanic(\"unreachable\")\n+\t}\n+}\n+\n+func condFromSSAIntCmpCond(origin ssa.IntegerCmpCond) cond {\n+\tswitch origin {\n+\tcase ssa.IntegerCmpCondEqual:\n+\t\treturn condZ\n+\tcase ssa.IntegerCmpCondNotEqual:\n+\t\treturn condNZ\n+\tcase ssa.IntegerCmpCondSignedLessThan:\n+\t\treturn condL\n+\tcase ssa.IntegerCmpCondSignedGreaterThanOrEqual:\n+\t\treturn condNL\n+\tcase ssa.IntegerCmpCondSignedGreaterThan:\n+\t\treturn condNLE\n+\tcase ssa.IntegerCmpCondSignedLessThanOrEqual:\n+\t\treturn condLE\n+\tcase ssa.IntegerCmpCondUnsignedLessThan:\n+\t\treturn condB\n+\tcase ssa.IntegerCmpCondUnsignedGreaterThanOrEqual:\n+\t\treturn condNB\n+\tcase ssa.IntegerCmpCondUnsignedGreaterThan:\n+\t\treturn condNBE\n+\tcase ssa.IntegerCmpCondUnsignedLessThanOrEqual:\n+\t\treturn condBE\n+\tdefault:\n+\t\tpanic(\"unreachable\")\n+\t}\n+}\n+\n+func condFromSSAFloatCmpCond(origin ssa.FloatCmpCond) cond {\n+\tswitch origin {\n+\tcase ssa.FloatCmpCondGreaterThanOrEqual:\n+\t\treturn condNB\n+\tcase ssa.FloatCmpCondGreaterThan:\n+\t\treturn condNBE\n+\tcase ssa.FloatCmpCondEqual, ssa.FloatCmpCondNotEqual, ssa.FloatCmpCondLessThan, ssa.FloatCmpCondLessThanOrEqual:\n+\t\tpanic(fmt.Sprintf(\"cond %s must be treated as a special case\", origin))\n+\tdefault:\n+\t\tpanic(\"unreachable\")\n+\t}\n+}\n+\n+func (c cond) encoding() byte {\n+\treturn byte(c)\n+}\n+\n+func (c cond) invert() cond {\n+\tswitch c {\n+\tcase condO:\n+\t\treturn condNO\n+\tcase condNO:\n+\t\treturn condO\n+\tcase condB:\n+\t\treturn condNB\n+\tcase condNB:\n+\t\treturn condB\n+\tcase condZ:\n+\t\treturn condNZ\n+\tcase condNZ:\n+\t\treturn condZ\n+\tcase condBE:\n+\t\treturn condNBE\n+\tcase condNBE:\n+\t\treturn condBE\n+\tcase condS:\n+\t\treturn condNS\n+\tcase condNS:\n+\t\treturn condS\n+\tcase condP:\n+\t\treturn condNP\n+\tcase condNP:\n+\t\treturn condP\n+\tcase condL:\n+\t\treturn condNL\n+\tcase condNL:\n+\t\treturn condL\n+\tcase condLE:\n+\t\treturn condNLE\n+\tcase condNLE:\n+\t\treturn condLE\n+\tdefault:\n+\t\tpanic(\"unreachable\")\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/ext.go",
          "status": "added",
          "additions": 35,
          "deletions": 0,
          "patch": "@@ -0,0 +1,35 @@\n+package amd64\n+\n+// extMode represents the mode of extension in movzx/movsx.\n+type extMode byte\n+\n+const (\n+\t// extModeBL represents Byte -> Longword.\n+\textModeBL extMode = iota\n+\t// extModeBQ represents Byte -> Quadword.\n+\textModeBQ\n+\t// extModeWL represents Word -> Longword.\n+\textModeWL\n+\t// extModeWQ represents Word -> Quadword.\n+\textModeWQ\n+\t// extModeLQ represents Longword -> Quadword.\n+\textModeLQ\n+)\n+\n+// String implements fmt.Stringer.\n+func (e extMode) String() string {\n+\tswitch e {\n+\tcase extModeBL:\n+\t\treturn \"bl\"\n+\tcase extModeBQ:\n+\t\treturn \"bq\"\n+\tcase extModeWL:\n+\t\treturn \"wl\"\n+\tcase extModeWQ:\n+\t\treturn \"wq\"\n+\tcase extModeLQ:\n+\t\treturn \"lq\"\n+\tdefault:\n+\t\tpanic(\"BUG: invalid ext mode\")\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/instr.go",
          "status": "added",
          "additions": 2472,
          "deletions": 0,
          "patch": "@@ -0,0 +1,2472 @@\n+package amd64\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+type instruction struct {\n+\tprev, next          *instruction\n+\top1, op2            operand\n+\tu1, u2              uint64\n+\tb1                  bool\n+\taddedBeforeRegAlloc bool\n+\tkind                instructionKind\n+}\n+\n+// Next implements regalloc.Instr.\n+func (i *instruction) Next() regalloc.Instr {\n+\treturn i.next\n+}\n+\n+// Prev implements regalloc.Instr.\n+func (i *instruction) Prev() regalloc.Instr {\n+\treturn i.prev\n+}\n+\n+// IsCall implements regalloc.Instr.\n+func (i *instruction) IsCall() bool { return i.kind == call }\n+\n+// IsIndirectCall implements regalloc.Instr.\n+func (i *instruction) IsIndirectCall() bool { return i.kind == callIndirect }\n+\n+// IsReturn implements regalloc.Instr.\n+func (i *instruction) IsReturn() bool { return i.kind == ret }\n+\n+// AddedBeforeRegAlloc implements regalloc.Instr.\n+func (i *instruction) AddedBeforeRegAlloc() bool { return i.addedBeforeRegAlloc }\n+\n+// String implements regalloc.Instr.\n+func (i *instruction) String() string {\n+\tswitch i.kind {\n+\tcase nop0:\n+\t\treturn \"nop\"\n+\tcase sourceOffsetInfo:\n+\t\treturn fmt.Sprintf(\"source_offset_info %d\", i.u1)\n+\tcase ret:\n+\t\treturn \"ret\"\n+\tcase imm:\n+\t\tif i.b1 {\n+\t\t\treturn fmt.Sprintf(\"movabsq $%d, %s\", int64(i.u1), i.op2.format(true))\n+\t\t} else {\n+\t\t\treturn fmt.Sprintf(\"movl $%d, %s\", int32(i.u1), i.op2.format(false))\n+\t\t}\n+\tcase aluRmiR:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", aluRmiROpcode(i.u1), i.op1.format(i.b1), i.op2.format(i.b1))\n+\tcase movRR:\n+\t\tif i.b1 {\n+\t\t\treturn fmt.Sprintf(\"movq %s, %s\", i.op1.format(true), i.op2.format(true))\n+\t\t} else {\n+\t\t\treturn fmt.Sprintf(\"movl %s, %s\", i.op1.format(false), i.op2.format(false))\n+\t\t}\n+\tcase xmmRmR:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", sseOpcode(i.u1), i.op1.format(false), i.op2.format(false))\n+\tcase gprToXmm:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", sseOpcode(i.u1), i.op1.format(i.b1), i.op2.format(i.b1))\n+\tcase xmmUnaryRmR:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", sseOpcode(i.u1), i.op1.format(false), i.op2.format(false))\n+\tcase xmmUnaryRmRImm:\n+\t\treturn fmt.Sprintf(\"%s $%d, %s, %s\", sseOpcode(i.u1), roundingMode(i.u2), i.op1.format(false), i.op2.format(false))\n+\tcase unaryRmR:\n+\t\tvar suffix string\n+\t\tif i.b1 {\n+\t\t\tsuffix = \"q\"\n+\t\t} else {\n+\t\t\tsuffix = \"l\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%s%s %s, %s\", unaryRmROpcode(i.u1), suffix, i.op1.format(i.b1), i.op2.format(i.b1))\n+\tcase not:\n+\t\tvar op string\n+\t\tif i.b1 {\n+\t\t\top = \"notq\"\n+\t\t} else {\n+\t\t\top = \"notl\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%s %s\", op, i.op1.format(i.b1))\n+\tcase neg:\n+\t\tvar op string\n+\t\tif i.b1 {\n+\t\t\top = \"negq\"\n+\t\t} else {\n+\t\t\top = \"negl\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%s %s\", op, i.op1.format(i.b1))\n+\tcase div:\n+\t\tvar prefix string\n+\t\tvar op string\n+\t\tif i.b1 {\n+\t\t\top = \"divq\"\n+\t\t} else {\n+\t\t\top = \"divl\"\n+\t\t}\n+\t\tif i.u1 != 0 {\n+\t\t\tprefix = \"i\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%s%s %s\", prefix, op, i.op1.format(i.b1))\n+\tcase mulHi:\n+\t\tsigned, _64 := i.u1 != 0, i.b1\n+\t\tvar op string\n+\t\tswitch {\n+\t\tcase signed && _64:\n+\t\t\top = \"imulq\"\n+\t\tcase !signed && _64:\n+\t\t\top = \"mulq\"\n+\t\tcase signed && !_64:\n+\t\t\top = \"imull\"\n+\t\tcase !signed && !_64:\n+\t\t\top = \"mull\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%s %s\", op, i.op1.format(i.b1))\n+\tcase signExtendData:\n+\t\tvar op string\n+\t\tif i.b1 {\n+\t\t\top = \"cqo\"\n+\t\t} else {\n+\t\t\top = \"cdq\"\n+\t\t}\n+\t\treturn op\n+\tcase movzxRmR:\n+\t\treturn fmt.Sprintf(\"movzx.%s %s, %s\", extMode(i.u1), i.op1.format(true), i.op2.format(true))\n+\tcase mov64MR:\n+\t\treturn fmt.Sprintf(\"movq %s, %s\", i.op1.format(true), i.op2.format(true))\n+\tcase lea:\n+\t\treturn fmt.Sprintf(\"lea %s, %s\", i.op1.format(true), i.op2.format(true))\n+\tcase movsxRmR:\n+\t\treturn fmt.Sprintf(\"movsx.%s %s, %s\", extMode(i.u1), i.op1.format(true), i.op2.format(true))\n+\tcase movRM:\n+\t\tvar suffix string\n+\t\tswitch i.u1 {\n+\t\tcase 1:\n+\t\t\tsuffix = \"b\"\n+\t\tcase 2:\n+\t\t\tsuffix = \"w\"\n+\t\tcase 4:\n+\t\t\tsuffix = \"l\"\n+\t\tcase 8:\n+\t\t\tsuffix = \"q\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"mov.%s %s, %s\", suffix, i.op1.format(true), i.op2.format(true))\n+\tcase shiftR:\n+\t\tvar suffix string\n+\t\tif i.b1 {\n+\t\t\tsuffix = \"q\"\n+\t\t} else {\n+\t\t\tsuffix = \"l\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%s%s %s, %s\", shiftROp(i.u1), suffix, i.op1.format(false), i.op2.format(i.b1))\n+\tcase xmmRmiReg:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", sseOpcode(i.u1), i.op1.format(true), i.op2.format(true))\n+\tcase cmpRmiR:\n+\t\tvar op, suffix string\n+\t\tif i.u1 != 0 {\n+\t\t\top = \"cmp\"\n+\t\t} else {\n+\t\t\top = \"test\"\n+\t\t}\n+\t\tif i.b1 {\n+\t\t\tsuffix = \"q\"\n+\t\t} else {\n+\t\t\tsuffix = \"l\"\n+\t\t}\n+\t\tif op == \"test\" && i.op1.kind == operandKindMem {\n+\t\t\t// Print consistently with AT&T syntax.\n+\t\t\treturn fmt.Sprintf(\"%s%s %s, %s\", op, suffix, i.op2.format(i.b1), i.op1.format(i.b1))\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%s%s %s, %s\", op, suffix, i.op1.format(i.b1), i.op2.format(i.b1))\n+\tcase setcc:\n+\t\treturn fmt.Sprintf(\"set%s %s\", cond(i.u1), i.op2.format(true))\n+\tcase cmove:\n+\t\tvar suffix string\n+\t\tif i.b1 {\n+\t\t\tsuffix = \"q\"\n+\t\t} else {\n+\t\t\tsuffix = \"l\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"cmov%s%s %s, %s\", cond(i.u1), suffix, i.op1.format(i.b1), i.op2.format(i.b1))\n+\tcase push64:\n+\t\treturn fmt.Sprintf(\"pushq %s\", i.op1.format(true))\n+\tcase pop64:\n+\t\treturn fmt.Sprintf(\"popq %s\", i.op1.format(true))\n+\tcase xmmMovRM:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", sseOpcode(i.u1), i.op1.format(true), i.op2.format(true))\n+\tcase xmmLoadConst:\n+\t\tpanic(\"TODO\")\n+\tcase xmmToGpr:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", sseOpcode(i.u1), i.op1.format(i.b1), i.op2.format(i.b1))\n+\tcase cvtUint64ToFloatSeq:\n+\t\tpanic(\"TODO\")\n+\tcase cvtFloatToSintSeq:\n+\t\tpanic(\"TODO\")\n+\tcase cvtFloatToUintSeq:\n+\t\tpanic(\"TODO\")\n+\tcase xmmMinMaxSeq:\n+\t\tpanic(\"TODO\")\n+\tcase xmmCmpRmR:\n+\t\treturn fmt.Sprintf(\"%s %s, %s\", sseOpcode(i.u1), i.op1.format(false), i.op2.format(false))\n+\tcase xmmRmRImm:\n+\t\top := sseOpcode(i.u1)\n+\t\tr1, r2 := i.op1.format(op == sseOpcodePextrq || op == sseOpcodePinsrq),\n+\t\t\ti.op2.format(op == sseOpcodePextrq || op == sseOpcodePinsrq)\n+\t\treturn fmt.Sprintf(\"%s $%d, %s, %s\", op, i.u2, r1, r2)\n+\tcase jmp:\n+\t\treturn fmt.Sprintf(\"jmp %s\", i.op1.format(true))\n+\tcase jmpIf:\n+\t\treturn fmt.Sprintf(\"j%s %s\", cond(i.u1), i.op1.format(true))\n+\tcase jmpTableIsland:\n+\t\treturn fmt.Sprintf(\"jump_table_island: jmp_table_index=%d\", i.u1)\n+\tcase exitSequence:\n+\t\treturn fmt.Sprintf(\"exit_sequence %s\", i.op1.format(true))\n+\tcase ud2:\n+\t\treturn \"ud2\"\n+\tcase call:\n+\t\treturn fmt.Sprintf(\"call %s\", ssa.FuncRef(i.u1))\n+\tcase callIndirect:\n+\t\treturn fmt.Sprintf(\"callq *%s\", i.op1.format(true))\n+\tcase xchg:\n+\t\tvar suffix string\n+\t\tswitch i.u1 {\n+\t\tcase 1:\n+\t\t\tsuffix = \"b\"\n+\t\tcase 2:\n+\t\t\tsuffix = \"w\"\n+\t\tcase 4:\n+\t\t\tsuffix = \"l\"\n+\t\tcase 8:\n+\t\t\tsuffix = \"q\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"xchg.%s %s, %s\", suffix, i.op1.format(true), i.op2.format(true))\n+\tcase zeros:\n+\t\treturn fmt.Sprintf(\"xor %s, %s\", i.op2.format(true), i.op2.format(true))\n+\tcase fcvtToSintSequence:\n+\t\texecCtx, src, tmpGp, tmpGp2, tmpXmm, src64, dst64, sat := i.fcvtToSintSequenceData()\n+\t\treturn fmt.Sprintf(\n+\t\t\t\"fcvtToSintSequence execCtx=%s, src=%s, tmpGp=%s, tmpGp2=%s, tmpXmm=%s, src64=%v, dst64=%v, sat=%v\",\n+\t\t\tformatVRegSized(execCtx, true),\n+\t\t\tformatVRegSized(src, true),\n+\t\t\tformatVRegSized(tmpGp, true),\n+\t\t\tformatVRegSized(tmpGp2, true),\n+\t\t\tformatVRegSized(tmpXmm, true), src64, dst64, sat)\n+\tcase fcvtToUintSequence:\n+\t\texecCtx, src, tmpGp, tmpGp2, tmpXmm, tmpXmm2, src64, dst64, sat := i.fcvtToUintSequenceData()\n+\t\treturn fmt.Sprintf(\n+\t\t\t\"fcvtToUintSequence execCtx=%s, src=%s, tmpGp=%s, tmpGp2=%s, tmpXmm=%s, tmpXmm2=%s, src64=%v, dst64=%v, sat=%v\",\n+\t\t\tformatVRegSized(execCtx, true),\n+\t\t\tformatVRegSized(src, true),\n+\t\t\tformatVRegSized(tmpGp, true),\n+\t\t\tformatVRegSized(tmpGp2, true),\n+\t\t\tformatVRegSized(tmpXmm, true),\n+\t\t\tformatVRegSized(tmpXmm2, true), src64, dst64, sat)\n+\tcase idivRemSequence:\n+\t\texecCtx, divisor, tmpGp, isDiv, signed, _64 := i.idivRemSequenceData()\n+\t\treturn fmt.Sprintf(\"idivRemSequence execCtx=%s, divisor=%s, tmpGp=%s, isDiv=%v, signed=%v, _64=%v\",\n+\t\t\tformatVRegSized(execCtx, true), formatVRegSized(divisor, _64), formatVRegSized(tmpGp, _64), isDiv, signed, _64)\n+\tcase defineUninitializedReg:\n+\t\treturn fmt.Sprintf(\"defineUninitializedReg %s\", i.op2.format(true))\n+\tcase xmmCMov:\n+\t\treturn fmt.Sprintf(\"xmmcmov%s %s, %s\", cond(i.u1), i.op1.format(true), i.op2.format(true))\n+\tcase blendvpd:\n+\t\treturn fmt.Sprintf(\"blendvpd %s, %s, %%xmm0\", i.op1.format(false), i.op2.format(false))\n+\tcase mfence:\n+\t\treturn \"mfence\"\n+\tcase lockcmpxchg:\n+\t\tvar suffix string\n+\t\tswitch i.u1 {\n+\t\tcase 1:\n+\t\t\tsuffix = \"b\"\n+\t\tcase 2:\n+\t\t\tsuffix = \"w\"\n+\t\tcase 4:\n+\t\t\tsuffix = \"l\"\n+\t\tcase 8:\n+\t\t\tsuffix = \"q\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"lock cmpxchg.%s %s, %s\", suffix, i.op1.format(true), i.op2.format(true))\n+\tcase lockxadd:\n+\t\tvar suffix string\n+\t\tswitch i.u1 {\n+\t\tcase 1:\n+\t\t\tsuffix = \"b\"\n+\t\tcase 2:\n+\t\t\tsuffix = \"w\"\n+\t\tcase 4:\n+\t\t\tsuffix = \"l\"\n+\t\tcase 8:\n+\t\t\tsuffix = \"q\"\n+\t\t}\n+\t\treturn fmt.Sprintf(\"lock xadd.%s %s, %s\", suffix, i.op1.format(true), i.op2.format(true))\n+\n+\tcase keepAlive:\n+\t\treturn fmt.Sprintf(\"keepAlive %s\", i.op1.format(true))\n+\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"BUG: %d\", int(i.kind)))\n+\t}\n+}\n+\n+// Defs implements regalloc.Instr.\n+func (i *instruction) Defs(regs *[]regalloc.VReg) []regalloc.VReg {\n+\t*regs = (*regs)[:0]\n+\tswitch dk := defKinds[i.kind]; dk {\n+\tcase defKindNone:\n+\tcase defKindOp2:\n+\t\t*regs = append(*regs, i.op2.reg())\n+\tcase defKindCall:\n+\t\t_, _, retIntRealRegs, retFloatRealRegs, _ := backend.ABIInfoFromUint64(i.u2)\n+\t\tfor i := byte(0); i < retIntRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[intArgResultRegs[i]])\n+\t\t}\n+\t\tfor i := byte(0); i < retFloatRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[floatArgResultRegs[i]])\n+\t\t}\n+\tcase defKindDivRem:\n+\t\t_, _, _, isDiv, _, _ := i.idivRemSequenceData()\n+\t\tif isDiv {\n+\t\t\t*regs = append(*regs, raxVReg)\n+\t\t} else {\n+\t\t\t*regs = append(*regs, rdxVReg)\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"BUG: invalid defKind \\\"%s\\\" for %s\", dk, i))\n+\t}\n+\treturn *regs\n+}\n+\n+// Uses implements regalloc.Instr.\n+func (i *instruction) Uses(regs *[]regalloc.VReg) []regalloc.VReg {\n+\t*regs = (*regs)[:0]\n+\tswitch uk := useKinds[i.kind]; uk {\n+\tcase useKindNone:\n+\tcase useKindOp1Op2Reg, useKindOp1RegOp2:\n+\t\topAny, opReg := &i.op1, &i.op2\n+\t\tif uk == useKindOp1RegOp2 {\n+\t\t\topAny, opReg = opReg, opAny\n+\t\t}\n+\t\t// The destination operand (op2) can be only reg,\n+\t\t// the source operand (op1) can be imm32, reg or mem.\n+\t\tswitch opAny.kind {\n+\t\tcase operandKindReg:\n+\t\t\t*regs = append(*regs, opAny.reg())\n+\t\tcase operandKindMem:\n+\t\t\topAny.addressMode().uses(regs)\n+\t\tcase operandKindImm32:\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\t\tif opReg.kind != operandKindReg {\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\t\t*regs = append(*regs, opReg.reg())\n+\tcase useKindOp1:\n+\t\top := i.op1\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\t*regs = append(*regs, op.reg())\n+\t\tcase operandKindMem:\n+\t\t\top.addressMode().uses(regs)\n+\t\tcase operandKindImm32, operandKindLabel:\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\tcase useKindCallInd:\n+\t\top := i.op1\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\t*regs = append(*regs, op.reg())\n+\t\tcase operandKindMem:\n+\t\t\top.addressMode().uses(regs)\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\t\tfallthrough\n+\tcase useKindCall:\n+\t\targIntRealRegs, argFloatRealRegs, _, _, _ := backend.ABIInfoFromUint64(i.u2)\n+\t\tfor i := byte(0); i < argIntRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[intArgResultRegs[i]])\n+\t\t}\n+\t\tfor i := byte(0); i < argFloatRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[floatArgResultRegs[i]])\n+\t\t}\n+\tcase useKindFcvtToSintSequence:\n+\t\texecCtx, src, tmpGp, tmpGp2, tmpXmm, _, _, _ := i.fcvtToSintSequenceData()\n+\t\t*regs = append(*regs, execCtx, src, tmpGp, tmpGp2, tmpXmm)\n+\tcase useKindFcvtToUintSequence:\n+\t\texecCtx, src, tmpGp, tmpGp2, tmpXmm, tmpXmm2, _, _, _ := i.fcvtToUintSequenceData()\n+\t\t*regs = append(*regs, execCtx, src, tmpGp, tmpGp2, tmpXmm, tmpXmm2)\n+\tcase useKindDivRem:\n+\t\texecCtx, divisor, tmpGp, _, _, _ := i.idivRemSequenceData()\n+\t\t// idiv uses rax and rdx as implicit operands.\n+\t\t*regs = append(*regs, raxVReg, rdxVReg, execCtx, divisor, tmpGp)\n+\tcase useKindBlendvpd:\n+\t\t*regs = append(*regs, xmm0VReg)\n+\n+\t\topAny, opReg := &i.op1, &i.op2\n+\t\tswitch opAny.kind {\n+\t\tcase operandKindReg:\n+\t\t\t*regs = append(*regs, opAny.reg())\n+\t\tcase operandKindMem:\n+\t\t\topAny.addressMode().uses(regs)\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\t\tif opReg.kind != operandKindReg {\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\t\t*regs = append(*regs, opReg.reg())\n+\n+\tcase useKindRaxOp1RegOp2:\n+\t\topReg, opAny := &i.op1, &i.op2\n+\t\t*regs = append(*regs, raxVReg, opReg.reg())\n+\t\tswitch opAny.kind {\n+\t\tcase operandKindReg:\n+\t\t\t*regs = append(*regs, opAny.reg())\n+\t\tcase operandKindMem:\n+\t\t\topAny.addressMode().uses(regs)\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\t\tif opReg.kind != operandKindReg {\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"BUG: invalid useKind %s for %s\", uk, i))\n+\t}\n+\treturn *regs\n+}\n+\n+// AssignUse implements regalloc.Instr.\n+func (i *instruction) AssignUse(index int, v regalloc.VReg) {\n+\tswitch uk := useKinds[i.kind]; uk {\n+\tcase useKindNone:\n+\tcase useKindCallInd:\n+\t\tif index != 0 {\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\top := &i.op1\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\top.setReg(v)\n+\t\tcase operandKindMem:\n+\t\t\top.addressMode().assignUses(index, v)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\tcase useKindOp1Op2Reg, useKindOp1RegOp2:\n+\t\top, opMustBeReg := &i.op1, &i.op2\n+\t\tif uk == useKindOp1RegOp2 {\n+\t\t\top, opMustBeReg = opMustBeReg, op\n+\t\t}\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\tif index == 0 {\n+\t\t\t\top.setReg(v)\n+\t\t\t} else if index == 1 {\n+\t\t\t\topMustBeReg.setReg(v)\n+\t\t\t} else {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\tcase operandKindMem:\n+\t\t\tnregs := op.addressMode().nregs()\n+\t\t\tif index < nregs {\n+\t\t\t\top.addressMode().assignUses(index, v)\n+\t\t\t} else if index == nregs {\n+\t\t\t\topMustBeReg.setReg(v)\n+\t\t\t} else {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\tcase operandKindImm32:\n+\t\t\tif index == 0 {\n+\t\t\t\topMustBeReg.setReg(v)\n+\t\t\t} else {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand pair: %s\", i))\n+\t\t}\n+\tcase useKindOp1:\n+\t\top := &i.op1\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\tif index != 0 {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t\top.setReg(v)\n+\t\tcase operandKindMem:\n+\t\t\top.addressMode().assignUses(index, v)\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", i))\n+\t\t}\n+\tcase useKindFcvtToSintSequence:\n+\t\tswitch index {\n+\t\tcase 0:\n+\t\t\ti.op1.addressMode().base = v\n+\t\tcase 1:\n+\t\t\ti.op1.addressMode().index = v\n+\t\tcase 2:\n+\t\t\ti.op2.addressMode().base = v\n+\t\tcase 3:\n+\t\t\ti.op2.addressMode().index = v\n+\t\tcase 4:\n+\t\t\ti.u1 = uint64(v)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\tcase useKindFcvtToUintSequence:\n+\t\tswitch index {\n+\t\tcase 0:\n+\t\t\ti.op1.addressMode().base = v\n+\t\tcase 1:\n+\t\t\ti.op1.addressMode().index = v\n+\t\tcase 2:\n+\t\t\ti.op2.addressMode().base = v\n+\t\tcase 3:\n+\t\t\ti.op2.addressMode().index = v\n+\t\tcase 4:\n+\t\t\ti.u1 = uint64(v)\n+\t\tcase 5:\n+\t\t\ti.u2 = uint64(v)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\tcase useKindDivRem:\n+\t\tswitch index {\n+\t\tcase 0:\n+\t\t\tif v != raxVReg {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\tcase 1:\n+\t\t\tif v != rdxVReg {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\tcase 2:\n+\t\t\ti.op1.setReg(v)\n+\t\tcase 3:\n+\t\t\ti.op2.setReg(v)\n+\t\tcase 4:\n+\t\t\ti.u1 = uint64(v)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\tcase useKindBlendvpd:\n+\t\top, opMustBeReg := &i.op1, &i.op2\n+\t\tif index == 0 {\n+\t\t\tif v.RealReg() != xmm0 {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t} else {\n+\t\t\tswitch op.kind {\n+\t\t\tcase operandKindReg:\n+\t\t\t\tswitch index {\n+\t\t\t\tcase 1:\n+\t\t\t\t\top.setReg(v)\n+\t\t\t\tcase 2:\n+\t\t\t\t\topMustBeReg.setReg(v)\n+\t\t\t\tdefault:\n+\t\t\t\t\tpanic(\"BUG\")\n+\t\t\t\t}\n+\t\t\tcase operandKindMem:\n+\t\t\t\tnregs := op.addressMode().nregs()\n+\t\t\t\tindex--\n+\t\t\t\tif index < nregs {\n+\t\t\t\t\top.addressMode().assignUses(index, v)\n+\t\t\t\t} else if index == nregs {\n+\t\t\t\t\topMustBeReg.setReg(v)\n+\t\t\t\t} else {\n+\t\t\t\t\tpanic(\"BUG\")\n+\t\t\t\t}\n+\t\t\tdefault:\n+\t\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand pair: %s\", i))\n+\t\t\t}\n+\t\t}\n+\n+\tcase useKindRaxOp1RegOp2:\n+\t\tswitch index {\n+\t\tcase 0:\n+\t\t\tif v.RealReg() != rax {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\tcase 1:\n+\t\t\ti.op1.setReg(v)\n+\t\tdefault:\n+\t\t\top := &i.op2\n+\t\t\tswitch op.kind {\n+\t\t\tcase operandKindReg:\n+\t\t\t\tswitch index {\n+\t\t\t\tcase 1:\n+\t\t\t\t\top.setReg(v)\n+\t\t\t\tcase 2:\n+\t\t\t\t\top.setReg(v)\n+\t\t\t\tdefault:\n+\t\t\t\t\tpanic(\"BUG\")\n+\t\t\t\t}\n+\t\t\tcase operandKindMem:\n+\t\t\t\tnregs := op.addressMode().nregs()\n+\t\t\t\tindex -= 2\n+\t\t\t\tif index < nregs {\n+\t\t\t\t\top.addressMode().assignUses(index, v)\n+\t\t\t\t} else if index == nregs {\n+\t\t\t\t\top.setReg(v)\n+\t\t\t\t} else {\n+\t\t\t\t\tpanic(\"BUG\")\n+\t\t\t\t}\n+\t\t\tdefault:\n+\t\t\t\tpanic(fmt.Sprintf(\"BUG: invalid operand pair: %s\", i))\n+\t\t\t}\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"BUG: invalid useKind %s for %s\", uk, i))\n+\t}\n+}\n+\n+// AssignDef implements regalloc.Instr.\n+func (i *instruction) AssignDef(reg regalloc.VReg) {\n+\tswitch dk := defKinds[i.kind]; dk {\n+\tcase defKindNone:\n+\tcase defKindOp2:\n+\t\ti.op2.setReg(reg)\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"BUG: invalid defKind \\\"%s\\\" for %s\", dk, i))\n+\t}\n+}\n+\n+// IsCopy implements regalloc.Instr.\n+func (i *instruction) IsCopy() bool {\n+\tk := i.kind\n+\tif k == movRR {\n+\t\treturn true\n+\t}\n+\tif k == xmmUnaryRmR {\n+\t\tif i.op1.kind == operandKindReg {\n+\t\t\tsse := sseOpcode(i.u1)\n+\t\t\treturn sse == sseOpcodeMovss || sse == sseOpcodeMovsd || sse == sseOpcodeMovdqu\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+func resetInstruction(i *instruction) {\n+\t*i = instruction{}\n+}\n+\n+func setNext(i *instruction, next *instruction) {\n+\ti.next = next\n+}\n+\n+func setPrev(i *instruction, prev *instruction) {\n+\ti.prev = prev\n+}\n+\n+func asNop(i *instruction) {\n+\ti.kind = nop0\n+}\n+\n+func (i *instruction) asNop0WithLabel(label backend.Label) *instruction { //nolint\n+\ti.kind = nop0\n+\ti.u1 = uint64(label)\n+\treturn i\n+}\n+\n+func (i *instruction) nop0Label() backend.Label {\n+\treturn backend.Label(i.u1)\n+}\n+\n+type instructionKind byte\n+\n+const (\n+\tnop0 instructionKind = iota + 1\n+\n+\t// Integer arithmetic/bit-twiddling: (add sub and or xor mul, etc.) (32 64) (reg addr imm) reg\n+\taluRmiR\n+\n+\t// Instructions on GPR that only read src and defines dst (dst is not modified): bsr, etc.\n+\tunaryRmR\n+\n+\t// Bitwise not\n+\tnot\n+\n+\t// Integer negation\n+\tneg\n+\n+\t// Integer quotient and remainder: (div idiv) $rax $rdx (reg addr)\n+\tdiv\n+\n+\t// The high bits (RDX) of a (un)signed multiply: RDX:RAX := RAX * rhs.\n+\tmulHi\n+\n+\t// Do a sign-extend based on the sign of the value in rax into rdx: (cwd cdq cqo)\n+\t// or al into ah: (cbw)\n+\tsignExtendData\n+\n+\t// Constant materialization: (imm32 imm64) reg.\n+\t// Either: movl $imm32, %reg32 or movabsq $imm64, %reg64.\n+\timm\n+\n+\t// GPR to GPR move: mov (64 32) reg reg.\n+\tmovRR\n+\n+\t// movzxRmR is zero-extended loads or move (R to R), except for 64 bits: movz (bl bq wl wq lq) addr reg.\n+\t// Note that the lq variant doesn't really exist since the default zero-extend rule makes it\n+\t// unnecessary. For that case we emit the equivalent \"movl AM, reg32\".\n+\tmovzxRmR\n+\n+\t// mov64MR is a plain 64-bit integer load, since movzxRmR can't represent that.\n+\tmov64MR\n+\n+\t// Loads the memory address of addr into dst.\n+\tlea\n+\n+\t// Sign-extended loads and moves: movs (bl bq wl wq lq) addr reg.\n+\tmovsxRmR\n+\n+\t// Integer stores: mov (b w l q) reg addr.\n+\tmovRM\n+\n+\t// Arithmetic shifts: (shl shr sar) (b w l q) imm reg.\n+\tshiftR\n+\n+\t// Arithmetic SIMD shifts.\n+\txmmRmiReg\n+\n+\t// Integer comparisons/tests: cmp or test (b w l q) (reg addr imm) reg.\n+\tcmpRmiR\n+\n+\t// Materializes the requested condition code in the destination reg.\n+\tsetcc\n+\n+\t// Integer conditional move.\n+\t// Overwrites the destination register.\n+\tcmove\n+\n+\t// pushq (reg addr imm)\n+\tpush64\n+\n+\t// popq reg\n+\tpop64\n+\n+\t// XMM (scalar or vector) binary op: (add sub and or xor mul adc? sbb?) (32 64) (reg addr) reg\n+\txmmRmR\n+\n+\t// XMM (scalar or vector) unary op: mov between XMM registers (32 64) (reg addr) reg.\n+\t//\n+\t// This differs from xmmRmR in that the dst register of xmmUnaryRmR is not used in the\n+\t// computation of the instruction dst value and so does not have to be a previously valid\n+\t// value. This is characteristic of mov instructions.\n+\txmmUnaryRmR\n+\n+\t// XMM (scalar or vector) unary op with immediate: roundss, roundsd, etc.\n+\t//\n+\t// This differs from XMM_RM_R_IMM in that the dst register of\n+\t// XmmUnaryRmRImm is not used in the computation of the instruction dst\n+\t// value and so does not have to be a previously valid value.\n+\txmmUnaryRmRImm\n+\n+\t// XMM (scalar or vector) unary op (from xmm to mem): stores, movd, movq\n+\txmmMovRM\n+\n+\t// XMM (vector) unary op (to move a constant value into an xmm register): movups\n+\txmmLoadConst\n+\n+\t// XMM (scalar) unary op (from xmm to integer reg): movd, movq, cvtts{s,d}2si\n+\txmmToGpr\n+\n+\t// XMM (scalar) unary op (from integer to float reg): movd, movq, cvtsi2s{s,d}\n+\tgprToXmm\n+\n+\t// Converts an unsigned int64 to a float32/float64.\n+\tcvtUint64ToFloatSeq\n+\n+\t// Converts a scalar xmm to a signed int32/int64.\n+\tcvtFloatToSintSeq\n+\n+\t// Converts a scalar xmm to an unsigned int32/int64.\n+\tcvtFloatToUintSeq\n+\n+\t// A sequence to compute min/max with the proper NaN semantics for xmm registers.\n+\txmmMinMaxSeq\n+\n+\t// Float comparisons/tests: cmp (b w l q) (reg addr imm) reg.\n+\txmmCmpRmR\n+\n+\t// A binary XMM instruction with an 8-bit immediate: e.g. cmp (ps pd) imm (reg addr) reg\n+\txmmRmRImm\n+\n+\t// Direct call: call simm32.\n+\t// Note that the offset is the relative to the *current RIP*, which points to the first byte of the next instruction.\n+\tcall\n+\n+\t// Indirect call: callq (reg mem).\n+\tcallIndirect\n+\n+\t// Return.\n+\tret\n+\n+\t// Jump: jmp (reg, mem, imm32 or label)\n+\tjmp\n+\n+\t// Jump conditionally: jcond cond label.\n+\tjmpIf\n+\n+\t// jmpTableIsland is to emit the jump table.\n+\tjmpTableIsland\n+\n+\t// exitSequence exits the execution and go back to the Go world.\n+\texitSequence\n+\n+\t// An instruction that will always trigger the illegal instruction exception.\n+\tud2\n+\n+\t// xchg is described in https://www.felixcloutier.com/x86/xchg.\n+\t// This instruction uses two operands, where one of them can be a memory address, and swaps their values.\n+\t// If the dst is a memory address, the execution is atomic.\n+\txchg\n+\n+\t// lockcmpxchg is the cmpxchg instruction https://www.felixcloutier.com/x86/cmpxchg with a lock prefix.\n+\tlockcmpxchg\n+\n+\t// zeros puts zeros into the destination register. This is implemented as xor reg, reg for\n+\t// either integer or XMM registers. The reason why we have this instruction instead of using aluRmiR\n+\t// is that it requires the already-defined registers. From reg alloc's perspective, this defines\n+\t// the destination register and takes no inputs.\n+\tzeros\n+\n+\t// sourceOffsetInfo is a dummy instruction to emit source offset info.\n+\t// The existence of this instruction does not affect the execution.\n+\tsourceOffsetInfo\n+\n+\t// defineUninitializedReg is a no-op instruction that defines a register without a defining instruction.\n+\tdefineUninitializedReg\n+\n+\t// fcvtToSintSequence is a sequence of instructions to convert a float to a signed integer.\n+\tfcvtToSintSequence\n+\n+\t// fcvtToUintSequence is a sequence of instructions to convert a float to an unsigned integer.\n+\tfcvtToUintSequence\n+\n+\t// xmmCMov is a conditional move instruction for XMM registers. Lowered after register allocation.\n+\txmmCMov\n+\n+\t// idivRemSequence is a sequence of instructions to compute both the quotient and remainder of a division.\n+\tidivRemSequence\n+\n+\t// blendvpd is https://www.felixcloutier.com/x86/blendvpd.\n+\tblendvpd\n+\n+\t// mfence is https://www.felixcloutier.com/x86/mfence\n+\tmfence\n+\n+\t// lockxadd is xadd https://www.felixcloutier.com/x86/xadd with a lock prefix.\n+\tlockxadd\n+\n+\t// keepAlive is a meta instruction that uses one register and does nothing.\n+\tkeepAlive\n+\n+\tinstrMax\n+)\n+\n+func (i *instruction) asMFence() *instruction {\n+\ti.kind = mfence\n+\treturn i\n+}\n+\n+func (i *instruction) asKeepAlive(r regalloc.VReg) *instruction {\n+\ti.kind = keepAlive\n+\ti.op1 = newOperandReg(r)\n+\treturn i\n+}\n+\n+func (i *instruction) asIdivRemSequence(execCtx, divisor, tmpGp regalloc.VReg, isDiv, signed, _64 bool) *instruction {\n+\ti.kind = idivRemSequence\n+\ti.op1 = newOperandReg(execCtx)\n+\ti.op2 = newOperandReg(divisor)\n+\ti.u1 = uint64(tmpGp)\n+\tif isDiv {\n+\t\ti.u2 |= 1\n+\t}\n+\tif signed {\n+\t\ti.u2 |= 2\n+\t}\n+\tif _64 {\n+\t\ti.u2 |= 4\n+\t}\n+\treturn i\n+}\n+\n+func (i *instruction) idivRemSequenceData() (\n+\texecCtx, divisor, tmpGp regalloc.VReg, isDiv, signed, _64 bool,\n+) {\n+\tif i.kind != idivRemSequence {\n+\t\tpanic(\"BUG\")\n+\t}\n+\treturn i.op1.reg(), i.op2.reg(), regalloc.VReg(i.u1), i.u2&1 != 0, i.u2&2 != 0, i.u2&4 != 0\n+}\n+\n+func (i *instruction) asXmmCMov(cc cond, x operand, rd regalloc.VReg, size byte) *instruction {\n+\ti.kind = xmmCMov\n+\ti.op1 = x\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(cc)\n+\ti.u2 = uint64(size)\n+\treturn i\n+}\n+\n+func (i *instruction) asDefineUninitializedReg(r regalloc.VReg) *instruction {\n+\ti.kind = defineUninitializedReg\n+\ti.op2 = newOperandReg(r)\n+\treturn i\n+}\n+\n+func (m *machine) allocateFcvtToUintSequence(\n+\texecCtx, src, tmpGp, tmpGp2, tmpXmm, tmpXmm2 regalloc.VReg,\n+\tsrc64, dst64, sat bool,\n+) *instruction {\n+\ti := m.allocateInstr()\n+\ti.kind = fcvtToUintSequence\n+\top1a := m.amodePool.Allocate()\n+\top2a := m.amodePool.Allocate()\n+\ti.op1 = newOperandMem(op1a)\n+\ti.op2 = newOperandMem(op2a)\n+\tif src64 {\n+\t\top1a.imm32 = 1\n+\t} else {\n+\t\top1a.imm32 = 0\n+\t}\n+\tif dst64 {\n+\t\top1a.imm32 |= 2\n+\t}\n+\tif sat {\n+\t\top1a.imm32 |= 4\n+\t}\n+\n+\top1a.base = execCtx\n+\top1a.index = src\n+\top2a.base = tmpGp\n+\top2a.index = tmpGp2\n+\ti.u1 = uint64(tmpXmm)\n+\ti.u2 = uint64(tmpXmm2)\n+\treturn i\n+}\n+\n+func (i *instruction) fcvtToUintSequenceData() (\n+\texecCtx, src, tmpGp, tmpGp2, tmpXmm, tmpXmm2 regalloc.VReg, src64, dst64, sat bool,\n+) {\n+\tif i.kind != fcvtToUintSequence {\n+\t\tpanic(\"BUG\")\n+\t}\n+\top1a := i.op1.addressMode()\n+\top2a := i.op2.addressMode()\n+\treturn op1a.base, op1a.index, op2a.base, op2a.index, regalloc.VReg(i.u1), regalloc.VReg(i.u2),\n+\t\top1a.imm32&1 != 0, op1a.imm32&2 != 0, op1a.imm32&4 != 0\n+}\n+\n+func (m *machine) allocateFcvtToSintSequence(\n+\texecCtx, src, tmpGp, tmpGp2, tmpXmm regalloc.VReg,\n+\tsrc64, dst64, sat bool,\n+) *instruction {\n+\ti := m.allocateInstr()\n+\ti.kind = fcvtToSintSequence\n+\top1a := m.amodePool.Allocate()\n+\top2a := m.amodePool.Allocate()\n+\ti.op1 = newOperandMem(op1a)\n+\ti.op2 = newOperandMem(op2a)\n+\top1a.base = execCtx\n+\top1a.index = src\n+\top2a.base = tmpGp\n+\top2a.index = tmpGp2\n+\ti.u1 = uint64(tmpXmm)\n+\tif src64 {\n+\t\ti.u2 = 1\n+\t} else {\n+\t\ti.u2 = 0\n+\t}\n+\tif dst64 {\n+\t\ti.u2 |= 2\n+\t}\n+\tif sat {\n+\t\ti.u2 |= 4\n+\t}\n+\treturn i\n+}\n+\n+func (i *instruction) fcvtToSintSequenceData() (\n+\texecCtx, src, tmpGp, tmpGp2, tmpXmm regalloc.VReg, src64, dst64, sat bool,\n+) {\n+\tif i.kind != fcvtToSintSequence {\n+\t\tpanic(\"BUG\")\n+\t}\n+\top1a := i.op1.addressMode()\n+\top2a := i.op2.addressMode()\n+\treturn op1a.base, op1a.index, op2a.base, op2a.index, regalloc.VReg(i.u1),\n+\t\ti.u2&1 != 0, i.u2&2 != 0, i.u2&4 != 0\n+}\n+\n+func (k instructionKind) String() string {\n+\tswitch k {\n+\tcase nop0:\n+\t\treturn \"nop\"\n+\tcase ret:\n+\t\treturn \"ret\"\n+\tcase imm:\n+\t\treturn \"imm\"\n+\tcase aluRmiR:\n+\t\treturn \"aluRmiR\"\n+\tcase movRR:\n+\t\treturn \"movRR\"\n+\tcase xmmRmR:\n+\t\treturn \"xmmRmR\"\n+\tcase gprToXmm:\n+\t\treturn \"gprToXmm\"\n+\tcase xmmUnaryRmR:\n+\t\treturn \"xmmUnaryRmR\"\n+\tcase xmmUnaryRmRImm:\n+\t\treturn \"xmmUnaryRmRImm\"\n+\tcase unaryRmR:\n+\t\treturn \"unaryRmR\"\n+\tcase not:\n+\t\treturn \"not\"\n+\tcase neg:\n+\t\treturn \"neg\"\n+\tcase div:\n+\t\treturn \"div\"\n+\tcase mulHi:\n+\t\treturn \"mulHi\"\n+\tcase signExtendData:\n+\t\treturn \"signExtendData\"\n+\tcase movzxRmR:\n+\t\treturn \"movzxRmR\"\n+\tcase mov64MR:\n+\t\treturn \"mov64MR\"\n+\tcase lea:\n+\t\treturn \"lea\"\n+\tcase movsxRmR:\n+\t\treturn \"movsxRmR\"\n+\tcase movRM:\n+\t\treturn \"movRM\"\n+\tcase shiftR:\n+\t\treturn \"shiftR\"\n+\tcase xmmRmiReg:\n+\t\treturn \"xmmRmiReg\"\n+\tcase cmpRmiR:\n+\t\treturn \"cmpRmiR\"\n+\tcase setcc:\n+\t\treturn \"setcc\"\n+\tcase cmove:\n+\t\treturn \"cmove\"\n+\tcase push64:\n+\t\treturn \"push64\"\n+\tcase pop64:\n+\t\treturn \"pop64\"\n+\tcase xmmMovRM:\n+\t\treturn \"xmmMovRM\"\n+\tcase xmmLoadConst:\n+\t\treturn \"xmmLoadConst\"\n+\tcase xmmToGpr:\n+\t\treturn \"xmmToGpr\"\n+\tcase cvtUint64ToFloatSeq:\n+\t\treturn \"cvtUint64ToFloatSeq\"\n+\tcase cvtFloatToSintSeq:\n+\t\treturn \"cvtFloatToSintSeq\"\n+\tcase cvtFloatToUintSeq:\n+\t\treturn \"cvtFloatToUintSeq\"\n+\tcase xmmMinMaxSeq:\n+\t\treturn \"xmmMinMaxSeq\"\n+\tcase xmmCmpRmR:\n+\t\treturn \"xmmCmpRmR\"\n+\tcase xmmRmRImm:\n+\t\treturn \"xmmRmRImm\"\n+\tcase jmpIf:\n+\t\treturn \"jmpIf\"\n+\tcase jmp:\n+\t\treturn \"jmp\"\n+\tcase jmpTableIsland:\n+\t\treturn \"jmpTableIsland\"\n+\tcase exitSequence:\n+\t\treturn \"exit_sequence\"\n+\tcase ud2:\n+\t\treturn \"ud2\"\n+\tcase xchg:\n+\t\treturn \"xchg\"\n+\tcase zeros:\n+\t\treturn \"zeros\"\n+\tcase fcvtToSintSequence:\n+\t\treturn \"fcvtToSintSequence\"\n+\tcase fcvtToUintSequence:\n+\t\treturn \"fcvtToUintSequence\"\n+\tcase xmmCMov:\n+\t\treturn \"xmmCMov\"\n+\tcase idivRemSequence:\n+\t\treturn \"idivRemSequence\"\n+\tcase mfence:\n+\t\treturn \"mfence\"\n+\tcase lockcmpxchg:\n+\t\treturn \"lockcmpxchg\"\n+\tcase lockxadd:\n+\t\treturn \"lockxadd\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+type aluRmiROpcode byte\n+\n+const (\n+\taluRmiROpcodeAdd aluRmiROpcode = iota + 1\n+\taluRmiROpcodeSub\n+\taluRmiROpcodeAnd\n+\taluRmiROpcodeOr\n+\taluRmiROpcodeXor\n+\taluRmiROpcodeMul\n+)\n+\n+func (a aluRmiROpcode) String() string {\n+\tswitch a {\n+\tcase aluRmiROpcodeAdd:\n+\t\treturn \"add\"\n+\tcase aluRmiROpcodeSub:\n+\t\treturn \"sub\"\n+\tcase aluRmiROpcodeAnd:\n+\t\treturn \"and\"\n+\tcase aluRmiROpcodeOr:\n+\t\treturn \"or\"\n+\tcase aluRmiROpcodeXor:\n+\t\treturn \"xor\"\n+\tcase aluRmiROpcodeMul:\n+\t\treturn \"imul\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+func (i *instruction) asJmpIf(cond cond, target operand) *instruction {\n+\ti.kind = jmpIf\n+\ti.u1 = uint64(cond)\n+\ti.op1 = target\n+\treturn i\n+}\n+\n+// asJmpTableSequence is used to emit the jump table.\n+// targetSliceIndex is the index of the target slice in machine.jmpTableTargets.\n+func (i *instruction) asJmpTableSequence(targetSliceIndex int, targetCount int) *instruction {\n+\ti.kind = jmpTableIsland\n+\ti.u1 = uint64(targetSliceIndex)\n+\ti.u2 = uint64(targetCount)\n+\treturn i\n+}\n+\n+func (i *instruction) asJmp(target operand) *instruction {\n+\ti.kind = jmp\n+\ti.op1 = target\n+\treturn i\n+}\n+\n+func (i *instruction) jmpLabel() backend.Label {\n+\tswitch i.kind {\n+\tcase jmp, jmpIf, lea, xmmUnaryRmR:\n+\t\treturn i.op1.label()\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+func (i *instruction) asLEA(target operand, rd regalloc.VReg) *instruction {\n+\ti.kind = lea\n+\ti.op1 = target\n+\ti.op2 = newOperandReg(rd)\n+\treturn i\n+}\n+\n+func (i *instruction) asCall(ref ssa.FuncRef, abi *backend.FunctionABI) *instruction {\n+\ti.kind = call\n+\ti.u1 = uint64(ref)\n+\tif abi != nil {\n+\t\ti.u2 = abi.ABIInfoAsUint64()\n+\t}\n+\treturn i\n+}\n+\n+func (i *instruction) asCallIndirect(ptr operand, abi *backend.FunctionABI) *instruction {\n+\tif ptr.kind != operandKindReg && ptr.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = callIndirect\n+\ti.op1 = ptr\n+\tif abi != nil {\n+\t\ti.u2 = abi.ABIInfoAsUint64()\n+\t}\n+\treturn i\n+}\n+\n+func (i *instruction) asRet() *instruction {\n+\ti.kind = ret\n+\treturn i\n+}\n+\n+func (i *instruction) asImm(dst regalloc.VReg, value uint64, _64 bool) *instruction {\n+\ti.kind = imm\n+\ti.op2 = newOperandReg(dst)\n+\ti.u1 = value\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asAluRmiR(op aluRmiROpcode, rm operand, rd regalloc.VReg, _64 bool) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem && rm.kind != operandKindImm32 {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = aluRmiR\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asZeros(dst regalloc.VReg) *instruction {\n+\ti.kind = zeros\n+\ti.op2 = newOperandReg(dst)\n+\treturn i\n+}\n+\n+func (i *instruction) asBlendvpd(rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = blendvpd\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\treturn i\n+}\n+\n+func (i *instruction) asXmmRmR(op sseOpcode, rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = xmmRmR\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\treturn i\n+}\n+\n+func (i *instruction) asXmmRmRImm(op sseOpcode, imm uint8, rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = xmmRmRImm\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\ti.u2 = uint64(imm)\n+\treturn i\n+}\n+\n+func (i *instruction) asGprToXmm(op sseOpcode, rm operand, rd regalloc.VReg, _64 bool) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = gprToXmm\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asEmitSourceOffsetInfo(l ssa.SourceOffset) *instruction {\n+\ti.kind = sourceOffsetInfo\n+\ti.u1 = uint64(l)\n+\treturn i\n+}\n+\n+func (i *instruction) sourceOffsetInfo() ssa.SourceOffset {\n+\treturn ssa.SourceOffset(i.u1)\n+}\n+\n+func (i *instruction) asXmmToGpr(op sseOpcode, rm, rd regalloc.VReg, _64 bool) *instruction {\n+\ti.kind = xmmToGpr\n+\ti.op1 = newOperandReg(rm)\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asMovRM(rm regalloc.VReg, rd operand, size byte) *instruction {\n+\tif rd.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = movRM\n+\ti.op1 = newOperandReg(rm)\n+\ti.op2 = rd\n+\ti.u1 = uint64(size)\n+\treturn i\n+}\n+\n+func (i *instruction) asMovsxRmR(ext extMode, src operand, rd regalloc.VReg) *instruction {\n+\tif src.kind != operandKindReg && src.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = movsxRmR\n+\ti.op1 = src\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(ext)\n+\treturn i\n+}\n+\n+func (i *instruction) asMovzxRmR(ext extMode, src operand, rd regalloc.VReg) *instruction {\n+\tif src.kind != operandKindReg && src.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = movzxRmR\n+\ti.op1 = src\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(ext)\n+\treturn i\n+}\n+\n+func (i *instruction) asSignExtendData(_64 bool) *instruction {\n+\ti.kind = signExtendData\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asUD2() *instruction {\n+\ti.kind = ud2\n+\treturn i\n+}\n+\n+func (i *instruction) asDiv(rn operand, signed bool, _64 bool) *instruction {\n+\ti.kind = div\n+\ti.op1 = rn\n+\ti.b1 = _64\n+\tif signed {\n+\t\ti.u1 = 1\n+\t}\n+\treturn i\n+}\n+\n+func (i *instruction) asMov64MR(rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = mov64MR\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\treturn i\n+}\n+\n+func (i *instruction) asMovRR(rm, rd regalloc.VReg, _64 bool) *instruction {\n+\ti.kind = movRR\n+\ti.op1 = newOperandReg(rm)\n+\ti.op2 = newOperandReg(rd)\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asNot(rm operand, _64 bool) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = not\n+\ti.op1 = rm\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asNeg(rm operand, _64 bool) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = neg\n+\ti.op1 = rm\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asMulHi(rm operand, signed, _64 bool) *instruction {\n+\tif rm.kind != operandKindReg && (rm.kind != operandKindMem) {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = mulHi\n+\ti.op1 = rm\n+\ti.b1 = _64\n+\tif signed {\n+\t\ti.u1 = 1\n+\t}\n+\treturn i\n+}\n+\n+func (i *instruction) asUnaryRmR(op unaryRmROpcode, rm operand, rd regalloc.VReg, _64 bool) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = unaryRmR\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asShiftR(op shiftROp, amount operand, rd regalloc.VReg, _64 bool) *instruction {\n+\tif amount.kind != operandKindReg && amount.kind != operandKindImm32 {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = shiftR\n+\ti.op1 = amount\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asXmmRmiReg(op sseOpcode, rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindImm32 && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = xmmRmiReg\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\treturn i\n+}\n+\n+func (i *instruction) asCmpRmiR(cmp bool, rm operand, rn regalloc.VReg, _64 bool) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindImm32 && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = cmpRmiR\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rn)\n+\tif cmp {\n+\t\ti.u1 = 1\n+\t}\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (i *instruction) asSetcc(c cond, rd regalloc.VReg) *instruction {\n+\ti.kind = setcc\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(c)\n+\treturn i\n+}\n+\n+func (i *instruction) asCmove(c cond, rm operand, rd regalloc.VReg, _64 bool) *instruction {\n+\ti.kind = cmove\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(c)\n+\ti.b1 = _64\n+\treturn i\n+}\n+\n+func (m *machine) allocateExitSeq(execCtx regalloc.VReg) *instruction {\n+\ti := m.allocateInstr()\n+\ti.kind = exitSequence\n+\ti.op1 = newOperandReg(execCtx)\n+\t// Allocate the address mode that will be used in encoding the exit sequence.\n+\ti.op2 = newOperandMem(m.amodePool.Allocate())\n+\treturn i\n+}\n+\n+func (i *instruction) asXmmUnaryRmR(op sseOpcode, rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = xmmUnaryRmR\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\treturn i\n+}\n+\n+func (i *instruction) asXmmUnaryRmRImm(op sseOpcode, imm byte, rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = xmmUnaryRmRImm\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\ti.u2 = uint64(imm)\n+\treturn i\n+}\n+\n+func (i *instruction) asXmmCmpRmR(op sseOpcode, rm operand, rd regalloc.VReg) *instruction {\n+\tif rm.kind != operandKindReg && rm.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = xmmCmpRmR\n+\ti.op1 = rm\n+\ti.op2 = newOperandReg(rd)\n+\ti.u1 = uint64(op)\n+\treturn i\n+}\n+\n+func (i *instruction) asXmmMovRM(op sseOpcode, rm regalloc.VReg, rd operand) *instruction {\n+\tif rd.kind != operandKindMem {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = xmmMovRM\n+\ti.op1 = newOperandReg(rm)\n+\ti.op2 = rd\n+\ti.u1 = uint64(op)\n+\treturn i\n+}\n+\n+func (i *instruction) asPop64(rm regalloc.VReg) *instruction {\n+\ti.kind = pop64\n+\ti.op1 = newOperandReg(rm)\n+\treturn i\n+}\n+\n+func (i *instruction) asPush64(op operand) *instruction {\n+\tif op.kind != operandKindReg && op.kind != operandKindMem && op.kind != operandKindImm32 {\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.kind = push64\n+\ti.op1 = op\n+\treturn i\n+}\n+\n+func (i *instruction) asXCHG(rm regalloc.VReg, rd operand, size byte) *instruction {\n+\ti.kind = xchg\n+\ti.op1 = newOperandReg(rm)\n+\ti.op2 = rd\n+\ti.u1 = uint64(size)\n+\treturn i\n+}\n+\n+func (i *instruction) asLockCmpXCHG(rm regalloc.VReg, rd *amode, size byte) *instruction {\n+\ti.kind = lockcmpxchg\n+\ti.op1 = newOperandReg(rm)\n+\ti.op2 = newOperandMem(rd)\n+\ti.u1 = uint64(size)\n+\treturn i\n+}\n+\n+func (i *instruction) asLockXAdd(rm regalloc.VReg, rd *amode, size byte) *instruction {\n+\ti.kind = lockxadd\n+\ti.op1 = newOperandReg(rm)\n+\ti.op2 = newOperandMem(rd)\n+\ti.u1 = uint64(size)\n+\treturn i\n+}\n+\n+type unaryRmROpcode byte\n+\n+const (\n+\tunaryRmROpcodeBsr unaryRmROpcode = iota\n+\tunaryRmROpcodeBsf\n+\tunaryRmROpcodeLzcnt\n+\tunaryRmROpcodeTzcnt\n+\tunaryRmROpcodePopcnt\n+)\n+\n+func (u unaryRmROpcode) String() string {\n+\tswitch u {\n+\tcase unaryRmROpcodeBsr:\n+\t\treturn \"bsr\"\n+\tcase unaryRmROpcodeBsf:\n+\t\treturn \"bsf\"\n+\tcase unaryRmROpcodeLzcnt:\n+\t\treturn \"lzcnt\"\n+\tcase unaryRmROpcodeTzcnt:\n+\t\treturn \"tzcnt\"\n+\tcase unaryRmROpcodePopcnt:\n+\t\treturn \"popcnt\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+type shiftROp byte\n+\n+const (\n+\tshiftROpRotateLeft           shiftROp = 0\n+\tshiftROpRotateRight          shiftROp = 1\n+\tshiftROpShiftLeft            shiftROp = 4\n+\tshiftROpShiftRightLogical    shiftROp = 5\n+\tshiftROpShiftRightArithmetic shiftROp = 7\n+)\n+\n+func (s shiftROp) String() string {\n+\tswitch s {\n+\tcase shiftROpRotateLeft:\n+\t\treturn \"rol\"\n+\tcase shiftROpRotateRight:\n+\t\treturn \"ror\"\n+\tcase shiftROpShiftLeft:\n+\t\treturn \"shl\"\n+\tcase shiftROpShiftRightLogical:\n+\t\treturn \"shr\"\n+\tcase shiftROpShiftRightArithmetic:\n+\t\treturn \"sar\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+type sseOpcode byte\n+\n+const (\n+\tsseOpcodeInvalid sseOpcode = iota\n+\tsseOpcodeAddps\n+\tsseOpcodeAddpd\n+\tsseOpcodeAddss\n+\tsseOpcodeAddsd\n+\tsseOpcodeAndps\n+\tsseOpcodeAndpd\n+\tsseOpcodeAndnps\n+\tsseOpcodeAndnpd\n+\tsseOpcodeBlendvps\n+\tsseOpcodeBlendvpd\n+\tsseOpcodeComiss\n+\tsseOpcodeComisd\n+\tsseOpcodeCmpps\n+\tsseOpcodeCmppd\n+\tsseOpcodeCmpss\n+\tsseOpcodeCmpsd\n+\tsseOpcodeCvtdq2ps\n+\tsseOpcodeCvtdq2pd\n+\tsseOpcodeCvtsd2ss\n+\tsseOpcodeCvtsd2si\n+\tsseOpcodeCvtsi2ss\n+\tsseOpcodeCvtsi2sd\n+\tsseOpcodeCvtss2si\n+\tsseOpcodeCvtss2sd\n+\tsseOpcodeCvttps2dq\n+\tsseOpcodeCvttss2si\n+\tsseOpcodeCvttsd2si\n+\tsseOpcodeDivps\n+\tsseOpcodeDivpd\n+\tsseOpcodeDivss\n+\tsseOpcodeDivsd\n+\tsseOpcodeInsertps\n+\tsseOpcodeMaxps\n+\tsseOpcodeMaxpd\n+\tsseOpcodeMaxss\n+\tsseOpcodeMaxsd\n+\tsseOpcodeMinps\n+\tsseOpcodeMinpd\n+\tsseOpcodeMinss\n+\tsseOpcodeMinsd\n+\tsseOpcodeMovaps\n+\tsseOpcodeMovapd\n+\tsseOpcodeMovd\n+\tsseOpcodeMovdqa\n+\tsseOpcodeMovdqu\n+\tsseOpcodeMovlhps\n+\tsseOpcodeMovmskps\n+\tsseOpcodeMovmskpd\n+\tsseOpcodeMovq\n+\tsseOpcodeMovss\n+\tsseOpcodeMovsd\n+\tsseOpcodeMovups\n+\tsseOpcodeMovupd\n+\tsseOpcodeMulps\n+\tsseOpcodeMulpd\n+\tsseOpcodeMulss\n+\tsseOpcodeMulsd\n+\tsseOpcodeOrps\n+\tsseOpcodeOrpd\n+\tsseOpcodePabsb\n+\tsseOpcodePabsw\n+\tsseOpcodePabsd\n+\tsseOpcodePackssdw\n+\tsseOpcodePacksswb\n+\tsseOpcodePackusdw\n+\tsseOpcodePackuswb\n+\tsseOpcodePaddb\n+\tsseOpcodePaddd\n+\tsseOpcodePaddq\n+\tsseOpcodePaddw\n+\tsseOpcodePaddsb\n+\tsseOpcodePaddsw\n+\tsseOpcodePaddusb\n+\tsseOpcodePaddusw\n+\tsseOpcodePalignr\n+\tsseOpcodePand\n+\tsseOpcodePandn\n+\tsseOpcodePavgb\n+\tsseOpcodePavgw\n+\tsseOpcodePcmpeqb\n+\tsseOpcodePcmpeqw\n+\tsseOpcodePcmpeqd\n+\tsseOpcodePcmpeqq\n+\tsseOpcodePcmpgtb\n+\tsseOpcodePcmpgtw\n+\tsseOpcodePcmpgtd\n+\tsseOpcodePcmpgtq\n+\tsseOpcodePextrb\n+\tsseOpcodePextrw\n+\tsseOpcodePextrd\n+\tsseOpcodePextrq\n+\tsseOpcodePinsrb\n+\tsseOpcodePinsrw\n+\tsseOpcodePinsrd\n+\tsseOpcodePinsrq\n+\tsseOpcodePmaddwd\n+\tsseOpcodePmaxsb\n+\tsseOpcodePmaxsw\n+\tsseOpcodePmaxsd\n+\tsseOpcodePmaxub\n+\tsseOpcodePmaxuw\n+\tsseOpcodePmaxud\n+\tsseOpcodePminsb\n+\tsseOpcodePminsw\n+\tsseOpcodePminsd\n+\tsseOpcodePminub\n+\tsseOpcodePminuw\n+\tsseOpcodePminud\n+\tsseOpcodePmovmskb\n+\tsseOpcodePmovsxbd\n+\tsseOpcodePmovsxbw\n+\tsseOpcodePmovsxbq\n+\tsseOpcodePmovsxwd\n+\tsseOpcodePmovsxwq\n+\tsseOpcodePmovsxdq\n+\tsseOpcodePmovzxbd\n+\tsseOpcodePmovzxbw\n+\tsseOpcodePmovzxbq\n+\tsseOpcodePmovzxwd\n+\tsseOpcodePmovzxwq\n+\tsseOpcodePmovzxdq\n+\tsseOpcodePmulld\n+\tsseOpcodePmullw\n+\tsseOpcodePmuludq\n+\tsseOpcodePor\n+\tsseOpcodePshufb\n+\tsseOpcodePshufd\n+\tsseOpcodePsllw\n+\tsseOpcodePslld\n+\tsseOpcodePsllq\n+\tsseOpcodePsraw\n+\tsseOpcodePsrad\n+\tsseOpcodePsrlw\n+\tsseOpcodePsrld\n+\tsseOpcodePsrlq\n+\tsseOpcodePsubb\n+\tsseOpcodePsubd\n+\tsseOpcodePsubq\n+\tsseOpcodePsubw\n+\tsseOpcodePsubsb\n+\tsseOpcodePsubsw\n+\tsseOpcodePsubusb\n+\tsseOpcodePsubusw\n+\tsseOpcodePtest\n+\tsseOpcodePunpckhbw\n+\tsseOpcodePunpcklbw\n+\tsseOpcodePxor\n+\tsseOpcodeRcpss\n+\tsseOpcodeRoundps\n+\tsseOpcodeRoundpd\n+\tsseOpcodeRoundss\n+\tsseOpcodeRoundsd\n+\tsseOpcodeRsqrtss\n+\tsseOpcodeSqrtps\n+\tsseOpcodeSqrtpd\n+\tsseOpcodeSqrtss\n+\tsseOpcodeSqrtsd\n+\tsseOpcodeSubps\n+\tsseOpcodeSubpd\n+\tsseOpcodeSubss\n+\tsseOpcodeSubsd\n+\tsseOpcodeUcomiss\n+\tsseOpcodeUcomisd\n+\tsseOpcodeXorps\n+\tsseOpcodeXorpd\n+\tsseOpcodePmulhrsw\n+\tsseOpcodeUnpcklps\n+\tsseOpcodeCvtps2pd\n+\tsseOpcodeCvtpd2ps\n+\tsseOpcodeCvttpd2dq\n+\tsseOpcodeShufps\n+\tsseOpcodePmaddubsw\n+)\n+\n+func (s sseOpcode) String() string {\n+\tswitch s {\n+\tcase sseOpcodeInvalid:\n+\t\treturn \"invalid\"\n+\tcase sseOpcodeAddps:\n+\t\treturn \"addps\"\n+\tcase sseOpcodeAddpd:\n+\t\treturn \"addpd\"\n+\tcase sseOpcodeAddss:\n+\t\treturn \"addss\"\n+\tcase sseOpcodeAddsd:\n+\t\treturn \"addsd\"\n+\tcase sseOpcodeAndps:\n+\t\treturn \"andps\"\n+\tcase sseOpcodeAndpd:\n+\t\treturn \"andpd\"\n+\tcase sseOpcodeAndnps:\n+\t\treturn \"andnps\"\n+\tcase sseOpcodeAndnpd:\n+\t\treturn \"andnpd\"\n+\tcase sseOpcodeBlendvps:\n+\t\treturn \"blendvps\"\n+\tcase sseOpcodeBlendvpd:\n+\t\treturn \"blendvpd\"\n+\tcase sseOpcodeComiss:\n+\t\treturn \"comiss\"\n+\tcase sseOpcodeComisd:\n+\t\treturn \"comisd\"\n+\tcase sseOpcodeCmpps:\n+\t\treturn \"cmpps\"\n+\tcase sseOpcodeCmppd:\n+\t\treturn \"cmppd\"\n+\tcase sseOpcodeCmpss:\n+\t\treturn \"cmpss\"\n+\tcase sseOpcodeCmpsd:\n+\t\treturn \"cmpsd\"\n+\tcase sseOpcodeCvtdq2ps:\n+\t\treturn \"cvtdq2ps\"\n+\tcase sseOpcodeCvtdq2pd:\n+\t\treturn \"cvtdq2pd\"\n+\tcase sseOpcodeCvtsd2ss:\n+\t\treturn \"cvtsd2ss\"\n+\tcase sseOpcodeCvtsd2si:\n+\t\treturn \"cvtsd2si\"\n+\tcase sseOpcodeCvtsi2ss:\n+\t\treturn \"cvtsi2ss\"\n+\tcase sseOpcodeCvtsi2sd:\n+\t\treturn \"cvtsi2sd\"\n+\tcase sseOpcodeCvtss2si:\n+\t\treturn \"cvtss2si\"\n+\tcase sseOpcodeCvtss2sd:\n+\t\treturn \"cvtss2sd\"\n+\tcase sseOpcodeCvttps2dq:\n+\t\treturn \"cvttps2dq\"\n+\tcase sseOpcodeCvttss2si:\n+\t\treturn \"cvttss2si\"\n+\tcase sseOpcodeCvttsd2si:\n+\t\treturn \"cvttsd2si\"\n+\tcase sseOpcodeDivps:\n+\t\treturn \"divps\"\n+\tcase sseOpcodeDivpd:\n+\t\treturn \"divpd\"\n+\tcase sseOpcodeDivss:\n+\t\treturn \"divss\"\n+\tcase sseOpcodeDivsd:\n+\t\treturn \"divsd\"\n+\tcase sseOpcodeInsertps:\n+\t\treturn \"insertps\"\n+\tcase sseOpcodeMaxps:\n+\t\treturn \"maxps\"\n+\tcase sseOpcodeMaxpd:\n+\t\treturn \"maxpd\"\n+\tcase sseOpcodeMaxss:\n+\t\treturn \"maxss\"\n+\tcase sseOpcodeMaxsd:\n+\t\treturn \"maxsd\"\n+\tcase sseOpcodeMinps:\n+\t\treturn \"minps\"\n+\tcase sseOpcodeMinpd:\n+\t\treturn \"minpd\"\n+\tcase sseOpcodeMinss:\n+\t\treturn \"minss\"\n+\tcase sseOpcodeMinsd:\n+\t\treturn \"minsd\"\n+\tcase sseOpcodeMovaps:\n+\t\treturn \"movaps\"\n+\tcase sseOpcodeMovapd:\n+\t\treturn \"movapd\"\n+\tcase sseOpcodeMovd:\n+\t\treturn \"movd\"\n+\tcase sseOpcodeMovdqa:\n+\t\treturn \"movdqa\"\n+\tcase sseOpcodeMovdqu:\n+\t\treturn \"movdqu\"\n+\tcase sseOpcodeMovlhps:\n+\t\treturn \"movlhps\"\n+\tcase sseOpcodeMovmskps:\n+\t\treturn \"movmskps\"\n+\tcase sseOpcodeMovmskpd:\n+\t\treturn \"movmskpd\"\n+\tcase sseOpcodeMovq:\n+\t\treturn \"movq\"\n+\tcase sseOpcodeMovss:\n+\t\treturn \"movss\"\n+\tcase sseOpcodeMovsd:\n+\t\treturn \"movsd\"\n+\tcase sseOpcodeMovups:\n+\t\treturn \"movups\"\n+\tcase sseOpcodeMovupd:\n+\t\treturn \"movupd\"\n+\tcase sseOpcodeMulps:\n+\t\treturn \"mulps\"\n+\tcase sseOpcodeMulpd:\n+\t\treturn \"mulpd\"\n+\tcase sseOpcodeMulss:\n+\t\treturn \"mulss\"\n+\tcase sseOpcodeMulsd:\n+\t\treturn \"mulsd\"\n+\tcase sseOpcodeOrps:\n+\t\treturn \"orps\"\n+\tcase sseOpcodeOrpd:\n+\t\treturn \"orpd\"\n+\tcase sseOpcodePabsb:\n+\t\treturn \"pabsb\"\n+\tcase sseOpcodePabsw:\n+\t\treturn \"pabsw\"\n+\tcase sseOpcodePabsd:\n+\t\treturn \"pabsd\"\n+\tcase sseOpcodePackssdw:\n+\t\treturn \"packssdw\"\n+\tcase sseOpcodePacksswb:\n+\t\treturn \"packsswb\"\n+\tcase sseOpcodePackusdw:\n+\t\treturn \"packusdw\"\n+\tcase sseOpcodePackuswb:\n+\t\treturn \"packuswb\"\n+\tcase sseOpcodePaddb:\n+\t\treturn \"paddb\"\n+\tcase sseOpcodePaddd:\n+\t\treturn \"paddd\"\n+\tcase sseOpcodePaddq:\n+\t\treturn \"paddq\"\n+\tcase sseOpcodePaddw:\n+\t\treturn \"paddw\"\n+\tcase sseOpcodePaddsb:\n+\t\treturn \"paddsb\"\n+\tcase sseOpcodePaddsw:\n+\t\treturn \"paddsw\"\n+\tcase sseOpcodePaddusb:\n+\t\treturn \"paddusb\"\n+\tcase sseOpcodePaddusw:\n+\t\treturn \"paddusw\"\n+\tcase sseOpcodePalignr:\n+\t\treturn \"palignr\"\n+\tcase sseOpcodePand:\n+\t\treturn \"pand\"\n+\tcase sseOpcodePandn:\n+\t\treturn \"pandn\"\n+\tcase sseOpcodePavgb:\n+\t\treturn \"pavgb\"\n+\tcase sseOpcodePavgw:\n+\t\treturn \"pavgw\"\n+\tcase sseOpcodePcmpeqb:\n+\t\treturn \"pcmpeqb\"\n+\tcase sseOpcodePcmpeqw:\n+\t\treturn \"pcmpeqw\"\n+\tcase sseOpcodePcmpeqd:\n+\t\treturn \"pcmpeqd\"\n+\tcase sseOpcodePcmpeqq:\n+\t\treturn \"pcmpeqq\"\n+\tcase sseOpcodePcmpgtb:\n+\t\treturn \"pcmpgtb\"\n+\tcase sseOpcodePcmpgtw:\n+\t\treturn \"pcmpgtw\"\n+\tcase sseOpcodePcmpgtd:\n+\t\treturn \"pcmpgtd\"\n+\tcase sseOpcodePcmpgtq:\n+\t\treturn \"pcmpgtq\"\n+\tcase sseOpcodePextrb:\n+\t\treturn \"pextrb\"\n+\tcase sseOpcodePextrw:\n+\t\treturn \"pextrw\"\n+\tcase sseOpcodePextrd:\n+\t\treturn \"pextrd\"\n+\tcase sseOpcodePextrq:\n+\t\treturn \"pextrq\"\n+\tcase sseOpcodePinsrb:\n+\t\treturn \"pinsrb\"\n+\tcase sseOpcodePinsrw:\n+\t\treturn \"pinsrw\"\n+\tcase sseOpcodePinsrd:\n+\t\treturn \"pinsrd\"\n+\tcase sseOpcodePinsrq:\n+\t\treturn \"pinsrq\"\n+\tcase sseOpcodePmaddwd:\n+\t\treturn \"pmaddwd\"\n+\tcase sseOpcodePmaxsb:\n+\t\treturn \"pmaxsb\"\n+\tcase sseOpcodePmaxsw:\n+\t\treturn \"pmaxsw\"\n+\tcase sseOpcodePmaxsd:\n+\t\treturn \"pmaxsd\"\n+\tcase sseOpcodePmaxub:\n+\t\treturn \"pmaxub\"\n+\tcase sseOpcodePmaxuw:\n+\t\treturn \"pmaxuw\"\n+\tcase sseOpcodePmaxud:\n+\t\treturn \"pmaxud\"\n+\tcase sseOpcodePminsb:\n+\t\treturn \"pminsb\"\n+\tcase sseOpcodePminsw:\n+\t\treturn \"pminsw\"\n+\tcase sseOpcodePminsd:\n+\t\treturn \"pminsd\"\n+\tcase sseOpcodePminub:\n+\t\treturn \"pminub\"\n+\tcase sseOpcodePminuw:\n+\t\treturn \"pminuw\"\n+\tcase sseOpcodePminud:\n+\t\treturn \"pminud\"\n+\tcase sseOpcodePmovmskb:\n+\t\treturn \"pmovmskb\"\n+\tcase sseOpcodePmovsxbd:\n+\t\treturn \"pmovsxbd\"\n+\tcase sseOpcodePmovsxbw:\n+\t\treturn \"pmovsxbw\"\n+\tcase sseOpcodePmovsxbq:\n+\t\treturn \"pmovsxbq\"\n+\tcase sseOpcodePmovsxwd:\n+\t\treturn \"pmovsxwd\"\n+\tcase sseOpcodePmovsxwq:\n+\t\treturn \"pmovsxwq\"\n+\tcase sseOpcodePmovsxdq:\n+\t\treturn \"pmovsxdq\"\n+\tcase sseOpcodePmovzxbd:\n+\t\treturn \"pmovzxbd\"\n+\tcase sseOpcodePmovzxbw:\n+\t\treturn \"pmovzxbw\"\n+\tcase sseOpcodePmovzxbq:\n+\t\treturn \"pmovzxbq\"\n+\tcase sseOpcodePmovzxwd:\n+\t\treturn \"pmovzxwd\"\n+\tcase sseOpcodePmovzxwq:\n+\t\treturn \"pmovzxwq\"\n+\tcase sseOpcodePmovzxdq:\n+\t\treturn \"pmovzxdq\"\n+\tcase sseOpcodePmulld:\n+\t\treturn \"pmulld\"\n+\tcase sseOpcodePmullw:\n+\t\treturn \"pmullw\"\n+\tcase sseOpcodePmuludq:\n+\t\treturn \"pmuludq\"\n+\tcase sseOpcodePor:\n+\t\treturn \"por\"\n+\tcase sseOpcodePshufb:\n+\t\treturn \"pshufb\"\n+\tcase sseOpcodePshufd:\n+\t\treturn \"pshufd\"\n+\tcase sseOpcodePsllw:\n+\t\treturn \"psllw\"\n+\tcase sseOpcodePslld:\n+\t\treturn \"pslld\"\n+\tcase sseOpcodePsllq:\n+\t\treturn \"psllq\"\n+\tcase sseOpcodePsraw:\n+\t\treturn \"psraw\"\n+\tcase sseOpcodePsrad:\n+\t\treturn \"psrad\"\n+\tcase sseOpcodePsrlw:\n+\t\treturn \"psrlw\"\n+\tcase sseOpcodePsrld:\n+\t\treturn \"psrld\"\n+\tcase sseOpcodePsrlq:\n+\t\treturn \"psrlq\"\n+\tcase sseOpcodePsubb:\n+\t\treturn \"psubb\"\n+\tcase sseOpcodePsubd:\n+\t\treturn \"psubd\"\n+\tcase sseOpcodePsubq:\n+\t\treturn \"psubq\"\n+\tcase sseOpcodePsubw:\n+\t\treturn \"psubw\"\n+\tcase sseOpcodePsubsb:\n+\t\treturn \"psubsb\"\n+\tcase sseOpcodePsubsw:\n+\t\treturn \"psubsw\"\n+\tcase sseOpcodePsubusb:\n+\t\treturn \"psubusb\"\n+\tcase sseOpcodePsubusw:\n+\t\treturn \"psubusw\"\n+\tcase sseOpcodePtest:\n+\t\treturn \"ptest\"\n+\tcase sseOpcodePunpckhbw:\n+\t\treturn \"punpckhbw\"\n+\tcase sseOpcodePunpcklbw:\n+\t\treturn \"punpcklbw\"\n+\tcase sseOpcodePxor:\n+\t\treturn \"pxor\"\n+\tcase sseOpcodeRcpss:\n+\t\treturn \"rcpss\"\n+\tcase sseOpcodeRoundps:\n+\t\treturn \"roundps\"\n+\tcase sseOpcodeRoundpd:\n+\t\treturn \"roundpd\"\n+\tcase sseOpcodeRoundss:\n+\t\treturn \"roundss\"\n+\tcase sseOpcodeRoundsd:\n+\t\treturn \"roundsd\"\n+\tcase sseOpcodeRsqrtss:\n+\t\treturn \"rsqrtss\"\n+\tcase sseOpcodeSqrtps:\n+\t\treturn \"sqrtps\"\n+\tcase sseOpcodeSqrtpd:\n+\t\treturn \"sqrtpd\"\n+\tcase sseOpcodeSqrtss:\n+\t\treturn \"sqrtss\"\n+\tcase sseOpcodeSqrtsd:\n+\t\treturn \"sqrtsd\"\n+\tcase sseOpcodeSubps:\n+\t\treturn \"subps\"\n+\tcase sseOpcodeSubpd:\n+\t\treturn \"subpd\"\n+\tcase sseOpcodeSubss:\n+\t\treturn \"subss\"\n+\tcase sseOpcodeSubsd:\n+\t\treturn \"subsd\"\n+\tcase sseOpcodeUcomiss:\n+\t\treturn \"ucomiss\"\n+\tcase sseOpcodeUcomisd:\n+\t\treturn \"ucomisd\"\n+\tcase sseOpcodeXorps:\n+\t\treturn \"xorps\"\n+\tcase sseOpcodeXorpd:\n+\t\treturn \"xorpd\"\n+\tcase sseOpcodePmulhrsw:\n+\t\treturn \"pmulhrsw\"\n+\tcase sseOpcodeUnpcklps:\n+\t\treturn \"unpcklps\"\n+\tcase sseOpcodeCvtps2pd:\n+\t\treturn \"cvtps2pd\"\n+\tcase sseOpcodeCvtpd2ps:\n+\t\treturn \"cvtpd2ps\"\n+\tcase sseOpcodeCvttpd2dq:\n+\t\treturn \"cvttpd2dq\"\n+\tcase sseOpcodeShufps:\n+\t\treturn \"shufps\"\n+\tcase sseOpcodePmaddubsw:\n+\t\treturn \"pmaddubsw\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+type roundingMode uint8\n+\n+const (\n+\troundingModeNearest roundingMode = iota\n+\troundingModeDown\n+\troundingModeUp\n+\troundingModeZero\n+)\n+\n+func (r roundingMode) String() string {\n+\tswitch r {\n+\tcase roundingModeNearest:\n+\t\treturn \"nearest\"\n+\tcase roundingModeDown:\n+\t\treturn \"down\"\n+\tcase roundingModeUp:\n+\t\treturn \"up\"\n+\tcase roundingModeZero:\n+\t\treturn \"zero\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+// cmpPred is the immediate value for a comparison operation in xmmRmRImm.\n+type cmpPred uint8\n+\n+const (\n+\t// cmpPredEQ_OQ is Equal (ordered, non-signaling)\n+\tcmpPredEQ_OQ cmpPred = iota\n+\t// cmpPredLT_OS is Less-than (ordered, signaling)\n+\tcmpPredLT_OS\n+\t// cmpPredLE_OS is Less-than-or-equal (ordered, signaling)\n+\tcmpPredLE_OS\n+\t// cmpPredUNORD_Q is Unordered (non-signaling)\n+\tcmpPredUNORD_Q\n+\t// cmpPredNEQ_UQ is Not-equal (unordered, non-signaling)\n+\tcmpPredNEQ_UQ\n+\t// cmpPredNLT_US is Not-less-than (unordered, signaling)\n+\tcmpPredNLT_US\n+\t// cmpPredNLE_US is Not-less-than-or-equal (unordered, signaling)\n+\tcmpPredNLE_US\n+\t// cmpPredORD_Q is Ordered (non-signaling)\n+\tcmpPredORD_Q\n+\t// cmpPredEQ_UQ is Equal (unordered, non-signaling)\n+\tcmpPredEQ_UQ\n+\t// cmpPredNGE_US is Not-greater-than-or-equal (unordered, signaling)\n+\tcmpPredNGE_US\n+\t// cmpPredNGT_US is Not-greater-than (unordered, signaling)\n+\tcmpPredNGT_US\n+\t// cmpPredFALSE_OQ is False (ordered, non-signaling)\n+\tcmpPredFALSE_OQ\n+\t// cmpPredNEQ_OQ is Not-equal (ordered, non-signaling)\n+\tcmpPredNEQ_OQ\n+\t// cmpPredGE_OS is Greater-than-or-equal (ordered, signaling)\n+\tcmpPredGE_OS\n+\t// cmpPredGT_OS is Greater-than (ordered, signaling)\n+\tcmpPredGT_OS\n+\t// cmpPredTRUE_UQ is True (unordered, non-signaling)\n+\tcmpPredTRUE_UQ\n+\t// Equal (ordered, signaling)\n+\tcmpPredEQ_OS\n+\t// Less-than (ordered, nonsignaling)\n+\tcmpPredLT_OQ\n+\t// Less-than-or-equal (ordered, nonsignaling)\n+\tcmpPredLE_OQ\n+\t// Unordered (signaling)\n+\tcmpPredUNORD_S\n+\t// Not-equal (unordered, signaling)\n+\tcmpPredNEQ_US\n+\t// Not-less-than (unordered, nonsignaling)\n+\tcmpPredNLT_UQ\n+\t// Not-less-than-or-equal (unordered, nonsignaling)\n+\tcmpPredNLE_UQ\n+\t// Ordered (signaling)\n+\tcmpPredORD_S\n+\t// Equal (unordered, signaling)\n+\tcmpPredEQ_US\n+\t// Not-greater-than-or-equal (unordered, non-signaling)\n+\tcmpPredNGE_UQ\n+\t// Not-greater-than (unordered, nonsignaling)\n+\tcmpPredNGT_UQ\n+\t// False (ordered, signaling)\n+\tcmpPredFALSE_OS\n+\t// Not-equal (ordered, signaling)\n+\tcmpPredNEQ_OS\n+\t// Greater-than-or-equal (ordered, nonsignaling)\n+\tcmpPredGE_OQ\n+\t// Greater-than (ordered, nonsignaling)\n+\tcmpPredGT_OQ\n+\t// True (unordered, signaling)\n+\tcmpPredTRUE_US\n+)\n+\n+func (r cmpPred) String() string {\n+\tswitch r {\n+\tcase cmpPredEQ_OQ:\n+\t\treturn \"eq_oq\"\n+\tcase cmpPredLT_OS:\n+\t\treturn \"lt_os\"\n+\tcase cmpPredLE_OS:\n+\t\treturn \"le_os\"\n+\tcase cmpPredUNORD_Q:\n+\t\treturn \"unord_q\"\n+\tcase cmpPredNEQ_UQ:\n+\t\treturn \"neq_uq\"\n+\tcase cmpPredNLT_US:\n+\t\treturn \"nlt_us\"\n+\tcase cmpPredNLE_US:\n+\t\treturn \"nle_us\"\n+\tcase cmpPredORD_Q:\n+\t\treturn \"ord_q\"\n+\tcase cmpPredEQ_UQ:\n+\t\treturn \"eq_uq\"\n+\tcase cmpPredNGE_US:\n+\t\treturn \"nge_us\"\n+\tcase cmpPredNGT_US:\n+\t\treturn \"ngt_us\"\n+\tcase cmpPredFALSE_OQ:\n+\t\treturn \"false_oq\"\n+\tcase cmpPredNEQ_OQ:\n+\t\treturn \"neq_oq\"\n+\tcase cmpPredGE_OS:\n+\t\treturn \"ge_os\"\n+\tcase cmpPredGT_OS:\n+\t\treturn \"gt_os\"\n+\tcase cmpPredTRUE_UQ:\n+\t\treturn \"true_uq\"\n+\tcase cmpPredEQ_OS:\n+\t\treturn \"eq_os\"\n+\tcase cmpPredLT_OQ:\n+\t\treturn \"lt_oq\"\n+\tcase cmpPredLE_OQ:\n+\t\treturn \"le_oq\"\n+\tcase cmpPredUNORD_S:\n+\t\treturn \"unord_s\"\n+\tcase cmpPredNEQ_US:\n+\t\treturn \"neq_us\"\n+\tcase cmpPredNLT_UQ:\n+\t\treturn \"nlt_uq\"\n+\tcase cmpPredNLE_UQ:\n+\t\treturn \"nle_uq\"\n+\tcase cmpPredORD_S:\n+\t\treturn \"ord_s\"\n+\tcase cmpPredEQ_US:\n+\t\treturn \"eq_us\"\n+\tcase cmpPredNGE_UQ:\n+\t\treturn \"nge_uq\"\n+\tcase cmpPredNGT_UQ:\n+\t\treturn \"ngt_uq\"\n+\tcase cmpPredFALSE_OS:\n+\t\treturn \"false_os\"\n+\tcase cmpPredNEQ_OS:\n+\t\treturn \"neq_os\"\n+\tcase cmpPredGE_OQ:\n+\t\treturn \"ge_oq\"\n+\tcase cmpPredGT_OQ:\n+\t\treturn \"gt_oq\"\n+\tcase cmpPredTRUE_US:\n+\t\treturn \"true_us\"\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+func linkInstr(prev, next *instruction) *instruction {\n+\tprev.next = next\n+\tnext.prev = prev\n+\treturn next\n+}\n+\n+type defKind byte\n+\n+const (\n+\tdefKindNone defKind = iota + 1\n+\tdefKindOp2\n+\tdefKindCall\n+\tdefKindDivRem\n+)\n+\n+var defKinds = [instrMax]defKind{\n+\tnop0:                   defKindNone,\n+\tret:                    defKindNone,\n+\tmovRR:                  defKindOp2,\n+\tmovRM:                  defKindNone,\n+\txmmMovRM:               defKindNone,\n+\taluRmiR:                defKindNone,\n+\tshiftR:                 defKindNone,\n+\timm:                    defKindOp2,\n+\tunaryRmR:               defKindOp2,\n+\txmmRmiReg:              defKindNone,\n+\txmmUnaryRmR:            defKindOp2,\n+\txmmUnaryRmRImm:         defKindOp2,\n+\txmmCmpRmR:              defKindNone,\n+\txmmRmR:                 defKindNone,\n+\txmmRmRImm:              defKindNone,\n+\tmov64MR:                defKindOp2,\n+\tmovsxRmR:               defKindOp2,\n+\tmovzxRmR:               defKindOp2,\n+\tgprToXmm:               defKindOp2,\n+\txmmToGpr:               defKindOp2,\n+\tcmove:                  defKindNone,\n+\tcall:                   defKindCall,\n+\tcallIndirect:           defKindCall,\n+\tud2:                    defKindNone,\n+\tjmp:                    defKindNone,\n+\tjmpIf:                  defKindNone,\n+\tjmpTableIsland:         defKindNone,\n+\tcmpRmiR:                defKindNone,\n+\texitSequence:           defKindNone,\n+\tlea:                    defKindOp2,\n+\tsetcc:                  defKindOp2,\n+\tzeros:                  defKindOp2,\n+\tsourceOffsetInfo:       defKindNone,\n+\tfcvtToSintSequence:     defKindNone,\n+\tdefineUninitializedReg: defKindOp2,\n+\tfcvtToUintSequence:     defKindNone,\n+\txmmCMov:                defKindOp2,\n+\tidivRemSequence:        defKindDivRem,\n+\tblendvpd:               defKindNone,\n+\tmfence:                 defKindNone,\n+\txchg:                   defKindNone,\n+\tlockcmpxchg:            defKindNone,\n+\tlockxadd:               defKindNone,\n+\tneg:                    defKindNone,\n+\tkeepAlive:              defKindNone,\n+}\n+\n+// String implements fmt.Stringer.\n+func (d defKind) String() string {\n+\tswitch d {\n+\tcase defKindNone:\n+\t\treturn \"none\"\n+\tcase defKindOp2:\n+\t\treturn \"op2\"\n+\tcase defKindCall:\n+\t\treturn \"call\"\n+\tcase defKindDivRem:\n+\t\treturn \"divrem\"\n+\tdefault:\n+\t\treturn \"invalid\"\n+\t}\n+}\n+\n+type useKind byte\n+\n+const (\n+\tuseKindNone useKind = iota + 1\n+\tuseKindOp1\n+\t// useKindOp1Op2Reg is Op1 can be any operand, Op2 must be a register.\n+\tuseKindOp1Op2Reg\n+\t// useKindOp1RegOp2 is Op1 must be a register, Op2 can be any operand.\n+\tuseKindOp1RegOp2\n+\t// useKindRaxOp1RegOp2 is Op1 must be a register, Op2 can be any operand, and RAX is used.\n+\tuseKindRaxOp1RegOp2\n+\tuseKindDivRem\n+\tuseKindBlendvpd\n+\tuseKindCall\n+\tuseKindCallInd\n+\tuseKindFcvtToSintSequence\n+\tuseKindFcvtToUintSequence\n+)\n+\n+var useKinds = [instrMax]useKind{\n+\tnop0:                   useKindNone,\n+\tret:                    useKindNone,\n+\tmovRR:                  useKindOp1,\n+\tmovRM:                  useKindOp1RegOp2,\n+\txmmMovRM:               useKindOp1RegOp2,\n+\tcmove:                  useKindOp1Op2Reg,\n+\taluRmiR:                useKindOp1Op2Reg,\n+\tshiftR:                 useKindOp1Op2Reg,\n+\timm:                    useKindNone,\n+\tunaryRmR:               useKindOp1,\n+\txmmRmiReg:              useKindOp1Op2Reg,\n+\txmmUnaryRmR:            useKindOp1,\n+\txmmUnaryRmRImm:         useKindOp1,\n+\txmmCmpRmR:              useKindOp1Op2Reg,\n+\txmmRmR:                 useKindOp1Op2Reg,\n+\txmmRmRImm:              useKindOp1Op2Reg,\n+\tmov64MR:                useKindOp1,\n+\tmovzxRmR:               useKindOp1,\n+\tmovsxRmR:               useKindOp1,\n+\tgprToXmm:               useKindOp1,\n+\txmmToGpr:               useKindOp1,\n+\tcall:                   useKindCall,\n+\tcallIndirect:           useKindCallInd,\n+\tud2:                    useKindNone,\n+\tjmpIf:                  useKindOp1,\n+\tjmp:                    useKindOp1,\n+\tcmpRmiR:                useKindOp1Op2Reg,\n+\texitSequence:           useKindOp1,\n+\tlea:                    useKindOp1,\n+\tjmpTableIsland:         useKindNone,\n+\tsetcc:                  useKindNone,\n+\tzeros:                  useKindNone,\n+\tsourceOffsetInfo:       useKindNone,\n+\tfcvtToSintSequence:     useKindFcvtToSintSequence,\n+\tdefineUninitializedReg: useKindNone,\n+\tfcvtToUintSequence:     useKindFcvtToUintSequence,\n+\txmmCMov:                useKindOp1,\n+\tidivRemSequence:        useKindDivRem,\n+\tblendvpd:               useKindBlendvpd,\n+\tmfence:                 useKindNone,\n+\txchg:                   useKindOp1RegOp2,\n+\tlockcmpxchg:            useKindRaxOp1RegOp2,\n+\tlockxadd:               useKindOp1RegOp2,\n+\tneg:                    useKindOp1,\n+\tkeepAlive:              useKindOp1,\n+}\n+\n+func (u useKind) String() string {\n+\tswitch u {\n+\tcase useKindNone:\n+\t\treturn \"none\"\n+\tcase useKindOp1:\n+\t\treturn \"op1\"\n+\tcase useKindOp1Op2Reg:\n+\t\treturn \"op1op2Reg\"\n+\tcase useKindOp1RegOp2:\n+\t\treturn \"op1RegOp2\"\n+\tcase useKindCall:\n+\t\treturn \"call\"\n+\tcase useKindCallInd:\n+\t\treturn \"callInd\"\n+\tdefault:\n+\t\treturn \"invalid\"\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/instr_encoding.go",
          "status": "added",
          "additions": 1683,
          "deletions": 0,
          "patch": "@@ -0,0 +1,1683 @@\n+package amd64\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+func (i *instruction) encode(c backend.Compiler) (needsLabelResolution bool) {\n+\tswitch kind := i.kind; kind {\n+\tcase nop0, sourceOffsetInfo, defineUninitializedReg, fcvtToSintSequence, fcvtToUintSequence, keepAlive:\n+\tcase ret:\n+\t\tencodeRet(c)\n+\tcase imm:\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\tcon := i.u1\n+\t\tif i.b1 { // 64 bit.\n+\t\t\tif lower32willSignExtendTo64(con) {\n+\t\t\t\t// Sign extend mov(imm32).\n+\t\t\t\tencodeRegReg(c,\n+\t\t\t\t\tlegacyPrefixesNone,\n+\t\t\t\t\t0xc7, 1,\n+\t\t\t\t\t0,\n+\t\t\t\t\tdst,\n+\t\t\t\t\trexInfo(0).setW(),\n+\t\t\t\t)\n+\t\t\t\tc.Emit4Bytes(uint32(con))\n+\t\t\t} else {\n+\t\t\t\tc.EmitByte(rexEncodingW | dst.rexBit())\n+\t\t\t\tc.EmitByte(0xb8 | dst.encoding())\n+\t\t\t\tc.Emit8Bytes(con)\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif dst.rexBit() > 0 {\n+\t\t\t\tc.EmitByte(rexEncodingDefault | 0x1)\n+\t\t\t}\n+\t\t\tc.EmitByte(0xb8 | dst.encoding())\n+\t\t\tc.Emit4Bytes(uint32(con))\n+\t\t}\n+\n+\tcase aluRmiR:\n+\t\tvar rex rexInfo\n+\t\tif i.b1 {\n+\t\t\trex = rex.setW()\n+\t\t} else {\n+\t\t\trex = rex.clearW()\n+\t\t}\n+\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\n+\t\taluOp := aluRmiROpcode(i.u1)\n+\t\tif aluOp == aluRmiROpcodeMul {\n+\t\t\top1 := i.op1\n+\t\t\tconst regMemOpc, regMemOpcNum = 0x0FAF, 2\n+\t\t\tswitch op1.kind {\n+\t\t\tcase operandKindReg:\n+\t\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\t\tencodeRegReg(c, legacyPrefixesNone, regMemOpc, regMemOpcNum, dst, src, rex)\n+\t\t\tcase operandKindMem:\n+\t\t\t\tm := i.op1.addressMode()\n+\t\t\t\tencodeRegMem(c, legacyPrefixesNone, regMemOpc, regMemOpcNum, dst, m, rex)\n+\t\t\tcase operandKindImm32:\n+\t\t\t\timm8 := lower8willSignExtendTo32(op1.imm32())\n+\t\t\t\tvar opc uint32\n+\t\t\t\tif imm8 {\n+\t\t\t\t\topc = 0x6b\n+\t\t\t\t} else {\n+\t\t\t\t\topc = 0x69\n+\t\t\t\t}\n+\t\t\t\tencodeRegReg(c, legacyPrefixesNone, opc, 1, dst, dst, rex)\n+\t\t\t\tif imm8 {\n+\t\t\t\t\tc.EmitByte(byte(op1.imm32()))\n+\t\t\t\t} else {\n+\t\t\t\t\tc.Emit4Bytes(op1.imm32())\n+\t\t\t\t}\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t\t}\n+\t\t} else {\n+\t\t\tconst opcodeNum = 1\n+\t\t\tvar opcR, opcM, subOpcImm uint32\n+\t\t\tswitch aluOp {\n+\t\t\tcase aluRmiROpcodeAdd:\n+\t\t\t\topcR, opcM, subOpcImm = 0x01, 0x03, 0x0\n+\t\t\tcase aluRmiROpcodeSub:\n+\t\t\t\topcR, opcM, subOpcImm = 0x29, 0x2b, 0x5\n+\t\t\tcase aluRmiROpcodeAnd:\n+\t\t\t\topcR, opcM, subOpcImm = 0x21, 0x23, 0x4\n+\t\t\tcase aluRmiROpcodeOr:\n+\t\t\t\topcR, opcM, subOpcImm = 0x09, 0x0b, 0x1\n+\t\t\tcase aluRmiROpcodeXor:\n+\t\t\t\topcR, opcM, subOpcImm = 0x31, 0x33, 0x6\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"BUG: invalid aluRmiROpcode\")\n+\t\t\t}\n+\n+\t\t\top1 := i.op1\n+\t\t\tswitch op1.kind {\n+\t\t\tcase operandKindReg:\n+\t\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\t\tencodeRegReg(c, legacyPrefixesNone, opcR, opcodeNum, src, dst, rex)\n+\t\t\tcase operandKindMem:\n+\t\t\t\tm := i.op1.addressMode()\n+\t\t\t\tencodeRegMem(c, legacyPrefixesNone, opcM, opcodeNum, dst, m, rex)\n+\t\t\tcase operandKindImm32:\n+\t\t\t\timm8 := lower8willSignExtendTo32(op1.imm32())\n+\t\t\t\tvar opc uint32\n+\t\t\t\tif imm8 {\n+\t\t\t\t\topc = 0x83\n+\t\t\t\t} else {\n+\t\t\t\t\topc = 0x81\n+\t\t\t\t}\n+\t\t\t\tencodeRegReg(c, legacyPrefixesNone, opc, opcodeNum, regEnc(subOpcImm), dst, rex)\n+\t\t\t\tif imm8 {\n+\t\t\t\t\tc.EmitByte(byte(op1.imm32()))\n+\t\t\t\t} else {\n+\t\t\t\t\tc.Emit4Bytes(op1.imm32())\n+\t\t\t\t}\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t\t}\n+\t\t}\n+\n+\tcase movRR:\n+\t\tsrc := regEncodings[i.op1.reg().RealReg()]\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\tvar rex rexInfo\n+\t\tif i.b1 {\n+\t\t\trex = rex.setW()\n+\t\t} else {\n+\t\t\trex = rex.clearW()\n+\t\t}\n+\t\tencodeRegReg(c, legacyPrefixesNone, 0x89, 1, src, dst, rex)\n+\n+\tcase xmmRmR, blendvpd:\n+\t\top := sseOpcode(i.u1)\n+\t\tvar legPrex legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tvar opcodeNum uint32\n+\t\tswitch op {\n+\t\tcase sseOpcodeAddps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F58, 2\n+\t\tcase sseOpcodeAddpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F58, 2\n+\t\tcase sseOpcodeAddss:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F58, 2\n+\t\tcase sseOpcodeAddsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F58, 2\n+\t\tcase sseOpcodeAndps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F54, 2\n+\t\tcase sseOpcodeAndpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F54, 2\n+\t\tcase sseOpcodeAndnps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F55, 2\n+\t\tcase sseOpcodeAndnpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F55, 2\n+\t\tcase sseOpcodeBlendvps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3814, 3\n+\t\tcase sseOpcodeBlendvpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3815, 3\n+\t\tcase sseOpcodeDivps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F5E, 2\n+\t\tcase sseOpcodeDivpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F5E, 2\n+\t\tcase sseOpcodeDivss:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F5E, 2\n+\t\tcase sseOpcodeDivsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F5E, 2\n+\t\tcase sseOpcodeMaxps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F5F, 2\n+\t\tcase sseOpcodeMaxpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F5F, 2\n+\t\tcase sseOpcodeMaxss:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F5F, 2\n+\t\tcase sseOpcodeMaxsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F5F, 2\n+\t\tcase sseOpcodeMinps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F5D, 2\n+\t\tcase sseOpcodeMinpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F5D, 2\n+\t\tcase sseOpcodeMinss:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F5D, 2\n+\t\tcase sseOpcodeMinsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F5D, 2\n+\t\tcase sseOpcodeMovlhps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F16, 2\n+\t\tcase sseOpcodeMovsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F10, 2\n+\t\tcase sseOpcodeMulps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F59, 2\n+\t\tcase sseOpcodeMulpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F59, 2\n+\t\tcase sseOpcodeMulss:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F59, 2\n+\t\tcase sseOpcodeMulsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F59, 2\n+\t\tcase sseOpcodeOrpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F56, 2\n+\t\tcase sseOpcodeOrps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F56, 2\n+\t\tcase sseOpcodePackssdw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F6B, 2\n+\t\tcase sseOpcodePacksswb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F63, 2\n+\t\tcase sseOpcodePackusdw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F382B, 3\n+\t\tcase sseOpcodePackuswb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F67, 2\n+\t\tcase sseOpcodePaddb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FFC, 2\n+\t\tcase sseOpcodePaddd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FFE, 2\n+\t\tcase sseOpcodePaddq:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FD4, 2\n+\t\tcase sseOpcodePaddw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FFD, 2\n+\t\tcase sseOpcodePaddsb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FEC, 2\n+\t\tcase sseOpcodePaddsw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FED, 2\n+\t\tcase sseOpcodePaddusb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FDC, 2\n+\t\tcase sseOpcodePaddusw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FDD, 2\n+\t\tcase sseOpcodePand:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FDB, 2\n+\t\tcase sseOpcodePandn:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FDF, 2\n+\t\tcase sseOpcodePavgb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FE0, 2\n+\t\tcase sseOpcodePavgw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FE3, 2\n+\t\tcase sseOpcodePcmpeqb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F74, 2\n+\t\tcase sseOpcodePcmpeqw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F75, 2\n+\t\tcase sseOpcodePcmpeqd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F76, 2\n+\t\tcase sseOpcodePcmpeqq:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3829, 3\n+\t\tcase sseOpcodePcmpgtb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F64, 2\n+\t\tcase sseOpcodePcmpgtw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F65, 2\n+\t\tcase sseOpcodePcmpgtd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F66, 2\n+\t\tcase sseOpcodePcmpgtq:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3837, 3\n+\t\tcase sseOpcodePmaddwd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FF5, 2\n+\t\tcase sseOpcodePmaxsb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F383C, 3\n+\t\tcase sseOpcodePmaxsw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FEE, 2\n+\t\tcase sseOpcodePmaxsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F383D, 3\n+\t\tcase sseOpcodePmaxub:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FDE, 2\n+\t\tcase sseOpcodePmaxuw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F383E, 3\n+\t\tcase sseOpcodePmaxud:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F383F, 3\n+\t\tcase sseOpcodePminsb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3838, 3\n+\t\tcase sseOpcodePminsw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FEA, 2\n+\t\tcase sseOpcodePminsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3839, 3\n+\t\tcase sseOpcodePminub:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FDA, 2\n+\t\tcase sseOpcodePminuw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F383A, 3\n+\t\tcase sseOpcodePminud:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F383B, 3\n+\t\tcase sseOpcodePmulld:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3840, 3\n+\t\tcase sseOpcodePmullw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FD5, 2\n+\t\tcase sseOpcodePmuludq:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FF4, 2\n+\t\tcase sseOpcodePor:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FEB, 2\n+\t\tcase sseOpcodePshufb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3800, 3\n+\t\tcase sseOpcodePsubb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FF8, 2\n+\t\tcase sseOpcodePsubd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FFA, 2\n+\t\tcase sseOpcodePsubq:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FFB, 2\n+\t\tcase sseOpcodePsubw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FF9, 2\n+\t\tcase sseOpcodePsubsb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FE8, 2\n+\t\tcase sseOpcodePsubsw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FE9, 2\n+\t\tcase sseOpcodePsubusb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FD8, 2\n+\t\tcase sseOpcodePsubusw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FD9, 2\n+\t\tcase sseOpcodePunpckhbw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F68, 2\n+\t\tcase sseOpcodePunpcklbw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F60, 2\n+\t\tcase sseOpcodePxor:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FEF, 2\n+\t\tcase sseOpcodeSubps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F5C, 2\n+\t\tcase sseOpcodeSubpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F5C, 2\n+\t\tcase sseOpcodeSubss:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F5C, 2\n+\t\tcase sseOpcodeSubsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F5C, 2\n+\t\tcase sseOpcodeXorps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F57, 2\n+\t\tcase sseOpcodeXorpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F57, 2\n+\t\tcase sseOpcodePmulhrsw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F380B, 3\n+\t\tcase sseOpcodeUnpcklps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0F14, 2\n+\t\tcase sseOpcodePmaddubsw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3804, 3\n+\t\tdefault:\n+\t\t\tif kind == blendvpd {\n+\t\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3815, 3\n+\t\t\t} else {\n+\t\t\t\tpanic(fmt.Sprintf(\"Unsupported sseOpcode: %s\", op))\n+\t\t\t}\n+\t\t}\n+\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\n+\t\trex := rexInfo(0).clearW()\n+\t\top1 := i.op1\n+\t\tif op1.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\tencodeRegReg(c, legPrex, opcode, opcodeNum, dst, src, rex)\n+\t\t} else if i.op1.kind == operandKindMem {\n+\t\t\tm := i.op1.addressMode()\n+\t\t\tencodeRegMem(c, legPrex, opcode, opcodeNum, dst, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase gprToXmm:\n+\t\tvar legPrefix legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tconst opcodeNum = 2\n+\t\tswitch sseOpcode(i.u1) {\n+\t\tcase sseOpcodeMovd, sseOpcodeMovq:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0x66, 0x0f6e\n+\t\tcase sseOpcodeCvtsi2ss:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0xF3, 0x0f2a\n+\t\tcase sseOpcodeCvtsi2sd:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0xF2, 0x0f2a\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"Unsupported sseOpcode: %s\", sseOpcode(i.u1)))\n+\t\t}\n+\n+\t\tvar rex rexInfo\n+\t\tif i.b1 {\n+\t\t\trex = rex.setW()\n+\t\t} else {\n+\t\t\trex = rex.clearW()\n+\t\t}\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\n+\t\top1 := i.op1\n+\t\tif op1.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\tencodeRegReg(c, legPrefix, opcode, opcodeNum, dst, src, rex)\n+\t\t} else if i.op1.kind == operandKindMem {\n+\t\t\tm := i.op1.addressMode()\n+\t\t\tencodeRegMem(c, legPrefix, opcode, opcodeNum, dst, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase xmmUnaryRmR:\n+\t\tvar prefix legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tvar opcodeNum uint32\n+\t\top := sseOpcode(i.u1)\n+\t\tswitch op {\n+\t\tcase sseOpcodeCvtss2sd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F5A, 2\n+\t\tcase sseOpcodeCvtsd2ss:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F5A, 2\n+\t\tcase sseOpcodeMovaps:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0F28, 2\n+\t\tcase sseOpcodeMovapd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F28, 2\n+\t\tcase sseOpcodeMovdqa:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F6F, 2\n+\t\tcase sseOpcodeMovdqu:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F6F, 2\n+\t\tcase sseOpcodeMovsd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F10, 2\n+\t\tcase sseOpcodeMovss:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F10, 2\n+\t\tcase sseOpcodeMovups:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0F10, 2\n+\t\tcase sseOpcodeMovupd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F10, 2\n+\t\tcase sseOpcodePabsb:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F381C, 3\n+\t\tcase sseOpcodePabsw:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F381D, 3\n+\t\tcase sseOpcodePabsd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F381E, 3\n+\t\tcase sseOpcodePmovsxbd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3821, 3\n+\t\tcase sseOpcodePmovsxbw:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3820, 3\n+\t\tcase sseOpcodePmovsxbq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3822, 3\n+\t\tcase sseOpcodePmovsxwd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3823, 3\n+\t\tcase sseOpcodePmovsxwq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3824, 3\n+\t\tcase sseOpcodePmovsxdq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3825, 3\n+\t\tcase sseOpcodePmovzxbd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3831, 3\n+\t\tcase sseOpcodePmovzxbw:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3830, 3\n+\t\tcase sseOpcodePmovzxbq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3832, 3\n+\t\tcase sseOpcodePmovzxwd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3833, 3\n+\t\tcase sseOpcodePmovzxwq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3834, 3\n+\t\tcase sseOpcodePmovzxdq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3835, 3\n+\t\tcase sseOpcodeSqrtps:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0F51, 2\n+\t\tcase sseOpcodeSqrtpd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F51, 2\n+\t\tcase sseOpcodeSqrtss:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F51, 2\n+\t\tcase sseOpcodeSqrtsd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF2, 0x0F51, 2\n+\t\tcase sseOpcodeXorps:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0F57, 2\n+\t\tcase sseOpcodeXorpd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F57, 2\n+\t\tcase sseOpcodeCvtdq2ps:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0F5B, 2\n+\t\tcase sseOpcodeCvtdq2pd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0FE6, 2\n+\t\tcase sseOpcodeCvtps2pd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0F5A, 2\n+\t\tcase sseOpcodeCvtpd2ps:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0F5A, 2\n+\t\tcase sseOpcodeCvttps2dq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0F5B, 2\n+\t\tcase sseOpcodeCvttpd2dq:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0FE6, 2\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"Unsupported sseOpcode: %s\", op))\n+\t\t}\n+\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\n+\t\trex := rexInfo(0).clearW()\n+\t\top1 := i.op1\n+\t\tif op1.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\tencodeRegReg(c, prefix, opcode, opcodeNum, dst, src, rex)\n+\t\t} else if i.op1.kind == operandKindMem {\n+\t\t\tm := i.op1.addressMode()\n+\t\t\tneedsLabelResolution = encodeRegMem(c, prefix, opcode, opcodeNum, dst, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase xmmUnaryRmRImm:\n+\t\tvar prefix legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tvar opcodeNum uint32\n+\t\top := sseOpcode(i.u1)\n+\t\tswitch op {\n+\t\tcase sseOpcodeRoundps:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0f3a08, 3\n+\t\tcase sseOpcodeRoundss:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0f3a0a, 3\n+\t\tcase sseOpcodeRoundpd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0f3a09, 3\n+\t\tcase sseOpcodeRoundsd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0f3a0b, 3\n+\t\t}\n+\t\trex := rexInfo(0).clearW()\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\top1 := i.op1\n+\t\tif op1.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\tencodeRegReg(c, prefix, opcode, opcodeNum, dst, src, rex)\n+\t\t} else if i.op1.kind == operandKindMem {\n+\t\t\tm := i.op1.addressMode()\n+\t\t\tencodeRegMem(c, prefix, opcode, opcodeNum, dst, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\t\tc.EmitByte(byte(i.u2))\n+\n+\tcase unaryRmR:\n+\t\tvar prefix legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tvar opcodeNum uint32\n+\t\top := unaryRmROpcode(i.u1)\n+\t\t// We assume size is either 32 or 64.\n+\t\tswitch op {\n+\t\tcase unaryRmROpcodeBsr:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0fbd, 2\n+\t\tcase unaryRmROpcodeBsf:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0fbc, 2\n+\t\tcase unaryRmROpcodeLzcnt:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0fbd, 2\n+\t\tcase unaryRmROpcodeTzcnt:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0fbc, 2\n+\t\tcase unaryRmROpcodePopcnt:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0xF3, 0x0fb8, 2\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"Unsupported unaryRmROpcode: %s\", op))\n+\t\t}\n+\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\n+\t\trex := rexInfo(0)\n+\t\tif i.b1 { // 64 bit.\n+\t\t\trex = rexInfo(0).setW()\n+\t\t} else {\n+\t\t\trex = rexInfo(0).clearW()\n+\t\t}\n+\t\top1 := i.op1\n+\t\tif op1.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\tencodeRegReg(c, prefix, opcode, opcodeNum, dst, src, rex)\n+\t\t} else if i.op1.kind == operandKindMem {\n+\t\t\tm := i.op1.addressMode()\n+\t\t\tencodeRegMem(c, prefix, opcode, opcodeNum, dst, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase not:\n+\t\tvar prefix legacyPrefixes\n+\t\tsrc := regEncodings[i.op1.reg().RealReg()]\n+\t\trex := rexInfo(0)\n+\t\tif i.b1 { // 64 bit.\n+\t\t\trex = rexInfo(0).setW()\n+\t\t} else {\n+\t\t\trex = rexInfo(0).clearW()\n+\t\t}\n+\t\tsubopcode := uint8(2)\n+\t\tencodeEncEnc(c, prefix, 0xf7, 1, subopcode, uint8(src), rex)\n+\n+\tcase neg:\n+\t\tvar prefix legacyPrefixes\n+\t\tsrc := regEncodings[i.op1.reg().RealReg()]\n+\t\trex := rexInfo(0)\n+\t\tif i.b1 { // 64 bit.\n+\t\t\trex = rexInfo(0).setW()\n+\t\t} else {\n+\t\t\trex = rexInfo(0).clearW()\n+\t\t}\n+\t\tsubopcode := uint8(3)\n+\t\tencodeEncEnc(c, prefix, 0xf7, 1, subopcode, uint8(src), rex)\n+\n+\tcase div:\n+\t\trex := rexInfo(0)\n+\t\tif i.b1 { // 64 bit.\n+\t\t\trex = rexInfo(0).setW()\n+\t\t} else {\n+\t\t\trex = rexInfo(0).clearW()\n+\t\t}\n+\t\tvar subopcode uint8\n+\t\tif i.u1 != 0 { // Signed.\n+\t\t\tsubopcode = 7\n+\t\t} else {\n+\t\t\tsubopcode = 6\n+\t\t}\n+\n+\t\tdivisor := i.op1\n+\t\tif divisor.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[divisor.reg().RealReg()]\n+\t\t\tencodeEncEnc(c, legacyPrefixesNone, 0xf7, 1, subopcode, uint8(src), rex)\n+\t\t} else if divisor.kind == operandKindMem {\n+\t\t\tm := divisor.addressMode()\n+\t\t\tencodeEncMem(c, legacyPrefixesNone, 0xf7, 1, subopcode, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase mulHi:\n+\t\tvar prefix legacyPrefixes\n+\t\trex := rexInfo(0)\n+\t\tif i.b1 { // 64 bit.\n+\t\t\trex = rexInfo(0).setW()\n+\t\t} else {\n+\t\t\trex = rexInfo(0).clearW()\n+\t\t}\n+\n+\t\tsigned := i.u1 != 0\n+\t\tvar subopcode uint8\n+\t\tif signed {\n+\t\t\tsubopcode = 5\n+\t\t} else {\n+\t\t\tsubopcode = 4\n+\t\t}\n+\n+\t\t// src1 is implicitly rax,\n+\t\t// dst_lo is implicitly rax,\n+\t\t// dst_hi is implicitly rdx.\n+\t\tsrc2 := i.op1\n+\t\tif src2.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[src2.reg().RealReg()]\n+\t\t\tencodeEncEnc(c, prefix, 0xf7, 1, subopcode, uint8(src), rex)\n+\t\t} else if src2.kind == operandKindMem {\n+\t\t\tm := src2.addressMode()\n+\t\t\tencodeEncMem(c, prefix, 0xf7, 1, subopcode, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase signExtendData:\n+\t\tif i.b1 { // 64 bit.\n+\t\t\tc.EmitByte(0x48)\n+\t\t\tc.EmitByte(0x99)\n+\t\t} else {\n+\t\t\tc.EmitByte(0x99)\n+\t\t}\n+\tcase movzxRmR, movsxRmR:\n+\t\tsigned := i.kind == movsxRmR\n+\n+\t\text := extMode(i.u1)\n+\t\tvar opcode uint32\n+\t\tvar opcodeNum uint32\n+\t\tvar rex rexInfo\n+\t\tswitch ext {\n+\t\tcase extModeBL:\n+\t\t\tif signed {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fbe, 2, rex.clearW()\n+\t\t\t} else {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fb6, 2, rex.clearW()\n+\t\t\t}\n+\t\tcase extModeBQ:\n+\t\t\tif signed {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fbe, 2, rex.setW()\n+\t\t\t} else {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fb6, 2, rex.setW()\n+\t\t\t}\n+\t\tcase extModeWL:\n+\t\t\tif signed {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fbf, 2, rex.clearW()\n+\t\t\t} else {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fb7, 2, rex.clearW()\n+\t\t\t}\n+\t\tcase extModeWQ:\n+\t\t\tif signed {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fbf, 2, rex.setW()\n+\t\t\t} else {\n+\t\t\t\topcode, opcodeNum, rex = 0x0fb7, 2, rex.setW()\n+\t\t\t}\n+\t\tcase extModeLQ:\n+\t\t\tif signed {\n+\t\t\t\topcode, opcodeNum, rex = 0x63, 1, rex.setW()\n+\t\t\t} else {\n+\t\t\t\topcode, opcodeNum, rex = 0x8b, 1, rex.clearW()\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid extMode\")\n+\t\t}\n+\n+\t\top := i.op1\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\tsrc := regEncodings[op.reg().RealReg()]\n+\t\t\tif ext == extModeBL || ext == extModeBQ {\n+\t\t\t\t// Some destinations must be encoded with REX.R = 1.\n+\t\t\t\tif e := src.encoding(); e >= 4 && e <= 7 {\n+\t\t\t\t\trex = rex.always()\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tencodeRegReg(c, legacyPrefixesNone, opcode, opcodeNum, dst, src, rex)\n+\t\tcase operandKindMem:\n+\t\t\tm := op.addressMode()\n+\t\t\tencodeRegMem(c, legacyPrefixesNone, opcode, opcodeNum, dst, m, rex)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase mov64MR:\n+\t\tm := i.op1.addressMode()\n+\t\tencodeLoad64(c, m, i.op2.reg().RealReg())\n+\n+\tcase lea:\n+\t\tneedsLabelResolution = true\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\trex := rexInfo(0).setW()\n+\t\tconst opcode, opcodeNum = 0x8d, 1\n+\t\tswitch i.op1.kind {\n+\t\tcase operandKindMem:\n+\t\t\ta := i.op1.addressMode()\n+\t\t\tencodeRegMem(c, legacyPrefixesNone, opcode, opcodeNum, dst, a, rex)\n+\t\tcase operandKindLabel:\n+\t\t\trex.encode(c, regRexBit(byte(dst)), 0)\n+\t\t\tc.EmitByte(byte((opcode) & 0xff))\n+\n+\t\t\t// Indicate \"LEAQ [RIP + 32bit displacement].\n+\t\t\t// https://wiki.osdev.org/X86-64_Instruction_Encoding#32.2F64-bit_addressing\n+\t\t\tc.EmitByte(encodeModRM(0b00, dst.encoding(), 0b101))\n+\n+\t\t\t// This will be resolved later, so we just emit a placeholder (0xffffffff for testing).\n+\t\t\tc.Emit4Bytes(0xffffffff)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase movRM:\n+\t\tm := i.op2.addressMode()\n+\t\tsrc := regEncodings[i.op1.reg().RealReg()]\n+\n+\t\tvar rex rexInfo\n+\t\tswitch i.u1 {\n+\t\tcase 1:\n+\t\t\tif e := src.encoding(); e >= 4 && e <= 7 {\n+\t\t\t\trex = rex.always()\n+\t\t\t}\n+\t\t\tencodeRegMem(c, legacyPrefixesNone, 0x88, 1, src, m, rex.clearW())\n+\t\tcase 2:\n+\t\t\tencodeRegMem(c, legacyPrefixes0x66, 0x89, 1, src, m, rex.clearW())\n+\t\tcase 4:\n+\t\t\tencodeRegMem(c, legacyPrefixesNone, 0x89, 1, src, m, rex.clearW())\n+\t\tcase 8:\n+\t\t\tencodeRegMem(c, legacyPrefixesNone, 0x89, 1, src, m, rex.setW())\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid size %d: %s\", i.u1, i.String()))\n+\t\t}\n+\n+\tcase shiftR:\n+\t\tsrc := regEncodings[i.op2.reg().RealReg()]\n+\t\tamount := i.op1\n+\n+\t\tvar opcode uint32\n+\t\tvar prefix legacyPrefixes\n+\t\trex := rexInfo(0)\n+\t\tif i.b1 { // 64 bit.\n+\t\t\trex = rexInfo(0).setW()\n+\t\t} else {\n+\t\t\trex = rexInfo(0).clearW()\n+\t\t}\n+\n+\t\tswitch amount.kind {\n+\t\tcase operandKindReg:\n+\t\t\tif amount.reg() != rcxVReg {\n+\t\t\t\tpanic(\"BUG: invalid reg operand: must be rcx\")\n+\t\t\t}\n+\t\t\topcode, prefix = 0xd3, legacyPrefixesNone\n+\t\t\tencodeEncEnc(c, prefix, opcode, 1, uint8(i.u1), uint8(src), rex)\n+\t\tcase operandKindImm32:\n+\t\t\topcode, prefix = 0xc1, legacyPrefixesNone\n+\t\t\tencodeEncEnc(c, prefix, opcode, 1, uint8(i.u1), uint8(src), rex)\n+\t\t\tc.EmitByte(byte(amount.imm32()))\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\tcase xmmRmiReg:\n+\t\tconst legPrefix = legacyPrefixes0x66\n+\t\trex := rexInfo(0).clearW()\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\n+\t\tvar opcode uint32\n+\t\tvar regDigit uint8\n+\n+\t\top := sseOpcode(i.u1)\n+\t\top1 := i.op1\n+\t\tif i.op1.kind == operandKindImm32 {\n+\t\t\tswitch op {\n+\t\t\tcase sseOpcodePsllw:\n+\t\t\t\topcode, regDigit = 0x0f71, 6\n+\t\t\tcase sseOpcodePslld:\n+\t\t\t\topcode, regDigit = 0x0f72, 6\n+\t\t\tcase sseOpcodePsllq:\n+\t\t\t\topcode, regDigit = 0x0f73, 6\n+\t\t\tcase sseOpcodePsraw:\n+\t\t\t\topcode, regDigit = 0x0f71, 4\n+\t\t\tcase sseOpcodePsrad:\n+\t\t\t\topcode, regDigit = 0x0f72, 4\n+\t\t\tcase sseOpcodePsrlw:\n+\t\t\t\topcode, regDigit = 0x0f71, 2\n+\t\t\tcase sseOpcodePsrld:\n+\t\t\t\topcode, regDigit = 0x0f72, 2\n+\t\t\tcase sseOpcodePsrlq:\n+\t\t\t\topcode, regDigit = 0x0f73, 2\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"invalid opcode\")\n+\t\t\t}\n+\n+\t\t\tencodeEncEnc(c, legPrefix, opcode, 2, regDigit, uint8(dst), rex)\n+\t\t\timm32 := op1.imm32()\n+\t\t\tif imm32 > 0xff&imm32 {\n+\t\t\t\tpanic(\"immediate value does not fit 1 byte\")\n+\t\t\t}\n+\t\t\tc.EmitByte(uint8(imm32))\n+\t\t} else {\n+\t\t\tswitch op {\n+\t\t\tcase sseOpcodePsllw:\n+\t\t\t\topcode = 0x0ff1\n+\t\t\tcase sseOpcodePslld:\n+\t\t\t\topcode = 0x0ff2\n+\t\t\tcase sseOpcodePsllq:\n+\t\t\t\topcode = 0x0ff3\n+\t\t\tcase sseOpcodePsraw:\n+\t\t\t\topcode = 0x0fe1\n+\t\t\tcase sseOpcodePsrad:\n+\t\t\t\topcode = 0x0fe2\n+\t\t\tcase sseOpcodePsrlw:\n+\t\t\t\topcode = 0x0fd1\n+\t\t\tcase sseOpcodePsrld:\n+\t\t\t\topcode = 0x0fd2\n+\t\t\tcase sseOpcodePsrlq:\n+\t\t\t\topcode = 0x0fd3\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"invalid opcode\")\n+\t\t\t}\n+\n+\t\t\tif op1.kind == operandKindReg {\n+\t\t\t\treg := regEncodings[op1.reg().RealReg()]\n+\t\t\t\tencodeRegReg(c, legPrefix, opcode, 2, dst, reg, rex)\n+\t\t\t} else if op1.kind == operandKindMem {\n+\t\t\t\tm := op1.addressMode()\n+\t\t\t\tencodeRegMem(c, legPrefix, opcode, 2, dst, m, rex)\n+\t\t\t} else {\n+\t\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t\t}\n+\t\t}\n+\n+\tcase cmpRmiR:\n+\t\tvar opcode uint32\n+\t\tisCmp := i.u1 != 0\n+\t\trex := rexInfo(0)\n+\t\t_64 := i.b1\n+\t\tif _64 { // 64 bit.\n+\t\t\trex = rex.setW()\n+\t\t} else {\n+\t\t\trex = rex.clearW()\n+\t\t}\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\top1 := i.op1\n+\t\tswitch op1.kind {\n+\t\tcase operandKindReg:\n+\t\t\treg := regEncodings[op1.reg().RealReg()]\n+\t\t\tif isCmp {\n+\t\t\t\topcode = 0x39\n+\t\t\t} else {\n+\t\t\t\topcode = 0x85\n+\t\t\t}\n+\t\t\t// Here we swap the encoding of the operands for CMP to be consistent with the output of LLVM/GCC.\n+\t\t\tencodeRegReg(c, legacyPrefixesNone, opcode, 1, reg, dst, rex)\n+\n+\t\tcase operandKindMem:\n+\t\t\tif isCmp {\n+\t\t\t\topcode = 0x3b\n+\t\t\t} else {\n+\t\t\t\topcode = 0x85\n+\t\t\t}\n+\t\t\tm := op1.addressMode()\n+\t\t\tencodeRegMem(c, legacyPrefixesNone, opcode, 1, dst, m, rex)\n+\n+\t\tcase operandKindImm32:\n+\t\t\timm32 := op1.imm32()\n+\t\t\tuseImm8 := isCmp && lower8willSignExtendTo32(imm32)\n+\t\t\tvar subopcode uint8\n+\n+\t\t\tswitch {\n+\t\t\tcase isCmp && useImm8:\n+\t\t\t\topcode, subopcode = 0x83, 7\n+\t\t\tcase isCmp && !useImm8:\n+\t\t\t\topcode, subopcode = 0x81, 7\n+\t\t\tdefault:\n+\t\t\t\topcode, subopcode = 0xf7, 0\n+\t\t\t}\n+\t\t\tencodeEncEnc(c, legacyPrefixesNone, opcode, 1, subopcode, uint8(dst), rex)\n+\t\t\tif useImm8 {\n+\t\t\t\tc.EmitByte(uint8(imm32))\n+\t\t\t} else {\n+\t\t\t\tc.Emit4Bytes(imm32)\n+\t\t\t}\n+\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\tcase setcc:\n+\t\tcc := cond(i.u1)\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\trex := rexInfo(0).clearW().always()\n+\t\topcode := uint32(0x0f90) + uint32(cc)\n+\t\tencodeEncEnc(c, legacyPrefixesNone, opcode, 2, 0, uint8(dst), rex)\n+\tcase cmove:\n+\t\tcc := cond(i.u1)\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\trex := rexInfo(0)\n+\t\tif i.b1 { // 64 bit.\n+\t\t\trex = rex.setW()\n+\t\t} else {\n+\t\t\trex = rex.clearW()\n+\t\t}\n+\t\topcode := uint32(0x0f40) + uint32(cc)\n+\t\tsrc := i.op1\n+\t\tswitch src.kind {\n+\t\tcase operandKindReg:\n+\t\t\tsrcReg := regEncodings[src.reg().RealReg()]\n+\t\t\tencodeRegReg(c, legacyPrefixesNone, opcode, 2, dst, srcReg, rex)\n+\t\tcase operandKindMem:\n+\t\t\tm := src.addressMode()\n+\t\t\tencodeRegMem(c, legacyPrefixesNone, opcode, 2, dst, m, rex)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\tcase push64:\n+\t\top := i.op1\n+\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\tdst := regEncodings[op.reg().RealReg()]\n+\t\t\tif dst.rexBit() > 0 {\n+\t\t\t\tc.EmitByte(rexEncodingDefault | 0x1)\n+\t\t\t}\n+\t\t\tc.EmitByte(0x50 | dst.encoding())\n+\t\tcase operandKindMem:\n+\t\t\tm := op.addressMode()\n+\t\t\tencodeRegMem(\n+\t\t\t\tc, legacyPrefixesNone, 0xff, 1, regEnc(6), m, rexInfo(0).clearW(),\n+\t\t\t)\n+\t\tcase operandKindImm32:\n+\t\t\tc.EmitByte(0x68)\n+\t\t\tc.Emit4Bytes(op.imm32())\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase pop64:\n+\t\tdst := regEncodings[i.op1.reg().RealReg()]\n+\t\tif dst.rexBit() > 0 {\n+\t\t\tc.EmitByte(rexEncodingDefault | 0x1)\n+\t\t}\n+\t\tc.EmitByte(0x58 | dst.encoding())\n+\n+\tcase xmmMovRM:\n+\t\tvar legPrefix legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tconst opcodeNum = 2\n+\t\tswitch sseOpcode(i.u1) {\n+\t\tcase sseOpcodeMovaps:\n+\t\t\tlegPrefix, opcode = legacyPrefixesNone, 0x0f29\n+\t\tcase sseOpcodeMovapd:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0x66, 0x0f29\n+\t\tcase sseOpcodeMovdqa:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0x66, 0x0f7f\n+\t\tcase sseOpcodeMovdqu:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0xF3, 0x0f7f\n+\t\tcase sseOpcodeMovss:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0xF3, 0x0f11\n+\t\tcase sseOpcodeMovsd:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0xF2, 0x0f11\n+\t\tcase sseOpcodeMovups:\n+\t\t\tlegPrefix, opcode = legacyPrefixesNone, 0x0f11\n+\t\tcase sseOpcodeMovupd:\n+\t\t\tlegPrefix, opcode = legacyPrefixes0x66, 0x0f11\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"Unsupported sseOpcode: %s\", sseOpcode(i.u1)))\n+\t\t}\n+\n+\t\tdst := regEncodings[i.op1.reg().RealReg()]\n+\t\tencodeRegMem(c, legPrefix, opcode, opcodeNum, dst, i.op2.addressMode(), rexInfo(0).clearW())\n+\tcase xmmLoadConst:\n+\t\tpanic(\"TODO\")\n+\tcase xmmToGpr:\n+\t\tvar legPrefix legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tvar argSwap bool\n+\t\tconst opcodeNum = 2\n+\t\tswitch sseOpcode(i.u1) {\n+\t\tcase sseOpcodeMovd, sseOpcodeMovq:\n+\t\t\tlegPrefix, opcode, argSwap = legacyPrefixes0x66, 0x0f7e, false\n+\t\tcase sseOpcodeMovmskps:\n+\t\t\tlegPrefix, opcode, argSwap = legacyPrefixesNone, 0x0f50, true\n+\t\tcase sseOpcodeMovmskpd:\n+\t\t\tlegPrefix, opcode, argSwap = legacyPrefixes0x66, 0x0f50, true\n+\t\tcase sseOpcodePmovmskb:\n+\t\t\tlegPrefix, opcode, argSwap = legacyPrefixes0x66, 0x0fd7, true\n+\t\tcase sseOpcodeCvttss2si:\n+\t\t\tlegPrefix, opcode, argSwap = legacyPrefixes0xF3, 0x0f2c, true\n+\t\tcase sseOpcodeCvttsd2si:\n+\t\t\tlegPrefix, opcode, argSwap = legacyPrefixes0xF2, 0x0f2c, true\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"Unsupported sseOpcode: %s\", sseOpcode(i.u1)))\n+\t\t}\n+\n+\t\tvar rex rexInfo\n+\t\tif i.b1 {\n+\t\t\trex = rex.setW()\n+\t\t} else {\n+\t\t\trex = rex.clearW()\n+\t\t}\n+\t\tsrc := regEncodings[i.op1.reg().RealReg()]\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\tif argSwap {\n+\t\t\tsrc, dst = dst, src\n+\t\t}\n+\t\tencodeRegReg(c, legPrefix, opcode, opcodeNum, src, dst, rex)\n+\n+\tcase cvtUint64ToFloatSeq:\n+\t\tpanic(\"TODO\")\n+\tcase cvtFloatToSintSeq:\n+\t\tpanic(\"TODO\")\n+\tcase cvtFloatToUintSeq:\n+\t\tpanic(\"TODO\")\n+\tcase xmmMinMaxSeq:\n+\t\tpanic(\"TODO\")\n+\tcase xmmCmpRmR:\n+\t\tvar prefix legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tvar opcodeNum uint32\n+\t\trex := rexInfo(0)\n+\t\t_64 := i.b1\n+\t\tif _64 { // 64 bit.\n+\t\t\trex = rex.setW()\n+\t\t} else {\n+\t\t\trex = rex.clearW()\n+\t\t}\n+\n+\t\top := sseOpcode(i.u1)\n+\t\tswitch op {\n+\t\tcase sseOpcodePtest:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0f3817, 3\n+\t\tcase sseOpcodeUcomisd:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixes0x66, 0x0f2e, 2\n+\t\tcase sseOpcodeUcomiss:\n+\t\t\tprefix, opcode, opcodeNum = legacyPrefixesNone, 0x0f2e, 2\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"Unsupported sseOpcode: %s\", op))\n+\t\t}\n+\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\t\top1 := i.op1\n+\t\tswitch op1.kind {\n+\t\tcase operandKindReg:\n+\t\t\treg := regEncodings[op1.reg().RealReg()]\n+\t\t\tencodeRegReg(c, prefix, opcode, opcodeNum, dst, reg, rex)\n+\n+\t\tcase operandKindMem:\n+\t\t\tm := op1.addressMode()\n+\t\t\tencodeRegMem(c, prefix, opcode, opcodeNum, dst, m, rex)\n+\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\tcase xmmRmRImm:\n+\t\top := sseOpcode(i.u1)\n+\t\tvar legPrex legacyPrefixes\n+\t\tvar opcode uint32\n+\t\tvar opcodeNum uint32\n+\t\tvar swap bool\n+\t\tswitch op {\n+\t\tcase sseOpcodeCmpps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0FC2, 2\n+\t\tcase sseOpcodeCmppd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FC2, 2\n+\t\tcase sseOpcodeCmpss:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF3, 0x0FC2, 2\n+\t\tcase sseOpcodeCmpsd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0xF2, 0x0FC2, 2\n+\t\tcase sseOpcodeInsertps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A21, 3\n+\t\tcase sseOpcodePalignr:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A0F, 3\n+\t\tcase sseOpcodePinsrb:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A20, 3\n+\t\tcase sseOpcodePinsrw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FC4, 2\n+\t\tcase sseOpcodePinsrd, sseOpcodePinsrq:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A22, 3\n+\t\tcase sseOpcodePextrb:\n+\t\t\tswap = true\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A14, 3\n+\t\tcase sseOpcodePextrw:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0FC5, 2\n+\t\tcase sseOpcodePextrd, sseOpcodePextrq:\n+\t\t\tswap = true\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A16, 3\n+\t\tcase sseOpcodePshufd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F70, 2\n+\t\tcase sseOpcodeRoundps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A08, 3\n+\t\tcase sseOpcodeRoundpd:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixes0x66, 0x0F3A09, 3\n+\t\tcase sseOpcodeShufps:\n+\t\t\tlegPrex, opcode, opcodeNum = legacyPrefixesNone, 0x0FC6, 2\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"Unsupported sseOpcode: %s\", op))\n+\t\t}\n+\n+\t\tdst := regEncodings[i.op2.reg().RealReg()]\n+\n+\t\tvar rex rexInfo\n+\t\tif op == sseOpcodePextrq || op == sseOpcodePinsrq {\n+\t\t\trex = rexInfo(0).setW()\n+\t\t} else {\n+\t\t\trex = rexInfo(0).clearW()\n+\t\t}\n+\t\top1 := i.op1\n+\t\tif op1.kind == operandKindReg {\n+\t\t\tsrc := regEncodings[op1.reg().RealReg()]\n+\t\t\tif swap {\n+\t\t\t\tsrc, dst = dst, src\n+\t\t\t}\n+\t\t\tencodeRegReg(c, legPrex, opcode, opcodeNum, dst, src, rex)\n+\t\t} else if i.op1.kind == operandKindMem {\n+\t\t\tif swap {\n+\t\t\t\tpanic(\"BUG: this is not possible to encode\")\n+\t\t\t}\n+\t\t\tm := i.op1.addressMode()\n+\t\t\tencodeRegMem(c, legPrex, opcode, opcodeNum, dst, m, rex)\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\t\tc.EmitByte(byte(i.u2))\n+\n+\tcase jmp:\n+\t\tconst (\n+\t\t\tregMemOpcode    = 0xff\n+\t\t\tregMemOpcodeNum = 1\n+\t\t\tregMemSubOpcode = 4\n+\t\t)\n+\t\top := i.op1\n+\t\tswitch op.kind {\n+\t\tcase operandKindLabel:\n+\t\t\tneedsLabelResolution = true\n+\t\t\tfallthrough\n+\t\tcase operandKindImm32:\n+\t\t\tc.EmitByte(0xe9)\n+\t\t\tc.Emit4Bytes(op.imm32())\n+\t\tcase operandKindMem:\n+\t\t\tm := op.addressMode()\n+\t\t\tencodeRegMem(c,\n+\t\t\t\tlegacyPrefixesNone,\n+\t\t\t\tregMemOpcode, regMemOpcodeNum,\n+\t\t\t\tregMemSubOpcode, m, rexInfo(0).clearW(),\n+\t\t\t)\n+\t\tcase operandKindReg:\n+\t\t\tr := op.reg().RealReg()\n+\t\t\tencodeRegReg(\n+\t\t\t\tc,\n+\t\t\t\tlegacyPrefixesNone,\n+\t\t\t\tregMemOpcode, regMemOpcodeNum,\n+\t\t\t\tregMemSubOpcode,\n+\t\t\t\tregEncodings[r], rexInfo(0).clearW(),\n+\t\t\t)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase jmpIf:\n+\t\top := i.op1\n+\t\tswitch op.kind {\n+\t\tcase operandKindLabel:\n+\t\t\tneedsLabelResolution = true\n+\t\t\tfallthrough\n+\t\tcase operandKindImm32:\n+\t\t\tc.EmitByte(0x0f)\n+\t\t\tc.EmitByte(0x80 | cond(i.u1).encoding())\n+\t\t\tc.Emit4Bytes(op.imm32())\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase jmpTableIsland:\n+\t\tneedsLabelResolution = true\n+\t\tfor tc := uint64(0); tc < i.u2; tc++ {\n+\t\t\tc.Emit8Bytes(0)\n+\t\t}\n+\n+\tcase exitSequence:\n+\t\texecCtx := i.op1.reg()\n+\t\tallocatedAmode := i.op2.addressMode()\n+\n+\t\t// Restore the RBP, RSP, and return to the Go code:\n+\t\t*allocatedAmode = amode{\n+\t\t\tkindWithShift: uint32(amodeImmReg), base: execCtx,\n+\t\t\timm32: wazevoapi.ExecutionContextOffsetOriginalFramePointer.U32(),\n+\t\t}\n+\t\tencodeLoad64(c, allocatedAmode, rbp)\n+\t\tallocatedAmode.imm32 = wazevoapi.ExecutionContextOffsetOriginalStackPointer.U32()\n+\t\tencodeLoad64(c, allocatedAmode, rsp)\n+\t\tencodeRet(c)\n+\n+\tcase ud2:\n+\t\tc.EmitByte(0x0f)\n+\t\tc.EmitByte(0x0b)\n+\n+\tcase call:\n+\t\tc.EmitByte(0xe8)\n+\t\t// Meaning that the call target is a function value, and requires relocation.\n+\t\tc.AddRelocationInfo(ssa.FuncRef(i.u1))\n+\t\t// Note that this is zero as a placeholder for the call target if it's a function value.\n+\t\tc.Emit4Bytes(uint32(i.u2))\n+\n+\tcase callIndirect:\n+\t\top := i.op1\n+\n+\t\tconst opcodeNum = 1\n+\t\tconst opcode = 0xff\n+\t\trex := rexInfo(0).clearW()\n+\t\tswitch op.kind {\n+\t\tcase operandKindReg:\n+\t\t\tdst := regEncodings[op.reg().RealReg()]\n+\t\t\tencodeRegReg(c,\n+\t\t\t\tlegacyPrefixesNone,\n+\t\t\t\topcode, opcodeNum,\n+\t\t\t\tregEnc(2),\n+\t\t\t\tdst,\n+\t\t\t\trex,\n+\t\t\t)\n+\t\tcase operandKindMem:\n+\t\t\tm := op.addressMode()\n+\t\t\tencodeRegMem(c,\n+\t\t\t\tlegacyPrefixesNone,\n+\t\t\t\topcode, opcodeNum,\n+\t\t\t\tregEnc(2),\n+\t\t\t\tm,\n+\t\t\t\trex,\n+\t\t\t)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase xchg:\n+\t\tsrc, dst := regEncodings[i.op1.reg().RealReg()], i.op2\n+\t\tsize := i.u1\n+\n+\t\tvar rex rexInfo\n+\t\tvar opcode uint32\n+\t\tlp := legacyPrefixesNone\n+\t\tswitch size {\n+\t\tcase 8:\n+\t\t\topcode = 0x87\n+\t\t\trex = rexInfo(0).setW()\n+\t\tcase 4:\n+\t\t\topcode = 0x87\n+\t\t\trex = rexInfo(0).clearW()\n+\t\tcase 2:\n+\t\t\tlp = legacyPrefixes0x66\n+\t\t\topcode = 0x87\n+\t\t\trex = rexInfo(0).clearW()\n+\t\tcase 1:\n+\t\t\topcode = 0x86\n+\t\t\tif i.op2.kind == operandKindReg {\n+\t\t\t\tpanic(\"TODO?: xchg on two 1-byte registers\")\n+\t\t\t}\n+\t\t\t// Some destinations must be encoded with REX.R = 1.\n+\t\t\tif e := src.encoding(); e >= 4 && e <= 7 {\n+\t\t\t\trex = rexInfo(0).always()\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid size %d: %s\", size, i.String()))\n+\t\t}\n+\n+\t\tswitch dst.kind {\n+\t\tcase operandKindMem:\n+\t\t\tm := dst.addressMode()\n+\t\t\tencodeRegMem(c, lp, opcode, 1, src, m, rex)\n+\t\tcase operandKindReg:\n+\t\t\tr := dst.reg().RealReg()\n+\t\t\tencodeRegReg(c, lp, opcode, 1, src, regEncodings[r], rex)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase lockcmpxchg:\n+\t\tsrc, dst := regEncodings[i.op1.reg().RealReg()], i.op2\n+\t\tsize := i.u1\n+\n+\t\tvar rex rexInfo\n+\t\tvar opcode uint32\n+\t\tlp := legacyPrefixes0xF0 // Lock prefix.\n+\t\tswitch size {\n+\t\tcase 8:\n+\t\t\topcode = 0x0FB1\n+\t\t\trex = rexInfo(0).setW()\n+\t\tcase 4:\n+\t\t\topcode = 0x0FB1\n+\t\t\trex = rexInfo(0).clearW()\n+\t\tcase 2:\n+\t\t\tlp = legacyPrefixes0x660xF0 // Legacy prefix + Lock prefix.\n+\t\t\topcode = 0x0FB1\n+\t\t\trex = rexInfo(0).clearW()\n+\t\tcase 1:\n+\t\t\topcode = 0x0FB0\n+\t\t\t// Some destinations must be encoded with REX.R = 1.\n+\t\t\tif e := src.encoding(); e >= 4 && e <= 7 {\n+\t\t\t\trex = rexInfo(0).always()\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid size %d: %s\", size, i.String()))\n+\t\t}\n+\n+\t\tswitch dst.kind {\n+\t\tcase operandKindMem:\n+\t\t\tm := dst.addressMode()\n+\t\t\tencodeRegMem(c, lp, opcode, 2, src, m, rex)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase lockxadd:\n+\t\tsrc, dst := regEncodings[i.op1.reg().RealReg()], i.op2\n+\t\tsize := i.u1\n+\n+\t\tvar rex rexInfo\n+\t\tvar opcode uint32\n+\t\tlp := legacyPrefixes0xF0 // Lock prefix.\n+\t\tswitch size {\n+\t\tcase 8:\n+\t\t\topcode = 0x0FC1\n+\t\t\trex = rexInfo(0).setW()\n+\t\tcase 4:\n+\t\t\topcode = 0x0FC1\n+\t\t\trex = rexInfo(0).clearW()\n+\t\tcase 2:\n+\t\t\tlp = legacyPrefixes0x660xF0 // Legacy prefix + Lock prefix.\n+\t\t\topcode = 0x0FC1\n+\t\t\trex = rexInfo(0).clearW()\n+\t\tcase 1:\n+\t\t\topcode = 0x0FC0\n+\t\t\t// Some destinations must be encoded with REX.R = 1.\n+\t\t\tif e := src.encoding(); e >= 4 && e <= 7 {\n+\t\t\t\trex = rexInfo(0).always()\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tpanic(fmt.Sprintf(\"BUG: invalid size %d: %s\", size, i.String()))\n+\t\t}\n+\n+\t\tswitch dst.kind {\n+\t\tcase operandKindMem:\n+\t\t\tm := dst.addressMode()\n+\t\t\tencodeRegMem(c, lp, opcode, 2, src, m, rex)\n+\t\tdefault:\n+\t\t\tpanic(\"BUG: invalid operand kind\")\n+\t\t}\n+\n+\tcase zeros:\n+\t\tr := i.op2.reg()\n+\t\tif r.RegType() == regalloc.RegTypeInt {\n+\t\t\ti.asAluRmiR(aluRmiROpcodeXor, newOperandReg(r), r, true)\n+\t\t} else {\n+\t\t\ti.asXmmRmR(sseOpcodePxor, newOperandReg(r), r)\n+\t\t}\n+\t\ti.encode(c)\n+\n+\tcase mfence:\n+\t\t// https://www.felixcloutier.com/x86/mfence\n+\t\tc.EmitByte(0x0f)\n+\t\tc.EmitByte(0xae)\n+\t\tc.EmitByte(0xf0)\n+\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"TODO: %v\", i.kind))\n+\t}\n+\treturn\n+}\n+\n+func encodeLoad64(c backend.Compiler, m *amode, rd regalloc.RealReg) {\n+\tdst := regEncodings[rd]\n+\tencodeRegMem(c, legacyPrefixesNone, 0x8b, 1, dst, m, rexInfo(0).setW())\n+}\n+\n+func encodeRet(c backend.Compiler) {\n+\tc.EmitByte(0xc3)\n+}\n+\n+func encodeEncEnc(\n+\tc backend.Compiler,\n+\tlegPrefixes legacyPrefixes,\n+\topcodes uint32,\n+\topcodeNum uint32,\n+\tr uint8,\n+\trm uint8,\n+\trex rexInfo,\n+) {\n+\tlegPrefixes.encode(c)\n+\trex.encode(c, r>>3, rm>>3)\n+\n+\tfor opcodeNum > 0 {\n+\t\topcodeNum--\n+\t\tc.EmitByte(byte((opcodes >> (opcodeNum << 3)) & 0xff))\n+\t}\n+\tc.EmitByte(encodeModRM(3, r&7, rm&7))\n+}\n+\n+func encodeRegReg(\n+\tc backend.Compiler,\n+\tlegPrefixes legacyPrefixes,\n+\topcodes uint32,\n+\topcodeNum uint32,\n+\tr regEnc,\n+\trm regEnc,\n+\trex rexInfo,\n+) {\n+\tencodeEncEnc(c, legPrefixes, opcodes, opcodeNum, uint8(r), uint8(rm), rex)\n+}\n+\n+func encodeModRM(mod byte, reg byte, rm byte) byte {\n+\treturn mod<<6 | reg<<3 | rm\n+}\n+\n+func encodeSIB(shift byte, encIndex byte, encBase byte) byte {\n+\treturn shift<<6 | encIndex<<3 | encBase\n+}\n+\n+func encodeRegMem(\n+\tc backend.Compiler, legPrefixes legacyPrefixes, opcodes uint32, opcodeNum uint32, r regEnc, m *amode, rex rexInfo,\n+) (needsLabelResolution bool) {\n+\tneedsLabelResolution = encodeEncMem(c, legPrefixes, opcodes, opcodeNum, uint8(r), m, rex)\n+\treturn\n+}\n+\n+func encodeEncMem(\n+\tc backend.Compiler, legPrefixes legacyPrefixes, opcodes uint32, opcodeNum uint32, r uint8, m *amode, rex rexInfo,\n+) (needsLabelResolution bool) {\n+\tlegPrefixes.encode(c)\n+\n+\tconst (\n+\t\tmodNoDisplacement    = 0b00\n+\t\tmodShortDisplacement = 0b01\n+\t\tmodLongDisplacement  = 0b10\n+\n+\t\tuseSBI = 4 // the encoding of rsp or r12 register.\n+\t)\n+\n+\tswitch m.kind() {\n+\tcase amodeImmReg, amodeImmRBP:\n+\t\tbase := m.base.RealReg()\n+\t\tbaseEnc := regEncodings[base]\n+\n+\t\trex.encode(c, regRexBit(r), baseEnc.rexBit())\n+\n+\t\tfor opcodeNum > 0 {\n+\t\t\topcodeNum--\n+\t\t\tc.EmitByte(byte((opcodes >> (opcodeNum << 3)) & 0xff))\n+\t\t}\n+\n+\t\t// SIB byte is the last byte of the memory encoding before the displacement\n+\t\tconst sibByte = 0x24 // == encodeSIB(0, 4, 4)\n+\n+\t\timmZero, baseRbp, baseR13 := m.imm32 == 0, base == rbp, base == r13\n+\t\tshort := lower8willSignExtendTo32(m.imm32)\n+\t\trspOrR12 := base == rsp || base == r12\n+\n+\t\tif immZero && !baseRbp && !baseR13 { // rbp or r13 can't be used as base for without displacement encoding.\n+\t\t\tc.EmitByte(encodeModRM(modNoDisplacement, regEncoding(r), baseEnc.encoding()))\n+\t\t\tif rspOrR12 {\n+\t\t\t\tc.EmitByte(sibByte)\n+\t\t\t}\n+\t\t} else if short { // Note: this includes the case where m.imm32 == 0 && base == rbp || base == r13.\n+\t\t\tc.EmitByte(encodeModRM(modShortDisplacement, regEncoding(r), baseEnc.encoding()))\n+\t\t\tif rspOrR12 {\n+\t\t\t\tc.EmitByte(sibByte)\n+\t\t\t}\n+\t\t\tc.EmitByte(byte(m.imm32))\n+\t\t} else {\n+\t\t\tc.EmitByte(encodeModRM(modLongDisplacement, regEncoding(r), baseEnc.encoding()))\n+\t\t\tif rspOrR12 {\n+\t\t\t\tc.EmitByte(sibByte)\n+\t\t\t}\n+\t\t\tc.Emit4Bytes(m.imm32)\n+\t\t}\n+\n+\tcase amodeRegRegShift:\n+\t\tbase := m.base.RealReg()\n+\t\tbaseEnc := regEncodings[base]\n+\t\tindex := m.index.RealReg()\n+\t\tindexEnc := regEncodings[index]\n+\n+\t\tif index == rsp {\n+\t\t\tpanic(\"BUG: rsp can't be used as index of addressing mode\")\n+\t\t}\n+\n+\t\trex.encodeForIndex(c, regEnc(r), indexEnc, baseEnc)\n+\n+\t\tfor opcodeNum > 0 {\n+\t\t\topcodeNum--\n+\t\t\tc.EmitByte(byte((opcodes >> (opcodeNum << 3)) & 0xff))\n+\t\t}\n+\n+\t\timmZero, baseRbp, baseR13 := m.imm32 == 0, base == rbp, base == r13\n+\t\tif immZero && !baseRbp && !baseR13 { // rbp or r13 can't be used as base for without displacement encoding. (curious why? because it's interpreted as RIP relative addressing).\n+\t\t\tc.EmitByte(encodeModRM(modNoDisplacement, regEncoding(r), useSBI))\n+\t\t\tc.EmitByte(encodeSIB(m.shift(), indexEnc.encoding(), baseEnc.encoding()))\n+\t\t} else if lower8willSignExtendTo32(m.imm32) {\n+\t\t\tc.EmitByte(encodeModRM(modShortDisplacement, regEncoding(r), useSBI))\n+\t\t\tc.EmitByte(encodeSIB(m.shift(), indexEnc.encoding(), baseEnc.encoding()))\n+\t\t\tc.EmitByte(byte(m.imm32))\n+\t\t} else {\n+\t\t\tc.EmitByte(encodeModRM(modLongDisplacement, regEncoding(r), useSBI))\n+\t\t\tc.EmitByte(encodeSIB(m.shift(), indexEnc.encoding(), baseEnc.encoding()))\n+\t\t\tc.Emit4Bytes(m.imm32)\n+\t\t}\n+\n+\tcase amodeRipRel:\n+\t\trex.encode(c, regRexBit(r), 0)\n+\t\tfor opcodeNum > 0 {\n+\t\t\topcodeNum--\n+\t\t\tc.EmitByte(byte((opcodes >> (opcodeNum << 3)) & 0xff))\n+\t\t}\n+\n+\t\t// Indicate \"LEAQ [RIP + 32bit displacement].\n+\t\t// https://wiki.osdev.org/X86-64_Instruction_Encoding#32.2F64-bit_addressing\n+\t\tc.EmitByte(encodeModRM(0b00, regEncoding(r), 0b101))\n+\n+\t\t// This will be resolved later, so we just emit a placeholder.\n+\t\tneedsLabelResolution = true\n+\t\tc.Emit4Bytes(0)\n+\n+\tdefault:\n+\t\tpanic(\"BUG: invalid addressing mode\")\n+\t}\n+\treturn\n+}\n+\n+const (\n+\trexEncodingDefault byte = 0x40\n+\trexEncodingW            = rexEncodingDefault | 0x08\n+)\n+\n+// rexInfo is a bit set to indicate:\n+//\n+//\t0x01: W bit must be cleared.\n+//\t0x02: REX prefix must be emitted.\n+type rexInfo byte\n+\n+func (ri rexInfo) setW() rexInfo {\n+\treturn ri | 0x01\n+}\n+\n+func (ri rexInfo) clearW() rexInfo {\n+\treturn ri & 0x02\n+}\n+\n+func (ri rexInfo) always() rexInfo {\n+\treturn ri | 0x02\n+}\n+\n+func (ri rexInfo) notAlways() rexInfo { //nolint\n+\treturn ri & 0x01\n+}\n+\n+func (ri rexInfo) encode(c backend.Compiler, r uint8, b uint8) {\n+\tvar w byte = 0\n+\tif ri&0x01 != 0 {\n+\t\tw = 0x01\n+\t}\n+\trex := rexEncodingDefault | w<<3 | r<<2 | b\n+\tif rex != rexEncodingDefault || ri&0x02 != 0 {\n+\t\tc.EmitByte(rex)\n+\t}\n+}\n+\n+func (ri rexInfo) encodeForIndex(c backend.Compiler, encR regEnc, encIndex regEnc, encBase regEnc) {\n+\tvar w byte = 0\n+\tif ri&0x01 != 0 {\n+\t\tw = 0x01\n+\t}\n+\tr := encR.rexBit()\n+\tx := encIndex.rexBit()\n+\tb := encBase.rexBit()\n+\trex := byte(0x40) | w<<3 | r<<2 | x<<1 | b\n+\tif rex != 0x40 || ri&0x02 != 0 {\n+\t\tc.EmitByte(rex)\n+\t}\n+}\n+\n+type regEnc byte\n+\n+func (r regEnc) rexBit() byte {\n+\treturn regRexBit(byte(r))\n+}\n+\n+func (r regEnc) encoding() byte {\n+\treturn regEncoding(byte(r))\n+}\n+\n+func regRexBit(r byte) byte {\n+\treturn r >> 3\n+}\n+\n+func regEncoding(r byte) byte {\n+\treturn r & 0x07\n+}\n+\n+var regEncodings = [...]regEnc{\n+\trax:   0b000,\n+\trcx:   0b001,\n+\trdx:   0b010,\n+\trbx:   0b011,\n+\trsp:   0b100,\n+\trbp:   0b101,\n+\trsi:   0b110,\n+\trdi:   0b111,\n+\tr8:    0b1000,\n+\tr9:    0b1001,\n+\tr10:   0b1010,\n+\tr11:   0b1011,\n+\tr12:   0b1100,\n+\tr13:   0b1101,\n+\tr14:   0b1110,\n+\tr15:   0b1111,\n+\txmm0:  0b000,\n+\txmm1:  0b001,\n+\txmm2:  0b010,\n+\txmm3:  0b011,\n+\txmm4:  0b100,\n+\txmm5:  0b101,\n+\txmm6:  0b110,\n+\txmm7:  0b111,\n+\txmm8:  0b1000,\n+\txmm9:  0b1001,\n+\txmm10: 0b1010,\n+\txmm11: 0b1011,\n+\txmm12: 0b1100,\n+\txmm13: 0b1101,\n+\txmm14: 0b1110,\n+\txmm15: 0b1111,\n+}\n+\n+type legacyPrefixes byte\n+\n+const (\n+\tlegacyPrefixesNone legacyPrefixes = iota\n+\tlegacyPrefixes0x66\n+\tlegacyPrefixes0xF0\n+\tlegacyPrefixes0x660xF0\n+\tlegacyPrefixes0xF2\n+\tlegacyPrefixes0xF3\n+)\n+\n+func (p legacyPrefixes) encode(c backend.Compiler) {\n+\tswitch p {\n+\tcase legacyPrefixesNone:\n+\tcase legacyPrefixes0x66:\n+\t\tc.EmitByte(0x66)\n+\tcase legacyPrefixes0xF0:\n+\t\tc.EmitByte(0xf0)\n+\tcase legacyPrefixes0x660xF0:\n+\t\tc.EmitByte(0x66)\n+\t\tc.EmitByte(0xf0)\n+\tcase legacyPrefixes0xF2:\n+\t\tc.EmitByte(0xf2)\n+\tcase legacyPrefixes0xF3:\n+\t\tc.EmitByte(0xf3)\n+\tdefault:\n+\t\tpanic(\"BUG: invalid legacy prefix\")\n+\t}\n+}\n+\n+func lower32willSignExtendTo64(x uint64) bool {\n+\txs := int64(x)\n+\treturn xs == int64(uint64(int32(xs)))\n+}\n+\n+func lower8willSignExtendTo32(x uint32) bool {\n+\txs := int32(x)\n+\treturn xs == ((xs << 24) >> 24)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/lower_constant.go",
          "status": "added",
          "additions": 71,
          "deletions": 0,
          "patch": "@@ -0,0 +1,71 @@\n+package amd64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+// lowerConstant allocates a new VReg and inserts the instruction to load the constant value.\n+func (m *machine) lowerConstant(instr *ssa.Instruction) (vr regalloc.VReg) {\n+\tval := instr.Return()\n+\tvalType := val.Type()\n+\n+\tvr = m.c.AllocateVReg(valType)\n+\tm.insertLoadConstant(instr, vr)\n+\treturn\n+}\n+\n+// InsertLoadConstantBlockArg implements backend.Machine.\n+func (m *machine) InsertLoadConstantBlockArg(instr *ssa.Instruction, vr regalloc.VReg) {\n+\tm.insertLoadConstant(instr, vr)\n+}\n+\n+func (m *machine) insertLoadConstant(instr *ssa.Instruction, vr regalloc.VReg) {\n+\tval := instr.Return()\n+\tvalType := val.Type()\n+\tv := instr.ConstantVal()\n+\n+\tbits := valType.Bits()\n+\tif bits < 64 { // Clear the redundant bits just in case it's unexpectedly sign-extended, etc.\n+\t\tv = v & ((1 << valType.Bits()) - 1)\n+\t}\n+\n+\tswitch valType {\n+\tcase ssa.TypeF32, ssa.TypeF64:\n+\t\tm.lowerFconst(vr, v, bits == 64)\n+\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\tm.lowerIconst(vr, v, bits == 64)\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+func (m *machine) lowerFconst(dst regalloc.VReg, c uint64, _64 bool) {\n+\tif c == 0 {\n+\t\txor := m.allocateInstr().asZeros(dst)\n+\t\tm.insert(xor)\n+\t} else {\n+\t\tvar tmpType ssa.Type\n+\t\tif _64 {\n+\t\t\ttmpType = ssa.TypeI64\n+\t\t} else {\n+\t\t\ttmpType = ssa.TypeI32\n+\t\t}\n+\t\ttmpInt := m.c.AllocateVReg(tmpType)\n+\t\tloadToGP := m.allocateInstr().asImm(tmpInt, c, _64)\n+\t\tm.insert(loadToGP)\n+\n+\t\tmovToXmm := m.allocateInstr().asGprToXmm(sseOpcodeMovq, newOperandReg(tmpInt), dst, _64)\n+\t\tm.insert(movToXmm)\n+\t}\n+}\n+\n+func (m *machine) lowerIconst(dst regalloc.VReg, c uint64, _64 bool) {\n+\ti := m.allocateInstr()\n+\tif c == 0 {\n+\t\ti.asZeros(dst)\n+\t} else {\n+\t\ti.asImm(dst, c, _64)\n+\t}\n+\tm.insert(i)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/lower_mem.go",
          "status": "added",
          "additions": 187,
          "deletions": 0,
          "patch": "@@ -0,0 +1,187 @@\n+package amd64\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+var addendsMatchOpcodes = [...]ssa.Opcode{ssa.OpcodeUExtend, ssa.OpcodeSExtend, ssa.OpcodeIadd, ssa.OpcodeIconst, ssa.OpcodeIshl}\n+\n+type addend struct {\n+\tr     regalloc.VReg\n+\toff   int64\n+\tshift byte\n+}\n+\n+func (a addend) String() string {\n+\treturn fmt.Sprintf(\"addend{r=%s, off=%d, shift=%d}\", a.r, a.off, a.shift)\n+}\n+\n+// lowerToAddressMode converts a pointer to an addressMode that can be used as an operand for load/store instructions.\n+func (m *machine) lowerToAddressMode(ptr ssa.Value, offsetBase uint32) (am *amode) {\n+\tdef := m.c.ValueDefinition(ptr)\n+\n+\tif offsetBase&0x80000000 != 0 {\n+\t\t// Special casing the huge base offset whose MSB is set. In x64, the immediate is always\n+\t\t// sign-extended, but our IR semantics requires the offset base is always unsigned.\n+\t\t// Note that this should be extremely rare or even this shouldn't hit in the real application,\n+\t\t// therefore we don't need to optimize this case in my opinion.\n+\n+\t\ta := m.lowerAddend(def)\n+\t\toff64 := a.off + int64(offsetBase)\n+\t\toffsetBaseReg := m.c.AllocateVReg(ssa.TypeI64)\n+\t\tm.lowerIconst(offsetBaseReg, uint64(off64), true)\n+\t\tif a.r != regalloc.VRegInvalid {\n+\t\t\treturn m.newAmodeRegRegShift(0, offsetBaseReg, a.r, a.shift)\n+\t\t} else {\n+\t\t\treturn m.newAmodeImmReg(0, offsetBaseReg)\n+\t\t}\n+\t}\n+\n+\tif op := m.c.MatchInstrOneOf(def, addendsMatchOpcodes[:]); op == ssa.OpcodeIadd {\n+\t\tadd := def.Instr\n+\t\tx, y := add.Arg2()\n+\t\txDef, yDef := m.c.ValueDefinition(x), m.c.ValueDefinition(y)\n+\t\tax := m.lowerAddend(xDef)\n+\t\tay := m.lowerAddend(yDef)\n+\t\tadd.MarkLowered()\n+\t\treturn m.lowerAddendsToAmode(ax, ay, offsetBase)\n+\t} else {\n+\t\t// If it is not an Iadd, then we lower the one addend.\n+\t\ta := m.lowerAddend(def)\n+\t\t// off is always 0 if r is valid.\n+\t\tif a.r != regalloc.VRegInvalid {\n+\t\t\tif a.shift != 0 {\n+\t\t\t\ttmpReg := m.c.AllocateVReg(ssa.TypeI64)\n+\t\t\t\tm.lowerIconst(tmpReg, 0, true)\n+\t\t\t\treturn m.newAmodeRegRegShift(offsetBase, tmpReg, a.r, a.shift)\n+\t\t\t}\n+\t\t\treturn m.newAmodeImmReg(offsetBase, a.r)\n+\t\t} else {\n+\t\t\toff64 := a.off + int64(offsetBase)\n+\t\t\ttmpReg := m.c.AllocateVReg(ssa.TypeI64)\n+\t\t\tm.lowerIconst(tmpReg, uint64(off64), true)\n+\t\t\treturn m.newAmodeImmReg(0, tmpReg)\n+\t\t}\n+\t}\n+}\n+\n+func (m *machine) lowerAddendsToAmode(x, y addend, offBase uint32) *amode {\n+\tif x.r != regalloc.VRegInvalid && x.off != 0 || y.r != regalloc.VRegInvalid && y.off != 0 {\n+\t\tpanic(\"invalid input\")\n+\t}\n+\n+\tu64 := uint64(x.off+y.off) + uint64(offBase)\n+\tif u64 != 0 {\n+\t\tif _, ok := asImm32(u64, false); !ok {\n+\t\t\ttmpReg := m.c.AllocateVReg(ssa.TypeI64)\n+\t\t\tm.lowerIconst(tmpReg, u64, true)\n+\t\t\t// Blank u64 as it has been already lowered.\n+\t\t\tu64 = 0\n+\n+\t\t\tif x.r == regalloc.VRegInvalid {\n+\t\t\t\tx.r = tmpReg\n+\t\t\t} else if y.r == regalloc.VRegInvalid {\n+\t\t\t\ty.r = tmpReg\n+\t\t\t} else {\n+\t\t\t\t// We already know that either rx or ry is invalid,\n+\t\t\t\t// so we overwrite it with the temporary register.\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tu32 := uint32(u64)\n+\tswitch {\n+\t// We assume rx, ry are valid iff offx, offy are 0.\n+\tcase x.r != regalloc.VRegInvalid && y.r != regalloc.VRegInvalid:\n+\t\tswitch {\n+\t\tcase x.shift != 0 && y.shift != 0:\n+\t\t\t// Cannot absorb two shifted registers, must lower one to a shift instruction.\n+\t\t\tshifted := m.allocateInstr()\n+\t\t\tshifted.asShiftR(shiftROpShiftLeft, newOperandImm32(uint32(x.shift)), x.r, true)\n+\t\t\tm.insert(shifted)\n+\n+\t\t\treturn m.newAmodeRegRegShift(u32, x.r, y.r, y.shift)\n+\t\tcase x.shift != 0 && y.shift == 0:\n+\t\t\t// Swap base and index.\n+\t\t\tx, y = y, x\n+\t\t\tfallthrough\n+\t\tdefault:\n+\t\t\treturn m.newAmodeRegRegShift(u32, x.r, y.r, y.shift)\n+\t\t}\n+\tcase x.r == regalloc.VRegInvalid && y.r != regalloc.VRegInvalid:\n+\t\tx, y = y, x\n+\t\tfallthrough\n+\tcase x.r != regalloc.VRegInvalid && y.r == regalloc.VRegInvalid:\n+\t\tif x.shift != 0 {\n+\t\t\tzero := m.c.AllocateVReg(ssa.TypeI64)\n+\t\t\tm.lowerIconst(zero, 0, true)\n+\t\t\treturn m.newAmodeRegRegShift(u32, zero, x.r, x.shift)\n+\t\t}\n+\t\treturn m.newAmodeImmReg(u32, x.r)\n+\tdefault: // Both are invalid: use the offset.\n+\t\ttmpReg := m.c.AllocateVReg(ssa.TypeI64)\n+\t\tm.lowerIconst(tmpReg, u64, true)\n+\t\treturn m.newAmodeImmReg(0, tmpReg)\n+\t}\n+}\n+\n+func (m *machine) lowerAddend(x *backend.SSAValueDefinition) addend {\n+\tif x.IsFromBlockParam() {\n+\t\treturn addend{x.BlkParamVReg, 0, 0}\n+\t}\n+\t// Ensure the addend is not referenced in multiple places; we will discard nested Iadds.\n+\top := m.c.MatchInstrOneOf(x, addendsMatchOpcodes[:])\n+\tif op != ssa.OpcodeInvalid && op != ssa.OpcodeIadd {\n+\t\treturn m.lowerAddendFromInstr(x.Instr)\n+\t}\n+\tp := m.getOperand_Reg(x)\n+\treturn addend{p.reg(), 0, 0}\n+}\n+\n+// lowerAddendFromInstr takes an instruction returns a Vreg and an offset that can be used in an address mode.\n+// The Vreg is regalloc.VRegInvalid if the addend cannot be lowered to a register.\n+// The offset is 0 if the addend can be lowered to a register.\n+func (m *machine) lowerAddendFromInstr(instr *ssa.Instruction) addend {\n+\tinstr.MarkLowered()\n+\tswitch op := instr.Opcode(); op {\n+\tcase ssa.OpcodeIconst:\n+\t\tu64 := instr.ConstantVal()\n+\t\tif instr.Return().Type().Bits() == 32 {\n+\t\t\treturn addend{regalloc.VRegInvalid, int64(int32(u64)), 0} // sign-extend.\n+\t\t} else {\n+\t\t\treturn addend{regalloc.VRegInvalid, int64(u64), 0}\n+\t\t}\n+\tcase ssa.OpcodeUExtend, ssa.OpcodeSExtend:\n+\t\tinput := instr.Arg()\n+\t\tinputDef := m.c.ValueDefinition(input)\n+\t\tif input.Type().Bits() != 32 {\n+\t\t\tpanic(\"BUG: invalid input type \" + input.Type().String())\n+\t\t}\n+\t\tconstInst := inputDef.IsFromInstr() && inputDef.Instr.Constant()\n+\t\tswitch {\n+\t\tcase constInst && op == ssa.OpcodeSExtend:\n+\t\t\treturn addend{regalloc.VRegInvalid, int64(uint32(inputDef.Instr.ConstantVal())), 0}\n+\t\tcase constInst && op == ssa.OpcodeUExtend:\n+\t\t\treturn addend{regalloc.VRegInvalid, int64(int32(inputDef.Instr.ConstantVal())), 0} // sign-extend!\n+\t\tdefault:\n+\t\t\tr := m.getOperand_Reg(inputDef)\n+\t\t\treturn addend{r.reg(), 0, 0}\n+\t\t}\n+\tcase ssa.OpcodeIshl:\n+\t\t// If the addend is a shift, we can only handle it if the shift amount is a constant.\n+\t\tx, amount := instr.Arg2()\n+\t\tamountDef := m.c.ValueDefinition(amount)\n+\t\tif amountDef.IsFromInstr() && amountDef.Instr.Constant() && amountDef.Instr.ConstantVal() <= 3 {\n+\t\t\tr := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\t\t\treturn addend{r.reg(), 0, uint8(amountDef.Instr.ConstantVal())}\n+\t\t}\n+\t\tr := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\t\treturn addend{r.reg(), 0, 0}\n+\t}\n+\tpanic(\"BUG: invalid opcode\")\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/machine.go",
          "status": "added",
          "additions": 3595,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/machine_pro_epi_logue.go",
          "status": "added",
          "additions": 304,
          "deletions": 0,
          "patch": "@@ -0,0 +1,304 @@\n+package amd64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+)\n+\n+// PostRegAlloc implements backend.Machine.\n+func (m *machine) PostRegAlloc() {\n+\tm.setupPrologue()\n+\tm.postRegAlloc()\n+}\n+\n+func (m *machine) setupPrologue() {\n+\tcur := m.ectx.RootInstr\n+\tprevInitInst := cur.next\n+\n+\t// At this point, we have the stack layout as follows:\n+\t//\n+\t//                   (high address)\n+\t//                 +-----------------+ <----- RBP (somewhere in the middle of the stack)\n+\t//                 |     .......     |\n+\t//                 |      ret Y      |\n+\t//                 |     .......     |\n+\t//                 |      ret 0      |\n+\t//                 |      arg X      |\n+\t//                 |     .......     |\n+\t//                 |      arg 1      |\n+\t//                 |      arg 0      |\n+\t//                 |   Return Addr   |\n+\t//       RSP ----> +-----------------+\n+\t//                    (low address)\n+\n+\t// First, we push the RBP, and update the RBP to the current RSP.\n+\t//\n+\t//                   (high address)                     (high address)\n+\t//       RBP ----> +-----------------+                +-----------------+\n+\t//                 |     .......     |                |     .......     |\n+\t//                 |      ret Y      |                |      ret Y      |\n+\t//                 |     .......     |                |     .......     |\n+\t//                 |      ret 0      |                |      ret 0      |\n+\t//                 |      arg X      |                |      arg X      |\n+\t//                 |     .......     |     ====>      |     .......     |\n+\t//                 |      arg 1      |                |      arg 1      |\n+\t//                 |      arg 0      |                |      arg 0      |\n+\t//                 |   Return Addr   |                |   Return Addr   |\n+\t//       RSP ----> +-----------------+                |    Caller_RBP   |\n+\t//                    (low address)                   +-----------------+ <----- RSP, RBP\n+\t//\n+\tcur = m.setupRBPRSP(cur)\n+\n+\tif !m.stackBoundsCheckDisabled {\n+\t\tcur = m.insertStackBoundsCheck(m.requiredStackSize(), cur)\n+\t}\n+\n+\t//\n+\t//            (high address)\n+\t//          +-----------------+                  +-----------------+\n+\t//          |     .......     |                  |     .......     |\n+\t//          |      ret Y      |                  |      ret Y      |\n+\t//          |     .......     |                  |     .......     |\n+\t//          |      ret 0      |                  |      ret 0      |\n+\t//          |      arg X      |                  |      arg X      |\n+\t//          |     .......     |                  |     .......     |\n+\t//          |      arg 1      |                  |      arg 1      |\n+\t//          |      arg 0      |                  |      arg 0      |\n+\t//          |      xxxxx      |                  |      xxxxx      |\n+\t//          |   Return Addr   |                  |   Return Addr   |\n+\t//          |    Caller_RBP   |      ====>       |    Caller_RBP   |\n+\t// RBP,RSP->+-----------------+                  +-----------------+ <----- RBP\n+\t//             (low address)                     |   clobbered M   |\n+\t//                                               |   clobbered 1   |\n+\t//                                               |   ...........   |\n+\t//                                               |   clobbered 0   |\n+\t//                                               +-----------------+ <----- RSP\n+\t//\n+\tif regs := m.clobberedRegs; len(regs) > 0 {\n+\t\tfor i := range regs {\n+\t\t\tr := regs[len(regs)-1-i] // Reverse order.\n+\t\t\tif r.RegType() == regalloc.RegTypeInt {\n+\t\t\t\tcur = linkInstr(cur, m.allocateInstr().asPush64(newOperandReg(r)))\n+\t\t\t} else {\n+\t\t\t\t// Push the XMM register is not supported by the PUSH instruction.\n+\t\t\t\tcur = m.addRSP(-16, cur)\n+\t\t\t\tpush := m.allocateInstr().asXmmMovRM(\n+\t\t\t\t\tsseOpcodeMovdqu, r, newOperandMem(m.newAmodeImmReg(0, rspVReg)),\n+\t\t\t\t)\n+\t\t\t\tcur = linkInstr(cur, push)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif size := m.spillSlotSize; size > 0 {\n+\t\t// Simply decrease the RSP to allocate the spill slots.\n+\t\t// \t\tsub $size, %rsp\n+\t\tcur = linkInstr(cur, m.allocateInstr().asAluRmiR(aluRmiROpcodeSub, newOperandImm32(uint32(size)), rspVReg, true))\n+\n+\t\t// At this point, we have the stack layout as follows:\n+\t\t//\n+\t\t//            (high address)\n+\t\t//          +-----------------+\n+\t\t//          |     .......     |\n+\t\t//          |      ret Y      |\n+\t\t//          |     .......     |\n+\t\t//          |      ret 0      |\n+\t\t//          |      arg X      |\n+\t\t//          |     .......     |\n+\t\t//          |      arg 1      |\n+\t\t//          |      arg 0      |\n+\t\t//          |   ReturnAddress |\n+\t\t//          |   Caller_RBP    |\n+\t\t//          +-----------------+ <--- RBP\n+\t\t//          |    clobbered M  |\n+\t\t//          |   ............  |\n+\t\t//          |    clobbered 1  |\n+\t\t//          |    clobbered 0  |\n+\t\t//          |   spill slot N  |\n+\t\t//          |   ............  |\n+\t\t//          |   spill slot 0  |\n+\t\t//          +-----------------+ <--- RSP\n+\t\t//             (low address)\n+\t}\n+\n+\tlinkInstr(cur, prevInitInst)\n+}\n+\n+// postRegAlloc does multiple things while walking through the instructions:\n+// 1. Inserts the epilogue code.\n+// 2. Removes the redundant copy instruction.\n+// 3. Inserts the dec/inc RSP instruction right before/after the call instruction.\n+// 4. Lowering that is supposed to be done after regalloc.\n+func (m *machine) postRegAlloc() {\n+\tectx := m.ectx\n+\tfor cur := ectx.RootInstr; cur != nil; cur = cur.next {\n+\t\tswitch k := cur.kind; k {\n+\t\tcase ret:\n+\t\t\tm.setupEpilogueAfter(cur.prev)\n+\t\t\tcontinue\n+\t\tcase fcvtToSintSequence, fcvtToUintSequence:\n+\t\t\tm.ectx.PendingInstructions = m.ectx.PendingInstructions[:0]\n+\t\t\tif k == fcvtToSintSequence {\n+\t\t\t\tm.lowerFcvtToSintSequenceAfterRegalloc(cur)\n+\t\t\t} else {\n+\t\t\t\tm.lowerFcvtToUintSequenceAfterRegalloc(cur)\n+\t\t\t}\n+\t\t\tprev := cur.prev\n+\t\t\tnext := cur.next\n+\t\t\tcur := prev\n+\t\t\tfor _, instr := range m.ectx.PendingInstructions {\n+\t\t\t\tcur = linkInstr(cur, instr)\n+\t\t\t}\n+\t\t\tlinkInstr(cur, next)\n+\t\t\tcontinue\n+\t\tcase xmmCMov:\n+\t\t\tm.ectx.PendingInstructions = m.ectx.PendingInstructions[:0]\n+\t\t\tm.lowerXmmCmovAfterRegAlloc(cur)\n+\t\t\tprev := cur.prev\n+\t\t\tnext := cur.next\n+\t\t\tcur := prev\n+\t\t\tfor _, instr := range m.ectx.PendingInstructions {\n+\t\t\t\tcur = linkInstr(cur, instr)\n+\t\t\t}\n+\t\t\tlinkInstr(cur, next)\n+\t\t\tcontinue\n+\t\tcase idivRemSequence:\n+\t\t\tm.ectx.PendingInstructions = m.ectx.PendingInstructions[:0]\n+\t\t\tm.lowerIDivRemSequenceAfterRegAlloc(cur)\n+\t\t\tprev := cur.prev\n+\t\t\tnext := cur.next\n+\t\t\tcur := prev\n+\t\t\tfor _, instr := range m.ectx.PendingInstructions {\n+\t\t\t\tcur = linkInstr(cur, instr)\n+\t\t\t}\n+\t\t\tlinkInstr(cur, next)\n+\t\t\tcontinue\n+\t\tcase call, callIndirect:\n+\t\t\t// At this point, reg alloc is done, therefore we can safely insert dec/inc RPS instruction\n+\t\t\t// right before/after the call instruction. If this is done before reg alloc, the stack slot\n+\t\t\t// can point to the wrong location and therefore results in a wrong value.\n+\t\t\tcall := cur\n+\t\t\tnext := call.next\n+\t\t\t_, _, _, _, size := backend.ABIInfoFromUint64(call.u2)\n+\t\t\tif size > 0 {\n+\t\t\t\tdec := m.allocateInstr().asAluRmiR(aluRmiROpcodeSub, newOperandImm32(size), rspVReg, true)\n+\t\t\t\tlinkInstr(call.prev, dec)\n+\t\t\t\tlinkInstr(dec, call)\n+\t\t\t\tinc := m.allocateInstr().asAluRmiR(aluRmiROpcodeAdd, newOperandImm32(size), rspVReg, true)\n+\t\t\t\tlinkInstr(call, inc)\n+\t\t\t\tlinkInstr(inc, next)\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// Removes the redundant copy instruction.\n+\t\tif cur.IsCopy() && cur.op1.reg().RealReg() == cur.op2.reg().RealReg() {\n+\t\t\tprev, next := cur.prev, cur.next\n+\t\t\t// Remove the copy instruction.\n+\t\t\tprev.next = next\n+\t\t\tif next != nil {\n+\t\t\t\tnext.prev = prev\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func (m *machine) setupEpilogueAfter(cur *instruction) {\n+\tprevNext := cur.next\n+\n+\t// At this point, we have the stack layout as follows:\n+\t//\n+\t//            (high address)\n+\t//          +-----------------+\n+\t//          |     .......     |\n+\t//          |      ret Y      |\n+\t//          |     .......     |\n+\t//          |      ret 0      |\n+\t//          |      arg X      |\n+\t//          |     .......     |\n+\t//          |      arg 1      |\n+\t//          |      arg 0      |\n+\t//          |   ReturnAddress |\n+\t//          |   Caller_RBP    |\n+\t//          +-----------------+ <--- RBP\n+\t//          |    clobbered M  |\n+\t//          |   ............  |\n+\t//          |    clobbered 1  |\n+\t//          |    clobbered 0  |\n+\t//          |   spill slot N  |\n+\t//          |   ............  |\n+\t//          |   spill slot 0  |\n+\t//          +-----------------+ <--- RSP\n+\t//             (low address)\n+\n+\tif size := m.spillSlotSize; size > 0 {\n+\t\t// Simply increase the RSP to free the spill slots.\n+\t\t// \t\tadd $size, %rsp\n+\t\tcur = linkInstr(cur, m.allocateInstr().asAluRmiR(aluRmiROpcodeAdd, newOperandImm32(uint32(size)), rspVReg, true))\n+\t}\n+\n+\t//\n+\t//             (high address)\n+\t//            +-----------------+                     +-----------------+\n+\t//            |     .......     |                     |     .......     |\n+\t//            |      ret Y      |                     |      ret Y      |\n+\t//            |     .......     |                     |     .......     |\n+\t//            |      ret 0      |                     |      ret 0      |\n+\t//            |      arg X      |                     |      arg X      |\n+\t//            |     .......     |                     |     .......     |\n+\t//            |      arg 1      |                     |      arg 1      |\n+\t//            |      arg 0      |                     |      arg 0      |\n+\t//            |   ReturnAddress |                     |   ReturnAddress |\n+\t//            |    Caller_RBP   |                     |    Caller_RBP   |\n+\t//   RBP ---> +-----------------+      ========>      +-----------------+ <---- RSP, RBP\n+\t//            |    clobbered M  |\n+\t//            |   ............  |\n+\t//            |    clobbered 1  |\n+\t//            |    clobbered 0  |\n+\t//   RSP ---> +-----------------+\n+\t//               (low address)\n+\t//\n+\tif regs := m.clobberedRegs; len(regs) > 0 {\n+\t\tfor _, r := range regs {\n+\t\t\tif r.RegType() == regalloc.RegTypeInt {\n+\t\t\t\tcur = linkInstr(cur, m.allocateInstr().asPop64(r))\n+\t\t\t} else {\n+\t\t\t\t// Pop the XMM register is not supported by the POP instruction.\n+\t\t\t\tpop := m.allocateInstr().asXmmUnaryRmR(\n+\t\t\t\t\tsseOpcodeMovdqu, newOperandMem(m.newAmodeImmReg(0, rspVReg)), r,\n+\t\t\t\t)\n+\t\t\t\tcur = linkInstr(cur, pop)\n+\t\t\t\tcur = m.addRSP(16, cur)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Now roll back the RSP to RBP, and pop the caller's RBP.\n+\tcur = m.revertRBPRSP(cur)\n+\n+\tlinkInstr(cur, prevNext)\n+}\n+\n+func (m *machine) addRSP(offset int32, cur *instruction) *instruction {\n+\tif offset == 0 {\n+\t\treturn cur\n+\t}\n+\topcode := aluRmiROpcodeAdd\n+\tif offset < 0 {\n+\t\topcode = aluRmiROpcodeSub\n+\t\toffset = -offset\n+\t}\n+\treturn linkInstr(cur, m.allocateInstr().asAluRmiR(opcode, newOperandImm32(uint32(offset)), rspVReg, true))\n+}\n+\n+func (m *machine) setupRBPRSP(cur *instruction) *instruction {\n+\tcur = linkInstr(cur, m.allocateInstr().asPush64(newOperandReg(rbpVReg)))\n+\tcur = linkInstr(cur, m.allocateInstr().asMovRR(rspVReg, rbpVReg, true))\n+\treturn cur\n+}\n+\n+func (m *machine) revertRBPRSP(cur *instruction) *instruction {\n+\tcur = linkInstr(cur, m.allocateInstr().asMovRR(rbpVReg, rspVReg, true))\n+\tcur = linkInstr(cur, m.allocateInstr().asPop64(rbpVReg))\n+\treturn cur\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/machine_regalloc.go",
          "status": "added",
          "additions": 153,
          "deletions": 0,
          "patch": "@@ -0,0 +1,153 @@\n+package amd64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+// InsertMoveBefore implements backend.RegAllocFunctionMachine.\n+func (m *machine) InsertMoveBefore(dst, src regalloc.VReg, instr *instruction) {\n+\ttyp := src.RegType()\n+\tif typ != dst.RegType() {\n+\t\tpanic(\"BUG: src and dst must have the same type\")\n+\t}\n+\n+\tmov := m.allocateInstr()\n+\tif typ == regalloc.RegTypeInt {\n+\t\tmov.asMovRR(src, dst, true)\n+\t} else {\n+\t\tmov.asXmmUnaryRmR(sseOpcodeMovdqu, newOperandReg(src), dst)\n+\t}\n+\n+\tcur := instr.prev\n+\tprevNext := cur.next\n+\tcur = linkInstr(cur, mov)\n+\tlinkInstr(cur, prevNext)\n+}\n+\n+// InsertStoreRegisterAt implements backend.RegAllocFunctionMachine.\n+func (m *machine) InsertStoreRegisterAt(v regalloc.VReg, instr *instruction, after bool) *instruction {\n+\tif !v.IsRealReg() {\n+\t\tpanic(\"BUG: VReg must be backed by real reg to be stored\")\n+\t}\n+\n+\ttyp := m.c.TypeOf(v)\n+\n+\tvar prevNext, cur *instruction\n+\tif after {\n+\t\tcur, prevNext = instr, instr.next\n+\t} else {\n+\t\tcur, prevNext = instr.prev, instr\n+\t}\n+\n+\toffsetFromSP := m.getVRegSpillSlotOffsetFromSP(v.ID(), typ.Size())\n+\tstore := m.allocateInstr()\n+\tmem := newOperandMem(m.newAmodeImmReg(uint32(offsetFromSP), rspVReg))\n+\tswitch typ {\n+\tcase ssa.TypeI32:\n+\t\tstore.asMovRM(v, mem, 4)\n+\tcase ssa.TypeI64:\n+\t\tstore.asMovRM(v, mem, 8)\n+\tcase ssa.TypeF32:\n+\t\tstore.asXmmMovRM(sseOpcodeMovss, v, mem)\n+\tcase ssa.TypeF64:\n+\t\tstore.asXmmMovRM(sseOpcodeMovsd, v, mem)\n+\tcase ssa.TypeV128:\n+\t\tstore.asXmmMovRM(sseOpcodeMovdqu, v, mem)\n+\t}\n+\n+\tcur = linkInstr(cur, store)\n+\treturn linkInstr(cur, prevNext)\n+}\n+\n+// InsertReloadRegisterAt implements backend.RegAllocFunctionMachine.\n+func (m *machine) InsertReloadRegisterAt(v regalloc.VReg, instr *instruction, after bool) *instruction {\n+\tif !v.IsRealReg() {\n+\t\tpanic(\"BUG: VReg must be backed by real reg to be stored\")\n+\t}\n+\n+\ttyp := m.c.TypeOf(v)\n+\tvar prevNext, cur *instruction\n+\tif after {\n+\t\tcur, prevNext = instr, instr.next\n+\t} else {\n+\t\tcur, prevNext = instr.prev, instr\n+\t}\n+\n+\t// Load the value to the temporary.\n+\tload := m.allocateInstr()\n+\toffsetFromSP := m.getVRegSpillSlotOffsetFromSP(v.ID(), typ.Size())\n+\ta := newOperandMem(m.newAmodeImmReg(uint32(offsetFromSP), rspVReg))\n+\tswitch typ {\n+\tcase ssa.TypeI32:\n+\t\tload.asMovzxRmR(extModeLQ, a, v)\n+\tcase ssa.TypeI64:\n+\t\tload.asMov64MR(a, v)\n+\tcase ssa.TypeF32:\n+\t\tload.asXmmUnaryRmR(sseOpcodeMovss, a, v)\n+\tcase ssa.TypeF64:\n+\t\tload.asXmmUnaryRmR(sseOpcodeMovsd, a, v)\n+\tcase ssa.TypeV128:\n+\t\tload.asXmmUnaryRmR(sseOpcodeMovdqu, a, v)\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\n+\tcur = linkInstr(cur, load)\n+\treturn linkInstr(cur, prevNext)\n+}\n+\n+// ClobberedRegisters implements backend.RegAllocFunctionMachine.\n+func (m *machine) ClobberedRegisters(regs []regalloc.VReg) {\n+\tm.clobberedRegs = append(m.clobberedRegs[:0], regs...)\n+}\n+\n+// Swap implements backend.RegAllocFunctionMachine.\n+func (m *machine) Swap(cur *instruction, x1, x2, tmp regalloc.VReg) {\n+\tif x1.RegType() == regalloc.RegTypeInt {\n+\t\tprevNext := cur.next\n+\t\txc := m.allocateInstr().asXCHG(x1, newOperandReg(x2), 8)\n+\t\tcur = linkInstr(cur, xc)\n+\t\tlinkInstr(cur, prevNext)\n+\t} else {\n+\t\tif tmp.Valid() {\n+\t\t\tprevNext := cur.next\n+\t\t\tm.InsertMoveBefore(tmp, x1, prevNext)\n+\t\t\tm.InsertMoveBefore(x1, x2, prevNext)\n+\t\t\tm.InsertMoveBefore(x2, tmp, prevNext)\n+\t\t} else {\n+\t\t\tprevNext := cur.next\n+\t\t\tr2 := x2.RealReg()\n+\t\t\t// Temporarily spill x1 to stack.\n+\t\t\tcur = m.InsertStoreRegisterAt(x1, cur, true).prev\n+\t\t\t// Then move x2 to x1.\n+\t\t\tcur = linkInstr(cur, m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqa, newOperandReg(x2), x1))\n+\t\t\tlinkInstr(cur, prevNext)\n+\t\t\t// Then reload the original value on x1 from stack to r2.\n+\t\t\tm.InsertReloadRegisterAt(x1.SetRealReg(r2), cur, true)\n+\t\t}\n+\t}\n+}\n+\n+// LastInstrForInsertion implements backend.RegAllocFunctionMachine.\n+func (m *machine) LastInstrForInsertion(begin, end *instruction) *instruction {\n+\tcur := end\n+\tfor cur.kind == nop0 {\n+\t\tcur = cur.prev\n+\t\tif cur == begin {\n+\t\t\treturn end\n+\t\t}\n+\t}\n+\tswitch cur.kind {\n+\tcase jmp:\n+\t\treturn cur\n+\tdefault:\n+\t\treturn end\n+\t}\n+}\n+\n+// SSABlockLabel implements backend.RegAllocFunctionMachine.\n+func (m *machine) SSABlockLabel(id ssa.BasicBlockID) backend.Label {\n+\treturn m.ectx.SsaBlockIDToLabels[id]\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/machine_vec.go",
          "status": "added",
          "additions": 992,
          "deletions": 0,
          "patch": "@@ -0,0 +1,992 @@\n+package amd64\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+var swizzleMask = [16]byte{\n+\t0x70, 0x70, 0x70, 0x70, 0x70, 0x70, 0x70, 0x70,\n+\t0x70, 0x70, 0x70, 0x70, 0x70, 0x70, 0x70, 0x70,\n+}\n+\n+func (m *machine) lowerSwizzle(x, y ssa.Value, ret ssa.Value) {\n+\tmasklabel := m.getOrAllocateConstLabel(&m.constSwizzleMaskConstIndex, swizzleMask[:])\n+\n+\t// Load mask to maskReg.\n+\tmaskReg := m.c.AllocateVReg(ssa.TypeV128)\n+\tloadMask := m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(masklabel)), maskReg)\n+\tm.insert(loadMask)\n+\n+\t// Copy x and y to tmp registers.\n+\txx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\ttmpDst := m.copyToTmp(xx.reg())\n+\tyy := m.getOperand_Reg(m.c.ValueDefinition(y))\n+\ttmpX := m.copyToTmp(yy.reg())\n+\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePaddusb, newOperandReg(maskReg), tmpX))\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePshufb, newOperandReg(tmpX), tmpDst))\n+\n+\t// Copy the result to the destination register.\n+\tm.copyTo(tmpDst, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerInsertLane(x, y ssa.Value, index byte, ret ssa.Value, lane ssa.VecLane) {\n+\t// Copy x to tmp.\n+\ttmpDst := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, m.getOperand_Mem_Reg(m.c.ValueDefinition(x)), tmpDst))\n+\n+\tyy := m.getOperand_Reg(m.c.ValueDefinition(y))\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrb, index, yy, tmpDst))\n+\tcase ssa.VecLaneI16x8:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrw, index, yy, tmpDst))\n+\tcase ssa.VecLaneI32x4:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrd, index, yy, tmpDst))\n+\tcase ssa.VecLaneI64x2:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrq, index, yy, tmpDst))\n+\tcase ssa.VecLaneF32x4:\n+\t\t// In INSERTPS instruction, the destination index is encoded at 4 and 5 bits of the argument.\n+\t\t// See https://www.felixcloutier.com/x86/insertps\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodeInsertps, index<<4, yy, tmpDst))\n+\tcase ssa.VecLaneF64x2:\n+\t\tif index == 0 {\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovsd, yy, tmpDst))\n+\t\t} else {\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeMovlhps, yy, tmpDst))\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\tm.copyTo(tmpDst, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerExtractLane(x ssa.Value, index byte, signed bool, ret ssa.Value, lane ssa.VecLane) {\n+\t// Pextr variants are used to extract a lane from a vector register.\n+\txx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\n+\ttmpDst := m.c.AllocateVReg(ret.Type())\n+\tm.insert(m.allocateInstr().asDefineUninitializedReg(tmpDst))\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePextrb, index, xx, tmpDst))\n+\t\tif signed {\n+\t\t\tm.insert(m.allocateInstr().asMovsxRmR(extModeBL, newOperandReg(tmpDst), tmpDst))\n+\t\t} else {\n+\t\t\tm.insert(m.allocateInstr().asMovzxRmR(extModeBL, newOperandReg(tmpDst), tmpDst))\n+\t\t}\n+\tcase ssa.VecLaneI16x8:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePextrw, index, xx, tmpDst))\n+\t\tif signed {\n+\t\t\tm.insert(m.allocateInstr().asMovsxRmR(extModeWL, newOperandReg(tmpDst), tmpDst))\n+\t\t} else {\n+\t\t\tm.insert(m.allocateInstr().asMovzxRmR(extModeWL, newOperandReg(tmpDst), tmpDst))\n+\t\t}\n+\tcase ssa.VecLaneI32x4:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePextrd, index, xx, tmpDst))\n+\tcase ssa.VecLaneI64x2:\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePextrq, index, xx, tmpDst))\n+\tcase ssa.VecLaneF32x4:\n+\t\tif index == 0 {\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovss, xx, tmpDst))\n+\t\t} else {\n+\t\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePshufd, index, xx, tmpDst))\n+\t\t}\n+\tcase ssa.VecLaneF64x2:\n+\t\tif index == 0 {\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovsd, xx, tmpDst))\n+\t\t} else {\n+\t\t\tm.copyTo(xx.reg(), tmpDst)\n+\t\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePshufd, 0b00_00_11_10, newOperandReg(tmpDst), tmpDst))\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\tm.copyTo(tmpDst, m.c.VRegOf(ret))\n+}\n+\n+var sqmulRoundSat = [16]byte{\n+\t0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80,\n+\t0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80,\n+}\n+\n+func (m *machine) lowerSqmulRoundSat(x, y, ret ssa.Value) {\n+\t// See https://github.com/WebAssembly/simd/pull/365 for the following logic.\n+\tmaskLabel := m.getOrAllocateConstLabel(&m.constSqmulRoundSatIndex, sqmulRoundSat[:])\n+\n+\ttmp := m.c.AllocateVReg(ssa.TypeV128)\n+\tloadMask := m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(maskLabel)), tmp)\n+\tm.insert(loadMask)\n+\n+\txx, yy := m.getOperand_Reg(m.c.ValueDefinition(x)), m.getOperand_Mem_Reg(m.c.ValueDefinition(y))\n+\ttmpX := m.copyToTmp(xx.reg())\n+\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePmulhrsw, yy, tmpX))\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePcmpeqd, newOperandReg(tmpX), tmp))\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePxor, newOperandReg(tmp), tmpX))\n+\n+\tm.copyTo(tmpX, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerVUshr(x, y, ret ssa.Value, lane ssa.VecLane) {\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tm.lowerVUshri8x16(x, y, ret)\n+\tcase ssa.VecLaneI16x8, ssa.VecLaneI32x4, ssa.VecLaneI64x2:\n+\t\tm.lowerShr(x, y, ret, lane, false)\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+}\n+\n+// i8x16LogicalSHRMaskTable is necessary for emulating non-existent packed bytes logical right shifts on amd64.\n+// The mask is applied after performing packed word shifts on the value to clear out the unnecessary bits.\n+var i8x16LogicalSHRMaskTable = [8 * 16]byte{ // (the number of possible shift amount 0, 1, ..., 7.) * 16 bytes.\n+\t0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, // for 0 shift\n+\t0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, // for 1 shift\n+\t0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, // for 2 shift\n+\t0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, // for 3 shift\n+\t0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, // for 4 shift\n+\t0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, // for 5 shift\n+\t0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, // for 6 shift\n+\t0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, // for 7 shift\n+}\n+\n+func (m *machine) lowerVUshri8x16(x, y, ret ssa.Value) {\n+\ttmpGpReg := m.c.AllocateVReg(ssa.TypeI32)\n+\t// Load the modulo 8 mask to tmpReg.\n+\tm.lowerIconst(tmpGpReg, 0x7, false)\n+\t// Take the modulo 8 of the shift amount.\n+\tshiftAmt := m.getOperand_Mem_Imm32_Reg(m.c.ValueDefinition(y))\n+\tm.insert(m.allocateInstr().asAluRmiR(aluRmiROpcodeAnd, shiftAmt, tmpGpReg, false))\n+\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\n+\tvecTmp := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.insert(m.allocateInstr().asGprToXmm(sseOpcodeMovd, newOperandReg(tmpGpReg), vecTmp, false))\n+\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePsrlw, newOperandReg(vecTmp), xx))\n+\n+\tmaskTableLabel := m.getOrAllocateConstLabel(&m.constI8x16LogicalSHRMaskTableIndex, i8x16LogicalSHRMaskTable[:])\n+\tbase := m.c.AllocateVReg(ssa.TypeI64)\n+\tlea := m.allocateInstr().asLEA(newOperandLabel(maskTableLabel), base)\n+\tm.insert(lea)\n+\n+\t// Shift tmpGpReg by 4 to multiply the shift amount by 16.\n+\tm.insert(m.allocateInstr().asShiftR(shiftROpShiftLeft, newOperandImm32(4), tmpGpReg, false))\n+\n+\tmem := m.newAmodeRegRegShift(0, base, tmpGpReg, 0)\n+\tloadMask := m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(mem), vecTmp)\n+\tm.insert(loadMask)\n+\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePand, newOperandReg(vecTmp), xx))\n+\tm.copyTo(xx, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerVSshr(x, y, ret ssa.Value, lane ssa.VecLane) {\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tm.lowerVSshri8x16(x, y, ret)\n+\tcase ssa.VecLaneI16x8, ssa.VecLaneI32x4:\n+\t\tm.lowerShr(x, y, ret, lane, true)\n+\tcase ssa.VecLaneI64x2:\n+\t\tm.lowerVSshri64x2(x, y, ret)\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+}\n+\n+func (m *machine) lowerVSshri8x16(x, y, ret ssa.Value) {\n+\tshiftAmtReg := m.c.AllocateVReg(ssa.TypeI32)\n+\t// Load the modulo 8 mask to tmpReg.\n+\tm.lowerIconst(shiftAmtReg, 0x7, false)\n+\t// Take the modulo 8 of the shift amount.\n+\tshiftAmt := m.getOperand_Mem_Imm32_Reg(m.c.ValueDefinition(y))\n+\tm.insert(m.allocateInstr().asAluRmiR(aluRmiROpcodeAnd, shiftAmt, shiftAmtReg, false))\n+\n+\t// Copy the x value to two temporary registers.\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\tvecTmp := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.copyTo(xx, vecTmp)\n+\n+\t// Assuming that we have\n+\t//  xx   = [b1, ..., b16]\n+\t//  vecTmp = [b1, ..., b16]\n+\t// at this point, then we use PUNPCKLBW and PUNPCKHBW to produce:\n+\t//  xx   = [b1, b1, b2, b2, ..., b8, b8]\n+\t//  vecTmp = [b9, b9, b10, b10, ..., b16, b16]\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePunpcklbw, newOperandReg(xx), xx))\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePunpckhbw, newOperandReg(vecTmp), vecTmp))\n+\n+\t// Adding 8 to the shift amount, and then move the amount to vecTmp2.\n+\tvecTmp2 := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.insert(m.allocateInstr().asAluRmiR(aluRmiROpcodeAdd, newOperandImm32(8), shiftAmtReg, false))\n+\tm.insert(m.allocateInstr().asGprToXmm(sseOpcodeMovd, newOperandReg(shiftAmtReg), vecTmp2, false))\n+\n+\t// Perform the word packed arithmetic right shifts on vreg and vecTmp.\n+\t// This changes these two registers as:\n+\t//  xx   = [xxx, b1 >> s, xxx, b2 >> s, ..., xxx, b8 >> s]\n+\t//  vecTmp = [xxx, b9 >> s, xxx, b10 >> s, ..., xxx, b16 >> s]\n+\t// where xxx is 1 or 0 depending on each byte's sign, and \">>\" is the arithmetic shift on a byte.\n+\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePsraw, newOperandReg(vecTmp2), xx))\n+\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePsraw, newOperandReg(vecTmp2), vecTmp))\n+\n+\t// Finally, we can get the result by packing these two word vectors.\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePacksswb, newOperandReg(vecTmp), xx))\n+\n+\tm.copyTo(xx, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerVSshri64x2(x, y, ret ssa.Value) {\n+\t// Load the shift amount to RCX.\n+\tshiftAmt := m.getOperand_Mem_Reg(m.c.ValueDefinition(y))\n+\tm.insert(m.allocateInstr().asMovzxRmR(extModeBQ, shiftAmt, rcxVReg))\n+\n+\ttmpGp := m.c.AllocateVReg(ssa.TypeI64)\n+\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txxReg := m.copyToTmp(_xx.reg())\n+\n+\tm.insert(m.allocateInstr().asDefineUninitializedReg(tmpGp))\n+\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePextrq, 0, newOperandReg(xxReg), tmpGp))\n+\tm.insert(m.allocateInstr().asShiftR(shiftROpShiftRightArithmetic, newOperandReg(rcxVReg), tmpGp, true))\n+\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrq, 0, newOperandReg(tmpGp), xxReg))\n+\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePextrq, 1, newOperandReg(xxReg), tmpGp))\n+\tm.insert(m.allocateInstr().asShiftR(shiftROpShiftRightArithmetic, newOperandReg(rcxVReg), tmpGp, true))\n+\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrq, 1, newOperandReg(tmpGp), xxReg))\n+\n+\tm.copyTo(xxReg, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerShr(x, y, ret ssa.Value, lane ssa.VecLane, signed bool) {\n+\tvar modulo uint64\n+\tvar shiftOp sseOpcode\n+\tswitch lane {\n+\tcase ssa.VecLaneI16x8:\n+\t\tmodulo = 0xf\n+\t\tif signed {\n+\t\t\tshiftOp = sseOpcodePsraw\n+\t\t} else {\n+\t\t\tshiftOp = sseOpcodePsrlw\n+\t\t}\n+\tcase ssa.VecLaneI32x4:\n+\t\tmodulo = 0x1f\n+\t\tif signed {\n+\t\t\tshiftOp = sseOpcodePsrad\n+\t\t} else {\n+\t\t\tshiftOp = sseOpcodePsrld\n+\t\t}\n+\tcase ssa.VecLaneI64x2:\n+\t\tmodulo = 0x3f\n+\t\tif signed {\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tshiftOp = sseOpcodePsrlq\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\n+\ttmpGpReg := m.c.AllocateVReg(ssa.TypeI32)\n+\t// Load the modulo 8 mask to tmpReg.\n+\tm.lowerIconst(tmpGpReg, modulo, false)\n+\t// Take the modulo 8 of the shift amount.\n+\tm.insert(m.allocateInstr().asAluRmiR(aluRmiROpcodeAnd,\n+\t\tm.getOperand_Mem_Imm32_Reg(m.c.ValueDefinition(y)), tmpGpReg, false))\n+\t// And move it to a xmm register.\n+\ttmpVec := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.insert(m.allocateInstr().asGprToXmm(sseOpcodeMovd, newOperandReg(tmpGpReg), tmpVec, false))\n+\n+\t// Then do the actual shift.\n+\tm.insert(m.allocateInstr().asXmmRmiReg(shiftOp, newOperandReg(tmpVec), xx))\n+\n+\tm.copyTo(xx, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerVIshl(x, y, ret ssa.Value, lane ssa.VecLane) {\n+\tvar modulo uint64\n+\tvar shiftOp sseOpcode\n+\tvar isI8x16 bool\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tisI8x16 = true\n+\t\tmodulo = 0x7\n+\t\tshiftOp = sseOpcodePsllw\n+\tcase ssa.VecLaneI16x8:\n+\t\tmodulo = 0xf\n+\t\tshiftOp = sseOpcodePsllw\n+\tcase ssa.VecLaneI32x4:\n+\t\tmodulo = 0x1f\n+\t\tshiftOp = sseOpcodePslld\n+\tcase ssa.VecLaneI64x2:\n+\t\tmodulo = 0x3f\n+\t\tshiftOp = sseOpcodePsllq\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\n+\ttmpGpReg := m.c.AllocateVReg(ssa.TypeI32)\n+\t// Load the modulo 8 mask to tmpReg.\n+\tm.lowerIconst(tmpGpReg, modulo, false)\n+\t// Take the modulo 8 of the shift amount.\n+\tm.insert(m.allocateInstr().asAluRmiR(aluRmiROpcodeAnd,\n+\t\tm.getOperand_Mem_Imm32_Reg(m.c.ValueDefinition(y)), tmpGpReg, false))\n+\t// And move it to a xmm register.\n+\ttmpVec := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.insert(m.allocateInstr().asGprToXmm(sseOpcodeMovd, newOperandReg(tmpGpReg), tmpVec, false))\n+\n+\t// Then do the actual shift.\n+\tm.insert(m.allocateInstr().asXmmRmiReg(shiftOp, newOperandReg(tmpVec), xx))\n+\n+\tif isI8x16 {\n+\t\tmaskTableLabel := m.getOrAllocateConstLabel(&m.constI8x16SHLMaskTableIndex, i8x16SHLMaskTable[:])\n+\t\tbase := m.c.AllocateVReg(ssa.TypeI64)\n+\t\tlea := m.allocateInstr().asLEA(newOperandLabel(maskTableLabel), base)\n+\t\tm.insert(lea)\n+\n+\t\t// Shift tmpGpReg by 4 to multiply the shift amount by 16.\n+\t\tm.insert(m.allocateInstr().asShiftR(shiftROpShiftLeft, newOperandImm32(4), tmpGpReg, false))\n+\n+\t\tmem := m.newAmodeRegRegShift(0, base, tmpGpReg, 0)\n+\t\tloadMask := m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(mem), tmpVec)\n+\t\tm.insert(loadMask)\n+\n+\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePand, newOperandReg(tmpVec), xx))\n+\t}\n+\n+\tm.copyTo(xx, m.c.VRegOf(ret))\n+}\n+\n+// i8x16SHLMaskTable is necessary for emulating non-existent packed bytes left shifts on amd64.\n+// The mask is applied after performing packed word shifts on the value to clear out the unnecessary bits.\n+var i8x16SHLMaskTable = [8 * 16]byte{ // (the number of possible shift amount 0, 1, ..., 7.) * 16 bytes.\n+\t0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, // for 0 shift\n+\t0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, // for 1 shift\n+\t0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, // for 2 shift\n+\t0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, // for 3 shift\n+\t0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, // for 4 shift\n+\t0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, // for 5 shift\n+\t0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, // for 6 shift\n+\t0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, // for 7 shift\n+}\n+\n+func (m *machine) lowerVRound(x, ret ssa.Value, imm byte, _64 bool) {\n+\txx := m.getOperand_Mem_Reg(m.c.ValueDefinition(x))\n+\tvar round sseOpcode\n+\tif _64 {\n+\t\tround = sseOpcodeRoundpd\n+\t} else {\n+\t\tround = sseOpcodeRoundps\n+\t}\n+\tm.insert(m.allocateInstr().asXmmUnaryRmRImm(round, imm, xx, m.c.VRegOf(ret)))\n+}\n+\n+var (\n+\tallOnesI8x16              = [16]byte{0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1, 0x1}\n+\tallOnesI16x8              = [16]byte{0x1, 0x0, 0x1, 0x0, 0x1, 0x0, 0x1, 0x0, 0x1, 0x0, 0x1, 0x0, 0x1, 0x0, 0x1, 0x0}\n+\textAddPairwiseI16x8uMask1 = [16]byte{0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80}\n+\textAddPairwiseI16x8uMask2 = [16]byte{0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00}\n+)\n+\n+func (m *machine) lowerExtIaddPairwise(x, ret ssa.Value, srcLane ssa.VecLane, signed bool) {\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\tswitch srcLane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tallOneReg := m.c.AllocateVReg(ssa.TypeV128)\n+\t\tmask := m.getOrAllocateConstLabel(&m.constAllOnesI8x16Index, allOnesI8x16[:])\n+\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(mask)), allOneReg))\n+\n+\t\tvar resultReg regalloc.VReg\n+\t\tif signed {\n+\t\t\tresultReg = allOneReg\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePmaddubsw, newOperandReg(xx), resultReg))\n+\t\t} else {\n+\t\t\t// Interpreter tmp (all ones) as signed byte meaning that all the multiply-add is unsigned.\n+\t\t\tresultReg = xx\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePmaddubsw, newOperandReg(allOneReg), resultReg))\n+\t\t}\n+\t\tm.copyTo(resultReg, m.c.VRegOf(ret))\n+\n+\tcase ssa.VecLaneI16x8:\n+\t\tif signed {\n+\t\t\tallOnesReg := m.c.AllocateVReg(ssa.TypeV128)\n+\t\t\tmask := m.getOrAllocateConstLabel(&m.constAllOnesI16x8Index, allOnesI16x8[:])\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(mask)), allOnesReg))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePmaddwd, newOperandReg(allOnesReg), xx))\n+\t\t\tm.copyTo(xx, m.c.VRegOf(ret))\n+\t\t} else {\n+\t\t\tmaskReg := m.c.AllocateVReg(ssa.TypeV128)\n+\t\t\tmask := m.getOrAllocateConstLabel(&m.constExtAddPairwiseI16x8uMask1Index, extAddPairwiseI16x8uMask1[:])\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(mask)), maskReg))\n+\n+\t\t\t// Flip the sign bits on xx.\n+\t\t\t//\n+\t\t\t// Assuming that xx = [w1, ..., w8], now we have,\n+\t\t\t// \txx[i] = int8(-w1) for i = 0...8\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePxor, newOperandReg(maskReg), xx))\n+\n+\t\t\tmask = m.getOrAllocateConstLabel(&m.constAllOnesI16x8Index, allOnesI16x8[:])\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(mask)), maskReg))\n+\n+\t\t\t// For i = 0,..4 (as this results in i32x4 lanes), now we have\n+\t\t\t// xx[i] = int32(-wn + -w(n+1)) = int32(-(wn + w(n+1)))\n+\t\t\t// c.assembler.CompileRegisterToRegister(amd64.PMADDWD, tmp, vr)\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePmaddwd, newOperandReg(maskReg), xx))\n+\n+\t\t\tmask = m.getOrAllocateConstLabel(&m.constExtAddPairwiseI16x8uMask2Index, extAddPairwiseI16x8uMask2[:])\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(mask)), maskReg))\n+\n+\t\t\t// vr[i] = int32(-(wn + w(n+1))) + int32(math.MaxInt16+1) = int32((wn + w(n+1))) = uint32(wn + w(n+1)).\n+\t\t\t// c.assembler.CompileRegisterToRegister(amd64.PADDD, tmp, vr)\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePaddd, newOperandReg(maskReg), xx))\n+\n+\t\t\tm.copyTo(xx, m.c.VRegOf(ret))\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", srcLane))\n+\t}\n+}\n+\n+func (m *machine) lowerWidenLow(x, ret ssa.Value, lane ssa.VecLane, signed bool) {\n+\tvar sseOp sseOpcode\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePmovsxbw\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePmovzxbw\n+\t\t}\n+\tcase ssa.VecLaneI16x8:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePmovsxwd\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePmovzxwd\n+\t\t}\n+\tcase ssa.VecLaneI32x4:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePmovsxdq\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePmovzxdq\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\txx := m.getOperand_Mem_Reg(m.c.ValueDefinition(x))\n+\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOp, xx, m.c.VRegOf(ret)))\n+}\n+\n+func (m *machine) lowerWidenHigh(x, ret ssa.Value, lane ssa.VecLane, signed bool) {\n+\ttmp := m.c.AllocateVReg(ssa.TypeV128)\n+\txx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\tm.copyTo(xx.reg(), tmp)\n+\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePalignr, 8, newOperandReg(tmp), tmp))\n+\n+\tvar sseOp sseOpcode\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePmovsxbw\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePmovzxbw\n+\t\t}\n+\tcase ssa.VecLaneI16x8:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePmovsxwd\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePmovzxwd\n+\t\t}\n+\tcase ssa.VecLaneI32x4:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePmovsxdq\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePmovzxdq\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOp, newOperandReg(tmp), m.c.VRegOf(ret)))\n+}\n+\n+func (m *machine) lowerLoadSplat(ptr ssa.Value, offset uint32, ret ssa.Value, lane ssa.VecLane) {\n+\ttmpDst, tmpGp := m.c.AllocateVReg(ssa.TypeV128), m.c.AllocateVReg(ssa.TypeI64)\n+\tam := newOperandMem(m.lowerToAddressMode(ptr, offset))\n+\n+\tm.insert(m.allocateInstr().asDefineUninitializedReg(tmpDst))\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\tm.insert(m.allocateInstr().asMovzxRmR(extModeBQ, am, tmpGp))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrb, 0, newOperandReg(tmpGp), tmpDst))\n+\t\ttmpZeroVec := m.c.AllocateVReg(ssa.TypeV128)\n+\t\tm.insert(m.allocateInstr().asZeros(tmpZeroVec))\n+\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePshufb, newOperandReg(tmpZeroVec), tmpDst))\n+\tcase ssa.VecLaneI16x8:\n+\t\tm.insert(m.allocateInstr().asMovzxRmR(extModeWQ, am, tmpGp))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrw, 0, newOperandReg(tmpGp), tmpDst))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrw, 1, newOperandReg(tmpGp), tmpDst))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePshufd, 0, newOperandReg(tmpDst), tmpDst))\n+\tcase ssa.VecLaneI32x4:\n+\t\tm.insert(m.allocateInstr().asMovzxRmR(extModeLQ, am, tmpGp))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrd, 0, newOperandReg(tmpGp), tmpDst))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePshufd, 0, newOperandReg(tmpDst), tmpDst))\n+\tcase ssa.VecLaneI64x2:\n+\t\tm.insert(m.allocateInstr().asMov64MR(am, tmpGp))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrq, 0, newOperandReg(tmpGp), tmpDst))\n+\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodePinsrq, 1, newOperandReg(tmpGp), tmpDst))\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\tm.copyTo(tmpDst, m.c.VRegOf(ret))\n+}\n+\n+var f64x2CvtFromIMask = [16]byte{\n+\t0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n+}\n+\n+func (m *machine) lowerVFcvtFromInt(x, ret ssa.Value, lane ssa.VecLane, signed bool) {\n+\tswitch lane {\n+\tcase ssa.VecLaneF32x4:\n+\t\tif signed {\n+\t\t\txx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvtdq2ps, xx, m.c.VRegOf(ret)))\n+\t\t} else {\n+\t\t\txx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\t\t\t// Copy the value to two temporary registers.\n+\t\t\ttmp := m.copyToTmp(xx.reg())\n+\t\t\ttmp2 := m.copyToTmp(xx.reg())\n+\n+\t\t\t// Clear the higher 16 bits of each 32-bit element.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePslld, newOperandImm32(0xa), tmp))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePsrld, newOperandImm32(0xa), tmp))\n+\n+\t\t\t// Subtract the higher 16-bits from tmp2: clear the lower 16-bits of tmp2.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePsubd, newOperandReg(tmp), tmp2))\n+\n+\t\t\t// Convert the lower 16-bits in tmp.\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvtdq2ps, newOperandReg(tmp), tmp))\n+\n+\t\t\t// Left shift by one and convert tmp2, meaning that halved conversion result of higher 16-bits in tmp2.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePsrld, newOperandImm32(1), tmp2))\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvtdq2ps, newOperandReg(tmp2), tmp2))\n+\n+\t\t\t// Double the converted halved higher 16bits.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeAddps, newOperandReg(tmp2), tmp2))\n+\n+\t\t\t// Get the conversion result by add tmp (holding lower 16-bit conversion) into tmp2.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeAddps, newOperandReg(tmp), tmp2))\n+\n+\t\t\tm.copyTo(tmp2, m.c.VRegOf(ret))\n+\t\t}\n+\tcase ssa.VecLaneF64x2:\n+\t\tif signed {\n+\t\t\txx := m.getOperand_Mem_Reg(m.c.ValueDefinition(x))\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvtdq2pd, xx, m.c.VRegOf(ret)))\n+\t\t} else {\n+\t\t\tmaskReg := m.c.AllocateVReg(ssa.TypeV128)\n+\t\t\tmaskLabel := m.getOrAllocateConstLabel(&m.constF64x2CvtFromIMaskIndex, f64x2CvtFromIMask[:])\n+\t\t\t// maskReg = [0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00]\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(maskLabel)), maskReg))\n+\n+\t\t\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\t\t\txx := m.copyToTmp(_xx.reg())\n+\n+\t\t\t// Given that we have xx = [d1, d2, d3, d4], this results in\n+\t\t\t//\txx = [d1, [0x00, 0x00, 0x30, 0x43], d2, [0x00, 0x00, 0x30, 0x43]]\n+\t\t\t//     = [float64(uint32(d1)) + 0x1.0p52, float64(uint32(d2)) + 0x1.0p52]\n+\t\t\t//     ^See https://stackoverflow.com/questions/13269523/can-all-32-bit-ints-be-exactly-represented-as-a-double\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeUnpcklps, newOperandReg(maskReg), xx))\n+\n+\t\t\t// maskReg = [float64(0x1.0p52), float64(0x1.0p52)]\n+\t\t\tmaskLabel = m.getOrAllocateConstLabel(&m.constTwop52Index, twop52[:])\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(maskLabel)), maskReg))\n+\n+\t\t\t// Now, we get the result as\n+\t\t\t// \txx = [float64(uint32(d1)), float64(uint32(d2))]\n+\t\t\t// because the following equality always satisfies:\n+\t\t\t//  float64(0x1.0p52 + float64(uint32(x))) - float64(0x1.0p52 + float64(uint32(y))) = float64(uint32(x)) - float64(uint32(y))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeSubpd, newOperandReg(maskReg), xx))\n+\n+\t\t\tm.copyTo(xx, m.c.VRegOf(ret))\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+}\n+\n+var (\n+\t// i32sMaxOnF64x2 holds math.MaxInt32(=2147483647.0) on two f64 lanes.\n+\ti32sMaxOnF64x2 = [16]byte{\n+\t\t0x00, 0x00, 0xc0, 0xff, 0xff, 0xff, 0xdf, 0x41, // float64(2147483647.0)\n+\t\t0x00, 0x00, 0xc0, 0xff, 0xff, 0xff, 0xdf, 0x41, // float64(2147483647.0)\n+\t}\n+\n+\t// i32sMaxOnF64x2 holds math.MaxUint32(=4294967295.0) on two f64 lanes.\n+\ti32uMaxOnF64x2 = [16]byte{\n+\t\t0x00, 0x00, 0xe0, 0xff, 0xff, 0xff, 0xef, 0x41, // float64(4294967295.0)\n+\t\t0x00, 0x00, 0xe0, 0xff, 0xff, 0xff, 0xef, 0x41, // float64(4294967295.0)\n+\t}\n+\n+\t// twop52 holds two float64(0x1.0p52) on two f64 lanes. 0x1.0p52 is special in the sense that\n+\t// with this exponent, the mantissa represents a corresponding uint32 number, and arithmetics,\n+\t// like addition or subtraction, the resulted floating point holds exactly the same\n+\t// bit representations in 32-bit integer on its mantissa.\n+\t//\n+\t// Note: the name twop52 is common across various compiler ecosystem.\n+\t// \tE.g. https://github.com/llvm/llvm-project/blob/92ab024f81e5b64e258b7c3baaf213c7c26fcf40/compiler-rt/lib/builtins/floatdidf.c#L28\n+\t// \tE.g. https://opensource.apple.com/source/clang/clang-425.0.24/src/projects/compiler-rt/lib/floatdidf.c.auto.html\n+\ttwop52 = [16]byte{\n+\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43, // float64(0x1.0p52)\n+\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43, // float64(0x1.0p52)\n+\t}\n+)\n+\n+func (m *machine) lowerVFcvtToIntSat(x, ret ssa.Value, lane ssa.VecLane, signed bool) {\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\n+\tswitch lane {\n+\tcase ssa.VecLaneF32x4:\n+\t\tif signed {\n+\t\t\ttmp := m.copyToTmp(xx)\n+\n+\t\t\t// Assuming we have xx = [v1, v2, v3, v4].\n+\t\t\t//\n+\t\t\t// Set all bits if lane is not NaN on tmp.\n+\t\t\t// tmp[i] = 0xffffffff  if vi != NaN\n+\t\t\t//        = 0           if vi == NaN\n+\t\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodeCmpps, uint8(cmpPredEQ_OQ), newOperandReg(tmp), tmp))\n+\n+\t\t\t// Clear NaN lanes on xx, meaning that\n+\t\t\t// \txx[i] = vi  if vi != NaN\n+\t\t\t//\t        0   if vi == NaN\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeAndps, newOperandReg(tmp), xx))\n+\n+\t\t\t// tmp[i] = ^vi         if vi != NaN\n+\t\t\t//        = 0xffffffff  if vi == NaN\n+\t\t\t// which means that tmp[i] & 0x80000000 != 0 if and only if vi is negative.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeXorps, newOperandReg(xx), tmp))\n+\n+\t\t\t// xx[i] = int32(vi)   if vi != NaN and xx is not overflowing.\n+\t\t\t//       = 0x80000000  if vi != NaN and xx is overflowing (See https://www.felixcloutier.com/x86/cvttps2dq)\n+\t\t\t//       = 0           if vi == NaN\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvttps2dq, newOperandReg(xx), xx))\n+\n+\t\t\t// Below, we have to convert 0x80000000 into 0x7FFFFFFF for positive overflowing lane.\n+\t\t\t//\n+\t\t\t// tmp[i] = 0x80000000                         if vi is positive\n+\t\t\t//        = any satisfying any&0x80000000 = 0  if vi is negative or zero.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeAndps, newOperandReg(xx), tmp))\n+\n+\t\t\t// Arithmetic right shifting tmp by 31, meaning that we have\n+\t\t\t// tmp[i] = 0xffffffff if vi is positive, 0 otherwise.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePsrad, newOperandImm32(0x1f), tmp))\n+\n+\t\t\t// Flipping 0x80000000 if vi is positive, otherwise keep intact.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePxor, newOperandReg(tmp), xx))\n+\t\t} else {\n+\t\t\ttmp := m.c.AllocateVReg(ssa.TypeV128)\n+\t\t\tm.insert(m.allocateInstr().asZeros(tmp))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeMaxps, newOperandReg(tmp), xx))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePcmpeqd, newOperandReg(tmp), tmp))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmiReg(sseOpcodePsrld, newOperandImm32(0x1), tmp))\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvtdq2ps, newOperandReg(tmp), tmp))\n+\t\t\ttmp2 := m.copyToTmp(xx)\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvttps2dq, newOperandReg(xx), xx))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeSubps, newOperandReg(tmp), tmp2))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodeCmpps, uint8(cmpPredLE_OS), newOperandReg(tmp2), tmp))\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvttps2dq, newOperandReg(tmp2), tmp2))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePxor, newOperandReg(tmp), tmp2))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePxor, newOperandReg(tmp), tmp))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePmaxsd, newOperandReg(tmp), tmp2))\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePaddd, newOperandReg(tmp2), xx))\n+\t\t}\n+\n+\tcase ssa.VecLaneF64x2:\n+\t\ttmp2 := m.c.AllocateVReg(ssa.TypeV128)\n+\t\tif signed {\n+\t\t\ttmp := m.copyToTmp(xx)\n+\n+\t\t\t// Set all bits for non-NaN lanes, zeros otherwise.\n+\t\t\t// I.e. tmp[i] = 0xffffffff_ffffffff if vi != NaN, 0 otherwise.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodeCmppd, uint8(cmpPredEQ_OQ), newOperandReg(tmp), tmp))\n+\n+\t\t\tmaskLabel := m.getOrAllocateConstLabel(&m.constI32sMaxOnF64x2Index, i32sMaxOnF64x2[:])\n+\t\t\t// Load the 2147483647 into tmp2's each lane.\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(maskLabel)), tmp2))\n+\n+\t\t\t// tmp[i] = 2147483647 if vi != NaN, 0 otherwise.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeAndps, newOperandReg(tmp2), tmp))\n+\n+\t\t\t// MINPD returns the source register's value as-is, so we have\n+\t\t\t//  xx[i] = vi   if vi != NaN\n+\t\t\t//        = 0    if vi == NaN\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeMinpd, newOperandReg(tmp), xx))\n+\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeCvttpd2dq, newOperandReg(xx), xx))\n+\t\t} else {\n+\t\t\ttmp := m.c.AllocateVReg(ssa.TypeV128)\n+\t\t\tm.insert(m.allocateInstr().asZeros(tmp))\n+\n+\t\t\t//  xx[i] = vi   if vi != NaN && vi > 0\n+\t\t\t//        = 0    if vi == NaN || vi <= 0\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeMaxpd, newOperandReg(tmp), xx))\n+\n+\t\t\t// tmp2[i] = float64(math.MaxUint32) = math.MaxUint32\n+\t\t\tmaskIndex := m.getOrAllocateConstLabel(&m.constI32uMaxOnF64x2Index, i32uMaxOnF64x2[:])\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(maskIndex)), tmp2))\n+\n+\t\t\t// xx[i] = vi   if vi != NaN && vi > 0 && vi <= math.MaxUint32\n+\t\t\t//       = 0    otherwise\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeMinpd, newOperandReg(tmp2), xx))\n+\n+\t\t\t// Round the floating points into integer.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodeRoundpd, 0x3, newOperandReg(xx), xx))\n+\n+\t\t\t// tmp2[i] = float64(0x1.0p52)\n+\t\t\tmaskIndex = m.getOrAllocateConstLabel(&m.constTwop52Index, twop52[:])\n+\t\t\tm.insert(m.allocateInstr().asXmmUnaryRmR(sseOpcodeMovdqu, newOperandMem(m.newAmodeRipRel(maskIndex)), tmp2))\n+\n+\t\t\t// xx[i] = float64(0x1.0p52) + float64(uint32(vi)) if vi != NaN && vi > 0 && vi <= math.MaxUint32\n+\t\t\t//       = 0                                       otherwise\n+\t\t\t//\n+\t\t\t// This means that xx[i] holds exactly the same bit of uint32(vi) in its lower 32-bits.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodeAddpd, newOperandReg(tmp2), xx))\n+\n+\t\t\t// At this point, we have\n+\t\t\t// \txx  = [uint32(v0), float64(0x1.0p52), uint32(v1), float64(0x1.0p52)]\n+\t\t\t//  tmp = [0, 0, 0, 0]\n+\t\t\t// as 32x4 lanes. Therefore, SHUFPS with 0b00_00_10_00 results in\n+\t\t\t//\txx = [xx[00], xx[10], tmp[00], tmp[00]] = [xx[00], xx[10], 0, 0]\n+\t\t\t// meaning that for i = 0 and 1, we have\n+\t\t\t//  xx[i] = uint32(vi) if vi != NaN && vi > 0 && vi <= math.MaxUint32\n+\t\t\t//        = 0          otherwise.\n+\t\t\tm.insert(m.allocateInstr().asXmmRmRImm(sseOpcodeShufps, 0b00_00_10_00, newOperandReg(tmp), xx))\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\n+\tm.copyTo(xx, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerNarrow(x, y, ret ssa.Value, lane ssa.VecLane, signed bool) {\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\tyy := m.getOperand_Mem_Reg(m.c.ValueDefinition(y))\n+\n+\tvar sseOp sseOpcode\n+\tswitch lane {\n+\tcase ssa.VecLaneI16x8:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePacksswb\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePackuswb\n+\t\t}\n+\tcase ssa.VecLaneI32x4:\n+\t\tif signed {\n+\t\t\tsseOp = sseOpcodePackssdw\n+\t\t} else {\n+\t\t\tsseOp = sseOpcodePackusdw\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"invalid lane type: %s\", lane))\n+\t}\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOp, yy, xx))\n+\tm.copyTo(xx, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerWideningPairwiseDotProductS(x, y, ret ssa.Value) {\n+\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\txx := m.copyToTmp(_xx.reg())\n+\tyy := m.getOperand_Mem_Reg(m.c.ValueDefinition(y))\n+\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePmaddwd, yy, xx))\n+\tm.copyTo(xx, m.c.VRegOf(ret))\n+}\n+\n+func (m *machine) lowerVIabs(instr *ssa.Instruction) {\n+\tx, lane := instr.ArgWithLane()\n+\trd := m.c.VRegOf(instr.Return())\n+\n+\tif lane == ssa.VecLaneI64x2 {\n+\t\t_xx := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\n+\t\tblendReg := xmm0VReg\n+\t\tm.insert(m.allocateInstr().asDefineUninitializedReg(blendReg))\n+\n+\t\ttmp := m.copyToTmp(_xx.reg())\n+\t\txx := m.copyToTmp(_xx.reg())\n+\n+\t\t// Clear all bits on blendReg.\n+\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePxor, newOperandReg(blendReg), blendReg))\n+\t\t// Subtract xx from blendMaskReg.\n+\t\tm.insert(m.allocateInstr().asXmmRmR(sseOpcodePsubq, newOperandReg(xx), blendReg))\n+\t\t// Copy the subtracted value ^^ back into tmp.\n+\t\tm.copyTo(blendReg, xx)\n+\n+\t\tm.insert(m.allocateInstr().asBlendvpd(newOperandReg(tmp), xx))\n+\n+\t\tm.copyTo(xx, rd)\n+\t} else {\n+\t\tvar vecOp sseOpcode\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI8x16:\n+\t\t\tvecOp = sseOpcodePabsb\n+\t\tcase ssa.VecLaneI16x8:\n+\t\t\tvecOp = sseOpcodePabsw\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tvecOp = sseOpcodePabsd\n+\t\t}\n+\t\trn := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\n+\t\ti := m.allocateInstr()\n+\t\ti.asXmmUnaryRmR(vecOp, rn, rd)\n+\t\tm.insert(i)\n+\t}\n+}\n+\n+func (m *machine) lowerVIpopcnt(instr *ssa.Instruction) {\n+\tx := instr.Arg()\n+\trn := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\trd := m.c.VRegOf(instr.Return())\n+\n+\ttmp1 := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.lowerVconst(tmp1, 0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f)\n+\n+\t// Copy input into tmp2.\n+\ttmp2 := m.copyToTmp(rn.reg())\n+\n+\t// Given that we have:\n+\t//  rm = [b1, ..., b16] where bn = hn:ln and hn and ln are higher and lower 4-bits of bn.\n+\t//\n+\t// Take PAND on tmp1 and tmp2, so that we mask out all the higher bits.\n+\t//  tmp2 = [l1, ..., l16].\n+\tpand := m.allocateInstr()\n+\tpand.asXmmRmR(sseOpcodePand, newOperandReg(tmp1), tmp2)\n+\tm.insert(pand)\n+\n+\t// Do logical (packed word) right shift by 4 on rm and PAND against the mask (tmp1); meaning that we have\n+\t//  tmp3 = [h1, ...., h16].\n+\ttmp3 := m.copyToTmp(rn.reg())\n+\tpsrlw := m.allocateInstr()\n+\tpsrlw.asXmmRmiReg(sseOpcodePsrlw, newOperandImm32(4), tmp3)\n+\tm.insert(psrlw)\n+\n+\tpand2 := m.allocateInstr()\n+\tpand2.asXmmRmR(sseOpcodePand, newOperandReg(tmp1), tmp3)\n+\tm.insert(pand2)\n+\n+\t// Read the popcntTable into tmp4, and we have\n+\t//  tmp4 = [0x00, 0x01, 0x01, 0x02, 0x01, 0x02, 0x02, 0x03, 0x01, 0x02, 0x02, 0x03, 0x02, 0x03, 0x03, 0x04]\n+\ttmp4 := m.c.AllocateVReg(ssa.TypeV128)\n+\tm.lowerVconst(tmp4, 0x03_02_02_01_02_01_01_00, 0x04_03_03_02_03_02_02_01)\n+\n+\t// Make a copy for later.\n+\ttmp5 := m.copyToTmp(tmp4)\n+\n+\t//  tmp4 = [popcnt(l1), ..., popcnt(l16)].\n+\tpshufb := m.allocateInstr()\n+\tpshufb.asXmmRmR(sseOpcodePshufb, newOperandReg(tmp2), tmp4)\n+\tm.insert(pshufb)\n+\n+\tpshufb2 := m.allocateInstr()\n+\tpshufb2.asXmmRmR(sseOpcodePshufb, newOperandReg(tmp3), tmp5)\n+\tm.insert(pshufb2)\n+\n+\t// tmp4 + tmp5 is the result.\n+\tpaddb := m.allocateInstr()\n+\tpaddb.asXmmRmR(sseOpcodePaddb, newOperandReg(tmp4), tmp5)\n+\tm.insert(paddb)\n+\n+\tm.copyTo(tmp5, rd)\n+}\n+\n+func (m *machine) lowerVImul(instr *ssa.Instruction) {\n+\tx, y, lane := instr.Arg2WithLane()\n+\trd := m.c.VRegOf(instr.Return())\n+\tif lane == ssa.VecLaneI64x2 {\n+\t\trn := m.getOperand_Reg(m.c.ValueDefinition(x))\n+\t\trm := m.getOperand_Reg(m.c.ValueDefinition(y))\n+\t\t// Assuming that we have\n+\t\t//\trm = [p1, p2] = [p1_lo, p1_hi, p2_lo, p2_high]\n+\t\t//  rn = [q1, q2] = [q1_lo, q1_hi, q2_lo, q2_high]\n+\t\t// where pN and qN are 64-bit (quad word) lane, and pN_lo, pN_hi, qN_lo and qN_hi are 32-bit (double word) lane.\n+\n+\t\t// Copy rn into tmp1.\n+\t\ttmp1 := m.copyToTmp(rn.reg())\n+\n+\t\t// And do the logical right shift by 32-bit on tmp1, which makes tmp1 = [0, p1_high, 0, p2_high]\n+\t\tshift := m.allocateInstr()\n+\t\tshift.asXmmRmiReg(sseOpcodePsrlq, newOperandImm32(32), tmp1)\n+\t\tm.insert(shift)\n+\n+\t\t// Execute \"pmuludq rm,tmp1\", which makes tmp1 = [p1_high*q1_lo, p2_high*q2_lo] where each lane is 64-bit.\n+\t\tmul := m.allocateInstr()\n+\t\tmul.asXmmRmR(sseOpcodePmuludq, rm, tmp1)\n+\t\tm.insert(mul)\n+\n+\t\t// Copy rm value into tmp2.\n+\t\ttmp2 := m.copyToTmp(rm.reg())\n+\n+\t\t// And do the logical right shift by 32-bit on tmp2, which makes tmp2 = [0, q1_high, 0, q2_high]\n+\t\tshift2 := m.allocateInstr()\n+\t\tshift2.asXmmRmiReg(sseOpcodePsrlq, newOperandImm32(32), tmp2)\n+\t\tm.insert(shift2)\n+\n+\t\t// Execute \"pmuludq rm,tmp2\", which makes tmp2 = [p1_lo*q1_high, p2_lo*q2_high] where each lane is 64-bit.\n+\t\tmul2 := m.allocateInstr()\n+\t\tmul2.asXmmRmR(sseOpcodePmuludq, rn, tmp2)\n+\t\tm.insert(mul2)\n+\n+\t\t// Adds tmp1 and tmp2 and do the logical left shift by 32-bit,\n+\t\t// which makes tmp1 = [(p1_lo*q1_high+p1_high*q1_lo)<<32, (p2_lo*q2_high+p2_high*q2_lo)<<32]\n+\t\tadd := m.allocateInstr()\n+\t\tadd.asXmmRmR(sseOpcodePaddq, newOperandReg(tmp2), tmp1)\n+\t\tm.insert(add)\n+\n+\t\tshift3 := m.allocateInstr()\n+\t\tshift3.asXmmRmiReg(sseOpcodePsllq, newOperandImm32(32), tmp1)\n+\t\tm.insert(shift3)\n+\n+\t\t// Copy rm value into tmp3.\n+\t\ttmp3 := m.copyToTmp(rm.reg())\n+\n+\t\t// \"pmuludq rm,tmp3\" makes tmp3 = [p1_lo*q1_lo, p2_lo*q2_lo] where each lane is 64-bit.\n+\t\tmul3 := m.allocateInstr()\n+\t\tmul3.asXmmRmR(sseOpcodePmuludq, rn, tmp3)\n+\t\tm.insert(mul3)\n+\n+\t\t// Finally, we get the result by computing tmp1 + tmp3,\n+\t\t// which makes tmp1 = [(p1_lo*q1_high+p1_high*q1_lo)<<32+p1_lo*q1_lo, (p2_lo*q2_high+p2_high*q2_lo)<<32+p2_lo*q2_lo]\n+\t\tadd2 := m.allocateInstr()\n+\t\tadd2.asXmmRmR(sseOpcodePaddq, newOperandReg(tmp3), tmp1)\n+\t\tm.insert(add2)\n+\n+\t\tm.copyTo(tmp1, rd)\n+\n+\t} else {\n+\t\tvar vecOp sseOpcode\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI16x8:\n+\t\t\tvecOp = sseOpcodePmullw\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tvecOp = sseOpcodePmulld\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported: \" + lane.String())\n+\t\t}\n+\t\tm.lowerVbBinOp(vecOp, x, y, instr.Return())\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/operands.go",
          "status": "added",
          "additions": 346,
          "deletions": 0,
          "patch": "@@ -0,0 +1,346 @@\n+package amd64\n+\n+import (\n+\t\"fmt\"\n+\t\"unsafe\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+type operand struct {\n+\tkind operandKind\n+\tdata uint64\n+}\n+\n+type operandKind byte\n+\n+const (\n+\t// operandKindReg is an operand which is an integer Register.\n+\toperandKindReg operandKind = iota + 1\n+\n+\t// operandKindMem is a value in Memory.\n+\t// 32, 64, or 128 bit value.\n+\toperandKindMem\n+\n+\t// operandKindImm32 is a signed-32-bit integer immediate value.\n+\toperandKindImm32\n+\n+\t// operandKindLabel is a label.\n+\toperandKindLabel\n+)\n+\n+// String implements fmt.Stringer.\n+func (o operandKind) String() string {\n+\tswitch o {\n+\tcase operandKindReg:\n+\t\treturn \"reg\"\n+\tcase operandKindMem:\n+\t\treturn \"mem\"\n+\tcase operandKindImm32:\n+\t\treturn \"imm32\"\n+\tcase operandKindLabel:\n+\t\treturn \"label\"\n+\tdefault:\n+\t\tpanic(\"BUG: invalid operand kind\")\n+\t}\n+}\n+\n+// format returns the string representation of the operand.\n+// _64 is only for the case where the operand is a register, and it's integer.\n+func (o *operand) format(_64 bool) string {\n+\tswitch o.kind {\n+\tcase operandKindReg:\n+\t\treturn formatVRegSized(o.reg(), _64)\n+\tcase operandKindMem:\n+\t\treturn o.addressMode().String()\n+\tcase operandKindImm32:\n+\t\treturn fmt.Sprintf(\"$%d\", int32(o.imm32()))\n+\tcase operandKindLabel:\n+\t\treturn backend.Label(o.imm32()).String()\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"BUG: invalid operand: %s\", o.kind))\n+\t}\n+}\n+\n+//go:inline\n+func (o *operand) reg() regalloc.VReg {\n+\treturn regalloc.VReg(o.data)\n+}\n+\n+//go:inline\n+func (o *operand) setReg(r regalloc.VReg) {\n+\to.data = uint64(r)\n+}\n+\n+//go:inline\n+func (o *operand) addressMode() *amode {\n+\treturn wazevoapi.PtrFromUintptr[amode](uintptr(o.data))\n+}\n+\n+//go:inline\n+func (o *operand) imm32() uint32 {\n+\treturn uint32(o.data)\n+}\n+\n+func (o *operand) label() backend.Label {\n+\tswitch o.kind {\n+\tcase operandKindLabel:\n+\t\treturn backend.Label(o.data)\n+\tcase operandKindMem:\n+\t\tmem := o.addressMode()\n+\t\tif mem.kind() != amodeRipRel {\n+\t\t\tpanic(\"BUG: invalid label\")\n+\t\t}\n+\t\treturn backend.Label(mem.imm32)\n+\tdefault:\n+\t\tpanic(\"BUG: invalid operand kind\")\n+\t}\n+}\n+\n+func newOperandLabel(label backend.Label) operand {\n+\treturn operand{kind: operandKindLabel, data: uint64(label)}\n+}\n+\n+func newOperandReg(r regalloc.VReg) operand {\n+\treturn operand{kind: operandKindReg, data: uint64(r)}\n+}\n+\n+func newOperandImm32(imm32 uint32) operand {\n+\treturn operand{kind: operandKindImm32, data: uint64(imm32)}\n+}\n+\n+func newOperandMem(amode *amode) operand {\n+\treturn operand{kind: operandKindMem, data: uint64(uintptr(unsafe.Pointer(amode)))}\n+}\n+\n+// amode is a memory operand (addressing mode).\n+type amode struct {\n+\tkindWithShift uint32\n+\timm32         uint32\n+\tbase          regalloc.VReg\n+\n+\t// For amodeRegRegShift:\n+\tindex regalloc.VReg\n+}\n+\n+type amodeKind byte\n+\n+const (\n+\t// amodeRegRegShift calculates sign-extend-32-to-64(Immediate) + base\n+\tamodeImmReg amodeKind = iota + 1\n+\n+\t// amodeImmRBP is the same as amodeImmReg, but the base register is fixed to RBP.\n+\t// The only differece is that it doesn't tell the register allocator to use RBP which is distracting for the\n+\t// register allocator.\n+\tamodeImmRBP\n+\n+\t// amodeRegRegShift calculates sign-extend-32-to-64(Immediate) + base + (Register2 << Shift)\n+\tamodeRegRegShift\n+\n+\t// amodeRipRel is a RIP-relative addressing mode specified by the label.\n+\tamodeRipRel\n+\n+\t// TODO: there are other addressing modes such as the one without base register.\n+)\n+\n+func (a *amode) kind() amodeKind {\n+\treturn amodeKind(a.kindWithShift & 0xff)\n+}\n+\n+func (a *amode) shift() byte {\n+\treturn byte(a.kindWithShift >> 8)\n+}\n+\n+func (a *amode) uses(rs *[]regalloc.VReg) {\n+\tswitch a.kind() {\n+\tcase amodeImmReg:\n+\t\t*rs = append(*rs, a.base)\n+\tcase amodeRegRegShift:\n+\t\t*rs = append(*rs, a.base, a.index)\n+\tcase amodeImmRBP, amodeRipRel:\n+\tdefault:\n+\t\tpanic(\"BUG: invalid amode kind\")\n+\t}\n+}\n+\n+func (a *amode) nregs() int {\n+\tswitch a.kind() {\n+\tcase amodeImmReg:\n+\t\treturn 1\n+\tcase amodeRegRegShift:\n+\t\treturn 2\n+\tcase amodeImmRBP, amodeRipRel:\n+\t\treturn 0\n+\tdefault:\n+\t\tpanic(\"BUG: invalid amode kind\")\n+\t}\n+}\n+\n+func (a *amode) assignUses(i int, reg regalloc.VReg) {\n+\tswitch a.kind() {\n+\tcase amodeImmReg:\n+\t\tif i == 0 {\n+\t\t\ta.base = reg\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid amode assignment\")\n+\t\t}\n+\tcase amodeRegRegShift:\n+\t\tif i == 0 {\n+\t\t\ta.base = reg\n+\t\t} else if i == 1 {\n+\t\t\ta.index = reg\n+\t\t} else {\n+\t\t\tpanic(\"BUG: invalid amode assignment\")\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"BUG: invalid amode assignment\")\n+\t}\n+}\n+\n+func (m *machine) newAmodeImmReg(imm32 uint32, base regalloc.VReg) *amode {\n+\tret := m.amodePool.Allocate()\n+\t*ret = amode{kindWithShift: uint32(amodeImmReg), imm32: imm32, base: base}\n+\treturn ret\n+}\n+\n+func (m *machine) newAmodeImmRBPReg(imm32 uint32) *amode {\n+\tret := m.amodePool.Allocate()\n+\t*ret = amode{kindWithShift: uint32(amodeImmRBP), imm32: imm32, base: rbpVReg}\n+\treturn ret\n+}\n+\n+func (m *machine) newAmodeRegRegShift(imm32 uint32, base, index regalloc.VReg, shift byte) *amode {\n+\tif shift > 3 {\n+\t\tpanic(fmt.Sprintf(\"BUG: invalid shift (must be 3>=): %d\", shift))\n+\t}\n+\tret := m.amodePool.Allocate()\n+\t*ret = amode{kindWithShift: uint32(amodeRegRegShift) | uint32(shift)<<8, imm32: imm32, base: base, index: index}\n+\treturn ret\n+}\n+\n+func (m *machine) newAmodeRipRel(label backend.Label) *amode {\n+\tret := m.amodePool.Allocate()\n+\t*ret = amode{kindWithShift: uint32(amodeRipRel), imm32: uint32(label)}\n+\treturn ret\n+}\n+\n+// String implements fmt.Stringer.\n+func (a *amode) String() string {\n+\tswitch a.kind() {\n+\tcase amodeImmReg, amodeImmRBP:\n+\t\tif a.imm32 == 0 {\n+\t\t\treturn fmt.Sprintf(\"(%s)\", formatVRegSized(a.base, true))\n+\t\t}\n+\t\treturn fmt.Sprintf(\"%d(%s)\", int32(a.imm32), formatVRegSized(a.base, true))\n+\tcase amodeRegRegShift:\n+\t\tshift := 1 << a.shift()\n+\t\tif a.imm32 == 0 {\n+\t\t\treturn fmt.Sprintf(\n+\t\t\t\t\"(%s,%s,%d)\",\n+\t\t\t\tformatVRegSized(a.base, true), formatVRegSized(a.index, true), shift)\n+\t\t}\n+\t\treturn fmt.Sprintf(\n+\t\t\t\"%d(%s,%s,%d)\",\n+\t\t\tint32(a.imm32), formatVRegSized(a.base, true), formatVRegSized(a.index, true), shift)\n+\tcase amodeRipRel:\n+\t\treturn fmt.Sprintf(\"%s(%%rip)\", backend.Label(a.imm32))\n+\tdefault:\n+\t\tpanic(\"BUG: invalid amode kind\")\n+\t}\n+}\n+\n+func (m *machine) getOperand_Mem_Reg(def *backend.SSAValueDefinition) (op operand) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn newOperandReg(def.BlkParamVReg)\n+\t}\n+\n+\tif def.SSAValue().Type() == ssa.TypeV128 {\n+\t\t// SIMD instructions require strict memory alignment, so we don't support the memory operand for V128 at the moment.\n+\t\treturn m.getOperand_Reg(def)\n+\t}\n+\n+\tif m.c.MatchInstr(def, ssa.OpcodeLoad) {\n+\t\tinstr := def.Instr\n+\t\tptr, offset, _ := instr.LoadData()\n+\t\top = newOperandMem(m.lowerToAddressMode(ptr, offset))\n+\t\tinstr.MarkLowered()\n+\t\treturn op\n+\t}\n+\treturn m.getOperand_Reg(def)\n+}\n+\n+func (m *machine) getOperand_Mem_Imm32_Reg(def *backend.SSAValueDefinition) (op operand) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn newOperandReg(def.BlkParamVReg)\n+\t}\n+\n+\tif m.c.MatchInstr(def, ssa.OpcodeLoad) {\n+\t\tinstr := def.Instr\n+\t\tptr, offset, _ := instr.LoadData()\n+\t\top = newOperandMem(m.lowerToAddressMode(ptr, offset))\n+\t\tinstr.MarkLowered()\n+\t\treturn op\n+\t}\n+\treturn m.getOperand_Imm32_Reg(def)\n+}\n+\n+func (m *machine) getOperand_Imm32_Reg(def *backend.SSAValueDefinition) (op operand) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn newOperandReg(def.BlkParamVReg)\n+\t}\n+\n+\tinstr := def.Instr\n+\tif instr.Constant() {\n+\t\t// If the operation is 64-bit, x64 sign-extends the 32-bit immediate value.\n+\t\t// Therefore, we need to check if the immediate value is within the 32-bit range and if the sign bit is set,\n+\t\t// we should not use the immediate value.\n+\t\tif op, ok := asImm32Operand(instr.ConstantVal(), instr.Return().Type() == ssa.TypeI32); ok {\n+\t\t\tinstr.MarkLowered()\n+\t\t\treturn op\n+\t\t}\n+\t}\n+\treturn m.getOperand_Reg(def)\n+}\n+\n+func asImm32Operand(val uint64, allowSignExt bool) (operand, bool) {\n+\tif imm32, ok := asImm32(val, allowSignExt); ok {\n+\t\treturn newOperandImm32(imm32), true\n+\t}\n+\treturn operand{}, false\n+}\n+\n+func asImm32(val uint64, allowSignExt bool) (uint32, bool) {\n+\tu32val := uint32(val)\n+\tif uint64(u32val) != val {\n+\t\treturn 0, false\n+\t}\n+\tif !allowSignExt && u32val&0x80000000 != 0 {\n+\t\treturn 0, false\n+\t}\n+\treturn u32val, true\n+}\n+\n+func (m *machine) getOperand_Reg(def *backend.SSAValueDefinition) (op operand) {\n+\tvar v regalloc.VReg\n+\tif def.IsFromBlockParam() {\n+\t\tv = def.BlkParamVReg\n+\t} else {\n+\t\tinstr := def.Instr\n+\t\tif instr.Constant() {\n+\t\t\t// We inline all the constant instructions so that we could reduce the register usage.\n+\t\t\tv = m.lowerConstant(instr)\n+\t\t\tinstr.MarkLowered()\n+\t\t} else {\n+\t\t\tif n := def.N; n == 0 {\n+\t\t\t\tv = m.c.VRegOf(instr.Return())\n+\t\t\t} else {\n+\t\t\t\t_, rs := instr.Returns()\n+\t\t\t\tv = m.c.VRegOf(rs[n-1])\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn newOperandReg(v)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/reflect.go",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": "@@ -0,0 +1,11 @@\n+//go:build !tinygo\n+\n+package amd64\n+\n+import \"reflect\"\n+\n+// setSliceLimits sets both Cap and Len for the given reflected slice.\n+func setSliceLimits(s *reflect.SliceHeader, limit uintptr) {\n+\ts.Len = int(limit)\n+\ts.Cap = int(limit)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/reflect_tinygo.go",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": "@@ -0,0 +1,11 @@\n+//go:build tinygo\n+\n+package amd64\n+\n+import \"reflect\"\n+\n+// setSliceLimits sets both Cap and Len for the given reflected slice.\n+func setSliceLimits(s *reflect.SliceHeader, limit uintptr) {\n+\ts.Len = limit\n+\ts.Len = limit\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/reg.go",
          "status": "added",
          "additions": 181,
          "deletions": 0,
          "patch": "@@ -0,0 +1,181 @@\n+package amd64\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+)\n+\n+// Amd64-specific registers.\n+const (\n+\t// rax is a gp register.\n+\trax = regalloc.RealRegInvalid + 1 + iota\n+\t// rcx is a gp register.\n+\trcx\n+\t// rdx is a gp register.\n+\trdx\n+\t// rbx is a gp register.\n+\trbx\n+\t// rsp is a gp register.\n+\trsp\n+\t// rbp is a gp register.\n+\trbp\n+\t// rsi is a gp register.\n+\trsi\n+\t// rdi is a gp register.\n+\trdi\n+\t// r8 is a gp register.\n+\tr8\n+\t// r9 is a gp register.\n+\tr9\n+\t// r10 is a gp register.\n+\tr10\n+\t// r11 is a gp register.\n+\tr11\n+\t// r12 is a gp register.\n+\tr12\n+\t// r13 is a gp register.\n+\tr13\n+\t// r14 is a gp register.\n+\tr14\n+\t// r15 is a gp register.\n+\tr15\n+\n+\t// xmm0 is a vector register.\n+\txmm0\n+\t// xmm1 is a vector register.\n+\txmm1\n+\t// xmm2 is a vector register.\n+\txmm2\n+\t// xmm3 is a vector register.\n+\txmm3\n+\t// xmm4 is a vector register.\n+\txmm4\n+\t// xmm5 is a vector register.\n+\txmm5\n+\t// xmm6 is a vector register.\n+\txmm6\n+\t// xmm7 is a vector register.\n+\txmm7\n+\t// xmm8 is a vector register.\n+\txmm8\n+\t// xmm9 is a vector register.\n+\txmm9\n+\t// xmm10 is a vector register.\n+\txmm10\n+\t// xmm11 is a vector register.\n+\txmm11\n+\t// xmm12 is a vector register.\n+\txmm12\n+\t// xmm13 is a vector register.\n+\txmm13\n+\t// xmm14 is a vector register.\n+\txmm14\n+\t// xmm15 is a vector register.\n+\txmm15\n+)\n+\n+var (\n+\traxVReg = regalloc.FromRealReg(rax, regalloc.RegTypeInt)\n+\trcxVReg = regalloc.FromRealReg(rcx, regalloc.RegTypeInt)\n+\trdxVReg = regalloc.FromRealReg(rdx, regalloc.RegTypeInt)\n+\trbxVReg = regalloc.FromRealReg(rbx, regalloc.RegTypeInt)\n+\trspVReg = regalloc.FromRealReg(rsp, regalloc.RegTypeInt)\n+\trbpVReg = regalloc.FromRealReg(rbp, regalloc.RegTypeInt)\n+\trsiVReg = regalloc.FromRealReg(rsi, regalloc.RegTypeInt)\n+\trdiVReg = regalloc.FromRealReg(rdi, regalloc.RegTypeInt)\n+\tr8VReg  = regalloc.FromRealReg(r8, regalloc.RegTypeInt)\n+\tr9VReg  = regalloc.FromRealReg(r9, regalloc.RegTypeInt)\n+\tr10VReg = regalloc.FromRealReg(r10, regalloc.RegTypeInt)\n+\tr11VReg = regalloc.FromRealReg(r11, regalloc.RegTypeInt)\n+\tr12VReg = regalloc.FromRealReg(r12, regalloc.RegTypeInt)\n+\tr13VReg = regalloc.FromRealReg(r13, regalloc.RegTypeInt)\n+\tr14VReg = regalloc.FromRealReg(r14, regalloc.RegTypeInt)\n+\tr15VReg = regalloc.FromRealReg(r15, regalloc.RegTypeInt)\n+\n+\txmm0VReg  = regalloc.FromRealReg(xmm0, regalloc.RegTypeFloat)\n+\txmm1VReg  = regalloc.FromRealReg(xmm1, regalloc.RegTypeFloat)\n+\txmm2VReg  = regalloc.FromRealReg(xmm2, regalloc.RegTypeFloat)\n+\txmm3VReg  = regalloc.FromRealReg(xmm3, regalloc.RegTypeFloat)\n+\txmm4VReg  = regalloc.FromRealReg(xmm4, regalloc.RegTypeFloat)\n+\txmm5VReg  = regalloc.FromRealReg(xmm5, regalloc.RegTypeFloat)\n+\txmm6VReg  = regalloc.FromRealReg(xmm6, regalloc.RegTypeFloat)\n+\txmm7VReg  = regalloc.FromRealReg(xmm7, regalloc.RegTypeFloat)\n+\txmm8VReg  = regalloc.FromRealReg(xmm8, regalloc.RegTypeFloat)\n+\txmm9VReg  = regalloc.FromRealReg(xmm9, regalloc.RegTypeFloat)\n+\txmm10VReg = regalloc.FromRealReg(xmm10, regalloc.RegTypeFloat)\n+\txmm11VReg = regalloc.FromRealReg(xmm11, regalloc.RegTypeFloat)\n+\txmm12VReg = regalloc.FromRealReg(xmm12, regalloc.RegTypeFloat)\n+\txmm13VReg = regalloc.FromRealReg(xmm13, regalloc.RegTypeFloat)\n+\txmm14VReg = regalloc.FromRealReg(xmm14, regalloc.RegTypeFloat)\n+\txmm15VReg = regalloc.FromRealReg(xmm15, regalloc.RegTypeFloat)\n+)\n+\n+var regNames = [...]string{\n+\trax:   \"rax\",\n+\trcx:   \"rcx\",\n+\trdx:   \"rdx\",\n+\trbx:   \"rbx\",\n+\trsp:   \"rsp\",\n+\trbp:   \"rbp\",\n+\trsi:   \"rsi\",\n+\trdi:   \"rdi\",\n+\tr8:    \"r8\",\n+\tr9:    \"r9\",\n+\tr10:   \"r10\",\n+\tr11:   \"r11\",\n+\tr12:   \"r12\",\n+\tr13:   \"r13\",\n+\tr14:   \"r14\",\n+\tr15:   \"r15\",\n+\txmm0:  \"xmm0\",\n+\txmm1:  \"xmm1\",\n+\txmm2:  \"xmm2\",\n+\txmm3:  \"xmm3\",\n+\txmm4:  \"xmm4\",\n+\txmm5:  \"xmm5\",\n+\txmm6:  \"xmm6\",\n+\txmm7:  \"xmm7\",\n+\txmm8:  \"xmm8\",\n+\txmm9:  \"xmm9\",\n+\txmm10: \"xmm10\",\n+\txmm11: \"xmm11\",\n+\txmm12: \"xmm12\",\n+\txmm13: \"xmm13\",\n+\txmm14: \"xmm14\",\n+\txmm15: \"xmm15\",\n+}\n+\n+func formatVRegSized(r regalloc.VReg, _64 bool) string {\n+\tif r.IsRealReg() {\n+\t\tif r.RegType() == regalloc.RegTypeInt {\n+\t\t\trr := r.RealReg()\n+\t\t\torig := regNames[rr]\n+\t\t\tif rr <= rdi {\n+\t\t\t\tif _64 {\n+\t\t\t\t\treturn \"%\" + orig\n+\t\t\t\t} else {\n+\t\t\t\t\treturn \"%e\" + orig[1:]\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tif _64 {\n+\t\t\t\t\treturn \"%\" + orig\n+\t\t\t\t} else {\n+\t\t\t\t\treturn \"%\" + orig + \"d\"\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\treturn \"%\" + regNames[r.RealReg()]\n+\t\t}\n+\t} else {\n+\t\tif r.RegType() == regalloc.RegTypeInt {\n+\t\t\tif _64 {\n+\t\t\t\treturn fmt.Sprintf(\"%%r%d?\", r.ID())\n+\t\t\t} else {\n+\t\t\t\treturn fmt.Sprintf(\"%%r%dd?\", r.ID())\n+\t\t\t}\n+\t\t} else {\n+\t\t\treturn fmt.Sprintf(\"%%xmm%d?\", r.ID())\n+\t\t}\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/amd64/stack.go",
          "status": "added",
          "additions": 128,
          "deletions": 0,
          "patch": "@@ -0,0 +1,128 @@\n+package amd64\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"reflect\"\n+\t\"unsafe\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/wasmdebug\"\n+)\n+\n+func stackView(rbp, top uintptr) []byte {\n+\tvar stackBuf []byte\n+\t{\n+\t\t// TODO: use unsafe.Slice after floor version is set to Go 1.20.\n+\t\thdr := (*reflect.SliceHeader)(unsafe.Pointer(&stackBuf))\n+\t\thdr.Data = rbp\n+\t\tsetSliceLimits(hdr, top-rbp)\n+\t}\n+\treturn stackBuf\n+}\n+\n+// UnwindStack implements wazevo.unwindStack.\n+func UnwindStack(_, rbp, top uintptr, returnAddresses []uintptr) []uintptr {\n+\tstackBuf := stackView(rbp, top)\n+\n+\tfor i := uint64(0); i < uint64(len(stackBuf)); {\n+\t\t//       (high address)\n+\t\t//    +-----------------+\n+\t\t//    |     .......     |\n+\t\t//    |      ret Y      |\n+\t\t//    |     .......     |\n+\t\t//    |      ret 0      |\n+\t\t//    |      arg X      |\n+\t\t//    |     .......     |\n+\t\t//    |      arg 1      |\n+\t\t//    |      arg 0      |\n+\t\t//    |  ReturnAddress  |\n+\t\t//    |   Caller_RBP    |\n+\t\t//    +-----------------+ <---- Caller_RBP\n+\t\t//    |   ...........   |\n+\t\t//    |   clobbered  M  |\n+\t\t//    |   ............  |\n+\t\t//    |   clobbered  0  |\n+\t\t//    |   spill slot N  |\n+\t\t//    |   ............  |\n+\t\t//    |   spill slot 0  |\n+\t\t//    |  ReturnAddress  |\n+\t\t//    |   Caller_RBP    |\n+\t\t//    +-----------------+ <---- RBP\n+\t\t//       (low address)\n+\n+\t\tcallerRBP := binary.LittleEndian.Uint64(stackBuf[i:])\n+\t\tretAddr := binary.LittleEndian.Uint64(stackBuf[i+8:])\n+\t\treturnAddresses = append(returnAddresses, uintptr(retAddr))\n+\t\ti = callerRBP - uint64(rbp)\n+\t\tif len(returnAddresses) == wasmdebug.MaxFrames {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\treturn returnAddresses\n+}\n+\n+// GoCallStackView implements wazevo.goCallStackView.\n+func GoCallStackView(stackPointerBeforeGoCall *uint64) []uint64 {\n+\t//                  (high address)\n+\t//              +-----------------+ <----+\n+\t//              |   xxxxxxxxxxx   |      | ;; optional unused space to make it 16-byte aligned.\n+\t//           ^  |  arg[N]/ret[M]  |      |\n+\t// sliceSize |  |  ............   |      | SizeInBytes/8\n+\t//           |  |  arg[1]/ret[1]  |      |\n+\t//           v  |  arg[0]/ret[0]  | <----+\n+\t//              |   SizeInBytes   |\n+\t//              +-----------------+ <---- stackPointerBeforeGoCall\n+\t//                 (low address)\n+\tdata := unsafe.Pointer(uintptr(unsafe.Pointer(stackPointerBeforeGoCall)) + 8)\n+\tsize := *stackPointerBeforeGoCall / 8\n+\treturn unsafe.Slice((*uint64)(data), int(size))\n+}\n+\n+func AdjustClonedStack(oldRsp, oldTop, rsp, rbp, top uintptr) {\n+\tdiff := uint64(rsp - oldRsp)\n+\n+\tnewBuf := stackView(rbp, top)\n+\tfor i := uint64(0); i < uint64(len(newBuf)); {\n+\t\t//       (high address)\n+\t\t//    +-----------------+\n+\t\t//    |     .......     |\n+\t\t//    |      ret Y      |\n+\t\t//    |     .......     |\n+\t\t//    |      ret 0      |\n+\t\t//    |      arg X      |\n+\t\t//    |     .......     |\n+\t\t//    |      arg 1      |\n+\t\t//    |      arg 0      |\n+\t\t//    |  ReturnAddress  |\n+\t\t//    |   Caller_RBP    |\n+\t\t//    +-----------------+ <---- Caller_RBP\n+\t\t//    |   ...........   |\n+\t\t//    |   clobbered  M  |\n+\t\t//    |   ............  |\n+\t\t//    |   clobbered  0  |\n+\t\t//    |   spill slot N  |\n+\t\t//    |   ............  |\n+\t\t//    |   spill slot 0  |\n+\t\t//    |  ReturnAddress  |\n+\t\t//    |   Caller_RBP    |\n+\t\t//    +-----------------+ <---- RBP\n+\t\t//       (low address)\n+\n+\t\tcallerRBP := binary.LittleEndian.Uint64(newBuf[i:])\n+\t\tif callerRBP == 0 {\n+\t\t\t// End of stack.\n+\t\t\tbreak\n+\t\t}\n+\t\tif i64 := int64(callerRBP); i64 < int64(oldRsp) || i64 >= int64(oldTop) {\n+\t\t\tpanic(\"BUG: callerRBP is out of range\")\n+\t\t}\n+\t\tif int(callerRBP) < 0 {\n+\t\t\tpanic(\"BUG: callerRBP is negative\")\n+\t\t}\n+\t\tadjustedCallerRBP := callerRBP + diff\n+\t\tif int(adjustedCallerRBP) < 0 {\n+\t\t\tpanic(\"BUG: adjustedCallerRBP is negative\")\n+\t\t}\n+\t\tbinary.LittleEndian.PutUint64(newBuf[i:], adjustedCallerRBP)\n+\t\ti = adjustedCallerRBP - uint64(rbp)\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/abi.go",
          "status": "added",
          "additions": 332,
          "deletions": 0,
          "patch": "@@ -0,0 +1,332 @@\n+package arm64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+// References:\n+// * https://github.com/golang/go/blob/49d42128fd8594c172162961ead19ac95e247d24/src/cmd/compile/abi-internal.md#arm64-architecture\n+// * https://developer.arm.com/documentation/102374/0101/Procedure-Call-Standard\n+\n+var (\n+\tintParamResultRegs   = []regalloc.RealReg{x0, x1, x2, x3, x4, x5, x6, x7}\n+\tfloatParamResultRegs = []regalloc.RealReg{v0, v1, v2, v3, v4, v5, v6, v7}\n+)\n+\n+var regInfo = &regalloc.RegisterInfo{\n+\tAllocatableRegisters: [regalloc.NumRegType][]regalloc.RealReg{\n+\t\t// We don't allocate:\n+\t\t// - x18: Reserved by the macOS: https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms#Respect-the-purpose-of-specific-CPU-registers\n+\t\t// - x28: Reserved by Go runtime.\n+\t\t// - x27(=tmpReg): because of the reason described on tmpReg.\n+\t\tregalloc.RegTypeInt: {\n+\t\t\tx8, x9, x10, x11, x12, x13, x14, x15,\n+\t\t\tx16, x17, x19, x20, x21, x22, x23, x24, x25,\n+\t\t\tx26, x29, x30,\n+\t\t\t// These are the argument/return registers. Less preferred in the allocation.\n+\t\t\tx7, x6, x5, x4, x3, x2, x1, x0,\n+\t\t},\n+\t\tregalloc.RegTypeFloat: {\n+\t\t\tv8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19,\n+\t\t\tv20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30,\n+\t\t\t// These are the argument/return registers. Less preferred in the allocation.\n+\t\t\tv7, v6, v5, v4, v3, v2, v1, v0,\n+\t\t},\n+\t},\n+\tCalleeSavedRegisters: regalloc.NewRegSet(\n+\t\tx19, x20, x21, x22, x23, x24, x25, x26, x28,\n+\t\tv18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31,\n+\t),\n+\tCallerSavedRegisters: regalloc.NewRegSet(\n+\t\tx0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x29, x30,\n+\t\tv0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17,\n+\t),\n+\tRealRegToVReg: []regalloc.VReg{\n+\t\tx0: x0VReg, x1: x1VReg, x2: x2VReg, x3: x3VReg, x4: x4VReg, x5: x5VReg, x6: x6VReg, x7: x7VReg, x8: x8VReg, x9: x9VReg, x10: x10VReg, x11: x11VReg, x12: x12VReg, x13: x13VReg, x14: x14VReg, x15: x15VReg, x16: x16VReg, x17: x17VReg, x18: x18VReg, x19: x19VReg, x20: x20VReg, x21: x21VReg, x22: x22VReg, x23: x23VReg, x24: x24VReg, x25: x25VReg, x26: x26VReg, x27: x27VReg, x28: x28VReg, x29: x29VReg, x30: x30VReg,\n+\t\tv0: v0VReg, v1: v1VReg, v2: v2VReg, v3: v3VReg, v4: v4VReg, v5: v5VReg, v6: v6VReg, v7: v7VReg, v8: v8VReg, v9: v9VReg, v10: v10VReg, v11: v11VReg, v12: v12VReg, v13: v13VReg, v14: v14VReg, v15: v15VReg, v16: v16VReg, v17: v17VReg, v18: v18VReg, v19: v19VReg, v20: v20VReg, v21: v21VReg, v22: v22VReg, v23: v23VReg, v24: v24VReg, v25: v25VReg, v26: v26VReg, v27: v27VReg, v28: v28VReg, v29: v29VReg, v30: v30VReg, v31: v31VReg,\n+\t},\n+\tRealRegName: func(r regalloc.RealReg) string { return regNames[r] },\n+\tRealRegType: func(r regalloc.RealReg) regalloc.RegType {\n+\t\tif r < v0 {\n+\t\t\treturn regalloc.RegTypeInt\n+\t\t}\n+\t\treturn regalloc.RegTypeFloat\n+\t},\n+}\n+\n+// ArgsResultsRegs implements backend.Machine.\n+func (m *machine) ArgsResultsRegs() (argResultInts, argResultFloats []regalloc.RealReg) {\n+\treturn intParamResultRegs, floatParamResultRegs\n+}\n+\n+// LowerParams implements backend.FunctionABI.\n+func (m *machine) LowerParams(args []ssa.Value) {\n+\ta := m.currentABI\n+\n+\tfor i, ssaArg := range args {\n+\t\tif !ssaArg.Valid() {\n+\t\t\tcontinue\n+\t\t}\n+\t\treg := m.compiler.VRegOf(ssaArg)\n+\t\targ := &a.Args[i]\n+\t\tif arg.Kind == backend.ABIArgKindReg {\n+\t\t\tm.InsertMove(reg, arg.Reg, arg.Type)\n+\t\t} else {\n+\t\t\t// TODO: we could use pair load if there's consecutive loads for the same type.\n+\t\t\t//\n+\t\t\t//            (high address)\n+\t\t\t//          +-----------------+\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      ret Y      |\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      ret 0      |\n+\t\t\t//          |      arg X      |\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      arg 1      |\n+\t\t\t//          |      arg 0      |    <-|\n+\t\t\t//          |   ReturnAddress |      |\n+\t\t\t//          +-----------------+      |\n+\t\t\t//          |   ...........   |      |\n+\t\t\t//          |   clobbered  M  |      |   argStackOffset: is unknown at this point of compilation.\n+\t\t\t//          |   ............  |      |\n+\t\t\t//          |   clobbered  0  |      |\n+\t\t\t//          |   spill slot N  |      |\n+\t\t\t//          |   ...........   |      |\n+\t\t\t//          |   spill slot 0  |      |\n+\t\t\t//   SP---> +-----------------+    <-+\n+\t\t\t//             (low address)\n+\n+\t\t\tbits := arg.Type.Bits()\n+\t\t\t// At this point of compilation, we don't yet know how much space exist below the return address.\n+\t\t\t// So we instruct the address mode to add the `argStackOffset` to the offset at the later phase of compilation.\n+\t\t\tamode := addressMode{imm: arg.Offset, rn: spVReg, kind: addressModeKindArgStackSpace}\n+\t\t\tload := m.allocateInstr()\n+\t\t\tswitch arg.Type {\n+\t\t\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\t\t\tload.asULoad(operandNR(reg), amode, bits)\n+\t\t\tcase ssa.TypeF32, ssa.TypeF64, ssa.TypeV128:\n+\t\t\t\tload.asFpuLoad(operandNR(reg), amode, bits)\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t\tm.insert(load)\n+\t\t\tm.unresolvedAddressModes = append(m.unresolvedAddressModes, load)\n+\t\t}\n+\t}\n+}\n+\n+// LowerReturns lowers the given returns.\n+func (m *machine) LowerReturns(rets []ssa.Value) {\n+\ta := m.currentABI\n+\n+\tl := len(rets) - 1\n+\tfor i := range rets {\n+\t\t// Reverse order in order to avoid overwriting the stack returns existing in the return registers.\n+\t\tret := rets[l-i]\n+\t\tr := &a.Rets[l-i]\n+\t\treg := m.compiler.VRegOf(ret)\n+\t\tif def := m.compiler.ValueDefinition(ret); def.IsFromInstr() {\n+\t\t\t// Constant instructions are inlined.\n+\t\t\tif inst := def.Instr; inst.Constant() {\n+\t\t\t\tval := inst.Return()\n+\t\t\t\tvalType := val.Type()\n+\t\t\t\tv := inst.ConstantVal()\n+\t\t\t\tm.insertLoadConstant(v, valType, reg)\n+\t\t\t}\n+\t\t}\n+\t\tif r.Kind == backend.ABIArgKindReg {\n+\t\t\tm.InsertMove(r.Reg, reg, ret.Type())\n+\t\t} else {\n+\t\t\t// TODO: we could use pair store if there's consecutive stores for the same type.\n+\t\t\t//\n+\t\t\t//            (high address)\n+\t\t\t//          +-----------------+\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      ret Y      |\n+\t\t\t//          |     .......     |\n+\t\t\t//          |      ret 0      |    <-+\n+\t\t\t//          |      arg X      |      |\n+\t\t\t//          |     .......     |      |\n+\t\t\t//          |      arg 1      |      |\n+\t\t\t//          |      arg 0      |      |\n+\t\t\t//          |   ReturnAddress |      |\n+\t\t\t//          +-----------------+      |\n+\t\t\t//          |   ...........   |      |\n+\t\t\t//          |   spill slot M  |      |   retStackOffset: is unknown at this point of compilation.\n+\t\t\t//          |   ............  |      |\n+\t\t\t//          |   spill slot 2  |      |\n+\t\t\t//          |   spill slot 1  |      |\n+\t\t\t//          |   clobbered 0   |      |\n+\t\t\t//          |   clobbered 1   |      |\n+\t\t\t//          |   ...........   |      |\n+\t\t\t//          |   clobbered N   |      |\n+\t\t\t//   SP---> +-----------------+    <-+\n+\t\t\t//             (low address)\n+\n+\t\t\tbits := r.Type.Bits()\n+\n+\t\t\t// At this point of compilation, we don't yet know how much space exist below the return address.\n+\t\t\t// So we instruct the address mode to add the `retStackOffset` to the offset at the later phase of compilation.\n+\t\t\tamode := addressMode{imm: r.Offset, rn: spVReg, kind: addressModeKindResultStackSpace}\n+\t\t\tstore := m.allocateInstr()\n+\t\t\tstore.asStore(operandNR(reg), amode, bits)\n+\t\t\tm.insert(store)\n+\t\t\tm.unresolvedAddressModes = append(m.unresolvedAddressModes, store)\n+\t\t}\n+\t}\n+}\n+\n+// callerGenVRegToFunctionArg is the opposite of GenFunctionArgToVReg, which is used to generate the\n+// caller side of the function call.\n+func (m *machine) callerGenVRegToFunctionArg(a *backend.FunctionABI, argIndex int, reg regalloc.VReg, def *backend.SSAValueDefinition, slotBegin int64) {\n+\targ := &a.Args[argIndex]\n+\tif def != nil && def.IsFromInstr() {\n+\t\t// Constant instructions are inlined.\n+\t\tif inst := def.Instr; inst.Constant() {\n+\t\t\tval := inst.Return()\n+\t\t\tvalType := val.Type()\n+\t\t\tv := inst.ConstantVal()\n+\t\t\tm.insertLoadConstant(v, valType, reg)\n+\t\t}\n+\t}\n+\tif arg.Kind == backend.ABIArgKindReg {\n+\t\tm.InsertMove(arg.Reg, reg, arg.Type)\n+\t} else {\n+\t\t// TODO: we could use pair store if there's consecutive stores for the same type.\n+\t\t//\n+\t\t// Note that at this point, stack pointer is already adjusted.\n+\t\tbits := arg.Type.Bits()\n+\t\tamode := m.resolveAddressModeForOffset(arg.Offset-slotBegin, bits, spVReg, false)\n+\t\tstore := m.allocateInstr()\n+\t\tstore.asStore(operandNR(reg), amode, bits)\n+\t\tm.insert(store)\n+\t}\n+}\n+\n+func (m *machine) callerGenFunctionReturnVReg(a *backend.FunctionABI, retIndex int, reg regalloc.VReg, slotBegin int64) {\n+\tr := &a.Rets[retIndex]\n+\tif r.Kind == backend.ABIArgKindReg {\n+\t\tm.InsertMove(reg, r.Reg, r.Type)\n+\t} else {\n+\t\t// TODO: we could use pair load if there's consecutive loads for the same type.\n+\t\tamode := m.resolveAddressModeForOffset(a.ArgStackSize+r.Offset-slotBegin, r.Type.Bits(), spVReg, false)\n+\t\tldr := m.allocateInstr()\n+\t\tswitch r.Type {\n+\t\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\t\tldr.asULoad(operandNR(reg), amode, r.Type.Bits())\n+\t\tcase ssa.TypeF32, ssa.TypeF64, ssa.TypeV128:\n+\t\t\tldr.asFpuLoad(operandNR(reg), amode, r.Type.Bits())\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tm.insert(ldr)\n+\t}\n+}\n+\n+func (m *machine) resolveAddressModeForOffsetAndInsert(cur *instruction, offset int64, dstBits byte, rn regalloc.VReg, allowTmpRegUse bool) (*instruction, addressMode) {\n+\texct := m.executableContext\n+\texct.PendingInstructions = exct.PendingInstructions[:0]\n+\tmode := m.resolveAddressModeForOffset(offset, dstBits, rn, allowTmpRegUse)\n+\tfor _, instr := range exct.PendingInstructions {\n+\t\tcur = linkInstr(cur, instr)\n+\t}\n+\treturn cur, mode\n+}\n+\n+func (m *machine) resolveAddressModeForOffset(offset int64, dstBits byte, rn regalloc.VReg, allowTmpRegUse bool) addressMode {\n+\tif rn.RegType() != regalloc.RegTypeInt {\n+\t\tpanic(\"BUG: rn should be a pointer: \" + formatVRegSized(rn, 64))\n+\t}\n+\tvar amode addressMode\n+\tif offsetFitsInAddressModeKindRegUnsignedImm12(dstBits, offset) {\n+\t\tamode = addressMode{kind: addressModeKindRegUnsignedImm12, rn: rn, imm: offset}\n+\t} else if offsetFitsInAddressModeKindRegSignedImm9(offset) {\n+\t\tamode = addressMode{kind: addressModeKindRegSignedImm9, rn: rn, imm: offset}\n+\t} else {\n+\t\tvar indexReg regalloc.VReg\n+\t\tif allowTmpRegUse {\n+\t\t\tm.lowerConstantI64(tmpRegVReg, offset)\n+\t\t\tindexReg = tmpRegVReg\n+\t\t} else {\n+\t\t\tindexReg = m.compiler.AllocateVReg(ssa.TypeI64)\n+\t\t\tm.lowerConstantI64(indexReg, offset)\n+\t\t}\n+\t\tamode = addressMode{kind: addressModeKindRegReg, rn: rn, rm: indexReg, extOp: extendOpUXTX /* indicates index rm is 64-bit */}\n+\t}\n+\treturn amode\n+}\n+\n+func (m *machine) lowerCall(si *ssa.Instruction) {\n+\tisDirectCall := si.Opcode() == ssa.OpcodeCall\n+\tvar indirectCalleePtr ssa.Value\n+\tvar directCallee ssa.FuncRef\n+\tvar sigID ssa.SignatureID\n+\tvar args []ssa.Value\n+\tif isDirectCall {\n+\t\tdirectCallee, sigID, args = si.CallData()\n+\t} else {\n+\t\tindirectCalleePtr, sigID, args = si.CallIndirectData()\n+\t}\n+\tcalleeABI := m.compiler.GetFunctionABI(m.compiler.SSABuilder().ResolveSignature(sigID))\n+\n+\tstackSlotSize := int64(calleeABI.AlignedArgResultStackSlotSize())\n+\tif m.maxRequiredStackSizeForCalls < stackSlotSize+16 {\n+\t\tm.maxRequiredStackSizeForCalls = stackSlotSize + 16 // return address frame.\n+\t}\n+\n+\tfor i, arg := range args {\n+\t\treg := m.compiler.VRegOf(arg)\n+\t\tdef := m.compiler.ValueDefinition(arg)\n+\t\tm.callerGenVRegToFunctionArg(calleeABI, i, reg, def, stackSlotSize)\n+\t}\n+\n+\tif isDirectCall {\n+\t\tcall := m.allocateInstr()\n+\t\tcall.asCall(directCallee, calleeABI)\n+\t\tm.insert(call)\n+\t} else {\n+\t\tptr := m.compiler.VRegOf(indirectCalleePtr)\n+\t\tcallInd := m.allocateInstr()\n+\t\tcallInd.asCallIndirect(ptr, calleeABI)\n+\t\tm.insert(callInd)\n+\t}\n+\n+\tvar index int\n+\tr1, rs := si.Returns()\n+\tif r1.Valid() {\n+\t\tm.callerGenFunctionReturnVReg(calleeABI, 0, m.compiler.VRegOf(r1), stackSlotSize)\n+\t\tindex++\n+\t}\n+\n+\tfor _, r := range rs {\n+\t\tm.callerGenFunctionReturnVReg(calleeABI, index, m.compiler.VRegOf(r), stackSlotSize)\n+\t\tindex++\n+\t}\n+}\n+\n+func (m *machine) insertAddOrSubStackPointer(rd regalloc.VReg, diff int64, add bool) {\n+\tif imm12Operand, ok := asImm12Operand(uint64(diff)); ok {\n+\t\talu := m.allocateInstr()\n+\t\tvar ao aluOp\n+\t\tif add {\n+\t\t\tao = aluOpAdd\n+\t\t} else {\n+\t\t\tao = aluOpSub\n+\t\t}\n+\t\talu.asALU(ao, operandNR(rd), operandNR(spVReg), imm12Operand, true)\n+\t\tm.insert(alu)\n+\t} else {\n+\t\tm.lowerConstantI64(tmpRegVReg, diff)\n+\t\talu := m.allocateInstr()\n+\t\tvar ao aluOp\n+\t\tif add {\n+\t\t\tao = aluOpAdd\n+\t\t} else {\n+\t\t\tao = aluOpSub\n+\t\t}\n+\t\talu.asALU(ao, operandNR(rd), operandNR(spVReg), operandNR(tmpRegVReg), true)\n+\t\tm.insert(alu)\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/abi_entry_arm64.go",
          "status": "added",
          "additions": 9,
          "deletions": 0,
          "patch": "@@ -0,0 +1,9 @@\n+package arm64\n+\n+// entrypoint enters the machine code generated by this backend which begins with the preamble generated by functionABI.EmitGoEntryPreamble below.\n+// This implements wazevo.entrypoint, and see the comments there for detail.\n+func entrypoint(preambleExecutable, functionExecutable *byte, executionContextPtr uintptr, moduleContextPtr *byte, paramResultPtr *uint64, goAllocatedStackSlicePtr uintptr)\n+\n+// afterGoFunctionCallEntrypoint enters the machine code after growing the stack.\n+// This implements wazevo.afterGoFunctionCallEntrypoint, and see the comments there for detail.\n+func afterGoFunctionCallEntrypoint(executable *byte, executionContextPtr uintptr, stackPointer, framePointer uintptr)"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/abi_entry_arm64.s",
          "status": "added",
          "additions": 29,
          "deletions": 0,
          "patch": "@@ -0,0 +1,29 @@\n+//go:build arm64\n+\n+#include \"funcdata.h\"\n+#include \"textflag.h\"\n+\n+// See the comments on EmitGoEntryPreamble for what this function is supposed to do.\n+TEXT \u00b7entrypoint(SB), NOSPLIT|NOFRAME, $0-48\n+\tMOVD preambleExecutable+0(FP), R27\n+\tMOVD functionExectuable+8(FP), R24\n+\tMOVD executionContextPtr+16(FP), R0\n+\tMOVD moduleContextPtr+24(FP), R1\n+\tMOVD paramResultSlicePtr+32(FP), R19\n+\tMOVD goAllocatedStackSlicePtr+40(FP), R26\n+\tJMP  (R27)\n+\n+TEXT \u00b7afterGoFunctionCallEntrypoint(SB), NOSPLIT|NOFRAME, $0-32\n+\tMOVD goCallReturnAddress+0(FP), R20\n+\tMOVD executionContextPtr+8(FP), R0\n+\tMOVD stackPointer+16(FP), R19\n+\n+\t// Save the current FP(R29), SP and LR(R30) into the wazevo.executionContext (stored in R0).\n+\tMOVD R29, 16(R0) // Store FP(R29) into [RO, #ExecutionContextOffsets.OriginalFramePointer]\n+\tMOVD RSP, R27    // Move SP to R27 (temporary register) since SP cannot be stored directly in str instructions.\n+\tMOVD R27, 24(R0) // Store R27 into [RO, #ExecutionContextOffsets.OriginalFramePointer]\n+\tMOVD R30, 32(R0) // Store R30 into [R0, #ExecutionContextOffsets.GoReturnAddress]\n+\n+\t// Load the new stack pointer (which sits somewhere in Go-allocated stack) into SP.\n+\tMOVD R19, RSP\n+\tJMP  (R20)"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/abi_entry_preamble.go",
          "status": "added",
          "additions": 230,
          "deletions": 0,
          "patch": "@@ -0,0 +1,230 @@\n+package arm64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+// CompileEntryPreamble implements backend.Machine. This assumes `entrypoint` function (in abi_go_entry_arm64.s) passes:\n+//\n+//  1. First (execution context ptr) and Second arguments are already passed in x0, and x1.\n+//  2. param/result slice ptr in x19; the pointer to []uint64{} which is used to pass arguments and accept return values.\n+//  3. Go-allocated stack slice ptr in x26.\n+//  4. Function executable in x24.\n+//\n+// also SP and FP are correct Go-runtime-based values, and LR is the return address to the Go-side caller.\n+func (m *machine) CompileEntryPreamble(signature *ssa.Signature) []byte {\n+\troot := m.constructEntryPreamble(signature)\n+\tm.encode(root)\n+\treturn m.compiler.Buf()\n+}\n+\n+var (\n+\texecutionContextPtrReg = x0VReg\n+\t// callee-saved regs so that they can be used in the prologue and epilogue.\n+\tparamResultSlicePtr      = x19VReg\n+\tsavedExecutionContextPtr = x20VReg\n+\t// goAllocatedStackPtr is not used in the epilogue.\n+\tgoAllocatedStackPtr = x26VReg\n+\t// paramResultSliceCopied is not used in the epilogue.\n+\tparamResultSliceCopied = x25VReg\n+\t// tmpRegVReg is not used in the epilogue.\n+\tfunctionExecutable = x24VReg\n+)\n+\n+func (m *machine) goEntryPreamblePassArg(cur *instruction, paramSlicePtr regalloc.VReg, arg *backend.ABIArg, argStartOffsetFromSP int64) *instruction {\n+\ttyp := arg.Type\n+\tbits := typ.Bits()\n+\tisStackArg := arg.Kind == backend.ABIArgKindStack\n+\n+\tvar loadTargetReg operand\n+\tif !isStackArg {\n+\t\tloadTargetReg = operandNR(arg.Reg)\n+\t} else {\n+\t\tswitch typ {\n+\t\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\t\tloadTargetReg = operandNR(x15VReg)\n+\t\tcase ssa.TypeF32, ssa.TypeF64, ssa.TypeV128:\n+\t\t\tloadTargetReg = operandNR(v15VReg)\n+\t\tdefault:\n+\t\t\tpanic(\"TODO?\")\n+\t\t}\n+\t}\n+\n+\tvar postIndexImm int64\n+\tif typ == ssa.TypeV128 {\n+\t\tpostIndexImm = 16 // v128 is represented as 2x64-bit in Go slice.\n+\t} else {\n+\t\tpostIndexImm = 8\n+\t}\n+\tloadMode := addressMode{kind: addressModeKindPostIndex, rn: paramSlicePtr, imm: postIndexImm}\n+\n+\tinstr := m.allocateInstr()\n+\tswitch typ {\n+\tcase ssa.TypeI32:\n+\t\tinstr.asULoad(loadTargetReg, loadMode, 32)\n+\tcase ssa.TypeI64:\n+\t\tinstr.asULoad(loadTargetReg, loadMode, 64)\n+\tcase ssa.TypeF32:\n+\t\tinstr.asFpuLoad(loadTargetReg, loadMode, 32)\n+\tcase ssa.TypeF64:\n+\t\tinstr.asFpuLoad(loadTargetReg, loadMode, 64)\n+\tcase ssa.TypeV128:\n+\t\tinstr.asFpuLoad(loadTargetReg, loadMode, 128)\n+\t}\n+\tcur = linkInstr(cur, instr)\n+\n+\tif isStackArg {\n+\t\tvar storeMode addressMode\n+\t\tcur, storeMode = m.resolveAddressModeForOffsetAndInsert(cur, argStartOffsetFromSP+arg.Offset, bits, spVReg, true)\n+\t\ttoStack := m.allocateInstr()\n+\t\ttoStack.asStore(loadTargetReg, storeMode, bits)\n+\t\tcur = linkInstr(cur, toStack)\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) goEntryPreamblePassResult(cur *instruction, resultSlicePtr regalloc.VReg, result *backend.ABIArg, resultStartOffsetFromSP int64) *instruction {\n+\tisStackArg := result.Kind == backend.ABIArgKindStack\n+\ttyp := result.Type\n+\tbits := typ.Bits()\n+\n+\tvar storeTargetReg operand\n+\tif !isStackArg {\n+\t\tstoreTargetReg = operandNR(result.Reg)\n+\t} else {\n+\t\tswitch typ {\n+\t\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\t\tstoreTargetReg = operandNR(x15VReg)\n+\t\tcase ssa.TypeF32, ssa.TypeF64, ssa.TypeV128:\n+\t\t\tstoreTargetReg = operandNR(v15VReg)\n+\t\tdefault:\n+\t\t\tpanic(\"TODO?\")\n+\t\t}\n+\t}\n+\n+\tvar postIndexImm int64\n+\tif typ == ssa.TypeV128 {\n+\t\tpostIndexImm = 16 // v128 is represented as 2x64-bit in Go slice.\n+\t} else {\n+\t\tpostIndexImm = 8\n+\t}\n+\n+\tif isStackArg {\n+\t\tvar loadMode addressMode\n+\t\tcur, loadMode = m.resolveAddressModeForOffsetAndInsert(cur, resultStartOffsetFromSP+result.Offset, bits, spVReg, true)\n+\t\ttoReg := m.allocateInstr()\n+\t\tswitch typ {\n+\t\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\t\ttoReg.asULoad(storeTargetReg, loadMode, bits)\n+\t\tcase ssa.TypeF32, ssa.TypeF64, ssa.TypeV128:\n+\t\t\ttoReg.asFpuLoad(storeTargetReg, loadMode, bits)\n+\t\tdefault:\n+\t\t\tpanic(\"TODO?\")\n+\t\t}\n+\t\tcur = linkInstr(cur, toReg)\n+\t}\n+\n+\tmode := addressMode{kind: addressModeKindPostIndex, rn: resultSlicePtr, imm: postIndexImm}\n+\tinstr := m.allocateInstr()\n+\tinstr.asStore(storeTargetReg, mode, bits)\n+\tcur = linkInstr(cur, instr)\n+\treturn cur\n+}\n+\n+func (m *machine) constructEntryPreamble(sig *ssa.Signature) (root *instruction) {\n+\tabi := backend.FunctionABI{}\n+\tabi.Init(sig, intParamResultRegs, floatParamResultRegs)\n+\n+\troot = m.allocateNop()\n+\n+\t//// ----------------------------------- prologue ----------------------------------- ////\n+\n+\t// First, we save executionContextPtrReg into a callee-saved register so that it can be used in epilogue as well.\n+\t// \t\tmov savedExecutionContextPtr, x0\n+\tcur := m.move64(savedExecutionContextPtr, executionContextPtrReg, root)\n+\n+\t// Next, save the current FP, SP and LR into the wazevo.executionContext:\n+\t// \t\tstr fp, [savedExecutionContextPtr, #OriginalFramePointer]\n+\t//      mov tmp, sp ;; sp cannot be str'ed directly.\n+\t// \t\tstr sp, [savedExecutionContextPtr, #OriginalStackPointer]\n+\t// \t\tstr lr, [savedExecutionContextPtr, #GoReturnAddress]\n+\tcur = m.loadOrStoreAtExecutionContext(fpVReg, wazevoapi.ExecutionContextOffsetOriginalFramePointer, true, cur)\n+\tcur = m.move64(tmpRegVReg, spVReg, cur)\n+\tcur = m.loadOrStoreAtExecutionContext(tmpRegVReg, wazevoapi.ExecutionContextOffsetOriginalStackPointer, true, cur)\n+\tcur = m.loadOrStoreAtExecutionContext(lrVReg, wazevoapi.ExecutionContextOffsetGoReturnAddress, true, cur)\n+\n+\t// Then, move the Go-allocated stack pointer to SP:\n+\t// \t\tmov sp, goAllocatedStackPtr\n+\tcur = m.move64(spVReg, goAllocatedStackPtr, cur)\n+\n+\tprReg := paramResultSlicePtr\n+\tif len(abi.Args) > 2 && len(abi.Rets) > 0 {\n+\t\t// paramResultSlicePtr is modified during the execution of goEntryPreamblePassArg,\n+\t\t// so copy it to another reg.\n+\t\tcur = m.move64(paramResultSliceCopied, paramResultSlicePtr, cur)\n+\t\tprReg = paramResultSliceCopied\n+\t}\n+\n+\tstackSlotSize := int64(abi.AlignedArgResultStackSlotSize())\n+\tfor i := range abi.Args {\n+\t\tif i < 2 {\n+\t\t\t// module context ptr and execution context ptr are passed in x0 and x1 by the Go assembly function.\n+\t\t\tcontinue\n+\t\t}\n+\t\targ := &abi.Args[i]\n+\t\tcur = m.goEntryPreamblePassArg(cur, prReg, arg, -stackSlotSize)\n+\t}\n+\n+\t// Call the real function.\n+\tbl := m.allocateInstr()\n+\tbl.asCallIndirect(functionExecutable, &abi)\n+\tcur = linkInstr(cur, bl)\n+\n+\t///// ----------------------------------- epilogue ----------------------------------- /////\n+\n+\t// Store the register results into paramResultSlicePtr.\n+\tfor i := range abi.Rets {\n+\t\tcur = m.goEntryPreamblePassResult(cur, paramResultSlicePtr, &abi.Rets[i], abi.ArgStackSize-stackSlotSize)\n+\t}\n+\n+\t// Finally, restore the FP, SP and LR, and return to the Go code.\n+\t// \t\tldr fp, [savedExecutionContextPtr, #OriginalFramePointer]\n+\t// \t\tldr tmp, [savedExecutionContextPtr, #OriginalStackPointer]\n+\t//      mov sp, tmp ;; sp cannot be str'ed directly.\n+\t// \t\tldr lr, [savedExecutionContextPtr, #GoReturnAddress]\n+\t// \t\tret ;; --> return to the Go code\n+\tcur = m.loadOrStoreAtExecutionContext(fpVReg, wazevoapi.ExecutionContextOffsetOriginalFramePointer, false, cur)\n+\tcur = m.loadOrStoreAtExecutionContext(tmpRegVReg, wazevoapi.ExecutionContextOffsetOriginalStackPointer, false, cur)\n+\tcur = m.move64(spVReg, tmpRegVReg, cur)\n+\tcur = m.loadOrStoreAtExecutionContext(lrVReg, wazevoapi.ExecutionContextOffsetGoReturnAddress, false, cur)\n+\tretInst := m.allocateInstr()\n+\tretInst.asRet()\n+\tlinkInstr(cur, retInst)\n+\treturn\n+}\n+\n+func (m *machine) move64(dst, src regalloc.VReg, prev *instruction) *instruction {\n+\tinstr := m.allocateInstr()\n+\tinstr.asMove64(dst, src)\n+\treturn linkInstr(prev, instr)\n+}\n+\n+func (m *machine) loadOrStoreAtExecutionContext(d regalloc.VReg, offset wazevoapi.Offset, store bool, prev *instruction) *instruction {\n+\tinstr := m.allocateInstr()\n+\tmode := addressMode{kind: addressModeKindRegUnsignedImm12, rn: savedExecutionContextPtr, imm: offset.I64()}\n+\tif store {\n+\t\tinstr.asStore(operandNR(d), mode, 64)\n+\t} else {\n+\t\tinstr.asULoad(operandNR(d), mode, 64)\n+\t}\n+\treturn linkInstr(prev, instr)\n+}\n+\n+func linkInstr(prev, next *instruction) *instruction {\n+\tprev.next = next\n+\tnext.prev = prev\n+\treturn next\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/abi_go_call.go",
          "status": "added",
          "additions": 428,
          "deletions": 0,
          "patch": "@@ -0,0 +1,428 @@\n+package arm64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+var calleeSavedRegistersSorted = []regalloc.VReg{\n+\tx19VReg, x20VReg, x21VReg, x22VReg, x23VReg, x24VReg, x25VReg, x26VReg, x28VReg,\n+\tv18VReg, v19VReg, v20VReg, v21VReg, v22VReg, v23VReg, v24VReg, v25VReg, v26VReg, v27VReg, v28VReg, v29VReg, v30VReg, v31VReg,\n+}\n+\n+// CompileGoFunctionTrampoline implements backend.Machine.\n+func (m *machine) CompileGoFunctionTrampoline(exitCode wazevoapi.ExitCode, sig *ssa.Signature, needModuleContextPtr bool) []byte {\n+\texct := m.executableContext\n+\targBegin := 1 // Skips exec context by default.\n+\tif needModuleContextPtr {\n+\t\targBegin++\n+\t}\n+\n+\tabi := &backend.FunctionABI{}\n+\tabi.Init(sig, intParamResultRegs, floatParamResultRegs)\n+\tm.currentABI = abi\n+\n+\tcur := m.allocateInstr()\n+\tcur.asNop0()\n+\texct.RootInstr = cur\n+\n+\t// Execution context is always the first argument.\n+\texecCtrPtr := x0VReg\n+\n+\t// In the following, we create the following stack layout:\n+\t//\n+\t//                   (high address)\n+\t//     SP ------> +-----------------+  <----+\n+\t//                |     .......     |       |\n+\t//                |      ret Y      |       |\n+\t//                |     .......     |       |\n+\t//                |      ret 0      |       |\n+\t//                |      arg X      |       |  size_of_arg_ret\n+\t//                |     .......     |       |\n+\t//                |      arg 1      |       |\n+\t//                |      arg 0      |  <----+ <-------- originalArg0Reg\n+\t//                | size_of_arg_ret |\n+\t//                |  ReturnAddress  |\n+\t//                +-----------------+ <----+\n+\t//                |      xxxx       |      |  ;; might be padded to make it 16-byte aligned.\n+\t//           +--->|  arg[N]/ret[M]  |      |\n+\t//  sliceSize|    |   ............  |      | goCallStackSize\n+\t//           |    |  arg[1]/ret[1]  |      |\n+\t//           +--->|  arg[0]/ret[0]  | <----+ <-------- arg0ret0AddrReg\n+\t//                |    sliceSize    |\n+\t//                |   frame_size    |\n+\t//                +-----------------+\n+\t//                   (low address)\n+\t//\n+\t// where the region of \"arg[0]/ret[0] ... arg[N]/ret[M]\" is the stack used by the Go functions,\n+\t// therefore will be accessed as the usual []uint64. So that's where we need to pass/receive\n+\t// the arguments/return values.\n+\n+\t// First of all, to update the SP, and create \"ReturnAddress + size_of_arg_ret\".\n+\tcur = m.createReturnAddrAndSizeOfArgRetSlot(cur)\n+\n+\tconst frameInfoSize = 16 // == frame_size + sliceSize.\n+\n+\t// Next, we should allocate the stack for the Go function call if necessary.\n+\tgoCallStackSize, sliceSizeInBytes := backend.GoFunctionCallRequiredStackSize(sig, argBegin)\n+\tcur = m.insertStackBoundsCheck(goCallStackSize+frameInfoSize, cur)\n+\n+\toriginalArg0Reg := x17VReg // Caller save, so we can use it for whatever we want.\n+\tif m.currentABI.AlignedArgResultStackSlotSize() > 0 {\n+\t\t// At this point, SP points to `ReturnAddress`, so add 16 to get the original arg 0 slot.\n+\t\tcur = m.addsAddOrSubStackPointer(cur, originalArg0Reg, frameInfoSize, true)\n+\t}\n+\n+\t// Save the callee saved registers.\n+\tcur = m.saveRegistersInExecutionContext(cur, calleeSavedRegistersSorted)\n+\n+\tif needModuleContextPtr {\n+\t\toffset := wazevoapi.ExecutionContextOffsetGoFunctionCallCalleeModuleContextOpaque.I64()\n+\t\tif !offsetFitsInAddressModeKindRegUnsignedImm12(64, offset) {\n+\t\t\tpanic(\"BUG: too large or un-aligned offset for goFunctionCallCalleeModuleContextOpaque in execution context\")\n+\t\t}\n+\n+\t\t// Module context is always the second argument.\n+\t\tmoduleCtrPtr := x1VReg\n+\t\tstore := m.allocateInstr()\n+\t\tamode := addressMode{kind: addressModeKindRegUnsignedImm12, rn: execCtrPtr, imm: offset}\n+\t\tstore.asStore(operandNR(moduleCtrPtr), amode, 64)\n+\t\tcur = linkInstr(cur, store)\n+\t}\n+\n+\t// Advances the stack pointer.\n+\tcur = m.addsAddOrSubStackPointer(cur, spVReg, goCallStackSize, false)\n+\n+\t// Copy the pointer to x15VReg.\n+\targ0ret0AddrReg := x15VReg // Caller save, so we can use it for whatever we want.\n+\tcopySp := m.allocateInstr()\n+\tcopySp.asMove64(arg0ret0AddrReg, spVReg)\n+\tcur = linkInstr(cur, copySp)\n+\n+\t// Next, we need to store all the arguments to the stack in the typical Wasm stack style.\n+\tfor i := range abi.Args[argBegin:] {\n+\t\targ := &abi.Args[argBegin+i]\n+\t\tstore := m.allocateInstr()\n+\t\tvar v regalloc.VReg\n+\t\tif arg.Kind == backend.ABIArgKindReg {\n+\t\t\tv = arg.Reg\n+\t\t} else {\n+\t\t\tcur, v = m.goFunctionCallLoadStackArg(cur, originalArg0Reg, arg,\n+\t\t\t\t// Caller save, so we can use it for whatever we want.\n+\t\t\t\tx11VReg, v11VReg)\n+\t\t}\n+\n+\t\tvar sizeInBits byte\n+\t\tif arg.Type == ssa.TypeV128 {\n+\t\t\tsizeInBits = 128\n+\t\t} else {\n+\t\t\tsizeInBits = 64\n+\t\t}\n+\t\tstore.asStore(operandNR(v),\n+\t\t\taddressMode{\n+\t\t\t\tkind: addressModeKindPostIndex,\n+\t\t\t\trn:   arg0ret0AddrReg, imm: int64(sizeInBits / 8),\n+\t\t\t}, sizeInBits)\n+\t\tcur = linkInstr(cur, store)\n+\t}\n+\n+\t// Finally, now that we've advanced SP to arg[0]/ret[0], we allocate `frame_size + sliceSize`.\n+\tvar frameSizeReg, sliceSizeReg regalloc.VReg\n+\tif goCallStackSize > 0 {\n+\t\tcur = m.lowerConstantI64AndInsert(cur, tmpRegVReg, goCallStackSize)\n+\t\tframeSizeReg = tmpRegVReg\n+\t\tcur = m.lowerConstantI64AndInsert(cur, x16VReg, sliceSizeInBytes/8)\n+\t\tsliceSizeReg = x16VReg\n+\t} else {\n+\t\tframeSizeReg = xzrVReg\n+\t\tsliceSizeReg = xzrVReg\n+\t}\n+\t_amode := addressModePreOrPostIndex(spVReg, -16, true)\n+\tstoreP := m.allocateInstr()\n+\tstoreP.asStorePair64(frameSizeReg, sliceSizeReg, _amode)\n+\tcur = linkInstr(cur, storeP)\n+\n+\t// Set the exit status on the execution context.\n+\tcur = m.setExitCode(cur, x0VReg, exitCode)\n+\n+\t// Save the current stack pointer.\n+\tcur = m.saveCurrentStackPointer(cur, x0VReg)\n+\n+\t// Exit the execution.\n+\tcur = m.storeReturnAddressAndExit(cur)\n+\n+\t// After the call, we need to restore the callee saved registers.\n+\tcur = m.restoreRegistersInExecutionContext(cur, calleeSavedRegistersSorted)\n+\n+\t// Get the pointer to the arg[0]/ret[0]: We need to skip `frame_size + sliceSize`.\n+\tif len(abi.Rets) > 0 {\n+\t\tcur = m.addsAddOrSubStackPointer(cur, arg0ret0AddrReg, frameInfoSize, true)\n+\t}\n+\n+\t// Advances the SP so that it points to `ReturnAddress`.\n+\tcur = m.addsAddOrSubStackPointer(cur, spVReg, frameInfoSize+goCallStackSize, true)\n+\tldr := m.allocateInstr()\n+\t// And load the return address.\n+\tldr.asULoad(operandNR(lrVReg),\n+\t\taddressModePreOrPostIndex(spVReg, 16 /* stack pointer must be 16-byte aligned. */, false /* increment after loads */), 64)\n+\tcur = linkInstr(cur, ldr)\n+\n+\toriginalRet0Reg := x17VReg // Caller save, so we can use it for whatever we want.\n+\tif m.currentABI.RetStackSize > 0 {\n+\t\tcur = m.addsAddOrSubStackPointer(cur, originalRet0Reg, m.currentABI.ArgStackSize, true)\n+\t}\n+\n+\t// Make the SP point to the original address (above the result slot).\n+\tif s := int64(m.currentABI.AlignedArgResultStackSlotSize()); s > 0 {\n+\t\tcur = m.addsAddOrSubStackPointer(cur, spVReg, s, true)\n+\t}\n+\n+\tfor i := range abi.Rets {\n+\t\tr := &abi.Rets[i]\n+\t\tif r.Kind == backend.ABIArgKindReg {\n+\t\t\tloadIntoReg := m.allocateInstr()\n+\t\t\tmode := addressMode{kind: addressModeKindPostIndex, rn: arg0ret0AddrReg}\n+\t\t\tswitch r.Type {\n+\t\t\tcase ssa.TypeI32:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoReg.asULoad(operandNR(r.Reg), mode, 32)\n+\t\t\tcase ssa.TypeI64:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoReg.asULoad(operandNR(r.Reg), mode, 64)\n+\t\t\tcase ssa.TypeF32:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoReg.asFpuLoad(operandNR(r.Reg), mode, 32)\n+\t\t\tcase ssa.TypeF64:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoReg.asFpuLoad(operandNR(r.Reg), mode, 64)\n+\t\t\tcase ssa.TypeV128:\n+\t\t\t\tmode.imm = 16\n+\t\t\t\tloadIntoReg.asFpuLoad(operandNR(r.Reg), mode, 128)\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"TODO\")\n+\t\t\t}\n+\t\t\tcur = linkInstr(cur, loadIntoReg)\n+\t\t} else {\n+\t\t\t// First we need to load the value to a temporary just like ^^.\n+\t\t\tintTmp, floatTmp := x11VReg, v11VReg\n+\t\t\tloadIntoTmpReg := m.allocateInstr()\n+\t\t\tmode := addressMode{kind: addressModeKindPostIndex, rn: arg0ret0AddrReg}\n+\t\t\tvar resultReg regalloc.VReg\n+\t\t\tswitch r.Type {\n+\t\t\tcase ssa.TypeI32:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoTmpReg.asULoad(operandNR(intTmp), mode, 32)\n+\t\t\t\tresultReg = intTmp\n+\t\t\tcase ssa.TypeI64:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoTmpReg.asULoad(operandNR(intTmp), mode, 64)\n+\t\t\t\tresultReg = intTmp\n+\t\t\tcase ssa.TypeF32:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoTmpReg.asFpuLoad(operandNR(floatTmp), mode, 32)\n+\t\t\t\tresultReg = floatTmp\n+\t\t\tcase ssa.TypeF64:\n+\t\t\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\t\t\tloadIntoTmpReg.asFpuLoad(operandNR(floatTmp), mode, 64)\n+\t\t\t\tresultReg = floatTmp\n+\t\t\tcase ssa.TypeV128:\n+\t\t\t\tmode.imm = 16\n+\t\t\t\tloadIntoTmpReg.asFpuLoad(operandNR(floatTmp), mode, 128)\n+\t\t\t\tresultReg = floatTmp\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"TODO\")\n+\t\t\t}\n+\t\t\tcur = linkInstr(cur, loadIntoTmpReg)\n+\t\t\tcur = m.goFunctionCallStoreStackResult(cur, originalRet0Reg, r, resultReg)\n+\t\t}\n+\t}\n+\n+\tret := m.allocateInstr()\n+\tret.asRet()\n+\tlinkInstr(cur, ret)\n+\n+\tm.encode(m.executableContext.RootInstr)\n+\treturn m.compiler.Buf()\n+}\n+\n+func (m *machine) saveRegistersInExecutionContext(cur *instruction, regs []regalloc.VReg) *instruction {\n+\toffset := wazevoapi.ExecutionContextOffsetSavedRegistersBegin.I64()\n+\tfor _, v := range regs {\n+\t\tstore := m.allocateInstr()\n+\t\tvar sizeInBits byte\n+\t\tswitch v.RegType() {\n+\t\tcase regalloc.RegTypeInt:\n+\t\t\tsizeInBits = 64\n+\t\tcase regalloc.RegTypeFloat:\n+\t\t\tsizeInBits = 128\n+\t\t}\n+\t\tstore.asStore(operandNR(v),\n+\t\t\taddressMode{\n+\t\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\t\t// Execution context is always the first argument.\n+\t\t\t\trn: x0VReg, imm: offset,\n+\t\t\t}, sizeInBits)\n+\t\tstore.prev = cur\n+\t\tcur.next = store\n+\t\tcur = store\n+\t\toffset += 16 // Imm12 must be aligned 16 for vector regs, so we unconditionally store regs at the offset of multiple of 16.\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) restoreRegistersInExecutionContext(cur *instruction, regs []regalloc.VReg) *instruction {\n+\toffset := wazevoapi.ExecutionContextOffsetSavedRegistersBegin.I64()\n+\tfor _, v := range regs {\n+\t\tload := m.allocateInstr()\n+\t\tvar as func(dst operand, amode addressMode, sizeInBits byte)\n+\t\tvar sizeInBits byte\n+\t\tswitch v.RegType() {\n+\t\tcase regalloc.RegTypeInt:\n+\t\t\tas = load.asULoad\n+\t\t\tsizeInBits = 64\n+\t\tcase regalloc.RegTypeFloat:\n+\t\t\tas = load.asFpuLoad\n+\t\t\tsizeInBits = 128\n+\t\t}\n+\t\tas(operandNR(v),\n+\t\t\taddressMode{\n+\t\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\t\t// Execution context is always the first argument.\n+\t\t\t\trn: x0VReg, imm: offset,\n+\t\t\t}, sizeInBits)\n+\t\tcur = linkInstr(cur, load)\n+\t\toffset += 16 // Imm12 must be aligned 16 for vector regs, so we unconditionally load regs at the offset of multiple of 16.\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) lowerConstantI64AndInsert(cur *instruction, dst regalloc.VReg, v int64) *instruction {\n+\texct := m.executableContext\n+\texct.PendingInstructions = exct.PendingInstructions[:0]\n+\tm.lowerConstantI64(dst, v)\n+\tfor _, instr := range exct.PendingInstructions {\n+\t\tcur = linkInstr(cur, instr)\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) lowerConstantI32AndInsert(cur *instruction, dst regalloc.VReg, v int32) *instruction {\n+\texct := m.executableContext\n+\texct.PendingInstructions = exct.PendingInstructions[:0]\n+\tm.lowerConstantI32(dst, v)\n+\tfor _, instr := range exct.PendingInstructions {\n+\t\tcur = linkInstr(cur, instr)\n+\t}\n+\treturn cur\n+}\n+\n+func (m *machine) setExitCode(cur *instruction, execCtr regalloc.VReg, exitCode wazevoapi.ExitCode) *instruction {\n+\tconstReg := x17VReg // caller-saved, so we can use it.\n+\tcur = m.lowerConstantI32AndInsert(cur, constReg, int32(exitCode))\n+\n+\t// Set the exit status on the execution context.\n+\tsetExistStatus := m.allocateInstr()\n+\tsetExistStatus.asStore(operandNR(constReg),\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   execCtr, imm: wazevoapi.ExecutionContextOffsetExitCodeOffset.I64(),\n+\t\t}, 32)\n+\tcur = linkInstr(cur, setExistStatus)\n+\treturn cur\n+}\n+\n+func (m *machine) storeReturnAddressAndExit(cur *instruction) *instruction {\n+\t// Read the return address into tmp, and store it in the execution context.\n+\tadr := m.allocateInstr()\n+\tadr.asAdr(tmpRegVReg, exitSequenceSize+8)\n+\tcur = linkInstr(cur, adr)\n+\n+\tstoreReturnAddr := m.allocateInstr()\n+\tstoreReturnAddr.asStore(operandNR(tmpRegVReg),\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\t// Execution context is always the first argument.\n+\t\t\trn: x0VReg, imm: wazevoapi.ExecutionContextOffsetGoCallReturnAddress.I64(),\n+\t\t}, 64)\n+\tcur = linkInstr(cur, storeReturnAddr)\n+\n+\t// Exit the execution.\n+\ttrapSeq := m.allocateInstr()\n+\ttrapSeq.asExitSequence(x0VReg)\n+\tcur = linkInstr(cur, trapSeq)\n+\treturn cur\n+}\n+\n+func (m *machine) saveCurrentStackPointer(cur *instruction, execCtr regalloc.VReg) *instruction {\n+\t// Save the current stack pointer:\n+\t// \tmov tmp, sp,\n+\t// \tstr tmp, [exec_ctx, #stackPointerBeforeGoCall]\n+\tmovSp := m.allocateInstr()\n+\tmovSp.asMove64(tmpRegVReg, spVReg)\n+\tcur = linkInstr(cur, movSp)\n+\n+\tstrSp := m.allocateInstr()\n+\tstrSp.asStore(operandNR(tmpRegVReg),\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   execCtr, imm: wazevoapi.ExecutionContextOffsetStackPointerBeforeGoCall.I64(),\n+\t\t}, 64)\n+\tcur = linkInstr(cur, strSp)\n+\treturn cur\n+}\n+\n+func (m *machine) goFunctionCallLoadStackArg(cur *instruction, originalArg0Reg regalloc.VReg, arg *backend.ABIArg, intVReg, floatVReg regalloc.VReg) (*instruction, regalloc.VReg) {\n+\tload := m.allocateInstr()\n+\tvar result regalloc.VReg\n+\tmode := addressMode{kind: addressModeKindPostIndex, rn: originalArg0Reg}\n+\tswitch arg.Type {\n+\tcase ssa.TypeI32:\n+\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\tload.asULoad(operandNR(intVReg), mode, 32)\n+\t\tresult = intVReg\n+\tcase ssa.TypeI64:\n+\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\tload.asULoad(operandNR(intVReg), mode, 64)\n+\t\tresult = intVReg\n+\tcase ssa.TypeF32:\n+\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\tload.asFpuLoad(operandNR(floatVReg), mode, 32)\n+\t\tresult = floatVReg\n+\tcase ssa.TypeF64:\n+\t\tmode.imm = 8 // We use uint64 for all basic types, except SIMD v128.\n+\t\tload.asFpuLoad(operandNR(floatVReg), mode, 64)\n+\t\tresult = floatVReg\n+\tcase ssa.TypeV128:\n+\t\tmode.imm = 16\n+\t\tload.asFpuLoad(operandNR(floatVReg), mode, 128)\n+\t\tresult = floatVReg\n+\tdefault:\n+\t\tpanic(\"TODO\")\n+\t}\n+\n+\tcur = linkInstr(cur, load)\n+\treturn cur, result\n+}\n+\n+func (m *machine) goFunctionCallStoreStackResult(cur *instruction, originalRet0Reg regalloc.VReg, result *backend.ABIArg, resultVReg regalloc.VReg) *instruction {\n+\tstore := m.allocateInstr()\n+\tmode := addressMode{kind: addressModeKindPostIndex, rn: originalRet0Reg}\n+\tvar sizeInBits byte\n+\tswitch result.Type {\n+\tcase ssa.TypeI32, ssa.TypeF32:\n+\t\tmode.imm = 8\n+\t\tsizeInBits = 32\n+\tcase ssa.TypeI64, ssa.TypeF64:\n+\t\tmode.imm = 8\n+\t\tsizeInBits = 64\n+\tcase ssa.TypeV128:\n+\t\tmode.imm = 16\n+\t\tsizeInBits = 128\n+\tdefault:\n+\t\tpanic(\"TODO\")\n+\t}\n+\tstore.asStore(operandNR(resultVReg), mode, sizeInBits)\n+\treturn linkInstr(cur, store)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/cond.go",
          "status": "added",
          "additions": 215,
          "deletions": 0,
          "patch": "@@ -0,0 +1,215 @@\n+package arm64\n+\n+import (\n+\t\"strconv\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+type (\n+\tcond     uint64\n+\tcondKind byte\n+)\n+\n+const (\n+\t// condKindRegisterZero represents a condition which checks if the register is zero.\n+\t// This indicates that the instruction must be encoded as CBZ:\n+\t// https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/CBZ--Compare-and-Branch-on-Zero-\n+\tcondKindRegisterZero condKind = iota\n+\t// condKindRegisterNotZero indicates that the instruction must be encoded as CBNZ:\n+\t// https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/CBNZ--Compare-and-Branch-on-Nonzero-\n+\tcondKindRegisterNotZero\n+\t// condKindCondFlagSet indicates that the instruction must be encoded as B.cond:\n+\t// https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/B-cond--Branch-conditionally-\n+\tcondKindCondFlagSet\n+)\n+\n+// kind returns the kind of condition which is stored in the first two bits.\n+func (c cond) kind() condKind {\n+\treturn condKind(c & 0b11)\n+}\n+\n+func (c cond) asUint64() uint64 {\n+\treturn uint64(c)\n+}\n+\n+// register returns the register for register conditions.\n+// This panics if the condition is not a register condition (condKindRegisterZero or condKindRegisterNotZero).\n+func (c cond) register() regalloc.VReg {\n+\tif c.kind() != condKindRegisterZero && c.kind() != condKindRegisterNotZero {\n+\t\tpanic(\"condition is not a register\")\n+\t}\n+\treturn regalloc.VReg(c >> 2)\n+}\n+\n+func registerAsRegZeroCond(r regalloc.VReg) cond {\n+\treturn cond(r)<<2 | cond(condKindRegisterZero)\n+}\n+\n+func registerAsRegNotZeroCond(r regalloc.VReg) cond {\n+\treturn cond(r)<<2 | cond(condKindRegisterNotZero)\n+}\n+\n+func (c cond) flag() condFlag {\n+\tif c.kind() != condKindCondFlagSet {\n+\t\tpanic(\"condition is not a flag\")\n+\t}\n+\treturn condFlag(c >> 2)\n+}\n+\n+func (c condFlag) asCond() cond {\n+\treturn cond(c)<<2 | cond(condKindCondFlagSet)\n+}\n+\n+// condFlag represents a condition flag for conditional branches.\n+// The value matches the encoding of condition flags in the ARM64 instruction set.\n+// https://developer.arm.com/documentation/den0024/a/The-A64-instruction-set/Data-processing-instructions/Conditional-instructions\n+type condFlag uint8\n+\n+const (\n+\teq condFlag = iota // eq represents \"equal\"\n+\tne                 // ne represents \"not equal\"\n+\ths                 // hs represents \"higher or same\"\n+\tlo                 // lo represents \"lower\"\n+\tmi                 // mi represents \"minus or negative result\"\n+\tpl                 // pl represents \"plus or positive result\"\n+\tvs                 // vs represents \"overflow set\"\n+\tvc                 // vc represents \"overflow clear\"\n+\thi                 // hi represents \"higher\"\n+\tls                 // ls represents \"lower or same\"\n+\tge                 // ge represents \"greater or equal\"\n+\tlt                 // lt represents \"less than\"\n+\tgt                 // gt represents \"greater than\"\n+\tle                 // le represents \"less than or equal\"\n+\tal                 // al represents \"always\"\n+\tnv                 // nv represents \"never\"\n+)\n+\n+// invert returns the inverted condition.\n+func (c condFlag) invert() condFlag {\n+\tswitch c {\n+\tcase eq:\n+\t\treturn ne\n+\tcase ne:\n+\t\treturn eq\n+\tcase hs:\n+\t\treturn lo\n+\tcase lo:\n+\t\treturn hs\n+\tcase mi:\n+\t\treturn pl\n+\tcase pl:\n+\t\treturn mi\n+\tcase vs:\n+\t\treturn vc\n+\tcase vc:\n+\t\treturn vs\n+\tcase hi:\n+\t\treturn ls\n+\tcase ls:\n+\t\treturn hi\n+\tcase ge:\n+\t\treturn lt\n+\tcase lt:\n+\t\treturn ge\n+\tcase gt:\n+\t\treturn le\n+\tcase le:\n+\t\treturn gt\n+\tcase al:\n+\t\treturn nv\n+\tcase nv:\n+\t\treturn al\n+\tdefault:\n+\t\tpanic(c)\n+\t}\n+}\n+\n+// String implements fmt.Stringer.\n+func (c condFlag) String() string {\n+\tswitch c {\n+\tcase eq:\n+\t\treturn \"eq\"\n+\tcase ne:\n+\t\treturn \"ne\"\n+\tcase hs:\n+\t\treturn \"hs\"\n+\tcase lo:\n+\t\treturn \"lo\"\n+\tcase mi:\n+\t\treturn \"mi\"\n+\tcase pl:\n+\t\treturn \"pl\"\n+\tcase vs:\n+\t\treturn \"vs\"\n+\tcase vc:\n+\t\treturn \"vc\"\n+\tcase hi:\n+\t\treturn \"hi\"\n+\tcase ls:\n+\t\treturn \"ls\"\n+\tcase ge:\n+\t\treturn \"ge\"\n+\tcase lt:\n+\t\treturn \"lt\"\n+\tcase gt:\n+\t\treturn \"gt\"\n+\tcase le:\n+\t\treturn \"le\"\n+\tcase al:\n+\t\treturn \"al\"\n+\tcase nv:\n+\t\treturn \"nv\"\n+\tdefault:\n+\t\tpanic(strconv.Itoa(int(c)))\n+\t}\n+}\n+\n+// condFlagFromSSAIntegerCmpCond returns the condition flag for the given ssa.IntegerCmpCond.\n+func condFlagFromSSAIntegerCmpCond(c ssa.IntegerCmpCond) condFlag {\n+\tswitch c {\n+\tcase ssa.IntegerCmpCondEqual:\n+\t\treturn eq\n+\tcase ssa.IntegerCmpCondNotEqual:\n+\t\treturn ne\n+\tcase ssa.IntegerCmpCondSignedLessThan:\n+\t\treturn lt\n+\tcase ssa.IntegerCmpCondSignedGreaterThanOrEqual:\n+\t\treturn ge\n+\tcase ssa.IntegerCmpCondSignedGreaterThan:\n+\t\treturn gt\n+\tcase ssa.IntegerCmpCondSignedLessThanOrEqual:\n+\t\treturn le\n+\tcase ssa.IntegerCmpCondUnsignedLessThan:\n+\t\treturn lo\n+\tcase ssa.IntegerCmpCondUnsignedGreaterThanOrEqual:\n+\t\treturn hs\n+\tcase ssa.IntegerCmpCondUnsignedGreaterThan:\n+\t\treturn hi\n+\tcase ssa.IntegerCmpCondUnsignedLessThanOrEqual:\n+\t\treturn ls\n+\tdefault:\n+\t\tpanic(c)\n+\t}\n+}\n+\n+// condFlagFromSSAFloatCmpCond returns the condition flag for the given ssa.FloatCmpCond.\n+func condFlagFromSSAFloatCmpCond(c ssa.FloatCmpCond) condFlag {\n+\tswitch c {\n+\tcase ssa.FloatCmpCondEqual:\n+\t\treturn eq\n+\tcase ssa.FloatCmpCondNotEqual:\n+\t\treturn ne\n+\tcase ssa.FloatCmpCondLessThan:\n+\t\treturn mi\n+\tcase ssa.FloatCmpCondLessThanOrEqual:\n+\t\treturn ls\n+\tcase ssa.FloatCmpCondGreaterThan:\n+\t\treturn gt\n+\tcase ssa.FloatCmpCondGreaterThanOrEqual:\n+\t\treturn ge\n+\tdefault:\n+\t\tpanic(c)\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/instr.go",
          "status": "added",
          "additions": 2545,
          "deletions": 0,
          "patch": "@@ -0,0 +1,2545 @@\n+package arm64\n+\n+import (\n+\t\"fmt\"\n+\t\"math\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+type (\n+\t// instruction represents either a real instruction in arm64, or the meta instructions\n+\t// that are convenient for code generation. For example, inline constants are also treated\n+\t// as instructions.\n+\t//\n+\t// Basically, each instruction knows how to get encoded in binaries. Hence, the final output of compilation\n+\t// can be considered equivalent to the sequence of such instructions.\n+\t//\n+\t// Each field is interpreted depending on the kind.\n+\t//\n+\t// TODO: optimize the layout later once the impl settles.\n+\tinstruction struct {\n+\t\tprev, next          *instruction\n+\t\tu1, u2, u3          uint64\n+\t\trd, rm, rn, ra      operand\n+\t\tamode               addressMode\n+\t\tkind                instructionKind\n+\t\taddedBeforeRegAlloc bool\n+\t}\n+\n+\t// instructionKind represents the kind of instruction.\n+\t// This controls how the instruction struct is interpreted.\n+\tinstructionKind byte\n+)\n+\n+func asNop0(i *instruction) {\n+\ti.kind = nop0\n+}\n+\n+func setNext(i, next *instruction) {\n+\ti.next = next\n+}\n+\n+func setPrev(i, prev *instruction) {\n+\ti.prev = prev\n+}\n+\n+// IsCall implements regalloc.Instr IsCall.\n+func (i *instruction) IsCall() bool {\n+\treturn i.kind == call\n+}\n+\n+// IsIndirectCall implements regalloc.Instr IsIndirectCall.\n+func (i *instruction) IsIndirectCall() bool {\n+\treturn i.kind == callInd\n+}\n+\n+// IsReturn implements regalloc.Instr IsReturn.\n+func (i *instruction) IsReturn() bool {\n+\treturn i.kind == ret\n+}\n+\n+// Next implements regalloc.Instr Next.\n+func (i *instruction) Next() regalloc.Instr {\n+\treturn i.next\n+}\n+\n+// Prev implements regalloc.Instr Prev.\n+func (i *instruction) Prev() regalloc.Instr {\n+\treturn i.prev\n+}\n+\n+// AddedBeforeRegAlloc implements regalloc.Instr AddedBeforeRegAlloc.\n+func (i *instruction) AddedBeforeRegAlloc() bool {\n+\treturn i.addedBeforeRegAlloc\n+}\n+\n+type defKind byte\n+\n+const (\n+\tdefKindNone defKind = iota + 1\n+\tdefKindRD\n+\tdefKindCall\n+)\n+\n+var defKinds = [numInstructionKinds]defKind{\n+\tadr:                  defKindRD,\n+\taluRRR:               defKindRD,\n+\taluRRRR:              defKindRD,\n+\taluRRImm12:           defKindRD,\n+\taluRRBitmaskImm:      defKindRD,\n+\taluRRRShift:          defKindRD,\n+\taluRRImmShift:        defKindRD,\n+\taluRRRExtend:         defKindRD,\n+\tbitRR:                defKindRD,\n+\tmovZ:                 defKindRD,\n+\tmovK:                 defKindRD,\n+\tmovN:                 defKindRD,\n+\tmov32:                defKindRD,\n+\tmov64:                defKindRD,\n+\tfpuMov64:             defKindRD,\n+\tfpuMov128:            defKindRD,\n+\tfpuRR:                defKindRD,\n+\tfpuRRR:               defKindRD,\n+\tnop0:                 defKindNone,\n+\tcall:                 defKindCall,\n+\tcallInd:              defKindCall,\n+\tret:                  defKindNone,\n+\tstore8:               defKindNone,\n+\tstore16:              defKindNone,\n+\tstore32:              defKindNone,\n+\tstore64:              defKindNone,\n+\texitSequence:         defKindNone,\n+\tcondBr:               defKindNone,\n+\tbr:                   defKindNone,\n+\tbrTableSequence:      defKindNone,\n+\tcSet:                 defKindRD,\n+\textend:               defKindRD,\n+\tfpuCmp:               defKindNone,\n+\tuLoad8:               defKindRD,\n+\tuLoad16:              defKindRD,\n+\tuLoad32:              defKindRD,\n+\tsLoad8:               defKindRD,\n+\tsLoad16:              defKindRD,\n+\tsLoad32:              defKindRD,\n+\tuLoad64:              defKindRD,\n+\tfpuLoad32:            defKindRD,\n+\tfpuLoad64:            defKindRD,\n+\tfpuLoad128:           defKindRD,\n+\tvecLoad1R:            defKindRD,\n+\tloadFpuConst32:       defKindRD,\n+\tloadFpuConst64:       defKindRD,\n+\tloadFpuConst128:      defKindRD,\n+\tfpuStore32:           defKindNone,\n+\tfpuStore64:           defKindNone,\n+\tfpuStore128:          defKindNone,\n+\tudf:                  defKindNone,\n+\tcSel:                 defKindRD,\n+\tfpuCSel:              defKindRD,\n+\tmovToVec:             defKindRD,\n+\tmovFromVec:           defKindRD,\n+\tmovFromVecSigned:     defKindRD,\n+\tvecDup:               defKindRD,\n+\tvecDupElement:        defKindRD,\n+\tvecExtract:           defKindRD,\n+\tvecMisc:              defKindRD,\n+\tvecMovElement:        defKindRD,\n+\tvecLanes:             defKindRD,\n+\tvecShiftImm:          defKindRD,\n+\tvecTbl:               defKindRD,\n+\tvecTbl2:              defKindRD,\n+\tvecPermute:           defKindRD,\n+\tvecRRR:               defKindRD,\n+\tvecRRRRewrite:        defKindNone,\n+\tfpuToInt:             defKindRD,\n+\tintToFpu:             defKindRD,\n+\tcCmpImm:              defKindNone,\n+\tmovToFPSR:            defKindNone,\n+\tmovFromFPSR:          defKindRD,\n+\temitSourceOffsetInfo: defKindNone,\n+\tatomicRmw:            defKindRD,\n+\tatomicCas:            defKindNone,\n+\tatomicLoad:           defKindRD,\n+\tatomicStore:          defKindNone,\n+\tdmb:                  defKindNone,\n+\tloadConstBlockArg:    defKindRD,\n+}\n+\n+// Defs returns the list of regalloc.VReg that are defined by the instruction.\n+// In order to reduce the number of allocations, the caller can pass the slice to be used.\n+func (i *instruction) Defs(regs *[]regalloc.VReg) []regalloc.VReg {\n+\t*regs = (*regs)[:0]\n+\tswitch defKinds[i.kind] {\n+\tcase defKindNone:\n+\tcase defKindRD:\n+\t\t*regs = append(*regs, i.rd.nr())\n+\tcase defKindCall:\n+\t\t_, _, retIntRealRegs, retFloatRealRegs, _ := backend.ABIInfoFromUint64(i.u2)\n+\t\tfor i := byte(0); i < retIntRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[intParamResultRegs[i]])\n+\t\t}\n+\t\tfor i := byte(0); i < retFloatRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[floatParamResultRegs[i]])\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"defKind for %v not defined\", i))\n+\t}\n+\treturn *regs\n+}\n+\n+// AssignDef implements regalloc.Instr AssignDef.\n+func (i *instruction) AssignDef(reg regalloc.VReg) {\n+\tswitch defKinds[i.kind] {\n+\tcase defKindNone:\n+\tcase defKindRD:\n+\t\ti.rd = i.rd.assignReg(reg)\n+\tcase defKindCall:\n+\t\tpanic(\"BUG: call instructions shouldn't be assigned\")\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"defKind for %v not defined\", i))\n+\t}\n+}\n+\n+type useKind byte\n+\n+const (\n+\tuseKindNone useKind = iota + 1\n+\tuseKindRN\n+\tuseKindRNRM\n+\tuseKindRNRMRA\n+\tuseKindRNRN1RM\n+\tuseKindCall\n+\tuseKindCallInd\n+\tuseKindAMode\n+\tuseKindRNAMode\n+\tuseKindCond\n+\t// useKindRDRewrite indicates an instruction where RD is used both as a source and destination.\n+\t// A temporary register for RD must be allocated explicitly with the source copied to this\n+\t// register before the instruction and the value copied from this register to the instruction\n+\t// return register.\n+\tuseKindRDRewrite\n+)\n+\n+var useKinds = [numInstructionKinds]useKind{\n+\tudf:                  useKindNone,\n+\taluRRR:               useKindRNRM,\n+\taluRRRR:              useKindRNRMRA,\n+\taluRRImm12:           useKindRN,\n+\taluRRBitmaskImm:      useKindRN,\n+\taluRRRShift:          useKindRNRM,\n+\taluRRImmShift:        useKindRN,\n+\taluRRRExtend:         useKindRNRM,\n+\tbitRR:                useKindRN,\n+\tmovZ:                 useKindNone,\n+\tmovK:                 useKindNone,\n+\tmovN:                 useKindNone,\n+\tmov32:                useKindRN,\n+\tmov64:                useKindRN,\n+\tfpuMov64:             useKindRN,\n+\tfpuMov128:            useKindRN,\n+\tfpuRR:                useKindRN,\n+\tfpuRRR:               useKindRNRM,\n+\tnop0:                 useKindNone,\n+\tcall:                 useKindCall,\n+\tcallInd:              useKindCallInd,\n+\tret:                  useKindNone,\n+\tstore8:               useKindRNAMode,\n+\tstore16:              useKindRNAMode,\n+\tstore32:              useKindRNAMode,\n+\tstore64:              useKindRNAMode,\n+\texitSequence:         useKindRN,\n+\tcondBr:               useKindCond,\n+\tbr:                   useKindNone,\n+\tbrTableSequence:      useKindRN,\n+\tcSet:                 useKindNone,\n+\textend:               useKindRN,\n+\tfpuCmp:               useKindRNRM,\n+\tuLoad8:               useKindAMode,\n+\tuLoad16:              useKindAMode,\n+\tuLoad32:              useKindAMode,\n+\tsLoad8:               useKindAMode,\n+\tsLoad16:              useKindAMode,\n+\tsLoad32:              useKindAMode,\n+\tuLoad64:              useKindAMode,\n+\tfpuLoad32:            useKindAMode,\n+\tfpuLoad64:            useKindAMode,\n+\tfpuLoad128:           useKindAMode,\n+\tfpuStore32:           useKindRNAMode,\n+\tfpuStore64:           useKindRNAMode,\n+\tfpuStore128:          useKindRNAMode,\n+\tloadFpuConst32:       useKindNone,\n+\tloadFpuConst64:       useKindNone,\n+\tloadFpuConst128:      useKindNone,\n+\tvecLoad1R:            useKindRN,\n+\tcSel:                 useKindRNRM,\n+\tfpuCSel:              useKindRNRM,\n+\tmovToVec:             useKindRN,\n+\tmovFromVec:           useKindRN,\n+\tmovFromVecSigned:     useKindRN,\n+\tvecDup:               useKindRN,\n+\tvecDupElement:        useKindRN,\n+\tvecExtract:           useKindRNRM,\n+\tcCmpImm:              useKindRN,\n+\tvecMisc:              useKindRN,\n+\tvecMovElement:        useKindRN,\n+\tvecLanes:             useKindRN,\n+\tvecShiftImm:          useKindRN,\n+\tvecTbl:               useKindRNRM,\n+\tvecTbl2:              useKindRNRN1RM,\n+\tvecRRR:               useKindRNRM,\n+\tvecRRRRewrite:        useKindRDRewrite,\n+\tvecPermute:           useKindRNRM,\n+\tfpuToInt:             useKindRN,\n+\tintToFpu:             useKindRN,\n+\tmovToFPSR:            useKindRN,\n+\tmovFromFPSR:          useKindNone,\n+\tadr:                  useKindNone,\n+\temitSourceOffsetInfo: useKindNone,\n+\tatomicRmw:            useKindRNRM,\n+\tatomicCas:            useKindRDRewrite,\n+\tatomicLoad:           useKindRN,\n+\tatomicStore:          useKindRNRM,\n+\tloadConstBlockArg:    useKindNone,\n+\tdmb:                  useKindNone,\n+}\n+\n+// Uses returns the list of regalloc.VReg that are used by the instruction.\n+// In order to reduce the number of allocations, the caller can pass the slice to be used.\n+func (i *instruction) Uses(regs *[]regalloc.VReg) []regalloc.VReg {\n+\t*regs = (*regs)[:0]\n+\tswitch useKinds[i.kind] {\n+\tcase useKindNone:\n+\tcase useKindRN:\n+\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\t*regs = append(*regs, rn)\n+\t\t}\n+\tcase useKindRNRM:\n+\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\t*regs = append(*regs, rn)\n+\t\t}\n+\t\tif rm := i.rm.reg(); rm.Valid() {\n+\t\t\t*regs = append(*regs, rm)\n+\t\t}\n+\tcase useKindRNRMRA:\n+\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\t*regs = append(*regs, rn)\n+\t\t}\n+\t\tif rm := i.rm.reg(); rm.Valid() {\n+\t\t\t*regs = append(*regs, rm)\n+\t\t}\n+\t\tif ra := i.ra.reg(); ra.Valid() {\n+\t\t\t*regs = append(*regs, ra)\n+\t\t}\n+\tcase useKindRNRN1RM:\n+\t\tif rn := i.rn.reg(); rn.Valid() && rn.IsRealReg() {\n+\t\t\trn1 := regalloc.FromRealReg(rn.RealReg()+1, rn.RegType())\n+\t\t\t*regs = append(*regs, rn, rn1)\n+\t\t}\n+\t\tif rm := i.rm.reg(); rm.Valid() {\n+\t\t\t*regs = append(*regs, rm)\n+\t\t}\n+\tcase useKindAMode:\n+\t\tif amodeRN := i.amode.rn; amodeRN.Valid() {\n+\t\t\t*regs = append(*regs, amodeRN)\n+\t\t}\n+\t\tif amodeRM := i.amode.rm; amodeRM.Valid() {\n+\t\t\t*regs = append(*regs, amodeRM)\n+\t\t}\n+\tcase useKindRNAMode:\n+\t\t*regs = append(*regs, i.rn.reg())\n+\t\tif amodeRN := i.amode.rn; amodeRN.Valid() {\n+\t\t\t*regs = append(*regs, amodeRN)\n+\t\t}\n+\t\tif amodeRM := i.amode.rm; amodeRM.Valid() {\n+\t\t\t*regs = append(*regs, amodeRM)\n+\t\t}\n+\tcase useKindCond:\n+\t\tcnd := cond(i.u1)\n+\t\tif cnd.kind() != condKindCondFlagSet {\n+\t\t\t*regs = append(*regs, cnd.register())\n+\t\t}\n+\tcase useKindCallInd:\n+\t\t*regs = append(*regs, i.rn.nr())\n+\t\tfallthrough\n+\tcase useKindCall:\n+\t\targIntRealRegs, argFloatRealRegs, _, _, _ := backend.ABIInfoFromUint64(i.u2)\n+\t\tfor i := byte(0); i < argIntRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[intParamResultRegs[i]])\n+\t\t}\n+\t\tfor i := byte(0); i < argFloatRealRegs; i++ {\n+\t\t\t*regs = append(*regs, regInfo.RealRegToVReg[floatParamResultRegs[i]])\n+\t\t}\n+\tcase useKindRDRewrite:\n+\t\t*regs = append(*regs, i.rn.reg())\n+\t\t*regs = append(*regs, i.rm.reg())\n+\t\t*regs = append(*regs, i.rd.reg())\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"useKind for %v not defined\", i))\n+\t}\n+\treturn *regs\n+}\n+\n+func (i *instruction) AssignUse(index int, reg regalloc.VReg) {\n+\tswitch useKinds[i.kind] {\n+\tcase useKindNone:\n+\tcase useKindRN:\n+\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\ti.rn = i.rn.assignReg(reg)\n+\t\t}\n+\tcase useKindRNRM:\n+\t\tif index == 0 {\n+\t\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\t\ti.rn = i.rn.assignReg(reg)\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif rm := i.rm.reg(); rm.Valid() {\n+\t\t\t\ti.rm = i.rm.assignReg(reg)\n+\t\t\t}\n+\t\t}\n+\tcase useKindRDRewrite:\n+\t\tif index == 0 {\n+\t\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\t\ti.rn = i.rn.assignReg(reg)\n+\t\t\t}\n+\t\t} else if index == 1 {\n+\t\t\tif rm := i.rm.reg(); rm.Valid() {\n+\t\t\t\ti.rm = i.rm.assignReg(reg)\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif rd := i.rd.reg(); rd.Valid() {\n+\t\t\t\ti.rd = i.rd.assignReg(reg)\n+\t\t\t}\n+\t\t}\n+\tcase useKindRNRN1RM:\n+\t\tif index == 0 {\n+\t\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\t\ti.rn = i.rn.assignReg(reg)\n+\t\t\t}\n+\t\t\tif rn1 := i.rn.reg() + 1; rn1.Valid() {\n+\t\t\t\ti.rm = i.rm.assignReg(reg + 1)\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif rm := i.rm.reg(); rm.Valid() {\n+\t\t\t\ti.rm = i.rm.assignReg(reg)\n+\t\t\t}\n+\t\t}\n+\tcase useKindRNRMRA:\n+\t\tif index == 0 {\n+\t\t\tif rn := i.rn.reg(); rn.Valid() {\n+\t\t\t\ti.rn = i.rn.assignReg(reg)\n+\t\t\t}\n+\t\t} else if index == 1 {\n+\t\t\tif rm := i.rm.reg(); rm.Valid() {\n+\t\t\t\ti.rm = i.rm.assignReg(reg)\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif ra := i.ra.reg(); ra.Valid() {\n+\t\t\t\ti.ra = i.ra.assignReg(reg)\n+\t\t\t}\n+\t\t}\n+\tcase useKindAMode:\n+\t\tif index == 0 {\n+\t\t\tif amodeRN := i.amode.rn; amodeRN.Valid() {\n+\t\t\t\ti.amode.rn = reg\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif amodeRM := i.amode.rm; amodeRM.Valid() {\n+\t\t\t\ti.amode.rm = reg\n+\t\t\t}\n+\t\t}\n+\tcase useKindRNAMode:\n+\t\tif index == 0 {\n+\t\t\ti.rn = i.rn.assignReg(reg)\n+\t\t} else if index == 1 {\n+\t\t\tif amodeRN := i.amode.rn; amodeRN.Valid() {\n+\t\t\t\ti.amode.rn = reg\n+\t\t\t} else {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif amodeRM := i.amode.rm; amodeRM.Valid() {\n+\t\t\t\ti.amode.rm = reg\n+\t\t\t} else {\n+\t\t\t\tpanic(\"BUG\")\n+\t\t\t}\n+\t\t}\n+\tcase useKindCond:\n+\t\tc := cond(i.u1)\n+\t\tswitch c.kind() {\n+\t\tcase condKindRegisterZero:\n+\t\t\ti.u1 = uint64(registerAsRegZeroCond(reg))\n+\t\tcase condKindRegisterNotZero:\n+\t\t\ti.u1 = uint64(registerAsRegNotZeroCond(reg))\n+\t\t}\n+\tcase useKindCall:\n+\t\tpanic(\"BUG: call instructions shouldn't be assigned\")\n+\tcase useKindCallInd:\n+\t\ti.rn = i.rn.assignReg(reg)\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"useKind for %v not defined\", i))\n+\t}\n+}\n+\n+func (i *instruction) asCall(ref ssa.FuncRef, abi *backend.FunctionABI) {\n+\ti.kind = call\n+\ti.u1 = uint64(ref)\n+\tif abi != nil {\n+\t\ti.u2 = abi.ABIInfoAsUint64()\n+\t}\n+}\n+\n+func (i *instruction) asCallIndirect(ptr regalloc.VReg, abi *backend.FunctionABI) {\n+\ti.kind = callInd\n+\ti.rn = operandNR(ptr)\n+\tif abi != nil {\n+\t\ti.u2 = abi.ABIInfoAsUint64()\n+\t}\n+}\n+\n+func (i *instruction) callFuncRef() ssa.FuncRef {\n+\treturn ssa.FuncRef(i.u1)\n+}\n+\n+// shift must be divided by 16 and must be in range 0-3 (if dst64bit is true) or 0-1 (if dst64bit is false)\n+func (i *instruction) asMOVZ(dst regalloc.VReg, imm uint64, shift uint64, dst64bit bool) {\n+\ti.kind = movZ\n+\ti.rd = operandNR(dst)\n+\ti.u1 = imm\n+\ti.u2 = shift\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+// shift must be divided by 16 and must be in range 0-3 (if dst64bit is true) or 0-1 (if dst64bit is false)\n+func (i *instruction) asMOVK(dst regalloc.VReg, imm uint64, shift uint64, dst64bit bool) {\n+\ti.kind = movK\n+\ti.rd = operandNR(dst)\n+\ti.u1 = imm\n+\ti.u2 = shift\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+// shift must be divided by 16 and must be in range 0-3 (if dst64bit is true) or 0-1 (if dst64bit is false)\n+func (i *instruction) asMOVN(dst regalloc.VReg, imm uint64, shift uint64, dst64bit bool) {\n+\ti.kind = movN\n+\ti.rd = operandNR(dst)\n+\ti.u1 = imm\n+\ti.u2 = shift\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asNop0() *instruction {\n+\ti.kind = nop0\n+\treturn i\n+}\n+\n+func (i *instruction) asNop0WithLabel(l label) {\n+\ti.kind = nop0\n+\ti.u1 = uint64(l)\n+}\n+\n+func (i *instruction) nop0Label() label {\n+\treturn label(i.u1)\n+}\n+\n+func (i *instruction) asRet() {\n+\ti.kind = ret\n+}\n+\n+func (i *instruction) asStorePair64(src1, src2 regalloc.VReg, amode addressMode) {\n+\ti.kind = storeP64\n+\ti.rn = operandNR(src1)\n+\ti.rm = operandNR(src2)\n+\ti.amode = amode\n+}\n+\n+func (i *instruction) asLoadPair64(src1, src2 regalloc.VReg, amode addressMode) {\n+\ti.kind = loadP64\n+\ti.rn = operandNR(src1)\n+\ti.rm = operandNR(src2)\n+\ti.amode = amode\n+}\n+\n+func (i *instruction) asStore(src operand, amode addressMode, sizeInBits byte) {\n+\tswitch sizeInBits {\n+\tcase 8:\n+\t\ti.kind = store8\n+\tcase 16:\n+\t\ti.kind = store16\n+\tcase 32:\n+\t\tif src.reg().RegType() == regalloc.RegTypeInt {\n+\t\t\ti.kind = store32\n+\t\t} else {\n+\t\t\ti.kind = fpuStore32\n+\t\t}\n+\tcase 64:\n+\t\tif src.reg().RegType() == regalloc.RegTypeInt {\n+\t\t\ti.kind = store64\n+\t\t} else {\n+\t\t\ti.kind = fpuStore64\n+\t\t}\n+\tcase 128:\n+\t\ti.kind = fpuStore128\n+\t}\n+\ti.rn = src\n+\ti.amode = amode\n+}\n+\n+func (i *instruction) asSLoad(dst operand, amode addressMode, sizeInBits byte) {\n+\tswitch sizeInBits {\n+\tcase 8:\n+\t\ti.kind = sLoad8\n+\tcase 16:\n+\t\ti.kind = sLoad16\n+\tcase 32:\n+\t\ti.kind = sLoad32\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.rd = dst\n+\ti.amode = amode\n+}\n+\n+func (i *instruction) asULoad(dst operand, amode addressMode, sizeInBits byte) {\n+\tswitch sizeInBits {\n+\tcase 8:\n+\t\ti.kind = uLoad8\n+\tcase 16:\n+\t\ti.kind = uLoad16\n+\tcase 32:\n+\t\ti.kind = uLoad32\n+\tcase 64:\n+\t\ti.kind = uLoad64\n+\t}\n+\ti.rd = dst\n+\ti.amode = amode\n+}\n+\n+func (i *instruction) asFpuLoad(dst operand, amode addressMode, sizeInBits byte) {\n+\tswitch sizeInBits {\n+\tcase 32:\n+\t\ti.kind = fpuLoad32\n+\tcase 64:\n+\t\ti.kind = fpuLoad64\n+\tcase 128:\n+\t\ti.kind = fpuLoad128\n+\t}\n+\ti.rd = dst\n+\ti.amode = amode\n+}\n+\n+func (i *instruction) asVecLoad1R(rd, rn operand, arr vecArrangement) {\n+\t// NOTE: currently only has support for no-offset loads, though it is suspicious that\n+\t// we would need to support offset load (that is only available for post-index).\n+\ti.kind = vecLoad1R\n+\ti.rd = rd\n+\ti.rn = rn\n+\ti.u1 = uint64(arr)\n+}\n+\n+func (i *instruction) asCSet(rd regalloc.VReg, mask bool, c condFlag) {\n+\ti.kind = cSet\n+\ti.rd = operandNR(rd)\n+\ti.u1 = uint64(c)\n+\tif mask {\n+\t\ti.u2 = 1\n+\t}\n+}\n+\n+func (i *instruction) asCSel(rd, rn, rm operand, c condFlag, _64bit bool) {\n+\ti.kind = cSel\n+\ti.rd = rd\n+\ti.rn = rn\n+\ti.rm = rm\n+\ti.u1 = uint64(c)\n+\tif _64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asFpuCSel(rd, rn, rm operand, c condFlag, _64bit bool) {\n+\ti.kind = fpuCSel\n+\ti.rd = rd\n+\ti.rn = rn\n+\ti.rm = rm\n+\ti.u1 = uint64(c)\n+\tif _64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asBr(target label) {\n+\tif target == labelReturn {\n+\t\tpanic(\"BUG: call site should special case for returnLabel\")\n+\t}\n+\ti.kind = br\n+\ti.u1 = uint64(target)\n+}\n+\n+func (i *instruction) asBrTableSequence(indexReg regalloc.VReg, targetIndex, targetCounts int) {\n+\ti.kind = brTableSequence\n+\ti.rn = operandNR(indexReg)\n+\ti.u1 = uint64(targetIndex)\n+\ti.u2 = uint64(targetCounts)\n+}\n+\n+func (i *instruction) brTableSequenceOffsetsResolved() {\n+\ti.u3 = 1 // indicate that the offsets are resolved, for debugging.\n+}\n+\n+func (i *instruction) brLabel() label {\n+\treturn label(i.u1)\n+}\n+\n+// brOffsetResolved is called when the target label is resolved.\n+func (i *instruction) brOffsetResolve(offset int64) {\n+\ti.u2 = uint64(offset)\n+\ti.u3 = 1 // indicate that the offset is resolved, for debugging.\n+}\n+\n+func (i *instruction) brOffset() int64 {\n+\treturn int64(i.u2)\n+}\n+\n+// asCondBr encodes a conditional branch instruction. is64bit is only needed when cond is not flag.\n+func (i *instruction) asCondBr(c cond, target label, is64bit bool) {\n+\ti.kind = condBr\n+\ti.u1 = c.asUint64()\n+\ti.u2 = uint64(target)\n+\tif is64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) setCondBrTargets(target label) {\n+\ti.u2 = uint64(target)\n+}\n+\n+func (i *instruction) condBrLabel() label {\n+\treturn label(i.u2)\n+}\n+\n+// condBrOffsetResolve is called when the target label is resolved.\n+func (i *instruction) condBrOffsetResolve(offset int64) {\n+\ti.rd.data = uint64(offset)\n+\ti.rd.data2 = 1 // indicate that the offset is resolved, for debugging.\n+}\n+\n+// condBrOffsetResolved returns true if condBrOffsetResolve is already called.\n+func (i *instruction) condBrOffsetResolved() bool {\n+\treturn i.rd.data2 == 1\n+}\n+\n+func (i *instruction) condBrOffset() int64 {\n+\treturn int64(i.rd.data)\n+}\n+\n+func (i *instruction) condBrCond() cond {\n+\treturn cond(i.u1)\n+}\n+\n+func (i *instruction) condBr64bit() bool {\n+\treturn i.u3 == 1\n+}\n+\n+func (i *instruction) asLoadFpuConst32(rd regalloc.VReg, raw uint64) {\n+\ti.kind = loadFpuConst32\n+\ti.u1 = raw\n+\ti.rd = operandNR(rd)\n+}\n+\n+func (i *instruction) asLoadFpuConst64(rd regalloc.VReg, raw uint64) {\n+\ti.kind = loadFpuConst64\n+\ti.u1 = raw\n+\ti.rd = operandNR(rd)\n+}\n+\n+func (i *instruction) asLoadFpuConst128(rd regalloc.VReg, lo, hi uint64) {\n+\ti.kind = loadFpuConst128\n+\ti.u1 = lo\n+\ti.u2 = hi\n+\ti.rd = operandNR(rd)\n+}\n+\n+func (i *instruction) asFpuCmp(rn, rm operand, is64bit bool) {\n+\ti.kind = fpuCmp\n+\ti.rn, i.rm = rn, rm\n+\tif is64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asCCmpImm(rn operand, imm uint64, c condFlag, flag byte, is64bit bool) {\n+\ti.kind = cCmpImm\n+\ti.rn = rn\n+\ti.rm.data = imm\n+\ti.u1 = uint64(c)\n+\ti.u2 = uint64(flag)\n+\tif is64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+// asALU setups a basic ALU instruction.\n+func (i *instruction) asALU(aluOp aluOp, rd, rn, rm operand, dst64bit bool) {\n+\tswitch rm.kind {\n+\tcase operandKindNR:\n+\t\ti.kind = aluRRR\n+\tcase operandKindSR:\n+\t\ti.kind = aluRRRShift\n+\tcase operandKindER:\n+\t\ti.kind = aluRRRExtend\n+\tcase operandKindImm12:\n+\t\ti.kind = aluRRImm12\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.u1 = uint64(aluOp)\n+\ti.rd, i.rn, i.rm = rd, rn, rm\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+// asALU setups a basic ALU instruction.\n+func (i *instruction) asALURRRR(aluOp aluOp, rd, rn, rm, ra operand, dst64bit bool) {\n+\ti.kind = aluRRRR\n+\ti.u1 = uint64(aluOp)\n+\ti.rd, i.rn, i.rm, i.ra = rd, rn, rm, ra\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+// asALUShift setups a shift based ALU instruction.\n+func (i *instruction) asALUShift(aluOp aluOp, rd, rn, rm operand, dst64bit bool) {\n+\tswitch rm.kind {\n+\tcase operandKindNR:\n+\t\ti.kind = aluRRR // If the shift amount op is a register, then the instruction is encoded as a normal ALU instruction with two register operands.\n+\tcase operandKindShiftImm:\n+\t\ti.kind = aluRRImmShift\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\ti.u1 = uint64(aluOp)\n+\ti.rd, i.rn, i.rm = rd, rn, rm\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asALUBitmaskImm(aluOp aluOp, rd, rn regalloc.VReg, imm uint64, dst64bit bool) {\n+\ti.kind = aluRRBitmaskImm\n+\ti.u1 = uint64(aluOp)\n+\ti.rn, i.rd = operandNR(rn), operandNR(rd)\n+\ti.u2 = imm\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asMovToFPSR(rn regalloc.VReg) {\n+\ti.kind = movToFPSR\n+\ti.rn = operandNR(rn)\n+}\n+\n+func (i *instruction) asMovFromFPSR(rd regalloc.VReg) {\n+\ti.kind = movFromFPSR\n+\ti.rd = operandNR(rd)\n+}\n+\n+func (i *instruction) asBitRR(bitOp bitOp, rd, rn regalloc.VReg, is64bit bool) {\n+\ti.kind = bitRR\n+\ti.rn, i.rd = operandNR(rn), operandNR(rd)\n+\ti.u1 = uint64(bitOp)\n+\tif is64bit {\n+\t\ti.u2 = 1\n+\t}\n+}\n+\n+func (i *instruction) asFpuRRR(op fpuBinOp, rd, rn, rm operand, dst64bit bool) {\n+\ti.kind = fpuRRR\n+\ti.u1 = uint64(op)\n+\ti.rd, i.rn, i.rm = rd, rn, rm\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asFpuRR(op fpuUniOp, rd, rn operand, dst64bit bool) {\n+\ti.kind = fpuRR\n+\ti.u1 = uint64(op)\n+\ti.rd, i.rn = rd, rn\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asExtend(rd, rn regalloc.VReg, fromBits, toBits byte, signed bool) {\n+\ti.kind = extend\n+\ti.rn, i.rd = operandNR(rn), operandNR(rd)\n+\ti.u1 = uint64(fromBits)\n+\ti.u2 = uint64(toBits)\n+\tif signed {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asMove32(rd, rn regalloc.VReg) {\n+\ti.kind = mov32\n+\ti.rn, i.rd = operandNR(rn), operandNR(rd)\n+}\n+\n+func (i *instruction) asMove64(rd, rn regalloc.VReg) *instruction {\n+\ti.kind = mov64\n+\ti.rn, i.rd = operandNR(rn), operandNR(rd)\n+\treturn i\n+}\n+\n+func (i *instruction) asFpuMov64(rd, rn regalloc.VReg) {\n+\ti.kind = fpuMov64\n+\ti.rn, i.rd = operandNR(rn), operandNR(rd)\n+}\n+\n+func (i *instruction) asFpuMov128(rd, rn regalloc.VReg) *instruction {\n+\ti.kind = fpuMov128\n+\ti.rn, i.rd = operandNR(rn), operandNR(rd)\n+\treturn i\n+}\n+\n+func (i *instruction) asMovToVec(rd, rn operand, arr vecArrangement, index vecIndex) {\n+\ti.kind = movToVec\n+\ti.rd = rd\n+\ti.rn = rn\n+\ti.u1, i.u2 = uint64(arr), uint64(index)\n+}\n+\n+func (i *instruction) asMovFromVec(rd, rn operand, arr vecArrangement, index vecIndex, signed bool) {\n+\tif signed {\n+\t\ti.kind = movFromVecSigned\n+\t} else {\n+\t\ti.kind = movFromVec\n+\t}\n+\ti.rd = rd\n+\ti.rn = rn\n+\ti.u1, i.u2 = uint64(arr), uint64(index)\n+}\n+\n+func (i *instruction) asVecDup(rd, rn operand, arr vecArrangement) {\n+\ti.kind = vecDup\n+\ti.u1 = uint64(arr)\n+\ti.rn, i.rd = rn, rd\n+}\n+\n+func (i *instruction) asVecDupElement(rd, rn operand, arr vecArrangement, index vecIndex) {\n+\ti.kind = vecDupElement\n+\ti.u1 = uint64(arr)\n+\ti.rn, i.rd = rn, rd\n+\ti.u2 = uint64(index)\n+}\n+\n+func (i *instruction) asVecExtract(rd, rn, rm operand, arr vecArrangement, index uint32) {\n+\ti.kind = vecExtract\n+\ti.u1 = uint64(arr)\n+\ti.rn, i.rm, i.rd = rn, rm, rd\n+\ti.u2 = uint64(index)\n+}\n+\n+func (i *instruction) asVecMovElement(rd, rn operand, arr vecArrangement, rdIndex, rnIndex vecIndex) {\n+\ti.kind = vecMovElement\n+\ti.u1 = uint64(arr)\n+\ti.u2, i.u3 = uint64(rdIndex), uint64(rnIndex)\n+\ti.rn, i.rd = rn, rd\n+}\n+\n+func (i *instruction) asVecMisc(op vecOp, rd, rn operand, arr vecArrangement) {\n+\ti.kind = vecMisc\n+\ti.u1 = uint64(op)\n+\ti.rn, i.rd = rn, rd\n+\ti.u2 = uint64(arr)\n+}\n+\n+func (i *instruction) asVecLanes(op vecOp, rd, rn operand, arr vecArrangement) {\n+\ti.kind = vecLanes\n+\ti.u1 = uint64(op)\n+\ti.rn, i.rd = rn, rd\n+\ti.u2 = uint64(arr)\n+}\n+\n+func (i *instruction) asVecShiftImm(op vecOp, rd, rn, rm operand, arr vecArrangement) *instruction {\n+\ti.kind = vecShiftImm\n+\ti.u1 = uint64(op)\n+\ti.rn, i.rm, i.rd = rn, rm, rd\n+\ti.u2 = uint64(arr)\n+\treturn i\n+}\n+\n+func (i *instruction) asVecTbl(nregs byte, rd, rn, rm operand, arr vecArrangement) {\n+\tswitch nregs {\n+\tcase 0, 1:\n+\t\ti.kind = vecTbl\n+\tcase 2:\n+\t\ti.kind = vecTbl2\n+\t\tif !rn.reg().IsRealReg() {\n+\t\t\tpanic(\"rn is not a RealReg\")\n+\t\t}\n+\t\tif rn.realReg() == v31 {\n+\t\t\tpanic(\"rn cannot be v31\")\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"unsupported number of registers %d\", nregs))\n+\t}\n+\ti.rn, i.rm, i.rd = rn, rm, rd\n+\ti.u2 = uint64(arr)\n+}\n+\n+func (i *instruction) asVecPermute(op vecOp, rd, rn, rm operand, arr vecArrangement) {\n+\ti.kind = vecPermute\n+\ti.u1 = uint64(op)\n+\ti.rn, i.rm, i.rd = rn, rm, rd\n+\ti.u2 = uint64(arr)\n+}\n+\n+func (i *instruction) asVecRRR(op vecOp, rd, rn, rm operand, arr vecArrangement) *instruction {\n+\ti.kind = vecRRR\n+\ti.u1 = uint64(op)\n+\ti.rn, i.rd, i.rm = rn, rd, rm\n+\ti.u2 = uint64(arr)\n+\treturn i\n+}\n+\n+// asVecRRRRewrite encodes a vector instruction that rewrites the destination register.\n+// IMPORTANT: the destination register must be already defined before this instruction.\n+func (i *instruction) asVecRRRRewrite(op vecOp, rd, rn, rm operand, arr vecArrangement) {\n+\ti.kind = vecRRRRewrite\n+\ti.u1 = uint64(op)\n+\ti.rn, i.rd, i.rm = rn, rd, rm\n+\ti.u2 = uint64(arr)\n+}\n+\n+func (i *instruction) IsCopy() bool {\n+\top := i.kind\n+\t// We do not include mov32 as it is not a copy instruction in the sense that it does not preserve the upper 32 bits,\n+\t// and it is only used in the translation of IReduce, not the actual copy indeed.\n+\treturn op == mov64 || op == fpuMov64 || op == fpuMov128\n+}\n+\n+// String implements fmt.Stringer.\n+func (i *instruction) String() (str string) {\n+\tis64SizeBitToSize := func(u3 uint64) byte {\n+\t\tif u3 == 0 {\n+\t\t\treturn 32\n+\t\t}\n+\t\treturn 64\n+\t}\n+\n+\tswitch i.kind {\n+\tcase nop0:\n+\t\tif i.u1 != 0 {\n+\t\t\tl := label(i.u1)\n+\t\t\tstr = fmt.Sprintf(\"%s:\", l)\n+\t\t} else {\n+\t\t\tstr = \"nop0\"\n+\t\t}\n+\tcase aluRRR:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\", aluOp(i.u1).String(),\n+\t\t\tformatVRegSized(i.rd.nr(), size), formatVRegSized(i.rn.nr(), size),\n+\t\t\ti.rm.format(size))\n+\tcase aluRRRR:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s, %s\", aluOp(i.u1).String(),\n+\t\t\tformatVRegSized(i.rd.nr(), size), formatVRegSized(i.rn.nr(), size), formatVRegSized(i.rm.nr(), size), formatVRegSized(i.ra.nr(), size))\n+\tcase aluRRImm12:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\", aluOp(i.u1).String(),\n+\t\t\tformatVRegSized(i.rd.nr(), size), formatVRegSized(i.rn.nr(), size), i.rm.format(size))\n+\tcase aluRRBitmaskImm:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\trd, rn := formatVRegSized(i.rd.nr(), size), formatVRegSized(i.rn.nr(), size)\n+\t\tif size == 32 {\n+\t\t\tstr = fmt.Sprintf(\"%s %s, %s, #%#x\", aluOp(i.u1).String(), rd, rn, uint32(i.u2))\n+\t\t} else {\n+\t\t\tstr = fmt.Sprintf(\"%s %s, %s, #%#x\", aluOp(i.u1).String(), rd, rn, i.u2)\n+\t\t}\n+\tcase aluRRImmShift:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %#x\",\n+\t\t\taluOp(i.u1).String(),\n+\t\t\tformatVRegSized(i.rd.nr(), size),\n+\t\t\tformatVRegSized(i.rn.nr(), size),\n+\t\t\ti.rm.shiftImm(),\n+\t\t)\n+\tcase aluRRRShift:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\",\n+\t\t\taluOp(i.u1).String(),\n+\t\t\tformatVRegSized(i.rd.nr(), size),\n+\t\t\tformatVRegSized(i.rn.nr(), size),\n+\t\t\ti.rm.format(size),\n+\t\t)\n+\tcase aluRRRExtend:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\", aluOp(i.u1).String(),\n+\t\t\tformatVRegSized(i.rd.nr(), size),\n+\t\t\tformatVRegSized(i.rn.nr(), size),\n+\t\t\t// Regardless of the source size, the register is formatted in 32-bit.\n+\t\t\ti.rm.format(32),\n+\t\t)\n+\tcase bitRR:\n+\t\tsize := is64SizeBitToSize(i.u2)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\",\n+\t\t\tbitOp(i.u1),\n+\t\t\tformatVRegSized(i.rd.nr(), size),\n+\t\t\tformatVRegSized(i.rn.nr(), size),\n+\t\t)\n+\tcase uLoad8:\n+\t\tstr = fmt.Sprintf(\"ldrb %s, %s\", formatVRegSized(i.rd.nr(), 32), i.amode.format(32))\n+\tcase sLoad8:\n+\t\tstr = fmt.Sprintf(\"ldrsb %s, %s\", formatVRegSized(i.rd.nr(), 32), i.amode.format(32))\n+\tcase uLoad16:\n+\t\tstr = fmt.Sprintf(\"ldrh %s, %s\", formatVRegSized(i.rd.nr(), 32), i.amode.format(32))\n+\tcase sLoad16:\n+\t\tstr = fmt.Sprintf(\"ldrsh %s, %s\", formatVRegSized(i.rd.nr(), 32), i.amode.format(32))\n+\tcase uLoad32:\n+\t\tstr = fmt.Sprintf(\"ldr %s, %s\", formatVRegSized(i.rd.nr(), 32), i.amode.format(32))\n+\tcase sLoad32:\n+\t\tstr = fmt.Sprintf(\"ldrs %s, %s\", formatVRegSized(i.rd.nr(), 32), i.amode.format(32))\n+\tcase uLoad64:\n+\t\tstr = fmt.Sprintf(\"ldr %s, %s\", formatVRegSized(i.rd.nr(), 64), i.amode.format(64))\n+\tcase store8:\n+\t\tstr = fmt.Sprintf(\"strb %s, %s\", formatVRegSized(i.rn.nr(), 32), i.amode.format(8))\n+\tcase store16:\n+\t\tstr = fmt.Sprintf(\"strh %s, %s\", formatVRegSized(i.rn.nr(), 32), i.amode.format(16))\n+\tcase store32:\n+\t\tstr = fmt.Sprintf(\"str %s, %s\", formatVRegSized(i.rn.nr(), 32), i.amode.format(32))\n+\tcase store64:\n+\t\tstr = fmt.Sprintf(\"str %s, %s\", formatVRegSized(i.rn.nr(), 64), i.amode.format(64))\n+\tcase storeP64:\n+\t\tstr = fmt.Sprintf(\"stp %s, %s, %s\",\n+\t\t\tformatVRegSized(i.rn.nr(), 64), formatVRegSized(i.rm.nr(), 64), i.amode.format(64))\n+\tcase loadP64:\n+\t\tstr = fmt.Sprintf(\"ldp %s, %s, %s\",\n+\t\t\tformatVRegSized(i.rn.nr(), 64), formatVRegSized(i.rm.nr(), 64), i.amode.format(64))\n+\tcase mov64:\n+\t\tstr = fmt.Sprintf(\"mov %s, %s\",\n+\t\t\tformatVRegSized(i.rd.nr(), 64),\n+\t\t\tformatVRegSized(i.rn.nr(), 64))\n+\tcase mov32:\n+\t\tstr = fmt.Sprintf(\"mov %s, %s\", formatVRegSized(i.rd.nr(), 32), formatVRegSized(i.rn.nr(), 32))\n+\tcase movZ:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"movz %s, #%#x, lsl %d\", formatVRegSized(i.rd.nr(), size), uint16(i.u1), i.u2*16)\n+\tcase movN:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"movn %s, #%#x, lsl %d\", formatVRegSized(i.rd.nr(), size), uint16(i.u1), i.u2*16)\n+\tcase movK:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"movk %s, #%#x, lsl %d\", formatVRegSized(i.rd.nr(), size), uint16(i.u1), i.u2*16)\n+\tcase extend:\n+\t\tfromBits, toBits := byte(i.u1), byte(i.u2)\n+\n+\t\tvar signedStr string\n+\t\tif i.u3 == 1 {\n+\t\t\tsignedStr = \"s\"\n+\t\t} else {\n+\t\t\tsignedStr = \"u\"\n+\t\t}\n+\t\tvar fromStr string\n+\t\tswitch fromBits {\n+\t\tcase 8:\n+\t\t\tfromStr = \"b\"\n+\t\tcase 16:\n+\t\t\tfromStr = \"h\"\n+\t\tcase 32:\n+\t\t\tfromStr = \"w\"\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%sxt%s %s, %s\", signedStr, fromStr, formatVRegSized(i.rd.nr(), toBits), formatVRegSized(i.rn.nr(), 32))\n+\tcase cSel:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"csel %s, %s, %s, %s\",\n+\t\t\tformatVRegSized(i.rd.nr(), size),\n+\t\t\tformatVRegSized(i.rn.nr(), size),\n+\t\t\tformatVRegSized(i.rm.nr(), size),\n+\t\t\tcondFlag(i.u1),\n+\t\t)\n+\tcase cSet:\n+\t\tif i.u2 != 0 {\n+\t\t\tstr = fmt.Sprintf(\"csetm %s, %s\", formatVRegSized(i.rd.nr(), 64), condFlag(i.u1))\n+\t\t} else {\n+\t\t\tstr = fmt.Sprintf(\"cset %s, %s\", formatVRegSized(i.rd.nr(), 64), condFlag(i.u1))\n+\t\t}\n+\tcase cCmpImm:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"ccmp %s, #%#x, #%#x, %s\",\n+\t\t\tformatVRegSized(i.rn.nr(), size), i.rm.data,\n+\t\t\ti.u2&0b1111,\n+\t\t\tcondFlag(i.u1))\n+\tcase fpuMov64:\n+\t\tstr = fmt.Sprintf(\"mov %s, %s\",\n+\t\t\tformatVRegVec(i.rd.nr(), vecArrangement8B, vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), vecArrangement8B, vecIndexNone))\n+\tcase fpuMov128:\n+\t\tstr = fmt.Sprintf(\"mov %s, %s\",\n+\t\t\tformatVRegVec(i.rd.nr(), vecArrangement16B, vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), vecArrangement16B, vecIndexNone))\n+\tcase fpuMovFromVec:\n+\t\tpanic(\"TODO\")\n+\tcase fpuRR:\n+\t\tdstSz := is64SizeBitToSize(i.u3)\n+\t\tsrcSz := dstSz\n+\t\top := fpuUniOp(i.u1)\n+\t\tswitch op {\n+\t\tcase fpuUniOpCvt32To64:\n+\t\t\tsrcSz = 32\n+\t\tcase fpuUniOpCvt64To32:\n+\t\t\tsrcSz = 64\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\", op.String(),\n+\t\t\tformatVRegSized(i.rd.nr(), dstSz), formatVRegSized(i.rn.nr(), srcSz))\n+\tcase fpuRRR:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\", fpuBinOp(i.u1).String(),\n+\t\t\tformatVRegSized(i.rd.nr(), size), formatVRegSized(i.rn.nr(), size), formatVRegSized(i.rm.nr(), size))\n+\tcase fpuRRI:\n+\t\tpanic(\"TODO\")\n+\tcase fpuRRRR:\n+\t\tpanic(\"TODO\")\n+\tcase fpuCmp:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"fcmp %s, %s\",\n+\t\t\tformatVRegSized(i.rn.nr(), size), formatVRegSized(i.rm.nr(), size))\n+\tcase fpuLoad32:\n+\t\tstr = fmt.Sprintf(\"ldr %s, %s\", formatVRegSized(i.rd.nr(), 32), i.amode.format(32))\n+\tcase fpuStore32:\n+\t\tstr = fmt.Sprintf(\"str %s, %s\", formatVRegSized(i.rn.nr(), 32), i.amode.format(64))\n+\tcase fpuLoad64:\n+\t\tstr = fmt.Sprintf(\"ldr %s, %s\", formatVRegSized(i.rd.nr(), 64), i.amode.format(64))\n+\tcase fpuStore64:\n+\t\tstr = fmt.Sprintf(\"str %s, %s\", formatVRegSized(i.rn.nr(), 64), i.amode.format(64))\n+\tcase fpuLoad128:\n+\t\tstr = fmt.Sprintf(\"ldr %s, %s\", formatVRegSized(i.rd.nr(), 128), i.amode.format(64))\n+\tcase fpuStore128:\n+\t\tstr = fmt.Sprintf(\"str %s, %s\", formatVRegSized(i.rn.nr(), 128), i.amode.format(64))\n+\tcase loadFpuConst32:\n+\t\tstr = fmt.Sprintf(\"ldr %s, #8; b 8; data.f32 %f\", formatVRegSized(i.rd.nr(), 32), math.Float32frombits(uint32(i.u1)))\n+\tcase loadFpuConst64:\n+\t\tstr = fmt.Sprintf(\"ldr %s, #8; b 16; data.f64 %f\", formatVRegSized(i.rd.nr(), 64), math.Float64frombits(i.u1))\n+\tcase loadFpuConst128:\n+\t\tstr = fmt.Sprintf(\"ldr %s, #8; b 32; data.v128  %016x %016x\",\n+\t\t\tformatVRegSized(i.rd.nr(), 128), i.u1, i.u2)\n+\tcase fpuToInt:\n+\t\tvar op, src, dst string\n+\t\tif signed := i.u1 == 1; signed {\n+\t\t\top = \"fcvtzs\"\n+\t\t} else {\n+\t\t\top = \"fcvtzu\"\n+\t\t}\n+\t\tif src64 := i.u2 == 1; src64 {\n+\t\t\tsrc = formatVRegWidthVec(i.rn.nr(), vecArrangementD)\n+\t\t} else {\n+\t\t\tsrc = formatVRegWidthVec(i.rn.nr(), vecArrangementS)\n+\t\t}\n+\t\tif dst64 := i.u3 == 1; dst64 {\n+\t\t\tdst = formatVRegSized(i.rd.nr(), 64)\n+\t\t} else {\n+\t\t\tdst = formatVRegSized(i.rd.nr(), 32)\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\", op, dst, src)\n+\n+\tcase intToFpu:\n+\t\tvar op, src, dst string\n+\t\tif signed := i.u1 == 1; signed {\n+\t\t\top = \"scvtf\"\n+\t\t} else {\n+\t\t\top = \"ucvtf\"\n+\t\t}\n+\t\tif src64 := i.u2 == 1; src64 {\n+\t\t\tsrc = formatVRegSized(i.rn.nr(), 64)\n+\t\t} else {\n+\t\t\tsrc = formatVRegSized(i.rn.nr(), 32)\n+\t\t}\n+\t\tif dst64 := i.u3 == 1; dst64 {\n+\t\t\tdst = formatVRegWidthVec(i.rd.nr(), vecArrangementD)\n+\t\t} else {\n+\t\t\tdst = formatVRegWidthVec(i.rd.nr(), vecArrangementS)\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\", op, dst, src)\n+\tcase fpuCSel:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tstr = fmt.Sprintf(\"fcsel %s, %s, %s, %s\",\n+\t\t\tformatVRegSized(i.rd.nr(), size),\n+\t\t\tformatVRegSized(i.rn.nr(), size),\n+\t\t\tformatVRegSized(i.rm.nr(), size),\n+\t\t\tcondFlag(i.u1),\n+\t\t)\n+\tcase movToVec:\n+\t\tvar size byte\n+\t\tarr := vecArrangement(i.u1)\n+\t\tswitch arr {\n+\t\tcase vecArrangementB, vecArrangementH, vecArrangementS:\n+\t\t\tsize = 32\n+\t\tcase vecArrangementD:\n+\t\t\tsize = 64\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement \" + arr.String())\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"ins %s, %s\", formatVRegVec(i.rd.nr(), arr, vecIndex(i.u2)), formatVRegSized(i.rn.nr(), size))\n+\tcase movFromVec, movFromVecSigned:\n+\t\tvar size byte\n+\t\tvar opcode string\n+\t\tarr := vecArrangement(i.u1)\n+\t\tsigned := i.kind == movFromVecSigned\n+\t\tswitch arr {\n+\t\tcase vecArrangementB, vecArrangementH, vecArrangementS:\n+\t\t\tsize = 32\n+\t\t\tif signed {\n+\t\t\t\topcode = \"smov\"\n+\t\t\t} else {\n+\t\t\t\topcode = \"umov\"\n+\t\t\t}\n+\t\tcase vecArrangementD:\n+\t\t\tsize = 64\n+\t\t\tif signed {\n+\t\t\t\topcode = \"smov\"\n+\t\t\t} else {\n+\t\t\t\topcode = \"mov\"\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement \" + arr.String())\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\", opcode, formatVRegSized(i.rd.nr(), size), formatVRegVec(i.rn.nr(), arr, vecIndex(i.u2)))\n+\tcase vecDup:\n+\t\tstr = fmt.Sprintf(\"dup %s, %s\",\n+\t\t\tformatVRegVec(i.rd.nr(), vecArrangement(i.u1), vecIndexNone),\n+\t\t\tformatVRegSized(i.rn.nr(), 64),\n+\t\t)\n+\tcase vecDupElement:\n+\t\tarr := vecArrangement(i.u1)\n+\t\tstr = fmt.Sprintf(\"dup %s, %s\",\n+\t\t\tformatVRegVec(i.rd.nr(), arr, vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), arr, vecIndex(i.u2)),\n+\t\t)\n+\tcase vecDupFromFpu:\n+\t\tpanic(\"TODO\")\n+\tcase vecExtract:\n+\t\tstr = fmt.Sprintf(\"ext %s, %s, %s, #%d\",\n+\t\t\tformatVRegVec(i.rd.nr(), vecArrangement(i.u1), vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), vecArrangement(i.u1), vecIndexNone),\n+\t\t\tformatVRegVec(i.rm.nr(), vecArrangement(i.u1), vecIndexNone),\n+\t\t\tuint32(i.u2),\n+\t\t)\n+\tcase vecExtend:\n+\t\tpanic(\"TODO\")\n+\tcase vecMovElement:\n+\t\tstr = fmt.Sprintf(\"mov %s, %s\",\n+\t\t\tformatVRegVec(i.rd.nr(), vecArrangement(i.u1), vecIndex(i.u2)),\n+\t\t\tformatVRegVec(i.rn.nr(), vecArrangement(i.u1), vecIndex(i.u3)),\n+\t\t)\n+\tcase vecMiscNarrow:\n+\t\tpanic(\"TODO\")\n+\tcase vecRRR, vecRRRRewrite:\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\",\n+\t\t\tvecOp(i.u1),\n+\t\t\tformatVRegVec(i.rd.nr(), vecArrangement(i.u2), vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), vecArrangement(i.u2), vecIndexNone),\n+\t\t\tformatVRegVec(i.rm.nr(), vecArrangement(i.u2), vecIndexNone),\n+\t\t)\n+\tcase vecMisc:\n+\t\tvop := vecOp(i.u1)\n+\t\tif vop == vecOpCmeq0 {\n+\t\t\tstr = fmt.Sprintf(\"cmeq %s, %s, #0\",\n+\t\t\t\tformatVRegVec(i.rd.nr(), vecArrangement(i.u2), vecIndexNone),\n+\t\t\t\tformatVRegVec(i.rn.nr(), vecArrangement(i.u2), vecIndexNone))\n+\t\t} else {\n+\t\t\tstr = fmt.Sprintf(\"%s %s, %s\",\n+\t\t\t\tvop,\n+\t\t\t\tformatVRegVec(i.rd.nr(), vecArrangement(i.u2), vecIndexNone),\n+\t\t\t\tformatVRegVec(i.rn.nr(), vecArrangement(i.u2), vecIndexNone))\n+\t\t}\n+\tcase vecLanes:\n+\t\tarr := vecArrangement(i.u2)\n+\t\tvar destArr vecArrangement\n+\t\tswitch arr {\n+\t\tcase vecArrangement8B, vecArrangement16B:\n+\t\t\tdestArr = vecArrangementH\n+\t\tcase vecArrangement4H, vecArrangement8H:\n+\t\t\tdestArr = vecArrangementS\n+\t\tcase vecArrangement4S:\n+\t\t\tdestArr = vecArrangementD\n+\t\tdefault:\n+\t\t\tpanic(\"invalid arrangement \" + arr.String())\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\",\n+\t\t\tvecOp(i.u1),\n+\t\t\tformatVRegWidthVec(i.rd.nr(), destArr),\n+\t\t\tformatVRegVec(i.rn.nr(), arr, vecIndexNone))\n+\tcase vecShiftImm:\n+\t\tarr := vecArrangement(i.u2)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, #%d\",\n+\t\t\tvecOp(i.u1),\n+\t\t\tformatVRegVec(i.rd.nr(), arr, vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), arr, vecIndexNone),\n+\t\t\ti.rm.shiftImm())\n+\tcase vecTbl:\n+\t\tarr := vecArrangement(i.u2)\n+\t\tstr = fmt.Sprintf(\"tbl %s, { %s }, %s\",\n+\t\t\tformatVRegVec(i.rd.nr(), arr, vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), vecArrangement16B, vecIndexNone),\n+\t\t\tformatVRegVec(i.rm.nr(), arr, vecIndexNone))\n+\tcase vecTbl2:\n+\t\tarr := vecArrangement(i.u2)\n+\t\trd, rn, rm := i.rd.nr(), i.rn.nr(), i.rm.nr()\n+\t\trn1 := regalloc.FromRealReg(rn.RealReg()+1, rn.RegType())\n+\t\tstr = fmt.Sprintf(\"tbl %s, { %s, %s }, %s\",\n+\t\t\tformatVRegVec(rd, arr, vecIndexNone),\n+\t\t\tformatVRegVec(rn, vecArrangement16B, vecIndexNone),\n+\t\t\tformatVRegVec(rn1, vecArrangement16B, vecIndexNone),\n+\t\t\tformatVRegVec(rm, arr, vecIndexNone))\n+\tcase vecPermute:\n+\t\tarr := vecArrangement(i.u2)\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\",\n+\t\t\tvecOp(i.u1),\n+\t\t\tformatVRegVec(i.rd.nr(), arr, vecIndexNone),\n+\t\t\tformatVRegVec(i.rn.nr(), arr, vecIndexNone),\n+\t\t\tformatVRegVec(i.rm.nr(), arr, vecIndexNone))\n+\tcase movToFPSR:\n+\t\tstr = fmt.Sprintf(\"msr fpsr, %s\", formatVRegSized(i.rn.nr(), 64))\n+\tcase movFromFPSR:\n+\t\tstr = fmt.Sprintf(\"mrs %s fpsr\", formatVRegSized(i.rd.nr(), 64))\n+\tcase call:\n+\t\tstr = fmt.Sprintf(\"bl %s\", ssa.FuncRef(i.u1))\n+\tcase callInd:\n+\t\tstr = fmt.Sprintf(\"bl %s\", formatVRegSized(i.rn.nr(), 64))\n+\tcase ret:\n+\t\tstr = \"ret\"\n+\tcase br:\n+\t\ttarget := label(i.u1)\n+\t\tif i.u3 != 0 {\n+\t\t\tstr = fmt.Sprintf(\"b #%#x (%s)\", i.brOffset(), target.String())\n+\t\t} else {\n+\t\t\tstr = fmt.Sprintf(\"b %s\", target.String())\n+\t\t}\n+\tcase condBr:\n+\t\tsize := is64SizeBitToSize(i.u3)\n+\t\tc := cond(i.u1)\n+\t\ttarget := label(i.u2)\n+\t\tswitch c.kind() {\n+\t\tcase condKindRegisterZero:\n+\t\t\tif !i.condBrOffsetResolved() {\n+\t\t\t\tstr = fmt.Sprintf(\"cbz %s, (%s)\", formatVRegSized(c.register(), size), target.String())\n+\t\t\t} else {\n+\t\t\t\tstr = fmt.Sprintf(\"cbz %s, #%#x %s\", formatVRegSized(c.register(), size), i.condBrOffset(), target.String())\n+\t\t\t}\n+\t\tcase condKindRegisterNotZero:\n+\t\t\tif offset := i.condBrOffset(); offset != 0 {\n+\t\t\t\tstr = fmt.Sprintf(\"cbnz %s, #%#x (%s)\", formatVRegSized(c.register(), size), offset, target.String())\n+\t\t\t} else {\n+\t\t\t\tstr = fmt.Sprintf(\"cbnz %s, %s\", formatVRegSized(c.register(), size), target.String())\n+\t\t\t}\n+\t\tcase condKindCondFlagSet:\n+\t\t\tif offset := i.condBrOffset(); offset != 0 {\n+\t\t\t\tif target == labelInvalid {\n+\t\t\t\t\tstr = fmt.Sprintf(\"b.%s #%#x\", c.flag(), offset)\n+\t\t\t\t} else {\n+\t\t\t\t\tstr = fmt.Sprintf(\"b.%s #%#x, (%s)\", c.flag(), offset, target.String())\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tstr = fmt.Sprintf(\"b.%s %s\", c.flag(), target.String())\n+\t\t\t}\n+\t\t}\n+\tcase adr:\n+\t\tstr = fmt.Sprintf(\"adr %s, #%#x\", formatVRegSized(i.rd.nr(), 64), int64(i.u1))\n+\tcase brTableSequence:\n+\t\ttargetIndex := i.u1\n+\t\tstr = fmt.Sprintf(\"br_table_sequence %s, table_index=%d\", formatVRegSized(i.rn.nr(), 64), targetIndex)\n+\tcase exitSequence:\n+\t\tstr = fmt.Sprintf(\"exit_sequence %s\", formatVRegSized(i.rn.nr(), 64))\n+\tcase atomicRmw:\n+\t\tm := atomicRmwOp(i.u1).String()\n+\t\tsize := byte(32)\n+\t\tswitch i.u2 {\n+\t\tcase 8:\n+\t\t\tsize = 64\n+\t\tcase 2:\n+\t\t\tm = m + \"h\"\n+\t\tcase 1:\n+\t\t\tm = m + \"b\"\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\", m, formatVRegSized(i.rm.nr(), size), formatVRegSized(i.rd.nr(), size), formatVRegSized(i.rn.nr(), 64))\n+\tcase atomicCas:\n+\t\tm := \"casal\"\n+\t\tsize := byte(32)\n+\t\tswitch i.u2 {\n+\t\tcase 8:\n+\t\t\tsize = 64\n+\t\tcase 2:\n+\t\t\tm = m + \"h\"\n+\t\tcase 1:\n+\t\t\tm = m + \"b\"\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s, %s\", m, formatVRegSized(i.rd.nr(), size), formatVRegSized(i.rm.nr(), size), formatVRegSized(i.rn.nr(), 64))\n+\tcase atomicLoad:\n+\t\tm := \"ldar\"\n+\t\tsize := byte(32)\n+\t\tswitch i.u2 {\n+\t\tcase 8:\n+\t\t\tsize = 64\n+\t\tcase 2:\n+\t\t\tm = m + \"h\"\n+\t\tcase 1:\n+\t\t\tm = m + \"b\"\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\", m, formatVRegSized(i.rd.nr(), size), formatVRegSized(i.rn.nr(), 64))\n+\tcase atomicStore:\n+\t\tm := \"stlr\"\n+\t\tsize := byte(32)\n+\t\tswitch i.u2 {\n+\t\tcase 8:\n+\t\t\tsize = 64\n+\t\tcase 2:\n+\t\t\tm = m + \"h\"\n+\t\tcase 1:\n+\t\t\tm = m + \"b\"\n+\t\t}\n+\t\tstr = fmt.Sprintf(\"%s %s, %s\", m, formatVRegSized(i.rm.nr(), size), formatVRegSized(i.rn.nr(), 64))\n+\tcase dmb:\n+\t\tstr = \"dmb\"\n+\tcase udf:\n+\t\tstr = \"udf\"\n+\tcase emitSourceOffsetInfo:\n+\t\tstr = fmt.Sprintf(\"source_offset_info %d\", ssa.SourceOffset(i.u1))\n+\tcase vecLoad1R:\n+\t\tstr = fmt.Sprintf(\"ld1r {%s}, [%s]\", formatVRegVec(i.rd.nr(), vecArrangement(i.u1), vecIndexNone), formatVRegSized(i.rn.nr(), 64))\n+\tcase loadConstBlockArg:\n+\t\tstr = fmt.Sprintf(\"load_const_block_arg %s, %#x\", formatVRegSized(i.rd.nr(), 64), i.u1)\n+\tdefault:\n+\t\tpanic(i.kind)\n+\t}\n+\treturn\n+}\n+\n+func (i *instruction) asAdr(rd regalloc.VReg, offset int64) {\n+\ti.kind = adr\n+\ti.rd = operandNR(rd)\n+\ti.u1 = uint64(offset)\n+}\n+\n+func (i *instruction) asAtomicRmw(op atomicRmwOp, rn, rs, rt operand, size uint64) {\n+\ti.kind = atomicRmw\n+\ti.rd, i.rn, i.rm = rt, rn, rs\n+\ti.u1 = uint64(op)\n+\ti.u2 = size\n+}\n+\n+func (i *instruction) asAtomicCas(rn, rs, rt operand, size uint64) {\n+\ti.kind = atomicCas\n+\ti.rm, i.rn, i.rd = rt, rn, rs\n+\ti.u2 = size\n+}\n+\n+func (i *instruction) asAtomicLoad(rn, rt operand, size uint64) {\n+\ti.kind = atomicLoad\n+\ti.rn, i.rd = rn, rt\n+\ti.u2 = size\n+}\n+\n+func (i *instruction) asAtomicStore(rn, rt operand, size uint64) {\n+\ti.kind = atomicStore\n+\ti.rn, i.rm = rn, rt\n+\ti.u2 = size\n+}\n+\n+func (i *instruction) asDMB() {\n+\ti.kind = dmb\n+}\n+\n+// TODO: delete unnecessary things.\n+const (\n+\t// nop0 represents a no-op of zero size.\n+\tnop0 instructionKind = iota + 1\n+\t// aluRRR represents an ALU operation with two register sources and a register destination.\n+\taluRRR\n+\t// aluRRRR represents an ALU operation with three register sources and a register destination.\n+\taluRRRR\n+\t// aluRRImm12 represents an ALU operation with a register source and an immediate-12 source, with a register destination.\n+\taluRRImm12\n+\t// aluRRBitmaskImm represents an ALU operation with a register source and a bitmask immediate, with a register destination.\n+\taluRRBitmaskImm\n+\t// aluRRImmShift represents an ALU operation with a register source and an immediate-shifted source, with a register destination.\n+\taluRRImmShift\n+\t// aluRRRShift represents an ALU operation with two register sources, one of which can be shifted, with a register destination.\n+\taluRRRShift\n+\t// aluRRRExtend represents an ALU operation with two register sources, one of which can be extended, with a register destination.\n+\taluRRRExtend\n+\t// bitRR represents a bit op instruction with a single register source.\n+\tbitRR\n+\t// uLoad8 represents an unsigned 8-bit load.\n+\tuLoad8\n+\t// sLoad8 represents a signed 8-bit load into 64-bit register.\n+\tsLoad8\n+\t// uLoad16 represents an unsigned 16-bit load into 64-bit register.\n+\tuLoad16\n+\t// sLoad16 represents a signed 16-bit load into 64-bit register.\n+\tsLoad16\n+\t// uLoad32 represents an unsigned 32-bit load into 64-bit register.\n+\tuLoad32\n+\t// sLoad32 represents a signed 32-bit load into 64-bit register.\n+\tsLoad32\n+\t// uLoad64 represents a 64-bit load.\n+\tuLoad64\n+\t// store8 represents an 8-bit store.\n+\tstore8\n+\t// store16 represents a 16-bit store.\n+\tstore16\n+\t// store32 represents a 32-bit store.\n+\tstore32\n+\t// store64 represents a 64-bit store.\n+\tstore64\n+\t// storeP64 represents a store of a pair of registers.\n+\tstoreP64\n+\t// loadP64 represents a load of a pair of registers.\n+\tloadP64\n+\t// mov64 represents a MOV instruction. These are encoded as ORR's but we keep them separate for better handling.\n+\tmov64\n+\t// mov32 represents a 32-bit MOV. This zeroes the top 32 bits of the destination.\n+\tmov32\n+\t// movZ represents a MOVZ with a 16-bit immediate.\n+\tmovZ\n+\t// movN represents a MOVN with a 16-bit immediate.\n+\tmovN\n+\t// movK represents a MOVK with a 16-bit immediate.\n+\tmovK\n+\t// extend represents a sign- or zero-extend operation.\n+\textend\n+\t// cSel represents a conditional-select operation.\n+\tcSel\n+\t// cSet represents a conditional-set operation.\n+\tcSet\n+\t// cCmpImm represents a conditional comparison with an immediate.\n+\tcCmpImm\n+\t// fpuMov64 represents a FPU move. Distinct from a vector-register move; moving just 64 bits appears to be significantly faster.\n+\tfpuMov64\n+\t// fpuMov128 represents a vector register move.\n+\tfpuMov128\n+\t// fpuMovFromVec represents a move to scalar from a vector element.\n+\tfpuMovFromVec\n+\t// fpuRR represents a 1-op FPU instruction.\n+\tfpuRR\n+\t// fpuRRR represents a 2-op FPU instruction.\n+\tfpuRRR\n+\t// fpuRRI represents a 2-op FPU instruction with immediate value.\n+\tfpuRRI\n+\t// fpuRRRR represents a 3-op FPU instruction.\n+\tfpuRRRR\n+\t// fpuCmp represents a FPU comparison, either 32 or 64 bit.\n+\tfpuCmp\n+\t// fpuLoad32 represents a floating-point load, single-precision (32 bit).\n+\tfpuLoad32\n+\t// fpuStore32 represents a floating-point store, single-precision (32 bit).\n+\tfpuStore32\n+\t// fpuLoad64 represents a floating-point load, double-precision (64 bit).\n+\tfpuLoad64\n+\t// fpuStore64 represents a floating-point store, double-precision (64 bit).\n+\tfpuStore64\n+\t// fpuLoad128 represents a floating-point/vector load, 128 bit.\n+\tfpuLoad128\n+\t// fpuStore128 represents a floating-point/vector store, 128 bit.\n+\tfpuStore128\n+\t// loadFpuConst32 represents a load of a 32-bit floating-point constant.\n+\tloadFpuConst32\n+\t// loadFpuConst64 represents a load of a 64-bit floating-point constant.\n+\tloadFpuConst64\n+\t// loadFpuConst128 represents a load of a 128-bit floating-point constant.\n+\tloadFpuConst128\n+\t// vecLoad1R represents a load of a one single-element structure that replicates to all lanes of a vector.\n+\tvecLoad1R\n+\t// fpuToInt represents a conversion from FP to integer.\n+\tfpuToInt\n+\t// intToFpu represents a conversion from integer to FP.\n+\tintToFpu\n+\t// fpuCSel represents a 32/64-bit FP conditional select.\n+\tfpuCSel\n+\t// movToVec represents a move to a vector element from a GPR.\n+\tmovToVec\n+\t// movFromVec represents an unsigned move from a vector element to a GPR.\n+\tmovFromVec\n+\t// movFromVecSigned represents a signed move from a vector element to a GPR.\n+\tmovFromVecSigned\n+\t// vecDup represents a duplication of general-purpose register to vector.\n+\tvecDup\n+\t// vecDupElement represents a duplication of a vector element to vector or scalar.\n+\tvecDupElement\n+\t// vecDupFromFpu represents a duplication of scalar to vector.\n+\tvecDupFromFpu\n+\t// vecExtract represents a vector extraction operation.\n+\tvecExtract\n+\t// vecExtend represents a vector extension operation.\n+\tvecExtend\n+\t// vecMovElement represents a move vector element to another vector element operation.\n+\tvecMovElement\n+\t// vecMiscNarrow represents a vector narrowing operation.\n+\tvecMiscNarrow\n+\t// vecRRR represents a vector ALU operation.\n+\tvecRRR\n+\t// vecRRRRewrite is exactly the same as vecRRR except that this rewrites the destination register.\n+\t// For example, BSL instruction rewrites the destination register, and the existing value influences the result.\n+\t// Therefore, the \"destination\" register in vecRRRRewrite will be treated as \"use\" which makes the register outlive\n+\t// the instruction while this instruction doesn't have \"def\" in the context of register allocation.\n+\tvecRRRRewrite\n+\t// vecMisc represents a vector two register miscellaneous instruction.\n+\tvecMisc\n+\t// vecLanes represents a vector instruction across lanes.\n+\tvecLanes\n+\t// vecShiftImm represents a SIMD scalar shift by immediate instruction.\n+\tvecShiftImm\n+\t// vecTbl represents a table vector lookup - single register table.\n+\tvecTbl\n+\t// vecTbl2 represents a table vector lookup - two register table.\n+\tvecTbl2\n+\t// vecPermute represents a vector permute instruction.\n+\tvecPermute\n+\t// movToNZCV represents a move to the FPSR.\n+\tmovToFPSR\n+\t// movFromNZCV represents a move from the FPSR.\n+\tmovFromFPSR\n+\t// call represents a machine call instruction.\n+\tcall\n+\t// callInd represents a machine indirect-call instruction.\n+\tcallInd\n+\t// ret represents a machine return instruction.\n+\tret\n+\t// br represents an unconditional branch.\n+\tbr\n+\t// condBr represents a conditional branch.\n+\tcondBr\n+\t// adr represents a compute the address (using a PC-relative offset) of a memory location.\n+\tadr\n+\t// brTableSequence represents a jump-table sequence.\n+\tbrTableSequence\n+\t// exitSequence consists of multiple instructions, and exits the execution immediately.\n+\t// See encodeExitSequence.\n+\texitSequence\n+\t// atomicRmw represents an atomic read-modify-write operation with two register sources and a register destination.\n+\tatomicRmw\n+\t// atomicCas represents an atomic compare-and-swap operation with three register sources. The value is loaded to\n+\t// the source register containing the comparison value.\n+\tatomicCas\n+\t// atomicLoad represents an atomic load with one source register and a register destination.\n+\tatomicLoad\n+\t// atomicStore represents an atomic store with two source registers and no destination.\n+\tatomicStore\n+\t// dmb represents the data memory barrier instruction in inner-shareable (ish) mode.\n+\tdmb\n+\t// UDF is the undefined instruction. For debugging only.\n+\tudf\n+\t// loadConstBlockArg represents a load of a constant block argument.\n+\tloadConstBlockArg\n+\n+\t// emitSourceOffsetInfo is a dummy instruction to emit source offset info.\n+\t// The existence of this instruction does not affect the execution.\n+\temitSourceOffsetInfo\n+\n+\t// ------------------- do not define below this line -------------------\n+\tnumInstructionKinds\n+)\n+\n+func (i *instruction) asLoadConstBlockArg(v uint64, typ ssa.Type, dst regalloc.VReg) *instruction {\n+\ti.kind = loadConstBlockArg\n+\ti.u1 = v\n+\ti.u2 = uint64(typ)\n+\ti.rd = operandNR(dst)\n+\treturn i\n+}\n+\n+func (i *instruction) loadConstBlockArgData() (v uint64, typ ssa.Type, dst regalloc.VReg) {\n+\treturn i.u1, ssa.Type(i.u2), i.rd.nr()\n+}\n+\n+func (i *instruction) asEmitSourceOffsetInfo(l ssa.SourceOffset) *instruction {\n+\ti.kind = emitSourceOffsetInfo\n+\ti.u1 = uint64(l)\n+\treturn i\n+}\n+\n+func (i *instruction) sourceOffsetInfo() ssa.SourceOffset {\n+\treturn ssa.SourceOffset(i.u1)\n+}\n+\n+func (i *instruction) asUDF() *instruction {\n+\ti.kind = udf\n+\treturn i\n+}\n+\n+func (i *instruction) asFpuToInt(rd, rn operand, rdSigned, src64bit, dst64bit bool) {\n+\ti.kind = fpuToInt\n+\ti.rn = rn\n+\ti.rd = rd\n+\tif rdSigned {\n+\t\ti.u1 = 1\n+\t}\n+\tif src64bit {\n+\t\ti.u2 = 1\n+\t}\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asIntToFpu(rd, rn operand, rnSigned, src64bit, dst64bit bool) {\n+\ti.kind = intToFpu\n+\ti.rn = rn\n+\ti.rd = rd\n+\tif rnSigned {\n+\t\ti.u1 = 1\n+\t}\n+\tif src64bit {\n+\t\ti.u2 = 1\n+\t}\n+\tif dst64bit {\n+\t\ti.u3 = 1\n+\t}\n+}\n+\n+func (i *instruction) asExitSequence(ctx regalloc.VReg) *instruction {\n+\ti.kind = exitSequence\n+\ti.rn = operandNR(ctx)\n+\treturn i\n+}\n+\n+// aluOp determines the type of ALU operation. Instructions whose kind is one of\n+// aluRRR, aluRRRR, aluRRImm12, aluRRBitmaskImm, aluRRImmShift, aluRRRShift and aluRRRExtend\n+// would use this type.\n+type aluOp int\n+\n+func (a aluOp) String() string {\n+\tswitch a {\n+\tcase aluOpAdd:\n+\t\treturn \"add\"\n+\tcase aluOpSub:\n+\t\treturn \"sub\"\n+\tcase aluOpOrr:\n+\t\treturn \"orr\"\n+\tcase aluOpOrn:\n+\t\treturn \"orn\"\n+\tcase aluOpAnd:\n+\t\treturn \"and\"\n+\tcase aluOpAnds:\n+\t\treturn \"ands\"\n+\tcase aluOpBic:\n+\t\treturn \"bic\"\n+\tcase aluOpEor:\n+\t\treturn \"eor\"\n+\tcase aluOpAddS:\n+\t\treturn \"adds\"\n+\tcase aluOpSubS:\n+\t\treturn \"subs\"\n+\tcase aluOpSMulH:\n+\t\treturn \"sMulH\"\n+\tcase aluOpUMulH:\n+\t\treturn \"uMulH\"\n+\tcase aluOpSDiv:\n+\t\treturn \"sdiv\"\n+\tcase aluOpUDiv:\n+\t\treturn \"udiv\"\n+\tcase aluOpRotR:\n+\t\treturn \"ror\"\n+\tcase aluOpLsr:\n+\t\treturn \"lsr\"\n+\tcase aluOpAsr:\n+\t\treturn \"asr\"\n+\tcase aluOpLsl:\n+\t\treturn \"lsl\"\n+\tcase aluOpMAdd:\n+\t\treturn \"madd\"\n+\tcase aluOpMSub:\n+\t\treturn \"msub\"\n+\t}\n+\tpanic(int(a))\n+}\n+\n+const (\n+\t// 32/64-bit Add.\n+\taluOpAdd aluOp = iota\n+\t// 32/64-bit Subtract.\n+\taluOpSub\n+\t// 32/64-bit Bitwise OR.\n+\taluOpOrr\n+\t// 32/64-bit Bitwise OR NOT.\n+\taluOpOrn\n+\t// 32/64-bit Bitwise AND.\n+\taluOpAnd\n+\t// 32/64-bit Bitwise ANDS.\n+\taluOpAnds\n+\t// 32/64-bit Bitwise AND NOT.\n+\taluOpBic\n+\t// 32/64-bit Bitwise XOR (Exclusive OR).\n+\taluOpEor\n+\t// 32/64-bit Add setting flags.\n+\taluOpAddS\n+\t// 32/64-bit Subtract setting flags.\n+\taluOpSubS\n+\t// Signed multiply, high-word result.\n+\taluOpSMulH\n+\t// Unsigned multiply, high-word result.\n+\taluOpUMulH\n+\t// 64-bit Signed divide.\n+\taluOpSDiv\n+\t// 64-bit Unsigned divide.\n+\taluOpUDiv\n+\t// 32/64-bit Rotate right.\n+\taluOpRotR\n+\t// 32/64-bit Logical shift right.\n+\taluOpLsr\n+\t// 32/64-bit Arithmetic shift right.\n+\taluOpAsr\n+\t// 32/64-bit Logical shift left.\n+\taluOpLsl /// Multiply-add\n+\n+\t// MAdd and MSub are only applicable for aluRRRR.\n+\taluOpMAdd\n+\taluOpMSub\n+)\n+\n+// vecOp determines the type of vector operation. Instructions whose kind is one of\n+// vecOpCnt would use this type.\n+type vecOp int\n+\n+// String implements fmt.Stringer.\n+func (b vecOp) String() string {\n+\tswitch b {\n+\tcase vecOpCnt:\n+\t\treturn \"cnt\"\n+\tcase vecOpCmeq:\n+\t\treturn \"cmeq\"\n+\tcase vecOpCmgt:\n+\t\treturn \"cmgt\"\n+\tcase vecOpCmhi:\n+\t\treturn \"cmhi\"\n+\tcase vecOpCmge:\n+\t\treturn \"cmge\"\n+\tcase vecOpCmhs:\n+\t\treturn \"cmhs\"\n+\tcase vecOpFcmeq:\n+\t\treturn \"fcmeq\"\n+\tcase vecOpFcmgt:\n+\t\treturn \"fcmgt\"\n+\tcase vecOpFcmge:\n+\t\treturn \"fcmge\"\n+\tcase vecOpCmeq0:\n+\t\treturn \"cmeq0\"\n+\tcase vecOpUaddlv:\n+\t\treturn \"uaddlv\"\n+\tcase vecOpBit:\n+\t\treturn \"bit\"\n+\tcase vecOpBic:\n+\t\treturn \"bic\"\n+\tcase vecOpBsl:\n+\t\treturn \"bsl\"\n+\tcase vecOpNot:\n+\t\treturn \"not\"\n+\tcase vecOpAnd:\n+\t\treturn \"and\"\n+\tcase vecOpOrr:\n+\t\treturn \"orr\"\n+\tcase vecOpEOR:\n+\t\treturn \"eor\"\n+\tcase vecOpFadd:\n+\t\treturn \"fadd\"\n+\tcase vecOpAdd:\n+\t\treturn \"add\"\n+\tcase vecOpAddp:\n+\t\treturn \"addp\"\n+\tcase vecOpAddv:\n+\t\treturn \"addv\"\n+\tcase vecOpSub:\n+\t\treturn \"sub\"\n+\tcase vecOpFsub:\n+\t\treturn \"fsub\"\n+\tcase vecOpSmin:\n+\t\treturn \"smin\"\n+\tcase vecOpUmin:\n+\t\treturn \"umin\"\n+\tcase vecOpUminv:\n+\t\treturn \"uminv\"\n+\tcase vecOpSmax:\n+\t\treturn \"smax\"\n+\tcase vecOpUmax:\n+\t\treturn \"umax\"\n+\tcase vecOpUmaxp:\n+\t\treturn \"umaxp\"\n+\tcase vecOpUrhadd:\n+\t\treturn \"urhadd\"\n+\tcase vecOpFmul:\n+\t\treturn \"fmul\"\n+\tcase vecOpSqrdmulh:\n+\t\treturn \"sqrdmulh\"\n+\tcase vecOpMul:\n+\t\treturn \"mul\"\n+\tcase vecOpUmlal:\n+\t\treturn \"umlal\"\n+\tcase vecOpFdiv:\n+\t\treturn \"fdiv\"\n+\tcase vecOpFsqrt:\n+\t\treturn \"fsqrt\"\n+\tcase vecOpAbs:\n+\t\treturn \"abs\"\n+\tcase vecOpFabs:\n+\t\treturn \"fabs\"\n+\tcase vecOpNeg:\n+\t\treturn \"neg\"\n+\tcase vecOpFneg:\n+\t\treturn \"fneg\"\n+\tcase vecOpFrintp:\n+\t\treturn \"frintp\"\n+\tcase vecOpFrintm:\n+\t\treturn \"frintm\"\n+\tcase vecOpFrintn:\n+\t\treturn \"frintn\"\n+\tcase vecOpFrintz:\n+\t\treturn \"frintz\"\n+\tcase vecOpFcvtl:\n+\t\treturn \"fcvtl\"\n+\tcase vecOpFcvtn:\n+\t\treturn \"fcvtn\"\n+\tcase vecOpFcvtzu:\n+\t\treturn \"fcvtzu\"\n+\tcase vecOpFcvtzs:\n+\t\treturn \"fcvtzs\"\n+\tcase vecOpScvtf:\n+\t\treturn \"scvtf\"\n+\tcase vecOpUcvtf:\n+\t\treturn \"ucvtf\"\n+\tcase vecOpSqxtn:\n+\t\treturn \"sqxtn\"\n+\tcase vecOpUqxtn:\n+\t\treturn \"uqxtn\"\n+\tcase vecOpSqxtun:\n+\t\treturn \"sqxtun\"\n+\tcase vecOpRev64:\n+\t\treturn \"rev64\"\n+\tcase vecOpXtn:\n+\t\treturn \"xtn\"\n+\tcase vecOpShll:\n+\t\treturn \"shll\"\n+\tcase vecOpSshl:\n+\t\treturn \"sshl\"\n+\tcase vecOpSshll:\n+\t\treturn \"sshll\"\n+\tcase vecOpUshl:\n+\t\treturn \"ushl\"\n+\tcase vecOpUshll:\n+\t\treturn \"ushll\"\n+\tcase vecOpSshr:\n+\t\treturn \"sshr\"\n+\tcase vecOpZip1:\n+\t\treturn \"zip1\"\n+\tcase vecOpFmin:\n+\t\treturn \"fmin\"\n+\tcase vecOpFmax:\n+\t\treturn \"fmax\"\n+\tcase vecOpSmull:\n+\t\treturn \"smull\"\n+\tcase vecOpSmull2:\n+\t\treturn \"smull2\"\n+\t}\n+\tpanic(int(b))\n+}\n+\n+const (\n+\tvecOpCnt vecOp = iota\n+\tvecOpCmeq0\n+\tvecOpCmeq\n+\tvecOpCmgt\n+\tvecOpCmhi\n+\tvecOpCmge\n+\tvecOpCmhs\n+\tvecOpFcmeq\n+\tvecOpFcmgt\n+\tvecOpFcmge\n+\tvecOpUaddlv\n+\tvecOpBit\n+\tvecOpBic\n+\tvecOpBsl\n+\tvecOpNot\n+\tvecOpAnd\n+\tvecOpOrr\n+\tvecOpEOR\n+\tvecOpAdd\n+\tvecOpFadd\n+\tvecOpAddv\n+\tvecOpSqadd\n+\tvecOpUqadd\n+\tvecOpAddp\n+\tvecOpSub\n+\tvecOpFsub\n+\tvecOpSqsub\n+\tvecOpUqsub\n+\tvecOpSmin\n+\tvecOpUmin\n+\tvecOpUminv\n+\tvecOpFmin\n+\tvecOpSmax\n+\tvecOpUmax\n+\tvecOpUmaxp\n+\tvecOpFmax\n+\tvecOpUrhadd\n+\tvecOpMul\n+\tvecOpFmul\n+\tvecOpSqrdmulh\n+\tvecOpUmlal\n+\tvecOpFdiv\n+\tvecOpFsqrt\n+\tvecOpAbs\n+\tvecOpFabs\n+\tvecOpNeg\n+\tvecOpFneg\n+\tvecOpFrintm\n+\tvecOpFrintn\n+\tvecOpFrintp\n+\tvecOpFrintz\n+\tvecOpFcvtl\n+\tvecOpFcvtn\n+\tvecOpFcvtzs\n+\tvecOpFcvtzu\n+\tvecOpScvtf\n+\tvecOpUcvtf\n+\tvecOpSqxtn\n+\tvecOpSqxtun\n+\tvecOpUqxtn\n+\tvecOpRev64\n+\tvecOpXtn\n+\tvecOpShll\n+\tvecOpSshl\n+\tvecOpSshll\n+\tvecOpUshl\n+\tvecOpUshll\n+\tvecOpSshr\n+\tvecOpZip1\n+\tvecOpSmull\n+\tvecOpSmull2\n+)\n+\n+// bitOp determines the type of bitwise operation. Instructions whose kind is one of\n+// bitOpRbit and bitOpClz would use this type.\n+type bitOp int\n+\n+// String implements fmt.Stringer.\n+func (b bitOp) String() string {\n+\tswitch b {\n+\tcase bitOpRbit:\n+\t\treturn \"rbit\"\n+\tcase bitOpClz:\n+\t\treturn \"clz\"\n+\t}\n+\tpanic(int(b))\n+}\n+\n+const (\n+\t// 32/64-bit Rbit.\n+\tbitOpRbit bitOp = iota\n+\t// 32/64-bit Clz.\n+\tbitOpClz\n+)\n+\n+// fpuUniOp represents a unary floating-point unit (FPU) operation.\n+type fpuUniOp byte\n+\n+const (\n+\tfpuUniOpNeg fpuUniOp = iota\n+\tfpuUniOpCvt32To64\n+\tfpuUniOpCvt64To32\n+\tfpuUniOpSqrt\n+\tfpuUniOpRoundPlus\n+\tfpuUniOpRoundMinus\n+\tfpuUniOpRoundZero\n+\tfpuUniOpRoundNearest\n+\tfpuUniOpAbs\n+)\n+\n+// String implements the fmt.Stringer.\n+func (f fpuUniOp) String() string {\n+\tswitch f {\n+\tcase fpuUniOpNeg:\n+\t\treturn \"fneg\"\n+\tcase fpuUniOpCvt32To64:\n+\t\treturn \"fcvt\"\n+\tcase fpuUniOpCvt64To32:\n+\t\treturn \"fcvt\"\n+\tcase fpuUniOpSqrt:\n+\t\treturn \"fsqrt\"\n+\tcase fpuUniOpRoundPlus:\n+\t\treturn \"frintp\"\n+\tcase fpuUniOpRoundMinus:\n+\t\treturn \"frintm\"\n+\tcase fpuUniOpRoundZero:\n+\t\treturn \"frintz\"\n+\tcase fpuUniOpRoundNearest:\n+\t\treturn \"frintn\"\n+\tcase fpuUniOpAbs:\n+\t\treturn \"fabs\"\n+\t}\n+\tpanic(int(f))\n+}\n+\n+// fpuBinOp represents a binary floating-point unit (FPU) operation.\n+type fpuBinOp byte\n+\n+const (\n+\tfpuBinOpAdd = iota\n+\tfpuBinOpSub\n+\tfpuBinOpMul\n+\tfpuBinOpDiv\n+\tfpuBinOpMax\n+\tfpuBinOpMin\n+)\n+\n+// String implements the fmt.Stringer.\n+func (f fpuBinOp) String() string {\n+\tswitch f {\n+\tcase fpuBinOpAdd:\n+\t\treturn \"fadd\"\n+\tcase fpuBinOpSub:\n+\t\treturn \"fsub\"\n+\tcase fpuBinOpMul:\n+\t\treturn \"fmul\"\n+\tcase fpuBinOpDiv:\n+\t\treturn \"fdiv\"\n+\tcase fpuBinOpMax:\n+\t\treturn \"fmax\"\n+\tcase fpuBinOpMin:\n+\t\treturn \"fmin\"\n+\t}\n+\tpanic(int(f))\n+}\n+\n+// extMode represents the mode of a register operand extension.\n+// For example, aluRRRExtend instructions need this info to determine the extensions.\n+type extMode byte\n+\n+const (\n+\textModeNone extMode = iota\n+\t// extModeZeroExtend64 suggests a zero-extension to 32 bits if the original bit size is less than 32.\n+\textModeZeroExtend32\n+\t// extModeSignExtend64 stands for a sign-extension to 32 bits if the original bit size is less than 32.\n+\textModeSignExtend32\n+\t// extModeZeroExtend64 suggests a zero-extension to 64 bits if the original bit size is less than 64.\n+\textModeZeroExtend64\n+\t// extModeSignExtend64 stands for a sign-extension to 64 bits if the original bit size is less than 64.\n+\textModeSignExtend64\n+)\n+\n+func (e extMode) bits() byte {\n+\tswitch e {\n+\tcase extModeZeroExtend32, extModeSignExtend32:\n+\t\treturn 32\n+\tcase extModeZeroExtend64, extModeSignExtend64:\n+\t\treturn 64\n+\tdefault:\n+\t\treturn 0\n+\t}\n+}\n+\n+func (e extMode) signed() bool {\n+\tswitch e {\n+\tcase extModeSignExtend32, extModeSignExtend64:\n+\t\treturn true\n+\tdefault:\n+\t\treturn false\n+\t}\n+}\n+\n+func extModeOf(t ssa.Type, signed bool) extMode {\n+\tswitch t.Bits() {\n+\tcase 32:\n+\t\tif signed {\n+\t\t\treturn extModeSignExtend32\n+\t\t}\n+\t\treturn extModeZeroExtend32\n+\tcase 64:\n+\t\tif signed {\n+\t\t\treturn extModeSignExtend64\n+\t\t}\n+\t\treturn extModeZeroExtend64\n+\tdefault:\n+\t\tpanic(\"TODO? do we need narrower than 32 bits?\")\n+\t}\n+}\n+\n+type extendOp byte\n+\n+const (\n+\textendOpUXTB extendOp = 0b000\n+\textendOpUXTH extendOp = 0b001\n+\textendOpUXTW extendOp = 0b010\n+\t// extendOpUXTX does nothing, but convenient symbol that officially exists. See:\n+\t// https://stackoverflow.com/questions/72041372/what-do-the-uxtx-and-sxtx-extensions-mean-for-32-bit-aarch64-adds-instruct\n+\textendOpUXTX extendOp = 0b011\n+\textendOpSXTB extendOp = 0b100\n+\textendOpSXTH extendOp = 0b101\n+\textendOpSXTW extendOp = 0b110\n+\t// extendOpSXTX does nothing, but convenient symbol that officially exists. See:\n+\t// https://stackoverflow.com/questions/72041372/what-do-the-uxtx-and-sxtx-extensions-mean-for-32-bit-aarch64-adds-instruct\n+\textendOpSXTX extendOp = 0b111\n+\textendOpNone extendOp = 0xff\n+)\n+\n+func (e extendOp) srcBits() byte {\n+\tswitch e {\n+\tcase extendOpUXTB, extendOpSXTB:\n+\t\treturn 8\n+\tcase extendOpUXTH, extendOpSXTH:\n+\t\treturn 16\n+\tcase extendOpUXTW, extendOpSXTW:\n+\t\treturn 32\n+\tcase extendOpUXTX, extendOpSXTX:\n+\t\treturn 64\n+\t}\n+\tpanic(int(e))\n+}\n+\n+func (e extendOp) String() string {\n+\tswitch e {\n+\tcase extendOpUXTB:\n+\t\treturn \"UXTB\"\n+\tcase extendOpUXTH:\n+\t\treturn \"UXTH\"\n+\tcase extendOpUXTW:\n+\t\treturn \"UXTW\"\n+\tcase extendOpUXTX:\n+\t\treturn \"UXTX\"\n+\tcase extendOpSXTB:\n+\t\treturn \"SXTB\"\n+\tcase extendOpSXTH:\n+\t\treturn \"SXTH\"\n+\tcase extendOpSXTW:\n+\t\treturn \"SXTW\"\n+\tcase extendOpSXTX:\n+\t\treturn \"SXTX\"\n+\t}\n+\tpanic(int(e))\n+}\n+\n+func extendOpFrom(signed bool, from byte) extendOp {\n+\tswitch from {\n+\tcase 8:\n+\t\tif signed {\n+\t\t\treturn extendOpSXTB\n+\t\t}\n+\t\treturn extendOpUXTB\n+\tcase 16:\n+\t\tif signed {\n+\t\t\treturn extendOpSXTH\n+\t\t}\n+\t\treturn extendOpUXTH\n+\tcase 32:\n+\t\tif signed {\n+\t\t\treturn extendOpSXTW\n+\t\t}\n+\t\treturn extendOpUXTW\n+\tcase 64:\n+\t\tif signed {\n+\t\t\treturn extendOpSXTX\n+\t\t}\n+\t\treturn extendOpUXTX\n+\t}\n+\tpanic(\"invalid extendOpFrom\")\n+}\n+\n+type shiftOp byte\n+\n+const (\n+\tshiftOpLSL shiftOp = 0b00\n+\tshiftOpLSR shiftOp = 0b01\n+\tshiftOpASR shiftOp = 0b10\n+\tshiftOpROR shiftOp = 0b11\n+)\n+\n+func (s shiftOp) String() string {\n+\tswitch s {\n+\tcase shiftOpLSL:\n+\t\treturn \"lsl\"\n+\tcase shiftOpLSR:\n+\t\treturn \"lsr\"\n+\tcase shiftOpASR:\n+\t\treturn \"asr\"\n+\tcase shiftOpROR:\n+\t\treturn \"ror\"\n+\t}\n+\tpanic(int(s))\n+}\n+\n+const exitSequenceSize = 6 * 4 // 6 instructions as in encodeExitSequence.\n+\n+// size returns the size of the instruction in encoded bytes.\n+func (i *instruction) size() int64 {\n+\tswitch i.kind {\n+\tcase exitSequence:\n+\t\treturn exitSequenceSize // 5 instructions as in encodeExitSequence.\n+\tcase nop0, loadConstBlockArg:\n+\t\treturn 0\n+\tcase emitSourceOffsetInfo:\n+\t\treturn 0\n+\tcase loadFpuConst32:\n+\t\tif i.u1 == 0 {\n+\t\t\treturn 4 // zero loading can be encoded as a single instruction.\n+\t\t}\n+\t\treturn 4 + 4 + 4\n+\tcase loadFpuConst64:\n+\t\tif i.u1 == 0 {\n+\t\t\treturn 4 // zero loading can be encoded as a single instruction.\n+\t\t}\n+\t\treturn 4 + 4 + 8\n+\tcase loadFpuConst128:\n+\t\tif i.u1 == 0 && i.u2 == 0 {\n+\t\t\treturn 4 // zero loading can be encoded as a single instruction.\n+\t\t}\n+\t\treturn 4 + 4 + 16\n+\tcase brTableSequence:\n+\t\treturn 4*4 + int64(i.u2)*4\n+\tdefault:\n+\t\treturn 4\n+\t}\n+}\n+\n+// vecArrangement is the arrangement of data within a vector register.\n+type vecArrangement byte\n+\n+const (\n+\t// vecArrangementNone is an arrangement indicating no data is stored.\n+\tvecArrangementNone vecArrangement = iota\n+\t// vecArrangement8B is an arrangement of 8 bytes (64-bit vector)\n+\tvecArrangement8B\n+\t// vecArrangement16B is an arrangement of 16 bytes (128-bit vector)\n+\tvecArrangement16B\n+\t// vecArrangement4H is an arrangement of 4 half precisions (64-bit vector)\n+\tvecArrangement4H\n+\t// vecArrangement8H is an arrangement of 8 half precisions (128-bit vector)\n+\tvecArrangement8H\n+\t// vecArrangement2S is an arrangement of 2 single precisions (64-bit vector)\n+\tvecArrangement2S\n+\t// vecArrangement4S is an arrangement of 4 single precisions (128-bit vector)\n+\tvecArrangement4S\n+\t// vecArrangement1D is an arrangement of 1 double precision (64-bit vector)\n+\tvecArrangement1D\n+\t// vecArrangement2D is an arrangement of 2 double precisions (128-bit vector)\n+\tvecArrangement2D\n+\n+\t// Assign each vector size specifier to a vector arrangement ID.\n+\t// Instructions can only have an arrangement or a size specifier, but not both, so it\n+\t// simplifies the internal representation of vector instructions by being able to\n+\t// store either into the same field.\n+\n+\t// vecArrangementB is a size specifier of byte\n+\tvecArrangementB\n+\t// vecArrangementH is a size specifier of word (16-bit)\n+\tvecArrangementH\n+\t// vecArrangementS is a size specifier of double word (32-bit)\n+\tvecArrangementS\n+\t// vecArrangementD is a size specifier of quad word (64-bit)\n+\tvecArrangementD\n+\t// vecArrangementQ is a size specifier of the entire vector (128-bit)\n+\tvecArrangementQ\n+)\n+\n+// String implements fmt.Stringer\n+func (v vecArrangement) String() (ret string) {\n+\tswitch v {\n+\tcase vecArrangement8B:\n+\t\tret = \"8B\"\n+\tcase vecArrangement16B:\n+\t\tret = \"16B\"\n+\tcase vecArrangement4H:\n+\t\tret = \"4H\"\n+\tcase vecArrangement8H:\n+\t\tret = \"8H\"\n+\tcase vecArrangement2S:\n+\t\tret = \"2S\"\n+\tcase vecArrangement4S:\n+\t\tret = \"4S\"\n+\tcase vecArrangement1D:\n+\t\tret = \"1D\"\n+\tcase vecArrangement2D:\n+\t\tret = \"2D\"\n+\tcase vecArrangementB:\n+\t\tret = \"B\"\n+\tcase vecArrangementH:\n+\t\tret = \"H\"\n+\tcase vecArrangementS:\n+\t\tret = \"S\"\n+\tcase vecArrangementD:\n+\t\tret = \"D\"\n+\tcase vecArrangementQ:\n+\t\tret = \"Q\"\n+\tcase vecArrangementNone:\n+\t\tret = \"none\"\n+\tdefault:\n+\t\tpanic(v)\n+\t}\n+\treturn\n+}\n+\n+// vecIndex is the index of an element of a vector register\n+type vecIndex byte\n+\n+// vecIndexNone indicates no vector index specified.\n+const vecIndexNone = ^vecIndex(0)\n+\n+func ssaLaneToArrangement(lane ssa.VecLane) vecArrangement {\n+\tswitch lane {\n+\tcase ssa.VecLaneI8x16:\n+\t\treturn vecArrangement16B\n+\tcase ssa.VecLaneI16x8:\n+\t\treturn vecArrangement8H\n+\tcase ssa.VecLaneI32x4:\n+\t\treturn vecArrangement4S\n+\tcase ssa.VecLaneI64x2:\n+\t\treturn vecArrangement2D\n+\tcase ssa.VecLaneF32x4:\n+\t\treturn vecArrangement4S\n+\tcase ssa.VecLaneF64x2:\n+\t\treturn vecArrangement2D\n+\tdefault:\n+\t\tpanic(lane)\n+\t}\n+}\n+\n+// atomicRmwOp is the type of atomic read-modify-write operation.\n+type atomicRmwOp byte\n+\n+const (\n+\t// atomicRmwOpAdd is an atomic add operation.\n+\tatomicRmwOpAdd atomicRmwOp = iota\n+\t// atomicRmwOpClr is an atomic clear operation, i.e. AND NOT.\n+\tatomicRmwOpClr\n+\t// atomicRmwOpSet is an atomic set operation, i.e. OR.\n+\tatomicRmwOpSet\n+\t// atomicRmwOpEor is an atomic exclusive OR operation.\n+\tatomicRmwOpEor\n+\t// atomicRmwOpSwp is an atomic swap operation.\n+\tatomicRmwOpSwp\n+)\n+\n+// String implements fmt.Stringer\n+func (a atomicRmwOp) String() string {\n+\tswitch a {\n+\tcase atomicRmwOpAdd:\n+\t\treturn \"ldaddal\"\n+\tcase atomicRmwOpClr:\n+\t\treturn \"ldclral\"\n+\tcase atomicRmwOpSet:\n+\t\treturn \"ldsetal\"\n+\tcase atomicRmwOpEor:\n+\t\treturn \"ldeoral\"\n+\tcase atomicRmwOpSwp:\n+\t\treturn \"swpal\"\n+\t}\n+\tpanic(fmt.Sprintf(\"unknown atomicRmwOp: %d\", a))\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/instr_encoding.go",
          "status": "added",
          "additions": 2351,
          "deletions": 0,
          "patch": "@@ -0,0 +1,2351 @@\n+package arm64\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+// Encode implements backend.Machine Encode.\n+func (m *machine) Encode(ctx context.Context) error {\n+\tm.resolveRelativeAddresses(ctx)\n+\tm.encode(m.executableContext.RootInstr)\n+\tif l := len(m.compiler.Buf()); l > maxFunctionExecutableSize {\n+\t\treturn fmt.Errorf(\"function size exceeds the limit: %d > %d\", l, maxFunctionExecutableSize)\n+\t}\n+\treturn nil\n+}\n+\n+func (m *machine) encode(root *instruction) {\n+\tfor cur := root; cur != nil; cur = cur.next {\n+\t\tcur.encode(m)\n+\t}\n+}\n+\n+func (i *instruction) encode(m *machine) {\n+\tc := m.compiler\n+\tswitch kind := i.kind; kind {\n+\tcase nop0, emitSourceOffsetInfo, loadConstBlockArg:\n+\tcase exitSequence:\n+\t\tencodeExitSequence(c, i.rn.reg())\n+\tcase ret:\n+\t\t// https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/RET--Return-from-subroutine-?lang=en\n+\t\tc.Emit4Bytes(encodeRet())\n+\tcase br:\n+\t\timm := i.brOffset()\n+\t\tc.Emit4Bytes(encodeUnconditionalBranch(false, imm))\n+\tcase call:\n+\t\t// We still don't know the exact address of the function to call, so we emit a placeholder.\n+\t\tc.AddRelocationInfo(i.callFuncRef())\n+\t\tc.Emit4Bytes(encodeUnconditionalBranch(true, 0)) // 0 = placeholder\n+\tcase callInd:\n+\t\tc.Emit4Bytes(encodeUnconditionalBranchReg(regNumberInEncoding[i.rn.realReg()], true))\n+\tcase store8, store16, store32, store64, fpuStore32, fpuStore64, fpuStore128:\n+\t\tc.Emit4Bytes(encodeLoadOrStore(i.kind, regNumberInEncoding[i.rn.realReg()], i.amode))\n+\tcase uLoad8, uLoad16, uLoad32, uLoad64, sLoad8, sLoad16, sLoad32, fpuLoad32, fpuLoad64, fpuLoad128:\n+\t\tc.Emit4Bytes(encodeLoadOrStore(i.kind, regNumberInEncoding[i.rd.realReg()], i.amode))\n+\tcase vecLoad1R:\n+\t\tc.Emit4Bytes(encodeVecLoad1R(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(i.u1)))\n+\tcase condBr:\n+\t\timm19 := i.condBrOffset()\n+\t\tif imm19%4 != 0 {\n+\t\t\tpanic(\"imm26 for branch must be a multiple of 4\")\n+\t\t}\n+\n+\t\timm19U32 := uint32(imm19/4) & 0b111_11111111_11111111\n+\t\tbrCond := i.condBrCond()\n+\t\tswitch brCond.kind() {\n+\t\tcase condKindRegisterZero:\n+\t\t\trt := regNumberInEncoding[brCond.register().RealReg()]\n+\t\t\tc.Emit4Bytes(encodeCBZCBNZ(rt, false, imm19U32, i.condBr64bit()))\n+\t\tcase condKindRegisterNotZero:\n+\t\t\trt := regNumberInEncoding[brCond.register().RealReg()]\n+\t\t\tc.Emit4Bytes(encodeCBZCBNZ(rt, true, imm19U32, i.condBr64bit()))\n+\t\tcase condKindCondFlagSet:\n+\t\t\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/B-cond--Branch-conditionally-\n+\t\t\tfl := brCond.flag()\n+\t\t\tc.Emit4Bytes(0b01010100<<24 | (imm19U32 << 5) | uint32(fl))\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\tcase movN:\n+\t\tc.Emit4Bytes(encodeMoveWideImmediate(0b00, regNumberInEncoding[i.rd.realReg()], i.u1, i.u2, i.u3))\n+\tcase movZ:\n+\t\tc.Emit4Bytes(encodeMoveWideImmediate(0b10, regNumberInEncoding[i.rd.realReg()], i.u1, i.u2, i.u3))\n+\tcase movK:\n+\t\tc.Emit4Bytes(encodeMoveWideImmediate(0b11, regNumberInEncoding[i.rd.realReg()], i.u1, i.u2, i.u3))\n+\tcase mov32:\n+\t\tto, from := i.rd.realReg(), i.rn.realReg()\n+\t\tc.Emit4Bytes(encodeAsMov32(regNumberInEncoding[from], regNumberInEncoding[to]))\n+\tcase mov64:\n+\t\tto, from := i.rd.realReg(), i.rn.realReg()\n+\t\ttoIsSp := to == sp\n+\t\tfromIsSp := from == sp\n+\t\tc.Emit4Bytes(encodeMov64(regNumberInEncoding[to], regNumberInEncoding[from], toIsSp, fromIsSp))\n+\tcase loadP64, storeP64:\n+\t\trt, rt2 := regNumberInEncoding[i.rn.realReg()], regNumberInEncoding[i.rm.realReg()]\n+\t\tamode := i.amode\n+\t\trn := regNumberInEncoding[amode.rn.RealReg()]\n+\t\tvar pre bool\n+\t\tswitch amode.kind {\n+\t\tcase addressModeKindPostIndex:\n+\t\tcase addressModeKindPreIndex:\n+\t\t\tpre = true\n+\t\tdefault:\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\tc.Emit4Bytes(encodePreOrPostIndexLoadStorePair64(pre, kind == loadP64, rn, rt, rt2, amode.imm))\n+\tcase loadFpuConst32:\n+\t\trd := regNumberInEncoding[i.rd.realReg()]\n+\t\tif i.u1 == 0 {\n+\t\t\tc.Emit4Bytes(encodeVecRRR(vecOpEOR, rd, rd, rd, vecArrangement8B))\n+\t\t} else {\n+\t\t\tencodeLoadFpuConst32(c, rd, i.u1)\n+\t\t}\n+\tcase loadFpuConst64:\n+\t\trd := regNumberInEncoding[i.rd.realReg()]\n+\t\tif i.u1 == 0 {\n+\t\t\tc.Emit4Bytes(encodeVecRRR(vecOpEOR, rd, rd, rd, vecArrangement8B))\n+\t\t} else {\n+\t\t\tencodeLoadFpuConst64(c, regNumberInEncoding[i.rd.realReg()], i.u1)\n+\t\t}\n+\tcase loadFpuConst128:\n+\t\trd := regNumberInEncoding[i.rd.realReg()]\n+\t\tlo, hi := i.u1, i.u2\n+\t\tif lo == 0 && hi == 0 {\n+\t\t\tc.Emit4Bytes(encodeVecRRR(vecOpEOR, rd, rd, rd, vecArrangement16B))\n+\t\t} else {\n+\t\t\tencodeLoadFpuConst128(c, rd, lo, hi)\n+\t\t}\n+\tcase aluRRRR:\n+\t\tc.Emit4Bytes(encodeAluRRRR(\n+\t\t\taluOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tregNumberInEncoding[i.ra.realReg()],\n+\t\t\tuint32(i.u3),\n+\t\t))\n+\tcase aluRRImmShift:\n+\t\tc.Emit4Bytes(encodeAluRRImm(\n+\t\t\taluOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tuint32(i.rm.shiftImm()),\n+\t\t\tuint32(i.u3),\n+\t\t))\n+\tcase aluRRR:\n+\t\trn := i.rn.realReg()\n+\t\tc.Emit4Bytes(encodeAluRRR(\n+\t\t\taluOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[rn],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\ti.u3 == 1,\n+\t\t\trn == sp,\n+\t\t))\n+\tcase aluRRRExtend:\n+\t\trm, exo, to := i.rm.er()\n+\t\tc.Emit4Bytes(encodeAluRRRExtend(\n+\t\t\taluOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[rm.RealReg()],\n+\t\t\texo,\n+\t\t\tto,\n+\t\t))\n+\tcase aluRRRShift:\n+\t\tr, amt, sop := i.rm.sr()\n+\t\tc.Emit4Bytes(encodeAluRRRShift(\n+\t\t\taluOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[r.RealReg()],\n+\t\t\tuint32(amt),\n+\t\t\tsop,\n+\t\t\ti.u3 == 1,\n+\t\t))\n+\tcase aluRRBitmaskImm:\n+\t\tc.Emit4Bytes(encodeAluBitmaskImmediate(\n+\t\t\taluOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\ti.u2,\n+\t\t\ti.u3 == 1,\n+\t\t))\n+\tcase bitRR:\n+\t\tc.Emit4Bytes(encodeBitRR(\n+\t\t\tbitOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tuint32(i.u2)),\n+\t\t)\n+\tcase aluRRImm12:\n+\t\timm12, shift := i.rm.imm12()\n+\t\tc.Emit4Bytes(encodeAluRRImm12(\n+\t\t\taluOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\timm12, shift,\n+\t\t\ti.u3 == 1,\n+\t\t))\n+\tcase fpuRRR:\n+\t\tc.Emit4Bytes(encodeFpuRRR(\n+\t\t\tfpuBinOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\ti.u3 == 1,\n+\t\t))\n+\tcase fpuMov64, fpuMov128:\n+\t\t// https://developer.arm.com/documentation/ddi0596/2021-12/SIMD-FP-Instructions/MOV--vector---Move-vector--an-alias-of-ORR--vector--register--\n+\t\trd := regNumberInEncoding[i.rd.realReg()]\n+\t\trn := regNumberInEncoding[i.rn.realReg()]\n+\t\tvar q uint32\n+\t\tif kind == fpuMov128 {\n+\t\t\tq = 0b1\n+\t\t}\n+\t\tc.Emit4Bytes(q<<30 | 0b1110101<<21 | rn<<16 | 0b000111<<10 | rn<<5 | rd)\n+\tcase cSet:\n+\t\trd := regNumberInEncoding[i.rd.realReg()]\n+\t\tcf := condFlag(i.u1)\n+\t\tif i.u2 == 1 {\n+\t\t\t// https://developer.arm.com/documentation/ddi0602/2022-03/Base-Instructions/CSETM--Conditional-Set-Mask--an-alias-of-CSINV-\n+\t\t\t// Note that we set 64bit version here.\n+\t\t\tc.Emit4Bytes(0b1101101010011111<<16 | uint32(cf.invert())<<12 | 0b011111<<5 | rd)\n+\t\t} else {\n+\t\t\t// https://developer.arm.com/documentation/ddi0602/2022-06/Base-Instructions/CSET--Conditional-Set--an-alias-of-CSINC-\n+\t\t\t// Note that we set 64bit version here.\n+\t\t\tc.Emit4Bytes(0b1001101010011111<<16 | uint32(cf.invert())<<12 | 0b111111<<5 | rd)\n+\t\t}\n+\tcase extend:\n+\t\tc.Emit4Bytes(encodeExtend(i.u3 == 1, byte(i.u1), byte(i.u2), regNumberInEncoding[i.rd.realReg()], regNumberInEncoding[i.rn.realReg()]))\n+\tcase fpuCmp:\n+\t\t// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/FCMP--Floating-point-quiet-Compare--scalar--?lang=en\n+\t\trn, rm := regNumberInEncoding[i.rn.realReg()], regNumberInEncoding[i.rm.realReg()]\n+\t\tvar ftype uint32\n+\t\tif i.u3 == 1 {\n+\t\t\tftype = 0b01 // double precision.\n+\t\t}\n+\t\tc.Emit4Bytes(0b1111<<25 | ftype<<22 | 1<<21 | rm<<16 | 0b1<<13 | rn<<5)\n+\tcase udf:\n+\t\t// https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/UDF--Permanently-Undefined-?lang=en\n+\t\tif wazevoapi.PrintMachineCodeHexPerFunctionDisassemblable {\n+\t\t\tc.Emit4Bytes(dummyInstruction)\n+\t\t} else {\n+\t\t\tc.Emit4Bytes(0)\n+\t\t}\n+\tcase adr:\n+\t\tc.Emit4Bytes(encodeAdr(regNumberInEncoding[i.rd.realReg()], uint32(i.u1)))\n+\tcase cSel:\n+\t\tc.Emit4Bytes(encodeConditionalSelect(\n+\t\t\tkind,\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tcondFlag(i.u1),\n+\t\t\ti.u3 == 1,\n+\t\t))\n+\tcase fpuCSel:\n+\t\tc.Emit4Bytes(encodeFpuCSel(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tcondFlag(i.u1),\n+\t\t\ti.u3 == 1,\n+\t\t))\n+\tcase movToVec:\n+\t\tc.Emit4Bytes(encodeMoveToVec(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(byte(i.u1)),\n+\t\t\tvecIndex(i.u2),\n+\t\t))\n+\tcase movFromVec, movFromVecSigned:\n+\t\tc.Emit4Bytes(encodeMoveFromVec(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(byte(i.u1)),\n+\t\t\tvecIndex(i.u2),\n+\t\t\ti.kind == movFromVecSigned,\n+\t\t))\n+\tcase vecDup:\n+\t\tc.Emit4Bytes(encodeVecDup(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(byte(i.u1))))\n+\tcase vecDupElement:\n+\t\tc.Emit4Bytes(encodeVecDupElement(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(byte(i.u1)),\n+\t\t\tvecIndex(i.u2)))\n+\tcase vecExtract:\n+\t\tc.Emit4Bytes(encodeVecExtract(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tvecArrangement(byte(i.u1)),\n+\t\t\tuint32(i.u2)))\n+\tcase vecPermute:\n+\t\tc.Emit4Bytes(encodeVecPermute(\n+\t\t\tvecOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tvecArrangement(byte(i.u2))))\n+\tcase vecMovElement:\n+\t\tc.Emit4Bytes(encodeVecMovElement(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(i.u1),\n+\t\t\tuint32(i.u2), uint32(i.u3),\n+\t\t))\n+\tcase vecMisc:\n+\t\tc.Emit4Bytes(encodeAdvancedSIMDTwoMisc(\n+\t\t\tvecOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(i.u2),\n+\t\t))\n+\tcase vecLanes:\n+\t\tc.Emit4Bytes(encodeVecLanes(\n+\t\t\tvecOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tvecArrangement(i.u2),\n+\t\t))\n+\tcase vecShiftImm:\n+\t\tc.Emit4Bytes(encodeVecShiftImm(\n+\t\t\tvecOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tuint32(i.rm.shiftImm()),\n+\t\t\tvecArrangement(i.u2),\n+\t\t))\n+\tcase vecTbl:\n+\t\tc.Emit4Bytes(encodeVecTbl(\n+\t\t\t1,\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tvecArrangement(i.u2)),\n+\t\t)\n+\tcase vecTbl2:\n+\t\tc.Emit4Bytes(encodeVecTbl(\n+\t\t\t2,\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tvecArrangement(i.u2)),\n+\t\t)\n+\tcase brTableSequence:\n+\t\ttargets := m.jmpTableTargets[i.u1]\n+\t\tencodeBrTableSequence(c, i.rn.reg(), targets)\n+\tcase fpuToInt, intToFpu:\n+\t\tc.Emit4Bytes(encodeCnvBetweenFloatInt(i))\n+\tcase fpuRR:\n+\t\tc.Emit4Bytes(encodeFloatDataOneSource(\n+\t\t\tfpuUniOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\ti.u3 == 1,\n+\t\t))\n+\tcase vecRRR:\n+\t\tif op := vecOp(i.u1); op == vecOpBsl || op == vecOpBit || op == vecOpUmlal {\n+\t\t\tpanic(fmt.Sprintf(\"vecOp %s must use vecRRRRewrite instead of vecRRR\", op.String()))\n+\t\t}\n+\t\tfallthrough\n+\tcase vecRRRRewrite:\n+\t\tc.Emit4Bytes(encodeVecRRR(\n+\t\t\tvecOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tvecArrangement(i.u2),\n+\t\t))\n+\tcase cCmpImm:\n+\t\t// Conditional compare (immediate) in https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en\n+\t\tsf := uint32(i.u3 & 0b1)\n+\t\tnzcv := uint32(i.u2 & 0b1111)\n+\t\tcond := uint32(condFlag(i.u1))\n+\t\timm := uint32(i.rm.data & 0b11111)\n+\t\trn := regNumberInEncoding[i.rn.realReg()]\n+\t\tc.Emit4Bytes(\n+\t\t\tsf<<31 | 0b111101001<<22 | imm<<16 | cond<<12 | 0b1<<11 | rn<<5 | nzcv,\n+\t\t)\n+\tcase movFromFPSR:\n+\t\trt := regNumberInEncoding[i.rd.realReg()]\n+\t\tc.Emit4Bytes(encodeSystemRegisterMove(rt, true))\n+\tcase movToFPSR:\n+\t\trt := regNumberInEncoding[i.rn.realReg()]\n+\t\tc.Emit4Bytes(encodeSystemRegisterMove(rt, false))\n+\tcase atomicRmw:\n+\t\tc.Emit4Bytes(encodeAtomicRmw(\n+\t\t\tatomicRmwOp(i.u1),\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tuint32(i.u2),\n+\t\t))\n+\tcase atomicCas:\n+\t\tc.Emit4Bytes(encodeAtomicCas(\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tuint32(i.u2),\n+\t\t))\n+\tcase atomicLoad:\n+\t\tc.Emit4Bytes(encodeAtomicLoadStore(\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rd.realReg()],\n+\t\t\tuint32(i.u2),\n+\t\t\t1,\n+\t\t))\n+\tcase atomicStore:\n+\t\tc.Emit4Bytes(encodeAtomicLoadStore(\n+\t\t\tregNumberInEncoding[i.rn.realReg()],\n+\t\t\tregNumberInEncoding[i.rm.realReg()],\n+\t\t\tuint32(i.u2),\n+\t\t\t0,\n+\t\t))\n+\tcase dmb:\n+\t\tc.Emit4Bytes(encodeDMB())\n+\tdefault:\n+\t\tpanic(i.String())\n+\t}\n+}\n+\n+func encodeMov64(rd, rn uint32, toIsSp, fromIsSp bool) uint32 {\n+\tif toIsSp || fromIsSp {\n+\t\t// This is an alias of ADD (immediate):\n+\t\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/MOV--to-from-SP---Move-between-register-and-stack-pointer--an-alias-of-ADD--immediate--\n+\t\treturn encodeAddSubtractImmediate(0b100, 0, 0, rn, rd)\n+\t} else {\n+\t\t// This is an alias of ORR (shifted register):\n+\t\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/MOV--register---Move--register---an-alias-of-ORR--shifted-register--\n+\t\treturn encodeLogicalShiftedRegister(0b101, 0, rn, 0, regNumberInEncoding[xzr], rd)\n+\t}\n+}\n+\n+// encodeSystemRegisterMove encodes as \"System register move\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Branches--Exception-Generating-and-System-instructions?lang=en\n+//\n+// Note that currently we only supports read/write of FPSR.\n+func encodeSystemRegisterMove(rt uint32, fromSystem bool) uint32 {\n+\tret := 0b11010101<<24 | 0b11011<<16 | 0b01000100<<8 | 0b001<<5 | rt\n+\tif fromSystem {\n+\t\tret |= 0b1 << 21\n+\t}\n+\treturn ret\n+}\n+\n+// encodeVecRRR encodes as either \"Advanced SIMD three *\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeVecRRR(op vecOp, rd, rn, rm uint32, arr vecArrangement) uint32 {\n+\tswitch op {\n+\tcase vecOpBit:\n+\t\t_, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00011, 0b10 /* always has size 0b10 */, 0b1, q)\n+\tcase vecOpBic:\n+\t\tif arr > vecArrangement16B {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\t_, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00011, 0b01 /* always has size 0b01 */, 0b0, q)\n+\tcase vecOpBsl:\n+\t\tif arr > vecArrangement16B {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\t_, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00011, 0b01 /* always has size 0b01 */, 0b1, q)\n+\tcase vecOpAnd:\n+\t\tif arr > vecArrangement16B {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\t_, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00011, 0b00 /* always has size 0b00 */, 0b0, q)\n+\tcase vecOpOrr:\n+\t\t_, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00011, 0b10 /* always has size 0b10 */, 0b0, q)\n+\tcase vecOpEOR:\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00011, size, 0b1, q)\n+\tcase vecOpCmeq:\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b10001, size, 0b1, q)\n+\tcase vecOpCmgt:\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00110, size, 0b0, q)\n+\tcase vecOpCmhi:\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00110, size, 0b1, q)\n+\tcase vecOpCmge:\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00111, size, 0b0, q)\n+\tcase vecOpCmhs:\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00111, size, 0b1, q)\n+\tcase vecOpFcmeq:\n+\t\tvar size, q uint32\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tsize, q = 0b00, 0b1\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tcase vecArrangement2D:\n+\t\t\tsize, q = 0b01, 0b1\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11100, size, 0b0, q)\n+\tcase vecOpFcmgt:\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11100, size, 0b1, q)\n+\tcase vecOpFcmge:\n+\t\tvar size, q uint32\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tsize, q = 0b00, 0b1\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tcase vecArrangement2D:\n+\t\t\tsize, q = 0b01, 0b1\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11100, size, 0b1, q)\n+\tcase vecOpAdd:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b10000, size, 0b0, q)\n+\tcase vecOpSqadd:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00001, size, 0b0, q)\n+\tcase vecOpUqadd:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00001, size, 0b1, q)\n+\tcase vecOpAddp:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b10111, size, 0b0, q)\n+\tcase vecOpSqsub:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00101, size, 0b0, q)\n+\tcase vecOpUqsub:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00101, size, 0b1, q)\n+\tcase vecOpSub:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b10000, size, 0b1, q)\n+\tcase vecOpFmin:\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11110, size, 0b0, q)\n+\tcase vecOpSmin:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b01101, size, 0b0, q)\n+\tcase vecOpUmin:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b01101, size, 0b1, q)\n+\tcase vecOpFmax:\n+\t\tvar size, q uint32\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tsize, q = 0b00, 0b1\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tcase vecArrangement2D:\n+\t\t\tsize, q = 0b01, 0b1\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11110, size, 0b0, q)\n+\tcase vecOpFadd:\n+\t\tvar size, q uint32\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tsize, q = 0b00, 0b1\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tcase vecArrangement2D:\n+\t\t\tsize, q = 0b01, 0b1\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11010, size, 0b0, q)\n+\tcase vecOpFsub:\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11010, size, 0b0, q)\n+\tcase vecOpFmul:\n+\t\tvar size, q uint32\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tsize, q = 0b00, 0b1\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tcase vecArrangement2D:\n+\t\t\tsize, q = 0b01, 0b1\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11011, size, 0b1, q)\n+\tcase vecOpSqrdmulh:\n+\t\tif arr < vecArrangement4H || arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b10110, size, 0b1, q)\n+\tcase vecOpFdiv:\n+\t\tvar size, q uint32\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tsize, q = 0b00, 0b1\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tcase vecArrangement2D:\n+\t\t\tsize, q = 0b01, 0b1\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b11111, size, 0b1, q)\n+\tcase vecOpSmax:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b01100, size, 0b0, q)\n+\tcase vecOpUmax:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b01100, size, 0b1, q)\n+\tcase vecOpUmaxp:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b10100, size, 0b1, q)\n+\tcase vecOpUrhadd:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b00010, size, 0b1, q)\n+\tcase vecOpMul:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b10011, size, 0b0, q)\n+\tcase vecOpUmlal:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeDifferent(rd, rn, rm, 0b1000, size, 0b1, q)\n+\tcase vecOpSshl:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b01000, size, 0b0, q)\n+\tcase vecOpUshl:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeSame(rd, rn, rm, 0b01000, size, 0b1, q)\n+\n+\tcase vecOpSmull:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, _ := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeDifferent(rd, rn, rm, 0b1100, size, 0b0, 0b0)\n+\n+\tcase vecOpSmull2:\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, _ := arrToSizeQEncoded(arr)\n+\t\treturn encodeAdvancedSIMDThreeDifferent(rd, rn, rm, 0b1100, size, 0b0, 0b1)\n+\n+\tdefault:\n+\t\tpanic(\"TODO: \" + op.String())\n+\t}\n+}\n+\n+func arrToSizeQEncoded(arr vecArrangement) (size, q uint32) {\n+\tswitch arr {\n+\tcase vecArrangement16B:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase vecArrangement8B:\n+\t\tsize = 0b00\n+\tcase vecArrangement8H:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase vecArrangement4H:\n+\t\tsize = 0b01\n+\tcase vecArrangement4S:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase vecArrangement2S:\n+\t\tsize = 0b10\n+\tcase vecArrangement2D:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase vecArrangement1D:\n+\t\tsize = 0b11\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\treturn\n+}\n+\n+// encodeAdvancedSIMDThreeSame encodes as \"Advanced SIMD three same\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeAdvancedSIMDThreeSame(rd, rn, rm, opcode, size, U, Q uint32) uint32 {\n+\treturn Q<<30 | U<<29 | 0b111<<25 | size<<22 | 0b1<<21 | rm<<16 | opcode<<11 | 0b1<<10 | rn<<5 | rd\n+}\n+\n+// encodeAdvancedSIMDThreeDifferent encodes as \"Advanced SIMD three different\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeAdvancedSIMDThreeDifferent(rd, rn, rm, opcode, size, U, Q uint32) uint32 {\n+\treturn Q<<30 | U<<29 | 0b111<<25 | size<<22 | 0b1<<21 | rm<<16 | opcode<<12 | rn<<5 | rd\n+}\n+\n+// encodeFloatDataOneSource encodes as \"Floating-point data-processing (1 source)\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en#simd-dp\n+func encodeFloatDataOneSource(op fpuUniOp, rd, rn uint32, dst64bit bool) uint32 {\n+\tvar opcode, ptype uint32\n+\tswitch op {\n+\tcase fpuUniOpCvt32To64:\n+\t\topcode = 0b000101\n+\tcase fpuUniOpCvt64To32:\n+\t\topcode = 0b000100\n+\t\tptype = 0b01\n+\tcase fpuUniOpNeg:\n+\t\topcode = 0b000010\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t}\n+\tcase fpuUniOpSqrt:\n+\t\topcode = 0b000011\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t}\n+\tcase fpuUniOpRoundPlus:\n+\t\topcode = 0b001001\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t}\n+\tcase fpuUniOpRoundMinus:\n+\t\topcode = 0b001010\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t}\n+\tcase fpuUniOpRoundZero:\n+\t\topcode = 0b001011\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t}\n+\tcase fpuUniOpRoundNearest:\n+\t\topcode = 0b001000\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t}\n+\tcase fpuUniOpAbs:\n+\t\topcode = 0b000001\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\treturn 0b1111<<25 | ptype<<22 | 0b1<<21 | opcode<<15 | 0b1<<14 | rn<<5 | rd\n+}\n+\n+// encodeCnvBetweenFloatInt encodes as \"Conversion between floating-point and integer\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeCnvBetweenFloatInt(i *instruction) uint32 {\n+\trd := regNumberInEncoding[i.rd.realReg()]\n+\trn := regNumberInEncoding[i.rn.realReg()]\n+\n+\tvar opcode uint32\n+\tvar rmode uint32\n+\tvar ptype uint32\n+\tvar sf uint32\n+\tswitch i.kind {\n+\tcase intToFpu: // Either UCVTF or SCVTF.\n+\t\trmode = 0b00\n+\n+\t\tsigned := i.u1 == 1\n+\t\tsrc64bit := i.u2 == 1\n+\t\tdst64bit := i.u3 == 1\n+\t\tif signed {\n+\t\t\topcode = 0b010\n+\t\t} else {\n+\t\t\topcode = 0b011\n+\t\t}\n+\t\tif src64bit {\n+\t\t\tsf = 0b1\n+\t\t}\n+\t\tif dst64bit {\n+\t\t\tptype = 0b01\n+\t\t} else {\n+\t\t\tptype = 0b00\n+\t\t}\n+\tcase fpuToInt: // Either FCVTZU or FCVTZS.\n+\t\trmode = 0b11\n+\n+\t\tsigned := i.u1 == 1\n+\t\tsrc64bit := i.u2 == 1\n+\t\tdst64bit := i.u3 == 1\n+\n+\t\tif signed {\n+\t\t\topcode = 0b000\n+\t\t} else {\n+\t\t\topcode = 0b001\n+\t\t}\n+\t\tif dst64bit {\n+\t\t\tsf = 0b1\n+\t\t}\n+\t\tif src64bit {\n+\t\t\tptype = 0b01\n+\t\t} else {\n+\t\t\tptype = 0b00\n+\t\t}\n+\t}\n+\treturn sf<<31 | 0b1111<<25 | ptype<<22 | 0b1<<21 | rmode<<19 | opcode<<16 | rn<<5 | rd\n+}\n+\n+// encodeAdr encodes a PC-relative ADR instruction.\n+// https://developer.arm.com/documentation/ddi0602/2022-06/Base-Instructions/ADR--Form-PC-relative-address-\n+func encodeAdr(rd uint32, offset uint32) uint32 {\n+\tif offset >= 1<<20 {\n+\t\tpanic(\"BUG: too large adr instruction\")\n+\t}\n+\treturn offset&0b11<<29 | 0b1<<28 | offset&0b1111111111_1111111100<<3 | rd\n+}\n+\n+// encodeFpuCSel encodes as \"Floating-point conditional select\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeFpuCSel(rd, rn, rm uint32, c condFlag, _64bit bool) uint32 {\n+\tvar ftype uint32\n+\tif _64bit {\n+\t\tftype = 0b01 // double precision.\n+\t}\n+\treturn 0b1111<<25 | ftype<<22 | 0b1<<21 | rm<<16 | uint32(c)<<12 | 0b11<<10 | rn<<5 | rd\n+}\n+\n+// encodeMoveToVec encodes as \"Move general-purpose register to a vector element\" (represented as `ins`) in\n+// https://developer.arm.com/documentation/dui0801/g/A64-SIMD-Vector-Instructions/MOV--vector--from-general-\n+// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/MOV--from-general---Move-general-purpose-register-to-a-vector-element--an-alias-of-INS--general--?lang=en\n+func encodeMoveToVec(rd, rn uint32, arr vecArrangement, index vecIndex) uint32 {\n+\tvar imm5 uint32\n+\tswitch arr {\n+\tcase vecArrangementB:\n+\t\timm5 |= 0b1\n+\t\timm5 |= uint32(index) << 1\n+\t\tif index > 0b1111 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 15\", index))\n+\t\t}\n+\tcase vecArrangementH:\n+\t\timm5 |= 0b10\n+\t\timm5 |= uint32(index) << 2\n+\t\tif index > 0b111 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 7\", index))\n+\t\t}\n+\tcase vecArrangementS:\n+\t\timm5 |= 0b100\n+\t\timm5 |= uint32(index) << 3\n+\t\tif index > 0b11 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 3\", index))\n+\t\t}\n+\tcase vecArrangementD:\n+\t\timm5 |= 0b1000\n+\t\timm5 |= uint32(index) << 4\n+\t\tif index > 0b1 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 1\", index))\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"Unsupported arrangement \" + arr.String())\n+\t}\n+\n+\treturn 0b01001110000<<21 | imm5<<16 | 0b000111<<10 | rn<<5 | rd\n+}\n+\n+// encodeMoveToVec encodes as \"Move vector element to another vector element, mov (element)\" (represented as `ins`) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/MOV--element---Move-vector-element-to-another-vector-element--an-alias-of-INS--element--?lang=en\n+// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/INS--element---Insert-vector-element-from-another-vector-element-?lang=en\n+func encodeVecMovElement(rd, rn uint32, arr vecArrangement, srcIndex, dstIndex uint32) uint32 {\n+\tvar imm4, imm5 uint32\n+\tswitch arr {\n+\tcase vecArrangementB:\n+\t\timm5 |= 0b1\n+\t\timm5 |= srcIndex << 1\n+\t\timm4 = dstIndex\n+\t\tif srcIndex > 0b1111 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 15\", srcIndex))\n+\t\t}\n+\tcase vecArrangementH:\n+\t\timm5 |= 0b10\n+\t\timm5 |= srcIndex << 2\n+\t\timm4 = dstIndex << 1\n+\t\tif srcIndex > 0b111 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 7\", srcIndex))\n+\t\t}\n+\tcase vecArrangementS:\n+\t\timm5 |= 0b100\n+\t\timm5 |= srcIndex << 3\n+\t\timm4 = dstIndex << 2\n+\t\tif srcIndex > 0b11 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 3\", srcIndex))\n+\t\t}\n+\tcase vecArrangementD:\n+\t\timm5 |= 0b1000\n+\t\timm5 |= srcIndex << 4\n+\t\timm4 = dstIndex << 3\n+\t\tif srcIndex > 0b1 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 1\", srcIndex))\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"Unsupported arrangement \" + arr.String())\n+\t}\n+\n+\treturn 0b01101110000<<21 | imm5<<16 | imm4<<11 | 0b1<<10 | rn<<5 | rd\n+}\n+\n+// encodeUnconditionalBranchReg encodes as \"Unconditional branch (register)\" in:\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Branches--Exception-Generating-and-System-instructions?lang=en\n+func encodeUnconditionalBranchReg(rn uint32, link bool) uint32 {\n+\tvar opc uint32\n+\tif link {\n+\t\topc = 0b0001\n+\t}\n+\treturn 0b1101011<<25 | opc<<21 | 0b11111<<16 | rn<<5\n+}\n+\n+// encodeMoveFromVec encodes as \"Move vector element to a general-purpose register\"\n+// (represented as `umov` when dest is 32-bit, `umov` otherwise) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/UMOV--Unsigned-Move-vector-element-to-general-purpose-register-?lang=en\n+// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/MOV--to-general---Move-vector-element-to-general-purpose-register--an-alias-of-UMOV-?lang=en\n+func encodeMoveFromVec(rd, rn uint32, arr vecArrangement, index vecIndex, signed bool) uint32 {\n+\tvar op, imm4, q, imm5 uint32\n+\tswitch {\n+\tcase arr == vecArrangementB:\n+\t\timm5 |= 0b1\n+\t\timm5 |= uint32(index) << 1\n+\t\tif index > 0b1111 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 15\", index))\n+\t\t}\n+\tcase arr == vecArrangementH:\n+\t\timm5 |= 0b10\n+\t\timm5 |= uint32(index) << 2\n+\t\tif index > 0b111 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 7\", index))\n+\t\t}\n+\tcase arr == vecArrangementS && signed:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase arr == vecArrangementS:\n+\t\timm5 |= 0b100\n+\t\timm5 |= uint32(index) << 3\n+\t\tif index > 0b11 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 3\", index))\n+\t\t}\n+\tcase arr == vecArrangementD && !signed:\n+\t\timm5 |= 0b1000\n+\t\timm5 |= uint32(index) << 4\n+\t\tq = 0b1\n+\t\tif index > 0b1 {\n+\t\t\tpanic(fmt.Sprintf(\"vector index is larger than the allowed bound: %d > 1\", index))\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"Unsupported arrangement \" + arr.String())\n+\t}\n+\tif signed {\n+\t\top, imm4 = 0, 0b0101\n+\t} else {\n+\t\top, imm4 = 0, 0b0111\n+\t}\n+\treturn op<<29 | 0b01110000<<21 | q<<30 | imm5<<16 | imm4<<11 | 1<<10 | rn<<5 | rd\n+}\n+\n+// encodeVecDup encodes as \"Duplicate general-purpose register to vector\" DUP (general)\n+// (represented as `dup`)\n+// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/DUP--general---Duplicate-general-purpose-register-to-vector-?lang=en\n+func encodeVecDup(rd, rn uint32, arr vecArrangement) uint32 {\n+\tvar q, imm5 uint32\n+\tswitch arr {\n+\tcase vecArrangement8B:\n+\t\tq, imm5 = 0b0, 0b1\n+\tcase vecArrangement16B:\n+\t\tq, imm5 = 0b1, 0b1\n+\tcase vecArrangement4H:\n+\t\tq, imm5 = 0b0, 0b10\n+\tcase vecArrangement8H:\n+\t\tq, imm5 = 0b1, 0b10\n+\tcase vecArrangement2S:\n+\t\tq, imm5 = 0b0, 0b100\n+\tcase vecArrangement4S:\n+\t\tq, imm5 = 0b1, 0b100\n+\tcase vecArrangement2D:\n+\t\tq, imm5 = 0b1, 0b1000\n+\tdefault:\n+\t\tpanic(\"Unsupported arrangement \" + arr.String())\n+\t}\n+\treturn q<<30 | 0b001110000<<21 | imm5<<16 | 0b000011<<10 | rn<<5 | rd\n+}\n+\n+// encodeVecDup encodes as \"Duplicate vector element to vector or scalar\" DUP (element).\n+// (represented as `dup`)\n+// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/DUP--element---Duplicate-vector-element-to-vector-or-scalar-\n+func encodeVecDupElement(rd, rn uint32, arr vecArrangement, srcIndex vecIndex) uint32 {\n+\tvar q, imm5 uint32\n+\tq = 0b1\n+\tswitch arr {\n+\tcase vecArrangementB:\n+\t\timm5 |= 0b1\n+\t\timm5 |= uint32(srcIndex) << 1\n+\tcase vecArrangementH:\n+\t\timm5 |= 0b10\n+\t\timm5 |= uint32(srcIndex) << 2\n+\tcase vecArrangementS:\n+\t\timm5 |= 0b100\n+\t\timm5 |= uint32(srcIndex) << 3\n+\tcase vecArrangementD:\n+\t\timm5 |= 0b1000\n+\t\timm5 |= uint32(srcIndex) << 4\n+\tdefault:\n+\t\tpanic(\"unsupported arrangement\" + arr.String())\n+\t}\n+\n+\treturn q<<30 | 0b001110000<<21 | imm5<<16 | 0b1<<10 | rn<<5 | rd\n+}\n+\n+// encodeVecExtract encodes as \"Advanced SIMD extract.\"\n+// Currently only `ext` is defined.\n+// https://developer.arm.com/documentation/ddi0602/2023-06/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en#simd-dp\n+// https://developer.arm.com/documentation/ddi0602/2023-06/SIMD-FP-Instructions/EXT--Extract-vector-from-pair-of-vectors-?lang=en\n+func encodeVecExtract(rd, rn, rm uint32, arr vecArrangement, index uint32) uint32 {\n+\tvar q, imm4 uint32\n+\tswitch arr {\n+\tcase vecArrangement8B:\n+\t\tq, imm4 = 0, 0b0111&uint32(index)\n+\tcase vecArrangement16B:\n+\t\tq, imm4 = 1, 0b1111&uint32(index)\n+\tdefault:\n+\t\tpanic(\"Unsupported arrangement \" + arr.String())\n+\t}\n+\treturn q<<30 | 0b101110000<<21 | rm<<16 | imm4<<11 | rn<<5 | rd\n+}\n+\n+// encodeVecPermute encodes as \"Advanced SIMD permute.\"\n+// https://developer.arm.com/documentation/ddi0602/2023-06/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en#simd-dp\n+func encodeVecPermute(op vecOp, rd, rn, rm uint32, arr vecArrangement) uint32 {\n+\tvar q, size, opcode uint32\n+\tswitch op {\n+\tcase vecOpZip1:\n+\t\topcode = 0b011\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tdefault:\n+\t\tpanic(\"TODO: \" + op.String())\n+\t}\n+\treturn q<<30 | 0b001110<<24 | size<<22 | rm<<16 | opcode<<12 | 0b10<<10 | rn<<5 | rd\n+}\n+\n+// encodeConditionalSelect encodes as \"Conditional select\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en#condsel\n+func encodeConditionalSelect(kind instructionKind, rd, rn, rm uint32, c condFlag, _64bit bool) uint32 {\n+\tif kind != cSel {\n+\t\tpanic(\"TODO: support other conditional select\")\n+\t}\n+\n+\tret := 0b110101<<23 | rm<<16 | uint32(c)<<12 | rn<<5 | rd\n+\tif _64bit {\n+\t\tret |= 0b1 << 31\n+\t}\n+\treturn ret\n+}\n+\n+const dummyInstruction uint32 = 0x14000000 // \"b 0\"\n+\n+// encodeLoadFpuConst32 encodes the following three instructions:\n+//\n+//\tldr s8, #8  ;; literal load of data.f32\n+//\tb 8           ;; skip the data\n+//\tdata.f32 xxxxxxx\n+func encodeLoadFpuConst32(c backend.Compiler, rd uint32, rawF32 uint64) {\n+\tc.Emit4Bytes(\n+\t\t// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/LDR--literal--SIMD-FP---Load-SIMD-FP-Register--PC-relative-literal--?lang=en\n+\t\t0b111<<26 | (0x8/4)<<5 | rd,\n+\t)\n+\tc.Emit4Bytes(encodeUnconditionalBranch(false, 8)) // b 8\n+\tif wazevoapi.PrintMachineCodeHexPerFunctionDisassemblable {\n+\t\t// Inlined data.f32 cannot be disassembled, so we add a dummy instruction here.\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t} else {\n+\t\tc.Emit4Bytes(uint32(rawF32)) // data.f32 xxxxxxx\n+\t}\n+}\n+\n+// encodeLoadFpuConst64 encodes the following three instructions:\n+//\n+//\tldr d8, #8  ;; literal load of data.f64\n+//\tb 12           ;; skip the data\n+//\tdata.f64 xxxxxxx\n+func encodeLoadFpuConst64(c backend.Compiler, rd uint32, rawF64 uint64) {\n+\tc.Emit4Bytes(\n+\t\t// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/LDR--literal--SIMD-FP---Load-SIMD-FP-Register--PC-relative-literal--?lang=en\n+\t\t0b1<<30 | 0b111<<26 | (0x8/4)<<5 | rd,\n+\t)\n+\tc.Emit4Bytes(encodeUnconditionalBranch(false, 12)) // b 12\n+\tif wazevoapi.PrintMachineCodeHexPerFunctionDisassemblable {\n+\t\t// Inlined data.f64 cannot be disassembled, so we add dummy instructions here.\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t} else {\n+\t\t// data.f64 xxxxxxx\n+\t\tc.Emit4Bytes(uint32(rawF64))\n+\t\tc.Emit4Bytes(uint32(rawF64 >> 32))\n+\t}\n+}\n+\n+// encodeLoadFpuConst128 encodes the following three instructions:\n+//\n+//\tldr v8, #8  ;; literal load of data.f64\n+//\tb 20           ;; skip the data\n+//\tdata.v128 xxxxxxx\n+func encodeLoadFpuConst128(c backend.Compiler, rd uint32, lo, hi uint64) {\n+\tc.Emit4Bytes(\n+\t\t// https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/LDR--literal--SIMD-FP---Load-SIMD-FP-Register--PC-relative-literal--?lang=en\n+\t\t0b1<<31 | 0b111<<26 | (0x8/4)<<5 | rd,\n+\t)\n+\tc.Emit4Bytes(encodeUnconditionalBranch(false, 20)) // b 20\n+\tif wazevoapi.PrintMachineCodeHexPerFunctionDisassemblable {\n+\t\t// Inlined data.v128 cannot be disassembled, so we add dummy instructions here.\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t} else {\n+\t\t// data.v128 xxxxxxx\n+\t\tc.Emit4Bytes(uint32(lo))\n+\t\tc.Emit4Bytes(uint32(lo >> 32))\n+\t\tc.Emit4Bytes(uint32(hi))\n+\t\tc.Emit4Bytes(uint32(hi >> 32))\n+\t}\n+}\n+\n+// encodeAluRRRR encodes as Data-processing (3 source) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en\n+func encodeAluRRRR(op aluOp, rd, rn, rm, ra, _64bit uint32) uint32 {\n+\tvar oO, op31 uint32\n+\tswitch op {\n+\tcase aluOpMAdd:\n+\t\top31, oO = 0b000, 0b0\n+\tcase aluOpMSub:\n+\t\top31, oO = 0b000, 0b1\n+\tdefault:\n+\t\tpanic(\"TODO/BUG\")\n+\t}\n+\treturn _64bit<<31 | 0b11011<<24 | op31<<21 | rm<<16 | oO<<15 | ra<<10 | rn<<5 | rd\n+}\n+\n+// encodeBitRR encodes as Data-processing (1 source) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en\n+func encodeBitRR(op bitOp, rd, rn, _64bit uint32) uint32 {\n+\tvar opcode2, opcode uint32\n+\tswitch op {\n+\tcase bitOpRbit:\n+\t\topcode2, opcode = 0b00000, 0b000000\n+\tcase bitOpClz:\n+\t\topcode2, opcode = 0b00000, 0b000100\n+\tdefault:\n+\t\tpanic(\"TODO/BUG\")\n+\t}\n+\treturn _64bit<<31 | 0b1_0_11010110<<21 | opcode2<<15 | opcode<<10 | rn<<5 | rd\n+}\n+\n+func encodeAsMov32(rn, rd uint32) uint32 {\n+\t// This is an alias of ORR (shifted register):\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/MOV--register---Move--register---an-alias-of-ORR--shifted-register--\n+\treturn encodeLogicalShiftedRegister(0b001, 0, rn, 0, regNumberInEncoding[xzr], rd)\n+}\n+\n+// encodeExtend encodes extension instructions.\n+func encodeExtend(signed bool, from, to byte, rd, rn uint32) uint32 {\n+\t// UTXB: https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/UXTB--Unsigned-Extend-Byte--an-alias-of-UBFM-?lang=en\n+\t// UTXH: https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/UXTH--Unsigned-Extend-Halfword--an-alias-of-UBFM-?lang=en\n+\t// STXB: https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/SXTB--Signed-Extend-Byte--an-alias-of-SBFM-\n+\t// STXH: https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/SXTH--Sign-Extend-Halfword--an-alias-of-SBFM-\n+\t// STXW: https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/SXTW--Sign-Extend-Word--an-alias-of-SBFM-\n+\tvar _31to10 uint32\n+\tswitch {\n+\tcase !signed && from == 8 && to == 32:\n+\t\t// 32-bit UXTB\n+\t\t_31to10 = 0b0101001100000000000111\n+\tcase !signed && from == 16 && to == 32:\n+\t\t// 32-bit UXTH\n+\t\t_31to10 = 0b0101001100000000001111\n+\tcase !signed && from == 8 && to == 64:\n+\t\t// 64-bit UXTB\n+\t\t_31to10 = 0b0101001100000000000111\n+\tcase !signed && from == 16 && to == 64:\n+\t\t// 64-bit UXTH\n+\t\t_31to10 = 0b0101001100000000001111\n+\tcase !signed && from == 32 && to == 64:\n+\t\treturn encodeAsMov32(rn, rd)\n+\tcase signed && from == 8 && to == 32:\n+\t\t// 32-bit SXTB\n+\t\t_31to10 = 0b0001001100000000000111\n+\tcase signed && from == 16 && to == 32:\n+\t\t// 32-bit SXTH\n+\t\t_31to10 = 0b0001001100000000001111\n+\tcase signed && from == 8 && to == 64:\n+\t\t// 64-bit SXTB\n+\t\t_31to10 = 0b1001001101000000000111\n+\tcase signed && from == 16 && to == 64:\n+\t\t// 64-bit SXTH\n+\t\t_31to10 = 0b1001001101000000001111\n+\tcase signed && from == 32 && to == 64:\n+\t\t// SXTW\n+\t\t_31to10 = 0b1001001101000000011111\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\treturn _31to10<<10 | rn<<5 | rd\n+}\n+\n+func encodeLoadOrStore(kind instructionKind, rt uint32, amode addressMode) uint32 {\n+\tvar _22to31 uint32\n+\tvar bits int64\n+\tswitch kind {\n+\tcase uLoad8:\n+\t\t_22to31 = 0b0011100001\n+\t\tbits = 8\n+\tcase sLoad8:\n+\t\t_22to31 = 0b0011100010\n+\t\tbits = 8\n+\tcase uLoad16:\n+\t\t_22to31 = 0b0111100001\n+\t\tbits = 16\n+\tcase sLoad16:\n+\t\t_22to31 = 0b0111100010\n+\t\tbits = 16\n+\tcase uLoad32:\n+\t\t_22to31 = 0b1011100001\n+\t\tbits = 32\n+\tcase sLoad32:\n+\t\t_22to31 = 0b1011100010\n+\t\tbits = 32\n+\tcase uLoad64:\n+\t\t_22to31 = 0b1111100001\n+\t\tbits = 64\n+\tcase fpuLoad32:\n+\t\t_22to31 = 0b1011110001\n+\t\tbits = 32\n+\tcase fpuLoad64:\n+\t\t_22to31 = 0b1111110001\n+\t\tbits = 64\n+\tcase fpuLoad128:\n+\t\t_22to31 = 0b0011110011\n+\t\tbits = 128\n+\tcase store8:\n+\t\t_22to31 = 0b0011100000\n+\t\tbits = 8\n+\tcase store16:\n+\t\t_22to31 = 0b0111100000\n+\t\tbits = 16\n+\tcase store32:\n+\t\t_22to31 = 0b1011100000\n+\t\tbits = 32\n+\tcase store64:\n+\t\t_22to31 = 0b1111100000\n+\t\tbits = 64\n+\tcase fpuStore32:\n+\t\t_22to31 = 0b1011110000\n+\t\tbits = 32\n+\tcase fpuStore64:\n+\t\t_22to31 = 0b1111110000\n+\t\tbits = 64\n+\tcase fpuStore128:\n+\t\t_22to31 = 0b0011110010\n+\t\tbits = 128\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\n+\tswitch amode.kind {\n+\tcase addressModeKindRegScaledExtended:\n+\t\treturn encodeLoadOrStoreExtended(_22to31,\n+\t\t\tregNumberInEncoding[amode.rn.RealReg()],\n+\t\t\tregNumberInEncoding[amode.rm.RealReg()],\n+\t\t\trt, true, amode.extOp)\n+\tcase addressModeKindRegScaled:\n+\t\treturn encodeLoadOrStoreExtended(_22to31,\n+\t\t\tregNumberInEncoding[amode.rn.RealReg()], regNumberInEncoding[amode.rm.RealReg()],\n+\t\t\trt, true, extendOpNone)\n+\tcase addressModeKindRegExtended:\n+\t\treturn encodeLoadOrStoreExtended(_22to31,\n+\t\t\tregNumberInEncoding[amode.rn.RealReg()], regNumberInEncoding[amode.rm.RealReg()],\n+\t\t\trt, false, amode.extOp)\n+\tcase addressModeKindRegReg:\n+\t\treturn encodeLoadOrStoreExtended(_22to31,\n+\t\t\tregNumberInEncoding[amode.rn.RealReg()], regNumberInEncoding[amode.rm.RealReg()],\n+\t\t\trt, false, extendOpNone)\n+\tcase addressModeKindRegSignedImm9:\n+\t\t// e.g. https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDUR--Load-Register--unscaled--\n+\t\treturn encodeLoadOrStoreSIMM9(_22to31, 0b00 /* unscaled */, regNumberInEncoding[amode.rn.RealReg()], rt, amode.imm)\n+\tcase addressModeKindPostIndex:\n+\t\treturn encodeLoadOrStoreSIMM9(_22to31, 0b01 /* post index */, regNumberInEncoding[amode.rn.RealReg()], rt, amode.imm)\n+\tcase addressModeKindPreIndex:\n+\t\treturn encodeLoadOrStoreSIMM9(_22to31, 0b11 /* pre index */, regNumberInEncoding[amode.rn.RealReg()], rt, amode.imm)\n+\tcase addressModeKindRegUnsignedImm12:\n+\t\t// \"unsigned immediate\" in https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Loads-and-Stores?lang=en\n+\t\trn := regNumberInEncoding[amode.rn.RealReg()]\n+\t\timm := amode.imm\n+\t\tdiv := bits / 8\n+\t\tif imm != 0 && !offsetFitsInAddressModeKindRegUnsignedImm12(byte(bits), imm) {\n+\t\t\tpanic(\"BUG\")\n+\t\t}\n+\t\timm /= div\n+\t\treturn _22to31<<22 | 0b1<<24 | uint32(imm&0b111111111111)<<10 | rn<<5 | rt\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+// encodeVecLoad1R encodes as Load one single-element structure and Replicate to all lanes (of one register) in\n+// https://developer.arm.com/documentation/ddi0596/2021-12/SIMD-FP-Instructions/LD1R--Load-one-single-element-structure-and-Replicate-to-all-lanes--of-one-register--?lang=en#sa_imm\n+func encodeVecLoad1R(rt, rn uint32, arr vecArrangement) uint32 {\n+\tsize, q := arrToSizeQEncoded(arr)\n+\treturn q<<30 | 0b001101010000001100<<12 | size<<10 | rn<<5 | rt\n+}\n+\n+// encodeAluBitmaskImmediate encodes as Logical (immediate) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Immediate?lang=en\n+func encodeAluBitmaskImmediate(op aluOp, rd, rn uint32, imm uint64, _64bit bool) uint32 {\n+\tvar _31to23 uint32\n+\tswitch op {\n+\tcase aluOpAnd:\n+\t\t_31to23 = 0b00_100100\n+\tcase aluOpOrr:\n+\t\t_31to23 = 0b01_100100\n+\tcase aluOpEor:\n+\t\t_31to23 = 0b10_100100\n+\tcase aluOpAnds:\n+\t\t_31to23 = 0b11_100100\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\tif _64bit {\n+\t\t_31to23 |= 0b1 << 8\n+\t}\n+\timmr, imms, N := bitmaskImmediate(imm, _64bit)\n+\treturn _31to23<<23 | uint32(N)<<22 | uint32(immr)<<16 | uint32(imms)<<10 | rn<<5 | rd\n+}\n+\n+func bitmaskImmediate(c uint64, is64bit bool) (immr, imms, N byte) {\n+\tvar size uint32\n+\tswitch {\n+\tcase c != c>>32|c<<32:\n+\t\tsize = 64\n+\tcase c != c>>16|c<<48:\n+\t\tsize = 32\n+\t\tc = uint64(int32(c))\n+\tcase c != c>>8|c<<56:\n+\t\tsize = 16\n+\t\tc = uint64(int16(c))\n+\tcase c != c>>4|c<<60:\n+\t\tsize = 8\n+\t\tc = uint64(int8(c))\n+\tcase c != c>>2|c<<62:\n+\t\tsize = 4\n+\t\tc = uint64(int64(c<<60) >> 60)\n+\tdefault:\n+\t\tsize = 2\n+\t\tc = uint64(int64(c<<62) >> 62)\n+\t}\n+\n+\tneg := false\n+\tif int64(c) < 0 {\n+\t\tc = ^c\n+\t\tneg = true\n+\t}\n+\n+\tonesSize, nonZeroPos := getOnesSequenceSize(c)\n+\tif neg {\n+\t\tnonZeroPos = onesSize + nonZeroPos\n+\t\tonesSize = size - onesSize\n+\t}\n+\n+\tvar mode byte = 32\n+\tif is64bit && size == 64 {\n+\t\tN, mode = 0b1, 64\n+\t}\n+\n+\timmr = byte((size - nonZeroPos) & (size - 1) & uint32(mode-1))\n+\timms = byte((onesSize - 1) | 63&^(size<<1-1))\n+\treturn\n+}\n+\n+func getOnesSequenceSize(x uint64) (size, nonZeroPos uint32) {\n+\t// Take 0b00111000 for example:\n+\ty := getLowestBit(x)               // = 0b0000100\n+\tnonZeroPos = setBitPos(y)          // = 2\n+\tsize = setBitPos(x+y) - nonZeroPos // = setBitPos(0b0100000) - 2 = 5 - 2 = 3\n+\treturn\n+}\n+\n+func setBitPos(x uint64) (ret uint32) {\n+\tfor ; ; ret++ {\n+\t\tif x == 0b1 {\n+\t\t\tbreak\n+\t\t}\n+\t\tx = x >> 1\n+\t}\n+\treturn\n+}\n+\n+// encodeLoadOrStoreExtended encodes store/load instruction as \"extended register offset\" in Load/store register (register offset):\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Loads-and-Stores?lang=en\n+func encodeLoadOrStoreExtended(_22to32 uint32, rn, rm, rt uint32, scaled bool, extOp extendOp) uint32 {\n+\tvar option uint32\n+\tswitch extOp {\n+\tcase extendOpUXTW:\n+\t\toption = 0b010\n+\tcase extendOpSXTW:\n+\t\toption = 0b110\n+\tcase extendOpNone:\n+\t\toption = 0b111\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\tvar s uint32\n+\tif scaled {\n+\t\ts = 0b1\n+\t}\n+\treturn _22to32<<22 | 0b1<<21 | rm<<16 | option<<13 | s<<12 | 0b10<<10 | rn<<5 | rt\n+}\n+\n+// encodeLoadOrStoreSIMM9 encodes store/load instruction as one of post-index, pre-index or unscaled immediate as in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Loads-and-Stores?lang=en\n+func encodeLoadOrStoreSIMM9(_22to32, _1011 uint32, rn, rt uint32, imm9 int64) uint32 {\n+\treturn _22to32<<22 | (uint32(imm9)&0b111111111)<<12 | _1011<<10 | rn<<5 | rt\n+}\n+\n+// encodeFpuRRR encodes as single or double precision (depending on `_64bit`) of Floating-point data-processing (2 source) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeFpuRRR(op fpuBinOp, rd, rn, rm uint32, _64bit bool) (ret uint32) {\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/SIMD-FP-Instructions/ADD--vector--Add-vectors--scalar--floating-point-and-integer-\n+\tvar opcode uint32\n+\tswitch op {\n+\tcase fpuBinOpAdd:\n+\t\topcode = 0b0010\n+\tcase fpuBinOpSub:\n+\t\topcode = 0b0011\n+\tcase fpuBinOpMul:\n+\t\topcode = 0b0000\n+\tcase fpuBinOpDiv:\n+\t\topcode = 0b0001\n+\tcase fpuBinOpMax:\n+\t\topcode = 0b0100\n+\tcase fpuBinOpMin:\n+\t\topcode = 0b0101\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\tvar ptype uint32\n+\tif _64bit {\n+\t\tptype = 0b01\n+\t}\n+\treturn 0b1111<<25 | ptype<<22 | 0b1<<21 | rm<<16 | opcode<<12 | 0b1<<11 | rn<<5 | rd\n+}\n+\n+// encodeAluRRImm12 encodes as Add/subtract (immediate) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Immediate?lang=en\n+func encodeAluRRImm12(op aluOp, rd, rn uint32, imm12 uint16, shiftBit byte, _64bit bool) uint32 {\n+\tvar _31to24 uint32\n+\tswitch op {\n+\tcase aluOpAdd:\n+\t\t_31to24 = 0b00_10001\n+\tcase aluOpAddS:\n+\t\t_31to24 = 0b01_10001\n+\tcase aluOpSub:\n+\t\t_31to24 = 0b10_10001\n+\tcase aluOpSubS:\n+\t\t_31to24 = 0b11_10001\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\tif _64bit {\n+\t\t_31to24 |= 0b1 << 7\n+\t}\n+\treturn _31to24<<24 | uint32(shiftBit)<<22 | uint32(imm12&0b111111111111)<<10 | rn<<5 | rd\n+}\n+\n+// encodeAluRRR encodes as Data Processing (shifted register), depending on aluOp.\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en#addsub_shift\n+func encodeAluRRRShift(op aluOp, rd, rn, rm, amount uint32, shiftOp shiftOp, _64bit bool) uint32 {\n+\tvar _31to24 uint32\n+\tvar opc, n uint32\n+\tswitch op {\n+\tcase aluOpAdd:\n+\t\t_31to24 = 0b00001011\n+\tcase aluOpAddS:\n+\t\t_31to24 = 0b00101011\n+\tcase aluOpSub:\n+\t\t_31to24 = 0b01001011\n+\tcase aluOpSubS:\n+\t\t_31to24 = 0b01101011\n+\tcase aluOpAnd, aluOpOrr, aluOpEor, aluOpAnds:\n+\t\t// \"Logical (shifted register)\".\n+\t\tswitch op {\n+\t\tcase aluOpAnd:\n+\t\t\t// all zeros\n+\t\tcase aluOpOrr:\n+\t\t\topc = 0b01\n+\t\tcase aluOpEor:\n+\t\t\topc = 0b10\n+\t\tcase aluOpAnds:\n+\t\t\topc = 0b11\n+\t\t}\n+\t\t_31to24 = 0b000_01010\n+\tdefault:\n+\t\tpanic(op.String())\n+\t}\n+\n+\tif _64bit {\n+\t\t_31to24 |= 0b1 << 7\n+\t}\n+\n+\tvar shift uint32\n+\tswitch shiftOp {\n+\tcase shiftOpLSL:\n+\t\tshift = 0b00\n+\tcase shiftOpLSR:\n+\t\tshift = 0b01\n+\tcase shiftOpASR:\n+\t\tshift = 0b10\n+\tdefault:\n+\t\tpanic(shiftOp.String())\n+\t}\n+\treturn opc<<29 | n<<21 | _31to24<<24 | shift<<22 | rm<<16 | (amount << 10) | (rn << 5) | rd\n+}\n+\n+// \"Add/subtract (extended register)\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en#addsub_ext\n+func encodeAluRRRExtend(ao aluOp, rd, rn, rm uint32, extOp extendOp, to byte) uint32 {\n+\tvar s, op uint32\n+\tswitch ao {\n+\tcase aluOpAdd:\n+\t\top = 0b0\n+\tcase aluOpAddS:\n+\t\top, s = 0b0, 0b1\n+\tcase aluOpSub:\n+\t\top = 0b1\n+\tcase aluOpSubS:\n+\t\top, s = 0b1, 0b1\n+\tdefault:\n+\t\tpanic(\"BUG: extended register operand can be used only for add/sub\")\n+\t}\n+\n+\tvar sf uint32\n+\tif to == 64 {\n+\t\tsf = 0b1\n+\t}\n+\n+\tvar option uint32\n+\tswitch extOp {\n+\tcase extendOpUXTB:\n+\t\toption = 0b000\n+\tcase extendOpUXTH:\n+\t\toption = 0b001\n+\tcase extendOpUXTW:\n+\t\toption = 0b010\n+\tcase extendOpSXTB:\n+\t\toption = 0b100\n+\tcase extendOpSXTH:\n+\t\toption = 0b101\n+\tcase extendOpSXTW:\n+\t\toption = 0b110\n+\tcase extendOpSXTX, extendOpUXTX:\n+\t\tpanic(fmt.Sprintf(\"%s is essentially noop, and should be handled much earlier than encoding\", extOp.String()))\n+\t}\n+\treturn sf<<31 | op<<30 | s<<29 | 0b1011001<<21 | rm<<16 | option<<13 | rn<<5 | rd\n+}\n+\n+// encodeAluRRR encodes as Data Processing (register), depending on aluOp.\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en\n+func encodeAluRRR(op aluOp, rd, rn, rm uint32, _64bit, isRnSp bool) uint32 {\n+\tvar _31to21, _15to10 uint32\n+\tswitch op {\n+\tcase aluOpAdd:\n+\t\tif isRnSp {\n+\t\t\t// \"Extended register\" with UXTW.\n+\t\t\t_31to21 = 0b00001011_001\n+\t\t\t_15to10 = 0b011000\n+\t\t} else {\n+\t\t\t// \"Shifted register\" with shift = 0\n+\t\t\t_31to21 = 0b00001011_000\n+\t\t}\n+\tcase aluOpAddS:\n+\t\tif isRnSp {\n+\t\t\tpanic(\"TODO\")\n+\t\t}\n+\t\t// \"Shifted register\" with shift = 0\n+\t\t_31to21 = 0b00101011_000\n+\tcase aluOpSub:\n+\t\tif isRnSp {\n+\t\t\t// \"Extended register\" with UXTW.\n+\t\t\t_31to21 = 0b01001011_001\n+\t\t\t_15to10 = 0b011000\n+\t\t} else {\n+\t\t\t// \"Shifted register\" with shift = 0\n+\t\t\t_31to21 = 0b01001011_000\n+\t\t}\n+\tcase aluOpSubS:\n+\t\tif isRnSp {\n+\t\t\tpanic(\"TODO\")\n+\t\t}\n+\t\t// \"Shifted register\" with shift = 0\n+\t\t_31to21 = 0b01101011_000\n+\tcase aluOpAnd, aluOpOrr, aluOpOrn, aluOpEor, aluOpAnds:\n+\t\t// \"Logical (shifted register)\".\n+\t\tvar opc, n uint32\n+\t\tswitch op {\n+\t\tcase aluOpAnd:\n+\t\t\t// all zeros\n+\t\tcase aluOpOrr:\n+\t\t\topc = 0b01\n+\t\tcase aluOpOrn:\n+\t\t\topc = 0b01\n+\t\t\tn = 1\n+\t\tcase aluOpEor:\n+\t\t\topc = 0b10\n+\t\tcase aluOpAnds:\n+\t\t\topc = 0b11\n+\t\t}\n+\t\t_31to21 = 0b000_01010_000 | opc<<8 | n\n+\tcase aluOpLsl, aluOpAsr, aluOpLsr, aluOpRotR:\n+\t\t// \"Data-processing (2 source)\".\n+\t\t_31to21 = 0b00011010_110\n+\t\tswitch op {\n+\t\tcase aluOpLsl:\n+\t\t\t_15to10 = 0b001000\n+\t\tcase aluOpLsr:\n+\t\t\t_15to10 = 0b001001\n+\t\tcase aluOpAsr:\n+\t\t\t_15to10 = 0b001010\n+\t\tcase aluOpRotR:\n+\t\t\t_15to10 = 0b001011\n+\t\t}\n+\tcase aluOpSDiv:\n+\t\t// \"Data-processing (2 source)\".\n+\t\t_31to21 = 0b11010110\n+\t\t_15to10 = 0b000011\n+\tcase aluOpUDiv:\n+\t\t// \"Data-processing (2 source)\".\n+\t\t_31to21 = 0b11010110\n+\t\t_15to10 = 0b000010\n+\tdefault:\n+\t\tpanic(op.String())\n+\t}\n+\tif _64bit {\n+\t\t_31to21 |= 0b1 << 10\n+\t}\n+\treturn _31to21<<21 | rm<<16 | (_15to10 << 10) | (rn << 5) | rd\n+}\n+\n+// encodeLogicalShiftedRegister encodes as Logical (shifted register) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Register?lang=en\n+func encodeLogicalShiftedRegister(sf_opc uint32, shift_N uint32, rm uint32, imm6 uint32, rn, rd uint32) (ret uint32) {\n+\tret = sf_opc << 29\n+\tret |= 0b01010 << 24\n+\tret |= shift_N << 21\n+\tret |= rm << 16\n+\tret |= imm6 << 10\n+\tret |= rn << 5\n+\tret |= rd\n+\treturn\n+}\n+\n+// encodeAddSubtractImmediate encodes as Add/subtract (immediate) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Immediate?lang=en\n+func encodeAddSubtractImmediate(sf_op_s uint32, sh uint32, imm12 uint32, rn, rd uint32) (ret uint32) {\n+\tret = sf_op_s << 29\n+\tret |= 0b100010 << 23\n+\tret |= sh << 22\n+\tret |= imm12 << 10\n+\tret |= rn << 5\n+\tret |= rd\n+\treturn\n+}\n+\n+// encodePreOrPostIndexLoadStorePair64 encodes as Load/store pair (pre/post-indexed) in\n+// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDP--Load-Pair-of-Registers-\n+// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/STP--Store-Pair-of-Registers-\n+func encodePreOrPostIndexLoadStorePair64(pre bool, load bool, rn, rt, rt2 uint32, imm7 int64) (ret uint32) {\n+\tif imm7%8 != 0 {\n+\t\tpanic(\"imm7 for pair load/store must be a multiple of 8\")\n+\t}\n+\timm7 /= 8\n+\tret = rt\n+\tret |= rn << 5\n+\tret |= rt2 << 10\n+\tret |= (uint32(imm7) & 0b1111111) << 15\n+\tif load {\n+\t\tret |= 0b1 << 22\n+\t}\n+\tret |= 0b101010001 << 23\n+\tif pre {\n+\t\tret |= 0b1 << 24\n+\t}\n+\treturn\n+}\n+\n+// encodeUnconditionalBranch encodes as B or BL instructions:\n+// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/B--Branch-\n+// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/BL--Branch-with-Link-\n+func encodeUnconditionalBranch(link bool, imm26 int64) (ret uint32) {\n+\tif imm26%4 != 0 {\n+\t\tpanic(\"imm26 for branch must be a multiple of 4\")\n+\t}\n+\timm26 /= 4\n+\tret = uint32(imm26 & 0b11_11111111_11111111_11111111)\n+\tret |= 0b101 << 26\n+\tif link {\n+\t\tret |= 0b1 << 31\n+\t}\n+\treturn\n+}\n+\n+// encodeCBZCBNZ encodes as either CBZ or CBNZ:\n+// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/CBZ--Compare-and-Branch-on-Zero-\n+// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/CBNZ--Compare-and-Branch-on-Nonzero-\n+func encodeCBZCBNZ(rt uint32, nz bool, imm19 uint32, _64bit bool) (ret uint32) {\n+\tret = rt\n+\tret |= imm19 << 5\n+\tif nz {\n+\t\tret |= 1 << 24\n+\t}\n+\tret |= 0b11010 << 25\n+\tif _64bit {\n+\t\tret |= 1 << 31\n+\t}\n+\treturn\n+}\n+\n+// encodeMoveWideImmediate encodes as either MOVZ, MOVN or MOVK, as Move wide (immediate) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Immediate?lang=en\n+//\n+// \"shift\" must have been divided by 16 at this point.\n+func encodeMoveWideImmediate(opc uint32, rd uint32, imm, shift, _64bit uint64) (ret uint32) {\n+\tret = rd\n+\tret |= uint32(imm&0xffff) << 5\n+\tret |= (uint32(shift)) << 21\n+\tret |= 0b100101 << 23\n+\tret |= opc << 29\n+\tret |= uint32(_64bit) << 31\n+\treturn\n+}\n+\n+// encodeAluRRImm encodes as \"Bitfield\" in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Immediate?lang=en#log_imm\n+func encodeAluRRImm(op aluOp, rd, rn, amount, _64bit uint32) uint32 {\n+\tvar opc uint32\n+\tvar immr, imms uint32\n+\tswitch op {\n+\tcase aluOpLsl:\n+\t\t// LSL (immediate) is an alias for UBFM.\n+\t\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/UBFM--Unsigned-Bitfield-Move-?lang=en\n+\t\topc = 0b10\n+\t\tif amount == 0 {\n+\t\t\t// This can be encoded as NOP, but we don't do it for consistency: lsr xn, xm, #0\n+\t\t\timmr = 0\n+\t\t\tif _64bit == 1 {\n+\t\t\t\timms = 0b111111\n+\t\t\t} else {\n+\t\t\t\timms = 0b11111\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif _64bit == 1 {\n+\t\t\t\timmr = 64 - amount\n+\t\t\t} else {\n+\t\t\t\timmr = (32 - amount) & 0b11111\n+\t\t\t}\n+\t\t\timms = immr - 1\n+\t\t}\n+\tcase aluOpLsr:\n+\t\t// LSR (immediate) is an alias for UBFM.\n+\t\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LSR--immediate---Logical-Shift-Right--immediate---an-alias-of-UBFM-?lang=en\n+\t\topc = 0b10\n+\t\timms, immr = 0b011111|_64bit<<5, amount\n+\tcase aluOpAsr:\n+\t\t// ASR (immediate) is an alias for SBFM.\n+\t\t// https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/SBFM--Signed-Bitfield-Move-?lang=en\n+\t\topc = 0b00\n+\t\timms, immr = 0b011111|_64bit<<5, amount\n+\tdefault:\n+\t\tpanic(op.String())\n+\t}\n+\treturn _64bit<<31 | opc<<29 | 0b100110<<23 | _64bit<<22 | immr<<16 | imms<<10 | rn<<5 | rd\n+}\n+\n+// encodeVecLanes encodes as Data Processing (Advanced SIMD across lanes) depending on vecOp in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeVecLanes(op vecOp, rd uint32, rn uint32, arr vecArrangement) uint32 {\n+\tvar u, q, size, opcode uint32\n+\tswitch arr {\n+\tcase vecArrangement8B:\n+\t\tq, size = 0b0, 0b00\n+\tcase vecArrangement16B:\n+\t\tq, size = 0b1, 0b00\n+\tcase vecArrangement4H:\n+\t\tq, size = 0, 0b01\n+\tcase vecArrangement8H:\n+\t\tq, size = 1, 0b01\n+\tcase vecArrangement4S:\n+\t\tq, size = 1, 0b10\n+\tdefault:\n+\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t}\n+\tswitch op {\n+\tcase vecOpUaddlv:\n+\t\tu, opcode = 1, 0b00011\n+\tcase vecOpUminv:\n+\t\tu, opcode = 1, 0b11010\n+\tcase vecOpAddv:\n+\t\tu, opcode = 0, 0b11011\n+\tdefault:\n+\t\tpanic(\"unsupported or illegal vecOp: \" + op.String())\n+\t}\n+\treturn q<<30 | u<<29 | 0b1110<<24 | size<<22 | 0b11000<<17 | opcode<<12 | 0b10<<10 | rn<<5 | rd\n+}\n+\n+// encodeVecLanes encodes as Data Processing (Advanced SIMD scalar shift by immediate) depending on vecOp in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en\n+func encodeVecShiftImm(op vecOp, rd uint32, rn, amount uint32, arr vecArrangement) uint32 {\n+\tvar u, q, immh, immb, opcode uint32\n+\tswitch op {\n+\tcase vecOpSshll:\n+\t\tu, opcode = 0b0, 0b10100\n+\tcase vecOpUshll:\n+\t\tu, opcode = 0b1, 0b10100\n+\tcase vecOpSshr:\n+\t\tu, opcode = 0, 0b00000\n+\tdefault:\n+\t\tpanic(\"unsupported or illegal vecOp: \" + op.String())\n+\t}\n+\tswitch arr {\n+\tcase vecArrangement16B:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase vecArrangement8B:\n+\t\timmh = 0b0001\n+\t\timmb = 8 - uint32(amount&0b111)\n+\tcase vecArrangement8H:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase vecArrangement4H:\n+\t\tv := 16 - uint32(amount&0b1111)\n+\t\timmb = v & 0b111\n+\t\timmh = 0b0010 | (v >> 3)\n+\tcase vecArrangement4S:\n+\t\tq = 0b1\n+\t\tfallthrough\n+\tcase vecArrangement2S:\n+\t\tv := 32 - uint32(amount&0b11111)\n+\t\timmb = v & 0b111\n+\t\timmh = 0b0100 | (v >> 3)\n+\tcase vecArrangement2D:\n+\t\tq = 0b1\n+\t\tv := 64 - uint32(amount&0b111111)\n+\t\timmb = v & 0b111\n+\t\timmh = 0b1000 | (v >> 3)\n+\tdefault:\n+\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t}\n+\treturn q<<30 | u<<29 | 0b011110<<23 | immh<<19 | immb<<16 | 0b000001<<10 | opcode<<11 | 0b1<<10 | rn<<5 | rd\n+}\n+\n+// encodeVecTbl encodes as Data Processing (Advanced SIMD table lookup) in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en#simd-dp\n+//\n+// Note: tblOp may encode tbl1, tbl2... in the future. Currently, it is ignored.\n+func encodeVecTbl(nregs, rd, rn, rm uint32, arr vecArrangement) uint32 {\n+\tvar q, op2, len, op uint32\n+\n+\tswitch nregs {\n+\tcase 1:\n+\t\t// tbl: single-register\n+\t\tlen = 0b00\n+\tcase 2:\n+\t\t// tbl2: 2-register table\n+\t\tlen = 0b01\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"unsupported number or registers %d\", nregs))\n+\t}\n+\tswitch arr {\n+\tcase vecArrangement8B:\n+\t\tq = 0b0\n+\tcase vecArrangement16B:\n+\t\tq = 0b1\n+\tdefault:\n+\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t}\n+\n+\treturn q<<30 | 0b001110<<24 | op2<<22 | rm<<16 | len<<13 | op<<12 | rn<<5 | rd\n+}\n+\n+// encodeVecMisc encodes as Data Processing (Advanced SIMD two-register miscellaneous) depending on vecOp in\n+// https://developer.arm.com/documentation/ddi0596/2020-12/Index-by-Encoding/Data-Processing----Scalar-Floating-Point-and-Advanced-SIMD?lang=en#simd-dp\n+func encodeAdvancedSIMDTwoMisc(op vecOp, rd, rn uint32, arr vecArrangement) uint32 {\n+\tvar q, u, size, opcode uint32\n+\tswitch op {\n+\tcase vecOpCnt:\n+\t\topcode = 0b00101\n+\t\tswitch arr {\n+\t\tcase vecArrangement8B:\n+\t\t\tq, size = 0b0, 0b00\n+\t\tcase vecArrangement16B:\n+\t\t\tq, size = 0b1, 0b00\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpCmeq0:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\topcode = 0b01001\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpNot:\n+\t\tu = 1\n+\t\topcode = 0b00101\n+\t\tswitch arr {\n+\t\tcase vecArrangement8B:\n+\t\t\tq, size = 0b0, 0b00\n+\t\tcase vecArrangement16B:\n+\t\t\tq, size = 0b1, 0b00\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpAbs:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\topcode = 0b01011\n+\t\tu = 0b0\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpNeg:\n+\t\tif arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\topcode = 0b01011\n+\t\tu = 0b1\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpFabs:\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\topcode = 0b01111\n+\t\tu = 0b0\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpFneg:\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\topcode = 0b01111\n+\t\tu = 0b1\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpFrintm:\n+\t\tu = 0b0\n+\t\topcode = 0b11001\n+\t\tswitch arr {\n+\t\tcase vecArrangement2S:\n+\t\t\tq, size = 0b0, 0b00\n+\t\tcase vecArrangement4S:\n+\t\t\tq, size = 0b1, 0b00\n+\t\tcase vecArrangement2D:\n+\t\t\tq, size = 0b1, 0b01\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpFrintn:\n+\t\tu = 0b0\n+\t\topcode = 0b11000\n+\t\tswitch arr {\n+\t\tcase vecArrangement2S:\n+\t\t\tq, size = 0b0, 0b00\n+\t\tcase vecArrangement4S:\n+\t\t\tq, size = 0b1, 0b00\n+\t\tcase vecArrangement2D:\n+\t\t\tq, size = 0b1, 0b01\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpFrintp:\n+\t\tu = 0b0\n+\t\topcode = 0b11000\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpFrintz:\n+\t\tu = 0b0\n+\t\topcode = 0b11001\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpFsqrt:\n+\t\tif arr < vecArrangement2S || arr == vecArrangement1D {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\topcode = 0b11111\n+\t\tu = 0b1\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpFcvtl:\n+\t\topcode = 0b10111\n+\t\tu = 0b0\n+\t\tswitch arr {\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b01, 0b0\n+\t\tcase vecArrangement4H:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpFcvtn:\n+\t\topcode = 0b10110\n+\t\tu = 0b0\n+\t\tswitch arr {\n+\t\tcase vecArrangement2S:\n+\t\t\tsize, q = 0b01, 0b0\n+\t\tcase vecArrangement4H:\n+\t\t\tsize, q = 0b00, 0b0\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpFcvtzs:\n+\t\topcode = 0b11011\n+\t\tu = 0b0\n+\t\tswitch arr {\n+\t\tcase vecArrangement2S:\n+\t\t\tq, size = 0b0, 0b10\n+\t\tcase vecArrangement4S:\n+\t\t\tq, size = 0b1, 0b10\n+\t\tcase vecArrangement2D:\n+\t\t\tq, size = 0b1, 0b11\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpFcvtzu:\n+\t\topcode = 0b11011\n+\t\tu = 0b1\n+\t\tswitch arr {\n+\t\tcase vecArrangement2S:\n+\t\t\tq, size = 0b0, 0b10\n+\t\tcase vecArrangement4S:\n+\t\t\tq, size = 0b1, 0b10\n+\t\tcase vecArrangement2D:\n+\t\t\tq, size = 0b1, 0b11\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpScvtf:\n+\t\topcode = 0b11101\n+\t\tu = 0b0\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tq, size = 0b1, 0b00\n+\t\tcase vecArrangement2S:\n+\t\t\tq, size = 0b0, 0b00\n+\t\tcase vecArrangement2D:\n+\t\t\tq, size = 0b1, 0b01\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpUcvtf:\n+\t\topcode = 0b11101\n+\t\tu = 0b1\n+\t\tswitch arr {\n+\t\tcase vecArrangement4S:\n+\t\t\tq, size = 0b1, 0b00\n+\t\tcase vecArrangement2S:\n+\t\t\tq, size = 0b0, 0b00\n+\t\tcase vecArrangement2D:\n+\t\t\tq, size = 0b1, 0b01\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tcase vecOpSqxtn:\n+\t\t// When q == 1 it encodes sqxtn2 (operates on upper 64 bits).\n+\t\topcode = 0b10100\n+\t\tu = 0b0\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpUqxtn:\n+\t\t// When q == 1 it encodes uqxtn2 (operates on upper 64 bits).\n+\t\topcode = 0b10100\n+\t\tu = 0b1\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpSqxtun:\n+\t\t// When q == 1 it encodes sqxtun2 (operates on upper 64 bits).\n+\t\topcode = 0b10010 // 0b10100\n+\t\tu = 0b1\n+\t\tif arr > vecArrangement4S {\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpRev64:\n+\t\topcode = 0b00000\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpXtn:\n+\t\tu = 0b0\n+\t\topcode = 0b10010\n+\t\tsize, q = arrToSizeQEncoded(arr)\n+\tcase vecOpShll:\n+\t\tu = 0b1\n+\t\topcode = 0b10011\n+\t\tswitch arr {\n+\t\tcase vecArrangement8B:\n+\t\t\tq, size = 0b0, 0b00\n+\t\tcase vecArrangement4H:\n+\t\t\tq, size = 0b0, 0b01\n+\t\tcase vecArrangement2S:\n+\t\t\tq, size = 0b0, 0b10\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported arrangement: \" + arr.String())\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"unsupported or illegal vecOp: \" + op.String())\n+\t}\n+\treturn q<<30 | u<<29 | 0b01110<<24 | size<<22 | 0b10000<<17 | opcode<<12 | 0b10<<10 | rn<<5 | rd\n+}\n+\n+// brTableSequenceOffsetTableBegin is the offset inside the brTableSequence where the table begins after 4 instructions\n+const brTableSequenceOffsetTableBegin = 16\n+\n+func encodeBrTableSequence(c backend.Compiler, index regalloc.VReg, targets []uint32) {\n+\ttmpRegNumber := regNumberInEncoding[tmp]\n+\tindexNumber := regNumberInEncoding[index.RealReg()]\n+\n+\t// adr tmpReg, PC+16 (PC+16 is the address of the first label offset)\n+\t// ldrsw index, [tmpReg, index, UXTW 2] ;; index = int64(*(tmpReg + index*8))\n+\t// add tmpReg, tmpReg, index\n+\t// br tmpReg\n+\t// [offset_to_l1, offset_to_l2, ..., offset_to_lN]\n+\tc.Emit4Bytes(encodeAdr(tmpRegNumber, 16))\n+\tc.Emit4Bytes(encodeLoadOrStore(sLoad32, indexNumber,\n+\t\taddressMode{kind: addressModeKindRegScaledExtended, rn: tmpRegVReg, rm: index, extOp: extendOpUXTW},\n+\t))\n+\tc.Emit4Bytes(encodeAluRRR(aluOpAdd, tmpRegNumber, tmpRegNumber, indexNumber, true, false))\n+\tc.Emit4Bytes(encodeUnconditionalBranchReg(tmpRegNumber, false))\n+\n+\t// Offsets are resolved in ResolveRelativeAddress phase.\n+\tfor _, offset := range targets {\n+\t\tif wazevoapi.PrintMachineCodeHexPerFunctionDisassemblable {\n+\t\t\t// Inlined offset tables cannot be disassembled properly, so pad dummy instructions to make the debugging easier.\n+\t\t\tc.Emit4Bytes(dummyInstruction)\n+\t\t} else {\n+\t\t\tc.Emit4Bytes(offset)\n+\t\t}\n+\t}\n+}\n+\n+// encodeExitSequence matches the implementation detail of functionABI.emitGoEntryPreamble.\n+func encodeExitSequence(c backend.Compiler, ctxReg regalloc.VReg) {\n+\t// Restore the FP, SP and LR, and return to the Go code:\n+\t// \t\tldr lr,  [ctxReg, #GoReturnAddress]\n+\t// \t\tldr fp,  [ctxReg, #OriginalFramePointer]\n+\t// \t\tldr tmp, [ctxReg, #OriginalStackPointer]\n+\t//      mov sp, tmp ;; sp cannot be str'ed directly.\n+\t// \t\tret ;; --> return to the Go code\n+\n+\tvar ctxEvicted bool\n+\tif ctx := ctxReg.RealReg(); ctx == fp || ctx == lr {\n+\t\t// In order to avoid overwriting the context register, we move ctxReg to tmp.\n+\t\tc.Emit4Bytes(encodeMov64(regNumberInEncoding[tmp], regNumberInEncoding[ctx], false, false))\n+\t\tctxReg = tmpRegVReg\n+\t\tctxEvicted = true\n+\t}\n+\n+\trestoreLr := encodeLoadOrStore(\n+\t\tuLoad64,\n+\t\tregNumberInEncoding[lr],\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   ctxReg,\n+\t\t\timm:  wazevoapi.ExecutionContextOffsetGoReturnAddress.I64(),\n+\t\t},\n+\t)\n+\n+\trestoreFp := encodeLoadOrStore(\n+\t\tuLoad64,\n+\t\tregNumberInEncoding[fp],\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   ctxReg,\n+\t\t\timm:  wazevoapi.ExecutionContextOffsetOriginalFramePointer.I64(),\n+\t\t},\n+\t)\n+\n+\trestoreSpToTmp := encodeLoadOrStore(\n+\t\tuLoad64,\n+\t\tregNumberInEncoding[tmp],\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   ctxReg,\n+\t\t\timm:  wazevoapi.ExecutionContextOffsetOriginalStackPointer.I64(),\n+\t\t},\n+\t)\n+\n+\tmovTmpToSp := encodeAddSubtractImmediate(0b100, 0, 0,\n+\t\tregNumberInEncoding[tmp], regNumberInEncoding[sp])\n+\n+\tc.Emit4Bytes(restoreFp)\n+\tc.Emit4Bytes(restoreLr)\n+\tc.Emit4Bytes(restoreSpToTmp)\n+\tc.Emit4Bytes(movTmpToSp)\n+\tc.Emit4Bytes(encodeRet())\n+\tif !ctxEvicted {\n+\t\t// In order to have the fixed-length exit sequence, we need to padd the binary.\n+\t\t// Since this will never be reached, we insert a dummy instruction.\n+\t\tc.Emit4Bytes(dummyInstruction)\n+\t}\n+}\n+\n+func encodeRet() uint32 {\n+\t// https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/RET--Return-from-subroutine-?lang=en\n+\treturn 0b1101011001011111<<16 | regNumberInEncoding[lr]<<5\n+}\n+\n+func encodeAtomicRmw(op atomicRmwOp, rs, rt, rn uint32, size uint32) uint32 {\n+\tvar _31to21, _15to10, sz uint32\n+\n+\tswitch size {\n+\tcase 8:\n+\t\tsz = 0b11\n+\tcase 4:\n+\t\tsz = 0b10\n+\tcase 2:\n+\t\tsz = 0b01\n+\tcase 1:\n+\t\tsz = 0b00\n+\t}\n+\n+\t_31to21 = 0b00111000_111 | sz<<9\n+\n+\tswitch op {\n+\tcase atomicRmwOpAdd:\n+\t\t_15to10 = 0b000000\n+\tcase atomicRmwOpClr:\n+\t\t_15to10 = 0b000100\n+\tcase atomicRmwOpSet:\n+\t\t_15to10 = 0b001100\n+\tcase atomicRmwOpEor:\n+\t\t_15to10 = 0b001000\n+\tcase atomicRmwOpSwp:\n+\t\t_15to10 = 0b100000\n+\t}\n+\n+\treturn _31to21<<21 | rs<<16 | _15to10<<10 | rn<<5 | rt\n+}\n+\n+func encodeAtomicCas(rs, rt, rn uint32, size uint32) uint32 {\n+\tvar _31to21, _15to10, sz uint32\n+\n+\tswitch size {\n+\tcase 8:\n+\t\tsz = 0b11\n+\tcase 4:\n+\t\tsz = 0b10\n+\tcase 2:\n+\t\tsz = 0b01\n+\tcase 1:\n+\t\tsz = 0b00\n+\t}\n+\n+\t_31to21 = 0b00001000_111 | sz<<9\n+\t_15to10 = 0b111111\n+\n+\treturn _31to21<<21 | rs<<16 | _15to10<<10 | rn<<5 | rt\n+}\n+\n+func encodeAtomicLoadStore(rn, rt, size, l uint32) uint32 {\n+\tvar _31to21, _20to16, _15to10, sz uint32\n+\n+\tswitch size {\n+\tcase 8:\n+\t\tsz = 0b11\n+\tcase 4:\n+\t\tsz = 0b10\n+\tcase 2:\n+\t\tsz = 0b01\n+\tcase 1:\n+\t\tsz = 0b00\n+\t}\n+\n+\t_31to21 = 0b00001000_100 | sz<<9 | l<<1\n+\t_20to16 = 0b11111\n+\t_15to10 = 0b111111\n+\n+\treturn _31to21<<21 | _20to16<<16 | _15to10<<10 | rn<<5 | rt\n+}\n+\n+func encodeDMB() uint32 {\n+\treturn 0b11010101000000110011101110111111\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/lower_constant.go",
          "status": "added",
          "additions": 301,
          "deletions": 0,
          "patch": "@@ -0,0 +1,301 @@\n+package arm64\n+\n+import (\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+// lowerConstant allocates a new VReg and inserts the instruction to load the constant value.\n+func (m *machine) lowerConstant(instr *ssa.Instruction) (vr regalloc.VReg) {\n+\tval := instr.Return()\n+\tvalType := val.Type()\n+\n+\tvr = m.compiler.AllocateVReg(valType)\n+\tv := instr.ConstantVal()\n+\tm.insertLoadConstant(v, valType, vr)\n+\treturn\n+}\n+\n+// InsertLoadConstantBlockArg implements backend.Machine.\n+func (m *machine) InsertLoadConstantBlockArg(instr *ssa.Instruction, vr regalloc.VReg) {\n+\tval := instr.Return()\n+\tvalType := val.Type()\n+\tv := instr.ConstantVal()\n+\tload := m.allocateInstr()\n+\tload.asLoadConstBlockArg(v, valType, vr)\n+\tm.insert(load)\n+}\n+\n+func (m *machine) lowerLoadConstantBlockArgAfterRegAlloc(i *instruction) {\n+\tv, typ, dst := i.loadConstBlockArgData()\n+\tm.insertLoadConstant(v, typ, dst)\n+}\n+\n+func (m *machine) insertLoadConstant(v uint64, valType ssa.Type, vr regalloc.VReg) {\n+\tif valType.Bits() < 64 { // Clear the redundant bits just in case it's unexpectedly sign-extended, etc.\n+\t\tv = v & ((1 << valType.Bits()) - 1)\n+\t}\n+\n+\tswitch valType {\n+\tcase ssa.TypeF32:\n+\t\tloadF := m.allocateInstr()\n+\t\tloadF.asLoadFpuConst32(vr, v)\n+\t\tm.insert(loadF)\n+\tcase ssa.TypeF64:\n+\t\tloadF := m.allocateInstr()\n+\t\tloadF.asLoadFpuConst64(vr, v)\n+\t\tm.insert(loadF)\n+\tcase ssa.TypeI32:\n+\t\tif v == 0 {\n+\t\t\tm.InsertMove(vr, xzrVReg, ssa.TypeI32)\n+\t\t} else {\n+\t\t\tm.lowerConstantI32(vr, int32(v))\n+\t\t}\n+\tcase ssa.TypeI64:\n+\t\tif v == 0 {\n+\t\t\tm.InsertMove(vr, xzrVReg, ssa.TypeI64)\n+\t\t} else {\n+\t\t\tm.lowerConstantI64(vr, int64(v))\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"TODO\")\n+\t}\n+}\n+\n+// The following logics are based on the old asm/arm64 package.\n+// https://github.com/tetratelabs/wazero/blob/39f2ff23a6d609e10c82b9cc0b981f6de5b87a9c/internal/asm/arm64/impl.go\n+\n+func (m *machine) lowerConstantI32(dst regalloc.VReg, c int32) {\n+\t// Following the logic here:\n+\t// https://github.com/golang/go/blob/release-branch.go1.15/src/cmd/internal/obj/arm64/asm7.go#L1637\n+\tic := int64(uint32(c))\n+\tif ic >= 0 && (ic <= 0xfff || (ic&0xfff) == 0 && (uint64(ic>>12) <= 0xfff)) {\n+\t\tif isBitMaskImmediate(uint64(c), false) {\n+\t\t\tm.lowerConstViaBitMaskImmediate(uint64(uint32(c)), dst, false)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\tif t := const16bitAligned(int64(uint32(c))); t >= 0 {\n+\t\t// If the const can fit within 16-bit alignment, for example, 0xffff, 0xffff_0000 or 0xffff_0000_0000_0000\n+\t\t// We could load it into temporary with movk.\n+\t\tm.insertMOVZ(dst, uint64(uint32(c)>>(16*t)), t, false)\n+\t} else if t := const16bitAligned(int64(^c)); t >= 0 {\n+\t\t// Also, if the inverse of the const can fit within 16-bit range, do the same ^^.\n+\t\tm.insertMOVN(dst, uint64(^c>>(16*t)), t, false)\n+\t} else if isBitMaskImmediate(uint64(uint32(c)), false) {\n+\t\tm.lowerConstViaBitMaskImmediate(uint64(c), dst, false)\n+\t} else {\n+\t\t// Otherwise, we use MOVZ and MOVK to load it.\n+\t\tc16 := uint16(c)\n+\t\tm.insertMOVZ(dst, uint64(c16), 0, false)\n+\t\tc16 = uint16(uint32(c) >> 16)\n+\t\tm.insertMOVK(dst, uint64(c16), 1, false)\n+\t}\n+}\n+\n+func (m *machine) lowerConstantI64(dst regalloc.VReg, c int64) {\n+\t// Following the logic here:\n+\t// https://github.com/golang/go/blob/release-branch.go1.15/src/cmd/internal/obj/arm64/asm7.go#L1798-L1852\n+\tif c >= 0 && (c <= 0xfff || (c&0xfff) == 0 && (uint64(c>>12) <= 0xfff)) {\n+\t\tif isBitMaskImmediate(uint64(c), true) {\n+\t\t\tm.lowerConstViaBitMaskImmediate(uint64(c), dst, true)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\tif t := const16bitAligned(c); t >= 0 {\n+\t\t// If the const can fit within 16-bit alignment, for example, 0xffff, 0xffff_0000 or 0xffff_0000_0000_0000\n+\t\t// We could load it into temporary with movk.\n+\t\tm.insertMOVZ(dst, uint64(c)>>(16*t), t, true)\n+\t} else if t := const16bitAligned(^c); t >= 0 {\n+\t\t// Also, if the reverse of the const can fit within 16-bit range, do the same ^^.\n+\t\tm.insertMOVN(dst, uint64(^c)>>(16*t), t, true)\n+\t} else if isBitMaskImmediate(uint64(c), true) {\n+\t\tm.lowerConstViaBitMaskImmediate(uint64(c), dst, true)\n+\t} else {\n+\t\tm.load64bitConst(c, dst)\n+\t}\n+}\n+\n+func (m *machine) lowerConstViaBitMaskImmediate(c uint64, dst regalloc.VReg, b64 bool) {\n+\tinstr := m.allocateInstr()\n+\tinstr.asALUBitmaskImm(aluOpOrr, dst, xzrVReg, c, b64)\n+\tm.insert(instr)\n+}\n+\n+// isBitMaskImmediate determines if the value can be encoded as \"bitmask immediate\".\n+//\n+//\tSuch an immediate is a 32-bit or 64-bit pattern viewed as a vector of identical elements of size e = 2, 4, 8, 16, 32, or 64 bits.\n+//\tEach element contains the same sub-pattern: a single run of 1 to e-1 non-zero bits, rotated by 0 to e-1 bits.\n+//\n+// See https://developer.arm.com/documentation/dui0802/b/A64-General-Instructions/MOV--bitmask-immediate-\n+func isBitMaskImmediate(x uint64, _64 bool) bool {\n+\t// All zeros and ones are not \"bitmask immediate\" by definition.\n+\tif x == 0 || (_64 && x == 0xffff_ffff_ffff_ffff) || (!_64 && x == 0xffff_ffff) {\n+\t\treturn false\n+\t}\n+\n+\tswitch {\n+\tcase x != x>>32|x<<32:\n+\t\t// e = 64\n+\tcase x != x>>16|x<<48:\n+\t\t// e = 32 (x == x>>32|x<<32).\n+\t\t// e.g. 0x00ff_ff00_00ff_ff00\n+\t\tx = uint64(int32(x))\n+\tcase x != x>>8|x<<56:\n+\t\t// e = 16 (x == x>>16|x<<48).\n+\t\t// e.g. 0x00ff_00ff_00ff_00ff\n+\t\tx = uint64(int16(x))\n+\tcase x != x>>4|x<<60:\n+\t\t// e = 8 (x == x>>8|x<<56).\n+\t\t// e.g. 0x0f0f_0f0f_0f0f_0f0f\n+\t\tx = uint64(int8(x))\n+\tdefault:\n+\t\t// e = 4 or 2.\n+\t\treturn true\n+\t}\n+\treturn sequenceOfSetbits(x) || sequenceOfSetbits(^x)\n+}\n+\n+// sequenceOfSetbits returns true if the number's binary representation is the sequence set bit (1).\n+// For example: 0b1110 -> true, 0b1010 -> false\n+func sequenceOfSetbits(x uint64) bool {\n+\ty := getLowestBit(x)\n+\t// If x is a sequence of set bit, this should results in the number\n+\t// with only one set bit (i.e. power of two).\n+\ty += x\n+\treturn (y-1)&y == 0\n+}\n+\n+func getLowestBit(x uint64) uint64 {\n+\treturn x & (^x + 1)\n+}\n+\n+// const16bitAligned check if the value is on the 16-bit alignment.\n+// If so, returns the shift num divided by 16, and otherwise -1.\n+func const16bitAligned(v int64) (ret int) {\n+\tret = -1\n+\tfor s := 0; s < 64; s += 16 {\n+\t\tif (uint64(v) &^ (uint64(0xffff) << uint(s))) == 0 {\n+\t\t\tret = s / 16\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\treturn\n+}\n+\n+// load64bitConst loads a 64-bit constant into the register, following the same logic to decide how to load large 64-bit\n+// consts as in the Go assembler.\n+//\n+// See https://github.com/golang/go/blob/release-branch.go1.15/src/cmd/internal/obj/arm64/asm7.go#L6632-L6759\n+func (m *machine) load64bitConst(c int64, dst regalloc.VReg) {\n+\tvar bits [4]uint64\n+\tvar zeros, negs int\n+\tfor i := 0; i < 4; i++ {\n+\t\tbits[i] = uint64(c) >> uint(i*16) & 0xffff\n+\t\tif v := bits[i]; v == 0 {\n+\t\t\tzeros++\n+\t\t} else if v == 0xffff {\n+\t\t\tnegs++\n+\t\t}\n+\t}\n+\n+\tif zeros == 3 {\n+\t\t// one MOVZ instruction.\n+\t\tfor i, v := range bits {\n+\t\t\tif v != 0 {\n+\t\t\t\tm.insertMOVZ(dst, v, i, true)\n+\t\t\t}\n+\t\t}\n+\t} else if negs == 3 {\n+\t\t// one MOVN instruction.\n+\t\tfor i, v := range bits {\n+\t\t\tif v != 0xffff {\n+\t\t\t\tv = ^v\n+\t\t\t\tm.insertMOVN(dst, v, i, true)\n+\t\t\t}\n+\t\t}\n+\t} else if zeros == 2 {\n+\t\t// one MOVZ then one OVK.\n+\t\tvar movz bool\n+\t\tfor i, v := range bits {\n+\t\t\tif !movz && v != 0 { // MOVZ.\n+\t\t\t\tm.insertMOVZ(dst, v, i, true)\n+\t\t\t\tmovz = true\n+\t\t\t} else if v != 0 {\n+\t\t\t\tm.insertMOVK(dst, v, i, true)\n+\t\t\t}\n+\t\t}\n+\n+\t} else if negs == 2 {\n+\t\t// one MOVN then one or two MOVK.\n+\t\tvar movn bool\n+\t\tfor i, v := range bits { // Emit MOVN.\n+\t\t\tif !movn && v != 0xffff {\n+\t\t\t\tv = ^v\n+\t\t\t\t// https://developer.arm.com/documentation/dui0802/a/A64-General-Instructions/MOVN\n+\t\t\t\tm.insertMOVN(dst, v, i, true)\n+\t\t\t\tmovn = true\n+\t\t\t} else if v != 0xffff {\n+\t\t\t\tm.insertMOVK(dst, v, i, true)\n+\t\t\t}\n+\t\t}\n+\n+\t} else if zeros == 1 {\n+\t\t// one MOVZ then two MOVK.\n+\t\tvar movz bool\n+\t\tfor i, v := range bits {\n+\t\t\tif !movz && v != 0 { // MOVZ.\n+\t\t\t\tm.insertMOVZ(dst, v, i, true)\n+\t\t\t\tmovz = true\n+\t\t\t} else if v != 0 {\n+\t\t\t\tm.insertMOVK(dst, v, i, true)\n+\t\t\t}\n+\t\t}\n+\n+\t} else if negs == 1 {\n+\t\t// one MOVN then two MOVK.\n+\t\tvar movn bool\n+\t\tfor i, v := range bits { // Emit MOVN.\n+\t\t\tif !movn && v != 0xffff {\n+\t\t\t\tv = ^v\n+\t\t\t\t// https://developer.arm.com/documentation/dui0802/a/A64-General-Instructions/MOVN\n+\t\t\t\tm.insertMOVN(dst, v, i, true)\n+\t\t\t\tmovn = true\n+\t\t\t} else if v != 0xffff {\n+\t\t\t\tm.insertMOVK(dst, v, i, true)\n+\t\t\t}\n+\t\t}\n+\n+\t} else {\n+\t\t// one MOVZ then up to three MOVK.\n+\t\tvar movz bool\n+\t\tfor i, v := range bits {\n+\t\t\tif !movz && v != 0 { // MOVZ.\n+\t\t\t\tm.insertMOVZ(dst, v, i, true)\n+\t\t\t\tmovz = true\n+\t\t\t} else if v != 0 {\n+\t\t\t\tm.insertMOVK(dst, v, i, true)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func (m *machine) insertMOVZ(dst regalloc.VReg, v uint64, shift int, dst64 bool) {\n+\tinstr := m.allocateInstr()\n+\tinstr.asMOVZ(dst, v, uint64(shift), dst64)\n+\tm.insert(instr)\n+}\n+\n+func (m *machine) insertMOVK(dst regalloc.VReg, v uint64, shift int, dst64 bool) {\n+\tinstr := m.allocateInstr()\n+\tinstr.asMOVK(dst, v, uint64(shift), dst64)\n+\tm.insert(instr)\n+}\n+\n+func (m *machine) insertMOVN(dst regalloc.VReg, v uint64, shift int, dst64 bool) {\n+\tinstr := m.allocateInstr()\n+\tinstr.asMOVN(dst, v, uint64(shift), dst64)\n+\tm.insert(instr)\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/lower_instr.go",
          "status": "added",
          "additions": 2221,
          "deletions": 0,
          "patch": "@@ -0,0 +1,2221 @@\n+package arm64\n+\n+// Files prefixed as lower_instr** do the instruction selection, meaning that lowering SSA level instructions\n+// into machine specific instructions.\n+//\n+// Importantly, what the lower** functions does includes tree-matching; find the pattern from the given instruction tree,\n+// and merge the multiple instructions if possible. It can be considered as \"N:1\" instruction selection.\n+\n+import (\n+\t\"fmt\"\n+\t\"math\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+// LowerSingleBranch implements backend.Machine.\n+func (m *machine) LowerSingleBranch(br *ssa.Instruction) {\n+\tectx := m.executableContext\n+\tswitch br.Opcode() {\n+\tcase ssa.OpcodeJump:\n+\t\t_, _, targetBlk := br.BranchData()\n+\t\tif br.IsFallthroughJump() {\n+\t\t\treturn\n+\t\t}\n+\t\tb := m.allocateInstr()\n+\t\ttarget := ectx.GetOrAllocateSSABlockLabel(targetBlk)\n+\t\tif target == labelReturn {\n+\t\t\tb.asRet()\n+\t\t} else {\n+\t\t\tb.asBr(target)\n+\t\t}\n+\t\tm.insert(b)\n+\tcase ssa.OpcodeBrTable:\n+\t\tm.lowerBrTable(br)\n+\tdefault:\n+\t\tpanic(\"BUG: unexpected branch opcode\" + br.Opcode().String())\n+\t}\n+}\n+\n+func (m *machine) lowerBrTable(i *ssa.Instruction) {\n+\tindex, targets := i.BrTableData()\n+\tindexOperand := m.getOperand_NR(m.compiler.ValueDefinition(index), extModeNone)\n+\n+\t// Firstly, we have to do the bounds check of the index, and\n+\t// set it to the default target (sitting at the end of the list) if it's out of bounds.\n+\n+\t// mov  maxIndexReg #maximum_index\n+\t// subs wzr, index, maxIndexReg\n+\t// csel adjustedIndex, maxIndexReg, index, hs ;; if index is higher or equal than maxIndexReg.\n+\tmaxIndexReg := m.compiler.AllocateVReg(ssa.TypeI32)\n+\tm.lowerConstantI32(maxIndexReg, int32(len(targets)-1))\n+\tsubs := m.allocateInstr()\n+\tsubs.asALU(aluOpSubS, operandNR(xzrVReg), indexOperand, operandNR(maxIndexReg), false)\n+\tm.insert(subs)\n+\tcsel := m.allocateInstr()\n+\tadjustedIndex := m.compiler.AllocateVReg(ssa.TypeI32)\n+\tcsel.asCSel(operandNR(adjustedIndex), operandNR(maxIndexReg), indexOperand, hs, false)\n+\tm.insert(csel)\n+\n+\tbrSequence := m.allocateInstr()\n+\n+\ttableIndex := m.addJmpTableTarget(targets)\n+\tbrSequence.asBrTableSequence(adjustedIndex, tableIndex, len(targets))\n+\tm.insert(brSequence)\n+}\n+\n+// LowerConditionalBranch implements backend.Machine.\n+func (m *machine) LowerConditionalBranch(b *ssa.Instruction) {\n+\texctx := m.executableContext\n+\tcval, args, targetBlk := b.BranchData()\n+\tif len(args) > 0 {\n+\t\tpanic(fmt.Sprintf(\n+\t\t\t\"conditional branch shouldn't have args; likely a bug in critical edge splitting: from %s to %s\",\n+\t\t\texctx.CurrentSSABlk,\n+\t\t\ttargetBlk,\n+\t\t))\n+\t}\n+\n+\ttarget := exctx.GetOrAllocateSSABlockLabel(targetBlk)\n+\tcvalDef := m.compiler.ValueDefinition(cval)\n+\n+\tswitch {\n+\tcase m.compiler.MatchInstr(cvalDef, ssa.OpcodeIcmp): // This case, we can use the ALU flag set by SUBS instruction.\n+\t\tcvalInstr := cvalDef.Instr\n+\t\tx, y, c := cvalInstr.IcmpData()\n+\t\tcc, signed := condFlagFromSSAIntegerCmpCond(c), c.Signed()\n+\t\tif b.Opcode() == ssa.OpcodeBrz {\n+\t\t\tcc = cc.invert()\n+\t\t}\n+\n+\t\tif !m.tryLowerBandToFlag(x, y) {\n+\t\t\tm.lowerIcmpToFlag(x, y, signed)\n+\t\t}\n+\t\tcbr := m.allocateInstr()\n+\t\tcbr.asCondBr(cc.asCond(), target, false /* ignored */)\n+\t\tm.insert(cbr)\n+\t\tcvalDef.Instr.MarkLowered()\n+\tcase m.compiler.MatchInstr(cvalDef, ssa.OpcodeFcmp): // This case we can use the Fpu flag directly.\n+\t\tcvalInstr := cvalDef.Instr\n+\t\tx, y, c := cvalInstr.FcmpData()\n+\t\tcc := condFlagFromSSAFloatCmpCond(c)\n+\t\tif b.Opcode() == ssa.OpcodeBrz {\n+\t\t\tcc = cc.invert()\n+\t\t}\n+\t\tm.lowerFcmpToFlag(x, y)\n+\t\tcbr := m.allocateInstr()\n+\t\tcbr.asCondBr(cc.asCond(), target, false /* ignored */)\n+\t\tm.insert(cbr)\n+\t\tcvalDef.Instr.MarkLowered()\n+\tdefault:\n+\t\trn := m.getOperand_NR(cvalDef, extModeNone)\n+\t\tvar c cond\n+\t\tif b.Opcode() == ssa.OpcodeBrz {\n+\t\t\tc = registerAsRegZeroCond(rn.nr())\n+\t\t} else {\n+\t\t\tc = registerAsRegNotZeroCond(rn.nr())\n+\t\t}\n+\t\tcbr := m.allocateInstr()\n+\t\tcbr.asCondBr(c, target, false)\n+\t\tm.insert(cbr)\n+\t}\n+}\n+\n+func (m *machine) tryLowerBandToFlag(x, y ssa.Value) (ok bool) {\n+\txx := m.compiler.ValueDefinition(x)\n+\tyy := m.compiler.ValueDefinition(y)\n+\tif xx.IsFromInstr() && xx.Instr.Constant() && xx.Instr.ConstantVal() == 0 {\n+\t\tif m.compiler.MatchInstr(yy, ssa.OpcodeBand) {\n+\t\t\tbandInstr := yy.Instr\n+\t\t\tm.lowerBitwiseAluOp(bandInstr, aluOpAnds, true)\n+\t\t\tok = true\n+\t\t\tbandInstr.MarkLowered()\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\tif yy.IsFromInstr() && yy.Instr.Constant() && yy.Instr.ConstantVal() == 0 {\n+\t\tif m.compiler.MatchInstr(xx, ssa.OpcodeBand) {\n+\t\t\tbandInstr := xx.Instr\n+\t\t\tm.lowerBitwiseAluOp(bandInstr, aluOpAnds, true)\n+\t\t\tok = true\n+\t\t\tbandInstr.MarkLowered()\n+\t\t\treturn\n+\t\t}\n+\t}\n+\treturn\n+}\n+\n+// LowerInstr implements backend.Machine.\n+func (m *machine) LowerInstr(instr *ssa.Instruction) {\n+\tif l := instr.SourceOffset(); l.Valid() {\n+\t\tinfo := m.allocateInstr().asEmitSourceOffsetInfo(l)\n+\t\tm.insert(info)\n+\t}\n+\n+\tswitch op := instr.Opcode(); op {\n+\tcase ssa.OpcodeBrz, ssa.OpcodeBrnz, ssa.OpcodeJump, ssa.OpcodeBrTable:\n+\t\tpanic(\"BUG: branching instructions are handled by LowerBranches\")\n+\tcase ssa.OpcodeReturn:\n+\t\tpanic(\"BUG: return must be handled by backend.Compiler\")\n+\tcase ssa.OpcodeIadd, ssa.OpcodeIsub:\n+\t\tm.lowerSubOrAdd(instr, op == ssa.OpcodeIadd)\n+\tcase ssa.OpcodeFadd, ssa.OpcodeFsub, ssa.OpcodeFmul, ssa.OpcodeFdiv, ssa.OpcodeFmax, ssa.OpcodeFmin:\n+\t\tm.lowerFpuBinOp(instr)\n+\tcase ssa.OpcodeIconst, ssa.OpcodeF32const, ssa.OpcodeF64const: // Constant instructions are inlined.\n+\tcase ssa.OpcodeExitWithCode:\n+\t\texecCtx, code := instr.ExitWithCodeData()\n+\t\tm.lowerExitWithCode(m.compiler.VRegOf(execCtx), code)\n+\tcase ssa.OpcodeExitIfTrueWithCode:\n+\t\texecCtx, c, code := instr.ExitIfTrueWithCodeData()\n+\t\tm.lowerExitIfTrueWithCode(m.compiler.VRegOf(execCtx), c, code)\n+\tcase ssa.OpcodeStore, ssa.OpcodeIstore8, ssa.OpcodeIstore16, ssa.OpcodeIstore32:\n+\t\tm.lowerStore(instr)\n+\tcase ssa.OpcodeLoad:\n+\t\tdst := instr.Return()\n+\t\tptr, offset, typ := instr.LoadData()\n+\t\tm.lowerLoad(ptr, offset, typ, dst)\n+\tcase ssa.OpcodeVZeroExtLoad:\n+\t\tdst := instr.Return()\n+\t\tptr, offset, typ := instr.VZeroExtLoadData()\n+\t\tm.lowerLoad(ptr, offset, typ, dst)\n+\tcase ssa.OpcodeUload8, ssa.OpcodeUload16, ssa.OpcodeUload32, ssa.OpcodeSload8, ssa.OpcodeSload16, ssa.OpcodeSload32:\n+\t\tptr, offset, _ := instr.LoadData()\n+\t\tret := m.compiler.VRegOf(instr.Return())\n+\t\tm.lowerExtLoad(op, ptr, offset, ret)\n+\tcase ssa.OpcodeCall, ssa.OpcodeCallIndirect:\n+\t\tm.lowerCall(instr)\n+\tcase ssa.OpcodeIcmp:\n+\t\tm.lowerIcmp(instr)\n+\tcase ssa.OpcodeVIcmp:\n+\t\tm.lowerVIcmp(instr)\n+\tcase ssa.OpcodeVFcmp:\n+\t\tm.lowerVFcmp(instr)\n+\tcase ssa.OpcodeVCeil:\n+\t\tm.lowerVecMisc(vecOpFrintp, instr)\n+\tcase ssa.OpcodeVFloor:\n+\t\tm.lowerVecMisc(vecOpFrintm, instr)\n+\tcase ssa.OpcodeVTrunc:\n+\t\tm.lowerVecMisc(vecOpFrintz, instr)\n+\tcase ssa.OpcodeVNearest:\n+\t\tm.lowerVecMisc(vecOpFrintn, instr)\n+\tcase ssa.OpcodeVMaxPseudo:\n+\t\tm.lowerVMinMaxPseudo(instr, true)\n+\tcase ssa.OpcodeVMinPseudo:\n+\t\tm.lowerVMinMaxPseudo(instr, false)\n+\tcase ssa.OpcodeBand:\n+\t\tm.lowerBitwiseAluOp(instr, aluOpAnd, false)\n+\tcase ssa.OpcodeBor:\n+\t\tm.lowerBitwiseAluOp(instr, aluOpOrr, false)\n+\tcase ssa.OpcodeBxor:\n+\t\tm.lowerBitwiseAluOp(instr, aluOpEor, false)\n+\tcase ssa.OpcodeIshl:\n+\t\tm.lowerShifts(instr, extModeNone, aluOpLsl)\n+\tcase ssa.OpcodeSshr:\n+\t\tif instr.Return().Type().Bits() == 64 {\n+\t\t\tm.lowerShifts(instr, extModeSignExtend64, aluOpAsr)\n+\t\t} else {\n+\t\t\tm.lowerShifts(instr, extModeSignExtend32, aluOpAsr)\n+\t\t}\n+\tcase ssa.OpcodeUshr:\n+\t\tif instr.Return().Type().Bits() == 64 {\n+\t\t\tm.lowerShifts(instr, extModeZeroExtend64, aluOpLsr)\n+\t\t} else {\n+\t\t\tm.lowerShifts(instr, extModeZeroExtend32, aluOpLsr)\n+\t\t}\n+\tcase ssa.OpcodeRotl:\n+\t\tm.lowerRotl(instr)\n+\tcase ssa.OpcodeRotr:\n+\t\tm.lowerRotr(instr)\n+\tcase ssa.OpcodeSExtend, ssa.OpcodeUExtend:\n+\t\tfrom, to, signed := instr.ExtendData()\n+\t\tm.lowerExtend(instr.Arg(), instr.Return(), from, to, signed)\n+\tcase ssa.OpcodeFcmp:\n+\t\tx, y, c := instr.FcmpData()\n+\t\tm.lowerFcmp(x, y, instr.Return(), c)\n+\tcase ssa.OpcodeImul:\n+\t\tx, y := instr.Arg2()\n+\t\tresult := instr.Return()\n+\t\tm.lowerImul(x, y, result)\n+\tcase ssa.OpcodeUndefined:\n+\t\tundef := m.allocateInstr()\n+\t\tundef.asUDF()\n+\t\tm.insert(undef)\n+\tcase ssa.OpcodeSelect:\n+\t\tc, x, y := instr.SelectData()\n+\t\tif x.Type() == ssa.TypeV128 {\n+\t\t\trc := m.getOperand_NR(m.compiler.ValueDefinition(c), extModeNone)\n+\t\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\t\tm.lowerSelectVec(rc, rn, rm, rd)\n+\t\t} else {\n+\t\t\tm.lowerSelect(c, x, y, instr.Return())\n+\t\t}\n+\tcase ssa.OpcodeClz:\n+\t\tx := instr.Arg()\n+\t\tresult := instr.Return()\n+\t\tm.lowerClz(x, result)\n+\tcase ssa.OpcodeCtz:\n+\t\tx := instr.Arg()\n+\t\tresult := instr.Return()\n+\t\tm.lowerCtz(x, result)\n+\tcase ssa.OpcodePopcnt:\n+\t\tx := instr.Arg()\n+\t\tresult := instr.Return()\n+\t\tm.lowerPopcnt(x, result)\n+\tcase ssa.OpcodeFcvtToSint, ssa.OpcodeFcvtToSintSat:\n+\t\tx, ctx := instr.Arg2()\n+\t\tresult := instr.Return()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(result))\n+\t\tctxVReg := m.compiler.VRegOf(ctx)\n+\t\tm.lowerFpuToInt(rd, rn, ctxVReg, true, x.Type() == ssa.TypeF64,\n+\t\t\tresult.Type().Bits() == 64, op == ssa.OpcodeFcvtToSintSat)\n+\tcase ssa.OpcodeFcvtToUint, ssa.OpcodeFcvtToUintSat:\n+\t\tx, ctx := instr.Arg2()\n+\t\tresult := instr.Return()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(result))\n+\t\tctxVReg := m.compiler.VRegOf(ctx)\n+\t\tm.lowerFpuToInt(rd, rn, ctxVReg, false, x.Type() == ssa.TypeF64,\n+\t\t\tresult.Type().Bits() == 64, op == ssa.OpcodeFcvtToUintSat)\n+\tcase ssa.OpcodeFcvtFromSint:\n+\t\tx := instr.Arg()\n+\t\tresult := instr.Return()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(result))\n+\t\tm.lowerIntToFpu(rd, rn, true, x.Type() == ssa.TypeI64, result.Type().Bits() == 64)\n+\tcase ssa.OpcodeFcvtFromUint:\n+\t\tx := instr.Arg()\n+\t\tresult := instr.Return()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(result))\n+\t\tm.lowerIntToFpu(rd, rn, false, x.Type() == ssa.TypeI64, result.Type().Bits() == 64)\n+\tcase ssa.OpcodeFdemote:\n+\t\tv := instr.Arg()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(v), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tcnt := m.allocateInstr()\n+\t\tcnt.asFpuRR(fpuUniOpCvt64To32, rd, rn, false)\n+\t\tm.insert(cnt)\n+\tcase ssa.OpcodeFpromote:\n+\t\tv := instr.Arg()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(v), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tcnt := m.allocateInstr()\n+\t\tcnt.asFpuRR(fpuUniOpCvt32To64, rd, rn, true)\n+\t\tm.insert(cnt)\n+\tcase ssa.OpcodeIreduce:\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(instr.Arg()), extModeNone)\n+\t\tretVal := instr.Return()\n+\t\trd := m.compiler.VRegOf(retVal)\n+\n+\t\tif retVal.Type() != ssa.TypeI32 {\n+\t\t\tpanic(\"TODO?: Ireduce to non-i32\")\n+\t\t}\n+\t\tmov := m.allocateInstr()\n+\t\tmov.asMove32(rd, rn.reg())\n+\t\tm.insert(mov)\n+\tcase ssa.OpcodeFneg:\n+\t\tm.lowerFpuUniOp(fpuUniOpNeg, instr.Arg(), instr.Return())\n+\tcase ssa.OpcodeSqrt:\n+\t\tm.lowerFpuUniOp(fpuUniOpSqrt, instr.Arg(), instr.Return())\n+\tcase ssa.OpcodeCeil:\n+\t\tm.lowerFpuUniOp(fpuUniOpRoundPlus, instr.Arg(), instr.Return())\n+\tcase ssa.OpcodeFloor:\n+\t\tm.lowerFpuUniOp(fpuUniOpRoundMinus, instr.Arg(), instr.Return())\n+\tcase ssa.OpcodeTrunc:\n+\t\tm.lowerFpuUniOp(fpuUniOpRoundZero, instr.Arg(), instr.Return())\n+\tcase ssa.OpcodeNearest:\n+\t\tm.lowerFpuUniOp(fpuUniOpRoundNearest, instr.Arg(), instr.Return())\n+\tcase ssa.OpcodeFabs:\n+\t\tm.lowerFpuUniOp(fpuUniOpAbs, instr.Arg(), instr.Return())\n+\tcase ssa.OpcodeBitcast:\n+\t\tm.lowerBitcast(instr)\n+\tcase ssa.OpcodeFcopysign:\n+\t\tx, y := instr.Arg2()\n+\t\tm.lowerFcopysign(x, y, instr.Return())\n+\tcase ssa.OpcodeSdiv, ssa.OpcodeUdiv:\n+\t\tx, y, ctx := instr.Arg3()\n+\t\tctxVReg := m.compiler.VRegOf(ctx)\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.lowerIDiv(ctxVReg, rd, rn, rm, x.Type() == ssa.TypeI64, op == ssa.OpcodeSdiv)\n+\tcase ssa.OpcodeSrem, ssa.OpcodeUrem:\n+\t\tx, y, ctx := instr.Arg3()\n+\t\tctxVReg := m.compiler.VRegOf(ctx)\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.lowerIRem(ctxVReg, rd, rn, rm, x.Type() == ssa.TypeI64, op == ssa.OpcodeSrem)\n+\tcase ssa.OpcodeVconst:\n+\t\tresult := m.compiler.VRegOf(instr.Return())\n+\t\tlo, hi := instr.VconstData()\n+\t\tv := m.allocateInstr()\n+\t\tv.asLoadFpuConst128(result, lo, hi)\n+\t\tm.insert(v)\n+\tcase ssa.OpcodeVbnot:\n+\t\tx := instr.Arg()\n+\t\tins := m.allocateInstr()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tins.asVecMisc(vecOpNot, rd, rn, vecArrangement16B)\n+\t\tm.insert(ins)\n+\tcase ssa.OpcodeVbxor:\n+\t\tx, y := instr.Arg2()\n+\t\tm.lowerVecRRR(vecOpEOR, x, y, instr.Return(), vecArrangement16B)\n+\tcase ssa.OpcodeVbor:\n+\t\tx, y := instr.Arg2()\n+\t\tm.lowerVecRRR(vecOpOrr, x, y, instr.Return(), vecArrangement16B)\n+\tcase ssa.OpcodeVband:\n+\t\tx, y := instr.Arg2()\n+\t\tm.lowerVecRRR(vecOpAnd, x, y, instr.Return(), vecArrangement16B)\n+\tcase ssa.OpcodeVbandnot:\n+\t\tx, y := instr.Arg2()\n+\t\tm.lowerVecRRR(vecOpBic, x, y, instr.Return(), vecArrangement16B)\n+\tcase ssa.OpcodeVbitselect:\n+\t\tc, x, y := instr.SelectData()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\tcreg := m.getOperand_NR(m.compiler.ValueDefinition(c), extModeNone)\n+\t\ttmp := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\t\t// creg is overwritten by BSL, so we need to move it to the result register before the instruction\n+\t\t// in case when it is used somewhere else.\n+\t\tmov := m.allocateInstr()\n+\t\tmov.asFpuMov128(tmp.nr(), creg.nr())\n+\t\tm.insert(mov)\n+\n+\t\tins := m.allocateInstr()\n+\t\tins.asVecRRRRewrite(vecOpBsl, tmp, rn, rm, vecArrangement16B)\n+\t\tm.insert(ins)\n+\n+\t\tmov2 := m.allocateInstr()\n+\t\trd := m.compiler.VRegOf(instr.Return())\n+\t\tmov2.asFpuMov128(rd, tmp.nr())\n+\t\tm.insert(mov2)\n+\tcase ssa.OpcodeVanyTrue, ssa.OpcodeVallTrue:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\tvar arr vecArrangement\n+\t\tif op == ssa.OpcodeVallTrue {\n+\t\t\tarr = ssaLaneToArrangement(lane)\n+\t\t}\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.lowerVcheckTrue(op, rm, rd, arr)\n+\tcase ssa.OpcodeVhighBits:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVhighBits(rm, rd, arr)\n+\tcase ssa.OpcodeVIadd:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpAdd, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeExtIaddPairwise:\n+\t\tv, lane, signed := instr.ExtIaddPairwiseData()\n+\t\tvv := m.getOperand_NR(m.compiler.ValueDefinition(v), extModeNone)\n+\n+\t\ttmpLo, tmpHi := operandNR(m.compiler.AllocateVReg(ssa.TypeV128)), operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\t\tvar widen vecOp\n+\t\tif signed {\n+\t\t\twiden = vecOpSshll\n+\t\t} else {\n+\t\t\twiden = vecOpUshll\n+\t\t}\n+\n+\t\tvar loArr, hiArr, dstArr vecArrangement\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI8x16:\n+\t\t\tloArr, hiArr, dstArr = vecArrangement8B, vecArrangement16B, vecArrangement8H\n+\t\tcase ssa.VecLaneI16x8:\n+\t\t\tloArr, hiArr, dstArr = vecArrangement4H, vecArrangement8H, vecArrangement4S\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tloArr, hiArr, dstArr = vecArrangement2S, vecArrangement4S, vecArrangement2D\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported lane \" + lane.String())\n+\t\t}\n+\n+\t\twidenLo := m.allocateInstr().asVecShiftImm(widen, tmpLo, vv, operandShiftImm(0), loArr)\n+\t\twidenHi := m.allocateInstr().asVecShiftImm(widen, tmpHi, vv, operandShiftImm(0), hiArr)\n+\t\taddp := m.allocateInstr().asVecRRR(vecOpAddp, operandNR(m.compiler.VRegOf(instr.Return())), tmpLo, tmpHi, dstArr)\n+\t\tm.insert(widenLo)\n+\t\tm.insert(widenHi)\n+\t\tm.insert(addp)\n+\n+\tcase ssa.OpcodeVSaddSat:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpSqadd, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVUaddSat:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpUqadd, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVIsub:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpSub, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVSsubSat:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpSqsub, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVUsubSat:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpUqsub, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVImin:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpSmin, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVUmin:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpUmin, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVImax:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpSmax, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVUmax:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpUmax, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVAvgRound:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpUrhadd, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVImul:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.lowerVIMul(rd, rn, rm, arr)\n+\tcase ssa.OpcodeVIabs:\n+\t\tm.lowerVecMisc(vecOpAbs, instr)\n+\tcase ssa.OpcodeVIneg:\n+\t\tm.lowerVecMisc(vecOpNeg, instr)\n+\tcase ssa.OpcodeVIpopcnt:\n+\t\tm.lowerVecMisc(vecOpCnt, instr)\n+\tcase ssa.OpcodeVIshl,\n+\t\tssa.OpcodeVSshr, ssa.OpcodeVUshr:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.lowerVShift(op, rd, rn, rm, arr)\n+\tcase ssa.OpcodeVSqrt:\n+\t\tm.lowerVecMisc(vecOpFsqrt, instr)\n+\tcase ssa.OpcodeVFabs:\n+\t\tm.lowerVecMisc(vecOpFabs, instr)\n+\tcase ssa.OpcodeVFneg:\n+\t\tm.lowerVecMisc(vecOpFneg, instr)\n+\tcase ssa.OpcodeVFmin:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpFmin, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVFmax:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpFmax, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVFadd:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpFadd, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVFsub:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpFsub, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVFmul:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpFmul, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeSqmulRoundSat:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpSqrdmulh, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVFdiv:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\tm.lowerVecRRR(vecOpFdiv, x, y, instr.Return(), arr)\n+\tcase ssa.OpcodeVFcvtToSintSat, ssa.OpcodeVFcvtToUintSat:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.lowerVfpuToInt(rd, rn, arr, op == ssa.OpcodeVFcvtToSintSat)\n+\tcase ssa.OpcodeVFcvtFromSint, ssa.OpcodeVFcvtFromUint:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\tarr := ssaLaneToArrangement(lane)\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.lowerVfpuFromInt(rd, rn, arr, op == ssa.OpcodeVFcvtFromSint)\n+\tcase ssa.OpcodeSwidenLow, ssa.OpcodeUwidenLow:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\n+\t\tvar arr vecArrangement\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI8x16:\n+\t\t\tarr = vecArrangement8B\n+\t\tcase ssa.VecLaneI16x8:\n+\t\t\tarr = vecArrangement4H\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tarr = vecArrangement2S\n+\t\t}\n+\n+\t\tshll := m.allocateInstr()\n+\t\tif signed := op == ssa.OpcodeSwidenLow; signed {\n+\t\t\tshll.asVecShiftImm(vecOpSshll, rd, rn, operandShiftImm(0), arr)\n+\t\t} else {\n+\t\t\tshll.asVecShiftImm(vecOpUshll, rd, rn, operandShiftImm(0), arr)\n+\t\t}\n+\t\tm.insert(shll)\n+\tcase ssa.OpcodeSwidenHigh, ssa.OpcodeUwidenHigh:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\n+\t\tarr := ssaLaneToArrangement(lane)\n+\n+\t\tshll := m.allocateInstr()\n+\t\tif signed := op == ssa.OpcodeSwidenHigh; signed {\n+\t\t\tshll.asVecShiftImm(vecOpSshll, rd, rn, operandShiftImm(0), arr)\n+\t\t} else {\n+\t\t\tshll.asVecShiftImm(vecOpUshll, rd, rn, operandShiftImm(0), arr)\n+\t\t}\n+\t\tm.insert(shll)\n+\n+\tcase ssa.OpcodeSnarrow, ssa.OpcodeUnarrow:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\tvar arr, arr2 vecArrangement\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI16x8: // I16x8\n+\t\t\tarr = vecArrangement8B\n+\t\t\tarr2 = vecArrangement16B // Implies sqxtn2.\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tarr = vecArrangement4H\n+\t\t\tarr2 = vecArrangement8H // Implies sqxtn2.\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported lane \" + lane.String())\n+\t\t}\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\n+\t\ttmp := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\t\tloQxtn := m.allocateInstr()\n+\t\thiQxtn := m.allocateInstr()\n+\t\tif signed := op == ssa.OpcodeSnarrow; signed {\n+\t\t\t// Narrow lanes on rn and write them into lower-half of rd.\n+\t\t\tloQxtn.asVecMisc(vecOpSqxtn, tmp, rn, arr) // low\n+\t\t\t// Narrow lanes on rm and write them into higher-half of rd.\n+\t\t\thiQxtn.asVecMisc(vecOpSqxtn, tmp, rm, arr2) // high (sqxtn2)\n+\t\t} else {\n+\t\t\t// Narrow lanes on rn and write them into lower-half of rd.\n+\t\t\tloQxtn.asVecMisc(vecOpSqxtun, tmp, rn, arr) // low\n+\t\t\t// Narrow lanes on rm and write them into higher-half of rd.\n+\t\t\thiQxtn.asVecMisc(vecOpSqxtun, tmp, rm, arr2) // high (sqxtn2)\n+\t\t}\n+\t\tm.insert(loQxtn)\n+\t\tm.insert(hiQxtn)\n+\n+\t\tmov := m.allocateInstr()\n+\t\tmov.asFpuMov128(rd.nr(), tmp.nr())\n+\t\tm.insert(mov)\n+\tcase ssa.OpcodeFvpromoteLow:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\tif lane != ssa.VecLaneF32x4 {\n+\t\t\tpanic(\"unsupported lane type \" + lane.String())\n+\t\t}\n+\t\tins := m.allocateInstr()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tins.asVecMisc(vecOpFcvtl, rd, rn, vecArrangement2S)\n+\t\tm.insert(ins)\n+\tcase ssa.OpcodeFvdemote:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\tif lane != ssa.VecLaneF64x2 {\n+\t\t\tpanic(\"unsupported lane type \" + lane.String())\n+\t\t}\n+\t\tins := m.allocateInstr()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tins.asVecMisc(vecOpFcvtn, rd, rn, vecArrangement2S)\n+\t\tm.insert(ins)\n+\tcase ssa.OpcodeExtractlane:\n+\t\tx, index, signed, lane := instr.ExtractlaneData()\n+\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\n+\t\tmov := m.allocateInstr()\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI8x16:\n+\t\t\tmov.asMovFromVec(rd, rn, vecArrangementB, vecIndex(index), signed)\n+\t\tcase ssa.VecLaneI16x8:\n+\t\t\tmov.asMovFromVec(rd, rn, vecArrangementH, vecIndex(index), signed)\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tmov.asMovFromVec(rd, rn, vecArrangementS, vecIndex(index), signed)\n+\t\tcase ssa.VecLaneI64x2:\n+\t\t\tmov.asMovFromVec(rd, rn, vecArrangementD, vecIndex(index), signed)\n+\t\tcase ssa.VecLaneF32x4:\n+\t\t\tmov.asVecMovElement(rd, rn, vecArrangementS, vecIndex(0), vecIndex(index))\n+\t\tcase ssa.VecLaneF64x2:\n+\t\t\tmov.asVecMovElement(rd, rn, vecArrangementD, vecIndex(0), vecIndex(index))\n+\t\tdefault:\n+\t\t\tpanic(\"unsupported lane: \" + lane.String())\n+\t\t}\n+\n+\t\tm.insert(mov)\n+\n+\tcase ssa.OpcodeInsertlane:\n+\t\tx, y, index, lane := instr.InsertlaneData()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\ttmpReg := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\t\t// Initially mov rn to tmp.\n+\t\tmov1 := m.allocateInstr()\n+\t\tmov1.asFpuMov128(tmpReg.nr(), rn.nr())\n+\t\tm.insert(mov1)\n+\n+\t\t// movToVec and vecMovElement do not clear the remaining bits to zero,\n+\t\t// thus, we can mov rm in-place to tmp.\n+\t\tmov2 := m.allocateInstr()\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI8x16:\n+\t\t\tmov2.asMovToVec(tmpReg, rm, vecArrangementB, vecIndex(index))\n+\t\tcase ssa.VecLaneI16x8:\n+\t\t\tmov2.asMovToVec(tmpReg, rm, vecArrangementH, vecIndex(index))\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tmov2.asMovToVec(tmpReg, rm, vecArrangementS, vecIndex(index))\n+\t\tcase ssa.VecLaneI64x2:\n+\t\t\tmov2.asMovToVec(tmpReg, rm, vecArrangementD, vecIndex(index))\n+\t\tcase ssa.VecLaneF32x4:\n+\t\t\tmov2.asVecMovElement(tmpReg, rm, vecArrangementS, vecIndex(index), vecIndex(0))\n+\t\tcase ssa.VecLaneF64x2:\n+\t\t\tmov2.asVecMovElement(tmpReg, rm, vecArrangementD, vecIndex(index), vecIndex(0))\n+\t\t}\n+\t\tm.insert(mov2)\n+\n+\t\t// Finally mov tmp to rd.\n+\t\tmov3 := m.allocateInstr()\n+\t\tmov3.asFpuMov128(rd.nr(), tmpReg.nr())\n+\t\tm.insert(mov3)\n+\n+\tcase ssa.OpcodeSwizzle:\n+\t\tx, y, lane := instr.Arg2WithLane()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\n+\t\tarr := ssaLaneToArrangement(lane)\n+\n+\t\t// tbl <rd>.<arr>, { <rn>.<arr> }, <rm>.<arr>\n+\t\ttbl1 := m.allocateInstr()\n+\t\ttbl1.asVecTbl(1, rd, rn, rm, arr)\n+\t\tm.insert(tbl1)\n+\n+\tcase ssa.OpcodeShuffle:\n+\t\tx, y, lane1, lane2 := instr.ShuffleData()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\n+\t\tm.lowerShuffle(rd, rn, rm, lane1, lane2)\n+\n+\tcase ssa.OpcodeSplat:\n+\t\tx, lane := instr.ArgWithLane()\n+\t\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\n+\t\tdup := m.allocateInstr()\n+\t\tswitch lane {\n+\t\tcase ssa.VecLaneI8x16:\n+\t\t\tdup.asVecDup(rd, rn, vecArrangement16B)\n+\t\tcase ssa.VecLaneI16x8:\n+\t\t\tdup.asVecDup(rd, rn, vecArrangement8H)\n+\t\tcase ssa.VecLaneI32x4:\n+\t\t\tdup.asVecDup(rd, rn, vecArrangement4S)\n+\t\tcase ssa.VecLaneI64x2:\n+\t\t\tdup.asVecDup(rd, rn, vecArrangement2D)\n+\t\tcase ssa.VecLaneF32x4:\n+\t\t\tdup.asVecDupElement(rd, rn, vecArrangementS, vecIndex(0))\n+\t\tcase ssa.VecLaneF64x2:\n+\t\t\tdup.asVecDupElement(rd, rn, vecArrangementD, vecIndex(0))\n+\t\t}\n+\t\tm.insert(dup)\n+\n+\tcase ssa.OpcodeWideningPairwiseDotProductS:\n+\t\tx, y := instr.Arg2()\n+\t\txx, yy := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone),\n+\t\t\tm.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\t\ttmp, tmp2 := operandNR(m.compiler.AllocateVReg(ssa.TypeV128)), operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\t\tm.insert(m.allocateInstr().asVecRRR(vecOpSmull, tmp, xx, yy, vecArrangement8H))\n+\t\tm.insert(m.allocateInstr().asVecRRR(vecOpSmull2, tmp2, xx, yy, vecArrangement8H))\n+\t\tm.insert(m.allocateInstr().asVecRRR(vecOpAddp, tmp, tmp, tmp2, vecArrangement4S))\n+\n+\t\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\t\tm.insert(m.allocateInstr().asFpuMov128(rd.nr(), tmp.nr()))\n+\n+\tcase ssa.OpcodeLoadSplat:\n+\t\tptr, offset, lane := instr.LoadSplatData()\n+\t\tm.lowerLoadSplat(ptr, offset, lane, instr.Return())\n+\n+\tcase ssa.OpcodeAtomicRmw:\n+\t\tm.lowerAtomicRmw(instr)\n+\n+\tcase ssa.OpcodeAtomicCas:\n+\t\tm.lowerAtomicCas(instr)\n+\n+\tcase ssa.OpcodeAtomicLoad:\n+\t\tm.lowerAtomicLoad(instr)\n+\n+\tcase ssa.OpcodeAtomicStore:\n+\t\tm.lowerAtomicStore(instr)\n+\n+\tcase ssa.OpcodeFence:\n+\t\tinstr := m.allocateInstr()\n+\t\tinstr.asDMB()\n+\t\tm.insert(instr)\n+\n+\tdefault:\n+\t\tpanic(\"TODO: lowering \" + op.String())\n+\t}\n+\tm.executableContext.FlushPendingInstructions()\n+}\n+\n+func (m *machine) lowerShuffle(rd, rn, rm operand, lane1, lane2 uint64) {\n+\t// `tbl2` requires 2 consecutive registers, so we arbitrarily pick v29, v30.\n+\tvReg, wReg := v29VReg, v30VReg\n+\n+\t// Initialize v29, v30 to rn, rm.\n+\tmovv := m.allocateInstr()\n+\tmovv.asFpuMov128(vReg, rn.nr())\n+\tm.insert(movv)\n+\n+\tmovw := m.allocateInstr()\n+\tmovw.asFpuMov128(wReg, rm.nr())\n+\tm.insert(movw)\n+\n+\t// `lane1`, `lane2` are already encoded as two u64s with the right layout:\n+\t//     lane1 := lane[7]<<56 | ... | lane[1]<<8 | lane[0]\n+\t//     lane2 := lane[15]<<56 | ... | lane[9]<<8 | lane[8]\n+\t// Thus, we can use loadFpuConst128.\n+\ttmp := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\tlfc := m.allocateInstr()\n+\tlfc.asLoadFpuConst128(tmp.nr(), lane1, lane2)\n+\tm.insert(lfc)\n+\n+\t// tbl <rd>.16b, { <vReg>.16B, <wReg>.16b }, <tmp>.16b\n+\ttbl2 := m.allocateInstr()\n+\ttbl2.asVecTbl(2, rd, operandNR(vReg), tmp, vecArrangement16B)\n+\tm.insert(tbl2)\n+}\n+\n+func (m *machine) lowerVShift(op ssa.Opcode, rd, rn, rm operand, arr vecArrangement) {\n+\tvar modulo byte\n+\tswitch arr {\n+\tcase vecArrangement16B:\n+\t\tmodulo = 0x7 // Modulo 8.\n+\tcase vecArrangement8H:\n+\t\tmodulo = 0xf // Modulo 16.\n+\tcase vecArrangement4S:\n+\t\tmodulo = 0x1f // Modulo 32.\n+\tcase vecArrangement2D:\n+\t\tmodulo = 0x3f // Modulo 64.\n+\tdefault:\n+\t\tpanic(\"unsupported arrangment \" + arr.String())\n+\t}\n+\n+\trtmp := operandNR(m.compiler.AllocateVReg(ssa.TypeI64))\n+\tvtmp := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\tand := m.allocateInstr()\n+\tand.asALUBitmaskImm(aluOpAnd, rtmp.nr(), rm.nr(), uint64(modulo), true)\n+\tm.insert(and)\n+\n+\tif op != ssa.OpcodeVIshl {\n+\t\t// Negate the amount to make this as right shift.\n+\t\tneg := m.allocateInstr()\n+\t\tneg.asALU(aluOpSub, rtmp, operandNR(xzrVReg), rtmp, true)\n+\t\tm.insert(neg)\n+\t}\n+\n+\t// Copy the shift amount into a vector register as sshl/ushl requires it to be there.\n+\tdup := m.allocateInstr()\n+\tdup.asVecDup(vtmp, rtmp, arr)\n+\tm.insert(dup)\n+\n+\tif op == ssa.OpcodeVIshl || op == ssa.OpcodeVSshr {\n+\t\tsshl := m.allocateInstr()\n+\t\tsshl.asVecRRR(vecOpSshl, rd, rn, vtmp, arr)\n+\t\tm.insert(sshl)\n+\t} else {\n+\t\tushl := m.allocateInstr()\n+\t\tushl.asVecRRR(vecOpUshl, rd, rn, vtmp, arr)\n+\t\tm.insert(ushl)\n+\t}\n+}\n+\n+func (m *machine) lowerVcheckTrue(op ssa.Opcode, rm, rd operand, arr vecArrangement) {\n+\ttmp := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\t// Special case VallTrue for i64x2.\n+\tif op == ssa.OpcodeVallTrue && arr == vecArrangement2D {\n+\t\t// \tcmeq v3?.2d, v2?.2d, #0\n+\t\t//\taddp v3?.2d, v3?.2d, v3?.2d\n+\t\t//\tfcmp v3?, v3?\n+\t\t//\tcset dst, eq\n+\n+\t\tins := m.allocateInstr()\n+\t\tins.asVecMisc(vecOpCmeq0, tmp, rm, vecArrangement2D)\n+\t\tm.insert(ins)\n+\n+\t\taddp := m.allocateInstr()\n+\t\taddp.asVecRRR(vecOpAddp, tmp, tmp, tmp, vecArrangement2D)\n+\t\tm.insert(addp)\n+\n+\t\tfcmp := m.allocateInstr()\n+\t\tfcmp.asFpuCmp(tmp, tmp, true)\n+\t\tm.insert(fcmp)\n+\n+\t\tcset := m.allocateInstr()\n+\t\tcset.asCSet(rd.nr(), false, eq)\n+\t\tm.insert(cset)\n+\n+\t\treturn\n+\t}\n+\n+\t// Create a scalar value with umaxp or uminv, then compare it against zero.\n+\tins := m.allocateInstr()\n+\tif op == ssa.OpcodeVanyTrue {\n+\t\t// \tumaxp v4?.16b, v2?.16b, v2?.16b\n+\t\tins.asVecRRR(vecOpUmaxp, tmp, rm, rm, vecArrangement16B)\n+\t} else {\n+\t\t// \tuminv d4?, v2?.4s\n+\t\tins.asVecLanes(vecOpUminv, tmp, rm, arr)\n+\t}\n+\tm.insert(ins)\n+\n+\t//\tmov x3?, v4?.d[0]\n+\t//\tccmp x3?, #0x0, #0x0, al\n+\t//\tcset x3?, ne\n+\t//\tmov x0, x3?\n+\n+\tmovv := m.allocateInstr()\n+\tmovv.asMovFromVec(rd, tmp, vecArrangementD, vecIndex(0), false)\n+\tm.insert(movv)\n+\n+\tfc := m.allocateInstr()\n+\tfc.asCCmpImm(rd, uint64(0), al, 0, true)\n+\tm.insert(fc)\n+\n+\tcset := m.allocateInstr()\n+\tcset.asCSet(rd.nr(), false, ne)\n+\tm.insert(cset)\n+}\n+\n+func (m *machine) lowerVhighBits(rm, rd operand, arr vecArrangement) {\n+\tr0 := operandNR(m.compiler.AllocateVReg(ssa.TypeI64))\n+\tv0 := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\tv1 := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\tswitch arr {\n+\tcase vecArrangement16B:\n+\t\t//\tsshr v6?.16b, v2?.16b, #7\n+\t\t//\tmovz x4?, #0x201, lsl 0\n+\t\t//\tmovk x4?, #0x804, lsl 16\n+\t\t//\tmovk x4?, #0x2010, lsl 32\n+\t\t//\tmovk x4?, #0x8040, lsl 48\n+\t\t//\tdup v5?.2d, x4?\n+\t\t//\tand v6?.16b, v6?.16b, v5?.16b\n+\t\t//\text v5?.16b, v6?.16b, v6?.16b, #8\n+\t\t//\tzip1 v5?.16b, v6?.16b, v5?.16b\n+\t\t//\taddv s5?, v5?.8h\n+\t\t//\tumov s3?, v5?.h[0]\n+\n+\t\t// Right arithmetic shift on the original vector and store the result into v1. So we have:\n+\t\t// v1[i] = 0xff if vi<0, 0 otherwise.\n+\t\tsshr := m.allocateInstr()\n+\t\tsshr.asVecShiftImm(vecOpSshr, v1, rm, operandShiftImm(7), vecArrangement16B)\n+\t\tm.insert(sshr)\n+\n+\t\t// Load the bit mask into r0.\n+\t\tm.insertMOVZ(r0.nr(), 0x0201, 0, true)\n+\t\tm.insertMOVK(r0.nr(), 0x0804, 1, true)\n+\t\tm.insertMOVK(r0.nr(), 0x2010, 2, true)\n+\t\tm.insertMOVK(r0.nr(), 0x8040, 3, true)\n+\n+\t\t// dup r0 to v0.\n+\t\tdup := m.allocateInstr()\n+\t\tdup.asVecDup(v0, r0, vecArrangement2D)\n+\t\tm.insert(dup)\n+\n+\t\t// Lane-wise logical AND with the bit mask, meaning that we have\n+\t\t// v[i] = (1 << i) if vi<0, 0 otherwise.\n+\t\t//\n+\t\t// Below, we use the following notation:\n+\t\t// wi := (1 << i) if vi<0, 0 otherwise.\n+\t\tand := m.allocateInstr()\n+\t\tand.asVecRRR(vecOpAnd, v1, v1, v0, vecArrangement16B)\n+\t\tm.insert(and)\n+\n+\t\t// Swap the lower and higher 8 byte elements, and write it into v0, meaning that we have\n+\t\t// v0[i] = w(i+8) if i < 8, w(i-8) otherwise.\n+\t\text := m.allocateInstr()\n+\t\text.asVecExtract(v0, v1, v1, vecArrangement16B, uint32(8))\n+\t\tm.insert(ext)\n+\n+\t\t// v = [w0, w8, ..., w7, w15]\n+\t\tzip1 := m.allocateInstr()\n+\t\tzip1.asVecPermute(vecOpZip1, v0, v1, v0, vecArrangement16B)\n+\t\tm.insert(zip1)\n+\n+\t\t// v.h[0] = w0 + ... + w15\n+\t\taddv := m.allocateInstr()\n+\t\taddv.asVecLanes(vecOpAddv, v0, v0, vecArrangement8H)\n+\t\tm.insert(addv)\n+\n+\t\t// Extract the v.h[0] as the result.\n+\t\tmovfv := m.allocateInstr()\n+\t\tmovfv.asMovFromVec(rd, v0, vecArrangementH, vecIndex(0), false)\n+\t\tm.insert(movfv)\n+\tcase vecArrangement8H:\n+\t\t//\tsshr v6?.8h, v2?.8h, #15\n+\t\t//\tmovz x4?, #0x1, lsl 0\n+\t\t//\tmovk x4?, #0x2, lsl 16\n+\t\t//\tmovk x4?, #0x4, lsl 32\n+\t\t//\tmovk x4?, #0x8, lsl 48\n+\t\t//\tdup v5?.2d, x4?\n+\t\t//\tlsl x4?, x4?, 0x4\n+\t\t//\tins v5?.d[1], x4?\n+\t\t//\tand v5?.16b, v6?.16b, v5?.16b\n+\t\t//\taddv s5?, v5?.8h\n+\t\t//\tumov s3?, v5?.h[0]\n+\n+\t\t// Right arithmetic shift on the original vector and store the result into v1. So we have:\n+\t\t// v[i] = 0xffff if vi<0, 0 otherwise.\n+\t\tsshr := m.allocateInstr()\n+\t\tsshr.asVecShiftImm(vecOpSshr, v1, rm, operandShiftImm(15), vecArrangement8H)\n+\t\tm.insert(sshr)\n+\n+\t\t// Load the bit mask into r0.\n+\t\tm.lowerConstantI64(r0.nr(), 0x0008000400020001)\n+\n+\t\t// dup r0 to vector v0.\n+\t\tdup := m.allocateInstr()\n+\t\tdup.asVecDup(v0, r0, vecArrangement2D)\n+\t\tm.insert(dup)\n+\n+\t\tlsl := m.allocateInstr()\n+\t\tlsl.asALUShift(aluOpLsl, r0, r0, operandShiftImm(4), true)\n+\t\tm.insert(lsl)\n+\n+\t\tmovv := m.allocateInstr()\n+\t\tmovv.asMovToVec(v0, r0, vecArrangementD, vecIndex(1))\n+\t\tm.insert(movv)\n+\n+\t\t// Lane-wise logical AND with the bitmask, meaning that we have\n+\t\t// v[i] = (1 << i)     if vi<0, 0 otherwise for i=0..3\n+\t\t//      = (1 << (i+4)) if vi<0, 0 otherwise for i=3..7\n+\t\tand := m.allocateInstr()\n+\t\tand.asVecRRR(vecOpAnd, v0, v1, v0, vecArrangement16B)\n+\t\tm.insert(and)\n+\n+\t\taddv := m.allocateInstr()\n+\t\taddv.asVecLanes(vecOpAddv, v0, v0, vecArrangement8H)\n+\t\tm.insert(addv)\n+\n+\t\tmovfv := m.allocateInstr()\n+\t\tmovfv.asMovFromVec(rd, v0, vecArrangementH, vecIndex(0), false)\n+\t\tm.insert(movfv)\n+\tcase vecArrangement4S:\n+\t\t// \tsshr v6?.8h, v2?.8h, #15\n+\t\t//\tmovz x4?, #0x1, lsl 0\n+\t\t//\tmovk x4?, #0x2, lsl 16\n+\t\t//\tmovk x4?, #0x4, lsl 32\n+\t\t//\tmovk x4?, #0x8, lsl 48\n+\t\t//\tdup v5?.2d, x4?\n+\t\t//\tlsl x4?, x4?, 0x4\n+\t\t//\tins v5?.d[1], x4?\n+\t\t//\tand v5?.16b, v6?.16b, v5?.16b\n+\t\t//\taddv s5?, v5?.8h\n+\t\t//\tumov s3?, v5?.h[0]\n+\n+\t\t// Right arithmetic shift on the original vector and store the result into v1. So we have:\n+\t\t// v[i] = 0xffffffff if vi<0, 0 otherwise.\n+\t\tsshr := m.allocateInstr()\n+\t\tsshr.asVecShiftImm(vecOpSshr, v1, rm, operandShiftImm(31), vecArrangement4S)\n+\t\tm.insert(sshr)\n+\n+\t\t// Load the bit mask into r0.\n+\t\tm.lowerConstantI64(r0.nr(), 0x0000000200000001)\n+\n+\t\t// dup r0 to vector v0.\n+\t\tdup := m.allocateInstr()\n+\t\tdup.asVecDup(v0, r0, vecArrangement2D)\n+\t\tm.insert(dup)\n+\n+\t\tlsl := m.allocateInstr()\n+\t\tlsl.asALUShift(aluOpLsl, r0, r0, operandShiftImm(2), true)\n+\t\tm.insert(lsl)\n+\n+\t\tmovv := m.allocateInstr()\n+\t\tmovv.asMovToVec(v0, r0, vecArrangementD, vecIndex(1))\n+\t\tm.insert(movv)\n+\n+\t\t// Lane-wise logical AND with the bitmask, meaning that we have\n+\t\t// v[i] = (1 << i)     if vi<0, 0 otherwise for i in [0, 1]\n+\t\t//      = (1 << (i+4)) if vi<0, 0 otherwise for i in [2, 3]\n+\t\tand := m.allocateInstr()\n+\t\tand.asVecRRR(vecOpAnd, v0, v1, v0, vecArrangement16B)\n+\t\tm.insert(and)\n+\n+\t\taddv := m.allocateInstr()\n+\t\taddv.asVecLanes(vecOpAddv, v0, v0, vecArrangement4S)\n+\t\tm.insert(addv)\n+\n+\t\tmovfv := m.allocateInstr()\n+\t\tmovfv.asMovFromVec(rd, v0, vecArrangementS, vecIndex(0), false)\n+\t\tm.insert(movfv)\n+\tcase vecArrangement2D:\n+\t\t// \tmov d3?, v2?.d[0]\n+\t\t//\tmov x4?, v2?.d[1]\n+\t\t//\tlsr x4?, x4?, 0x3f\n+\t\t//\tlsr d3?, d3?, 0x3f\n+\t\t//\tadd s3?, s3?, w4?, lsl #1\n+\n+\t\t// Move the lower 64-bit int into result.\n+\t\tmovv0 := m.allocateInstr()\n+\t\tmovv0.asMovFromVec(rd, rm, vecArrangementD, vecIndex(0), false)\n+\t\tm.insert(movv0)\n+\n+\t\t// Move the higher 64-bit int into r0.\n+\t\tmovv1 := m.allocateInstr()\n+\t\tmovv1.asMovFromVec(r0, rm, vecArrangementD, vecIndex(1), false)\n+\t\tm.insert(movv1)\n+\n+\t\t// Move the sign bit into the least significant bit.\n+\t\tlsr1 := m.allocateInstr()\n+\t\tlsr1.asALUShift(aluOpLsr, r0, r0, operandShiftImm(63), true)\n+\t\tm.insert(lsr1)\n+\n+\t\tlsr2 := m.allocateInstr()\n+\t\tlsr2.asALUShift(aluOpLsr, rd, rd, operandShiftImm(63), true)\n+\t\tm.insert(lsr2)\n+\n+\t\t// rd = (r0<<1) | rd\n+\t\tlsl := m.allocateInstr()\n+\t\tlsl.asALU(aluOpAdd, rd, rd, operandSR(r0.nr(), 1, shiftOpLSL), false)\n+\t\tm.insert(lsl)\n+\tdefault:\n+\t\tpanic(\"Unsupported \" + arr.String())\n+\t}\n+}\n+\n+func (m *machine) lowerVecMisc(op vecOp, instr *ssa.Instruction) {\n+\tx, lane := instr.ArgWithLane()\n+\tarr := ssaLaneToArrangement(lane)\n+\tins := m.allocateInstr()\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\tins.asVecMisc(op, rd, rn, arr)\n+\tm.insert(ins)\n+}\n+\n+func (m *machine) lowerVecRRR(op vecOp, x, y, ret ssa.Value, arr vecArrangement) {\n+\tins := m.allocateInstr()\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(ret))\n+\tins.asVecRRR(op, rd, rn, rm, arr)\n+\tm.insert(ins)\n+}\n+\n+func (m *machine) lowerVIMul(rd, rn, rm operand, arr vecArrangement) {\n+\tif arr != vecArrangement2D {\n+\t\tmul := m.allocateInstr()\n+\t\tmul.asVecRRR(vecOpMul, rd, rn, rm, arr)\n+\t\tm.insert(mul)\n+\t} else {\n+\t\ttmp1 := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\t\ttmp2 := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\t\ttmp3 := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\t\ttmpRes := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\t\t// Following the algorithm in https://chromium-review.googlesource.com/c/v8/v8/+/1781696\n+\t\trev64 := m.allocateInstr()\n+\t\trev64.asVecMisc(vecOpRev64, tmp2, rm, vecArrangement4S)\n+\t\tm.insert(rev64)\n+\n+\t\tmul := m.allocateInstr()\n+\t\tmul.asVecRRR(vecOpMul, tmp2, tmp2, rn, vecArrangement4S)\n+\t\tm.insert(mul)\n+\n+\t\txtn1 := m.allocateInstr()\n+\t\txtn1.asVecMisc(vecOpXtn, tmp1, rn, vecArrangement2S)\n+\t\tm.insert(xtn1)\n+\n+\t\taddp := m.allocateInstr()\n+\t\taddp.asVecRRR(vecOpAddp, tmp2, tmp2, tmp2, vecArrangement4S)\n+\t\tm.insert(addp)\n+\n+\t\txtn2 := m.allocateInstr()\n+\t\txtn2.asVecMisc(vecOpXtn, tmp3, rm, vecArrangement2S)\n+\t\tm.insert(xtn2)\n+\n+\t\t// Note: do not write the result directly into result yet. This is the same reason as in bsl.\n+\t\t// In short, in UMLAL instruction, the result register is also one of the source register, and\n+\t\t// the value on the result register is significant.\n+\t\tshll := m.allocateInstr()\n+\t\tshll.asVecMisc(vecOpShll, tmpRes, tmp2, vecArrangement2S)\n+\t\tm.insert(shll)\n+\n+\t\tumlal := m.allocateInstr()\n+\t\tumlal.asVecRRRRewrite(vecOpUmlal, tmpRes, tmp3, tmp1, vecArrangement2S)\n+\t\tm.insert(umlal)\n+\n+\t\tmov := m.allocateInstr()\n+\t\tmov.asFpuMov128(rd.nr(), tmpRes.nr())\n+\t\tm.insert(mov)\n+\t}\n+}\n+\n+func (m *machine) lowerVMinMaxPseudo(instr *ssa.Instruction, max bool) {\n+\tx, y, lane := instr.Arg2WithLane()\n+\tarr := ssaLaneToArrangement(lane)\n+\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\n+\t// Note: this usage of tmp is important.\n+\t// BSL modifies the destination register, so we need to use a temporary register so that\n+\t// the actual definition of the destination register happens *after* the BSL instruction.\n+\t// That way, we can force the spill instruction to be inserted after the BSL instruction.\n+\ttmp := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\n+\tfcmgt := m.allocateInstr()\n+\tif max {\n+\t\tfcmgt.asVecRRR(vecOpFcmgt, tmp, rm, rn, arr)\n+\t} else {\n+\t\t// If min, swap the args.\n+\t\tfcmgt.asVecRRR(vecOpFcmgt, tmp, rn, rm, arr)\n+\t}\n+\tm.insert(fcmgt)\n+\n+\tbsl := m.allocateInstr()\n+\tbsl.asVecRRRRewrite(vecOpBsl, tmp, rm, rn, vecArrangement16B)\n+\tm.insert(bsl)\n+\n+\tres := operandNR(m.compiler.VRegOf(instr.Return()))\n+\tmov2 := m.allocateInstr()\n+\tmov2.asFpuMov128(res.nr(), tmp.nr())\n+\tm.insert(mov2)\n+}\n+\n+func (m *machine) lowerIRem(execCtxVReg regalloc.VReg, rd, rn, rm operand, _64bit, signed bool) {\n+\tdiv := m.allocateInstr()\n+\n+\tif signed {\n+\t\tdiv.asALU(aluOpSDiv, rd, rn, rm, _64bit)\n+\t} else {\n+\t\tdiv.asALU(aluOpUDiv, rd, rn, rm, _64bit)\n+\t}\n+\tm.insert(div)\n+\n+\t// Check if rm is zero:\n+\tm.exitIfNot(execCtxVReg, registerAsRegNotZeroCond(rm.nr()), _64bit, wazevoapi.ExitCodeIntegerDivisionByZero)\n+\n+\t// rd = rn-rd*rm by MSUB instruction.\n+\tmsub := m.allocateInstr()\n+\tmsub.asALURRRR(aluOpMSub, rd, rd, rm, rn, _64bit)\n+\tm.insert(msub)\n+}\n+\n+func (m *machine) lowerIDiv(execCtxVReg regalloc.VReg, rd, rn, rm operand, _64bit, signed bool) {\n+\tdiv := m.allocateInstr()\n+\n+\tif signed {\n+\t\tdiv.asALU(aluOpSDiv, rd, rn, rm, _64bit)\n+\t} else {\n+\t\tdiv.asALU(aluOpUDiv, rd, rn, rm, _64bit)\n+\t}\n+\tm.insert(div)\n+\n+\t// Check if rm is zero:\n+\tm.exitIfNot(execCtxVReg, registerAsRegNotZeroCond(rm.nr()), _64bit, wazevoapi.ExitCodeIntegerDivisionByZero)\n+\n+\tif signed {\n+\t\t// We need to check the signed overflow which happens iff \"math.MinInt{32,64} / -1\"\n+\t\tminusOneCheck := m.allocateInstr()\n+\t\t// Sets eq condition if rm == -1.\n+\t\tminusOneCheck.asALU(aluOpAddS, operandNR(xzrVReg), rm, operandImm12(1, 0), _64bit)\n+\t\tm.insert(minusOneCheck)\n+\n+\t\tccmp := m.allocateInstr()\n+\t\t// If eq condition is set, sets the flag by the result based on \"rn - 1\", otherwise clears the flag.\n+\t\tccmp.asCCmpImm(rn, 1, eq, 0, _64bit)\n+\t\tm.insert(ccmp)\n+\n+\t\t// Check the overflow flag.\n+\t\tm.exitIfNot(execCtxVReg, vs.invert().asCond(), false, wazevoapi.ExitCodeIntegerOverflow)\n+\t}\n+}\n+\n+// exitIfNot emits a conditional branch to exit if the condition is not met.\n+// If `c` (cond type) is a register, `cond64bit` must be chosen to indicate whether the register is 32-bit or 64-bit.\n+// Otherwise, `cond64bit` is ignored.\n+func (m *machine) exitIfNot(execCtxVReg regalloc.VReg, c cond, cond64bit bool, code wazevoapi.ExitCode) {\n+\texecCtxTmp := m.copyToTmp(execCtxVReg)\n+\n+\tcbr := m.allocateInstr()\n+\tm.insert(cbr)\n+\tm.lowerExitWithCode(execCtxTmp, code)\n+\t// Conditional branch target is after exit.\n+\tl := m.insertBrTargetLabel()\n+\tcbr.asCondBr(c, l, cond64bit)\n+}\n+\n+func (m *machine) lowerFcopysign(x, y, ret ssa.Value) {\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\tvar tmpI, tmpF operand\n+\t_64 := x.Type() == ssa.TypeF64\n+\tif _64 {\n+\t\ttmpF = operandNR(m.compiler.AllocateVReg(ssa.TypeF64))\n+\t\ttmpI = operandNR(m.compiler.AllocateVReg(ssa.TypeI64))\n+\t} else {\n+\t\ttmpF = operandNR(m.compiler.AllocateVReg(ssa.TypeF32))\n+\t\ttmpI = operandNR(m.compiler.AllocateVReg(ssa.TypeI32))\n+\t}\n+\trd := m.compiler.VRegOf(ret)\n+\tm.lowerFcopysignImpl(operandNR(rd), rn, rm, tmpI, tmpF, _64)\n+}\n+\n+func (m *machine) lowerFcopysignImpl(rd, rn, rm, tmpI, tmpF operand, _64bit bool) {\n+\t// This is exactly the same code emitted by GCC for \"__builtin_copysign\":\n+\t//\n+\t//    mov     x0, -9223372036854775808\n+\t//    fmov    d2, x0\n+\t//    vbit    v0.8b, v1.8b, v2.8b\n+\t//\n+\n+\tsetMSB := m.allocateInstr()\n+\tif _64bit {\n+\t\tm.lowerConstantI64(tmpI.nr(), math.MinInt64)\n+\t\tsetMSB.asMovToVec(tmpF, tmpI, vecArrangementD, vecIndex(0))\n+\t} else {\n+\t\tm.lowerConstantI32(tmpI.nr(), math.MinInt32)\n+\t\tsetMSB.asMovToVec(tmpF, tmpI, vecArrangementS, vecIndex(0))\n+\t}\n+\tm.insert(setMSB)\n+\n+\ttmpReg := operandNR(m.compiler.AllocateVReg(ssa.TypeF64))\n+\n+\tmov := m.allocateInstr()\n+\tmov.asFpuMov64(tmpReg.nr(), rn.nr())\n+\tm.insert(mov)\n+\n+\tvbit := m.allocateInstr()\n+\tvbit.asVecRRRRewrite(vecOpBit, tmpReg, rm, tmpF, vecArrangement8B)\n+\tm.insert(vbit)\n+\n+\tmovDst := m.allocateInstr()\n+\tmovDst.asFpuMov64(rd.nr(), tmpReg.nr())\n+\tm.insert(movDst)\n+}\n+\n+func (m *machine) lowerBitcast(instr *ssa.Instruction) {\n+\tv, dstType := instr.BitcastData()\n+\tsrcType := v.Type()\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(v), extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(instr.Return()))\n+\tsrcInt := srcType.IsInt()\n+\tdstInt := dstType.IsInt()\n+\tswitch {\n+\tcase srcInt && !dstInt: // Int to Float:\n+\t\tmov := m.allocateInstr()\n+\t\tvar arr vecArrangement\n+\t\tif srcType.Bits() == 64 {\n+\t\t\tarr = vecArrangementD\n+\t\t} else {\n+\t\t\tarr = vecArrangementS\n+\t\t}\n+\t\tmov.asMovToVec(rd, rn, arr, vecIndex(0))\n+\t\tm.insert(mov)\n+\tcase !srcInt && dstInt: // Float to Int:\n+\t\tmov := m.allocateInstr()\n+\t\tvar arr vecArrangement\n+\t\tif dstType.Bits() == 64 {\n+\t\t\tarr = vecArrangementD\n+\t\t} else {\n+\t\t\tarr = vecArrangementS\n+\t\t}\n+\t\tmov.asMovFromVec(rd, rn, arr, vecIndex(0), false)\n+\t\tm.insert(mov)\n+\tdefault:\n+\t\tpanic(\"TODO?BUG?\")\n+\t}\n+}\n+\n+func (m *machine) lowerFpuUniOp(op fpuUniOp, in, out ssa.Value) {\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(in), extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(out))\n+\n+\tneg := m.allocateInstr()\n+\tneg.asFpuRR(op, rd, rn, in.Type().Bits() == 64)\n+\tm.insert(neg)\n+}\n+\n+func (m *machine) lowerFpuToInt(rd, rn operand, ctx regalloc.VReg, signed, src64bit, dst64bit, nonTrapping bool) {\n+\tif !nonTrapping {\n+\t\t// First of all, we have to clear the FPU flags.\n+\t\tflagClear := m.allocateInstr()\n+\t\tflagClear.asMovToFPSR(xzrVReg)\n+\t\tm.insert(flagClear)\n+\t}\n+\n+\t// Then, do the conversion which doesn't trap inherently.\n+\tcvt := m.allocateInstr()\n+\tcvt.asFpuToInt(rd, rn, signed, src64bit, dst64bit)\n+\tm.insert(cvt)\n+\n+\tif !nonTrapping {\n+\t\ttmpReg := m.compiler.AllocateVReg(ssa.TypeI64)\n+\n+\t\t// After the conversion, check the FPU flags.\n+\t\tgetFlag := m.allocateInstr()\n+\t\tgetFlag.asMovFromFPSR(tmpReg)\n+\t\tm.insert(getFlag)\n+\n+\t\texecCtx := m.copyToTmp(ctx)\n+\t\t_rn := operandNR(m.copyToTmp(rn.nr()))\n+\n+\t\t// Check if the conversion was undefined by comparing the status with 1.\n+\t\t// See https://developer.arm.com/documentation/ddi0595/2020-12/AArch64-Registers/FPSR--Floating-point-Status-Register\n+\t\talu := m.allocateInstr()\n+\t\talu.asALU(aluOpSubS, operandNR(xzrVReg), operandNR(tmpReg), operandImm12(1, 0), true)\n+\t\tm.insert(alu)\n+\n+\t\t// If it is not undefined, we can return the result.\n+\t\tok := m.allocateInstr()\n+\t\tm.insert(ok)\n+\n+\t\t// Otherwise, we have to choose the status depending on it is overflow or NaN conversion.\n+\n+\t\t// Comparing itself to check if it is a NaN.\n+\t\tfpuCmp := m.allocateInstr()\n+\t\tfpuCmp.asFpuCmp(_rn, _rn, src64bit)\n+\t\tm.insert(fpuCmp)\n+\t\t// If the VC flag is not set (== VS flag is set), it is a NaN.\n+\t\tm.exitIfNot(execCtx, vc.asCond(), false, wazevoapi.ExitCodeInvalidConversionToInteger)\n+\t\t// Otherwise, it is an overflow.\n+\t\tm.lowerExitWithCode(execCtx, wazevoapi.ExitCodeIntegerOverflow)\n+\n+\t\t// Conditional branch target is after exit.\n+\t\tl := m.insertBrTargetLabel()\n+\t\tok.asCondBr(ne.asCond(), l, false /* ignored */)\n+\t}\n+}\n+\n+func (m *machine) lowerIntToFpu(rd, rn operand, signed, src64bit, dst64bit bool) {\n+\tcvt := m.allocateInstr()\n+\tcvt.asIntToFpu(rd, rn, signed, src64bit, dst64bit)\n+\tm.insert(cvt)\n+}\n+\n+func (m *machine) lowerFpuBinOp(si *ssa.Instruction) {\n+\tinstr := m.allocateInstr()\n+\tvar op fpuBinOp\n+\tswitch si.Opcode() {\n+\tcase ssa.OpcodeFadd:\n+\t\top = fpuBinOpAdd\n+\tcase ssa.OpcodeFsub:\n+\t\top = fpuBinOpSub\n+\tcase ssa.OpcodeFmul:\n+\t\top = fpuBinOpMul\n+\tcase ssa.OpcodeFdiv:\n+\t\top = fpuBinOpDiv\n+\tcase ssa.OpcodeFmax:\n+\t\top = fpuBinOpMax\n+\tcase ssa.OpcodeFmin:\n+\t\top = fpuBinOpMin\n+\t}\n+\tx, y := si.Arg2()\n+\txDef, yDef := m.compiler.ValueDefinition(x), m.compiler.ValueDefinition(y)\n+\trn := m.getOperand_NR(xDef, extModeNone)\n+\trm := m.getOperand_NR(yDef, extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(si.Return()))\n+\tinstr.asFpuRRR(op, rd, rn, rm, x.Type().Bits() == 64)\n+\tm.insert(instr)\n+}\n+\n+func (m *machine) lowerSubOrAdd(si *ssa.Instruction, add bool) {\n+\tx, y := si.Arg2()\n+\tif !x.Type().IsInt() {\n+\t\tpanic(\"BUG?\")\n+\t}\n+\n+\txDef, yDef := m.compiler.ValueDefinition(x), m.compiler.ValueDefinition(y)\n+\trn := m.getOperand_NR(xDef, extModeNone)\n+\trm, yNegated := m.getOperand_MaybeNegatedImm12_ER_SR_NR(yDef, extModeNone)\n+\n+\tvar aop aluOp\n+\tswitch {\n+\tcase add && !yNegated: // rn+rm = x+y\n+\t\taop = aluOpAdd\n+\tcase add && yNegated: // rn-rm = x-(-y) = x+y\n+\t\taop = aluOpSub\n+\tcase !add && !yNegated: // rn-rm = x-y\n+\t\taop = aluOpSub\n+\tcase !add && yNegated: // rn+rm = x-(-y) = x-y\n+\t\taop = aluOpAdd\n+\t}\n+\trd := operandNR(m.compiler.VRegOf(si.Return()))\n+\talu := m.allocateInstr()\n+\talu.asALU(aop, rd, rn, rm, x.Type().Bits() == 64)\n+\tm.insert(alu)\n+}\n+\n+// InsertMove implements backend.Machine.\n+func (m *machine) InsertMove(dst, src regalloc.VReg, typ ssa.Type) {\n+\tinstr := m.allocateInstr()\n+\tswitch typ {\n+\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\tinstr.asMove64(dst, src)\n+\tcase ssa.TypeF32, ssa.TypeF64:\n+\t\tinstr.asFpuMov64(dst, src)\n+\tcase ssa.TypeV128:\n+\t\tinstr.asFpuMov128(dst, src)\n+\tdefault:\n+\t\tpanic(\"TODO\")\n+\t}\n+\tm.insert(instr)\n+}\n+\n+func (m *machine) lowerIcmp(si *ssa.Instruction) {\n+\tx, y, c := si.IcmpData()\n+\tflag := condFlagFromSSAIntegerCmpCond(c)\n+\n+\tin64bit := x.Type().Bits() == 64\n+\tvar ext extMode\n+\tif in64bit {\n+\t\tif c.Signed() {\n+\t\t\text = extModeSignExtend64\n+\t\t} else {\n+\t\t\text = extModeZeroExtend64\n+\t\t}\n+\t} else {\n+\t\tif c.Signed() {\n+\t\t\text = extModeSignExtend32\n+\t\t} else {\n+\t\t\text = extModeZeroExtend32\n+\t\t}\n+\t}\n+\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), ext)\n+\trm := m.getOperand_Imm12_ER_SR_NR(m.compiler.ValueDefinition(y), ext)\n+\talu := m.allocateInstr()\n+\talu.asALU(aluOpSubS, operandNR(xzrVReg), rn, rm, in64bit)\n+\tm.insert(alu)\n+\n+\tcset := m.allocateInstr()\n+\tcset.asCSet(m.compiler.VRegOf(si.Return()), false, flag)\n+\tm.insert(cset)\n+}\n+\n+func (m *machine) lowerVIcmp(si *ssa.Instruction) {\n+\tx, y, c, lane := si.VIcmpData()\n+\tflag := condFlagFromSSAIntegerCmpCond(c)\n+\tarr := ssaLaneToArrangement(lane)\n+\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(si.Return()))\n+\n+\tswitch flag {\n+\tcase eq:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmeq, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase ne:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmeq, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\t\tnot := m.allocateInstr()\n+\t\tnot.asVecMisc(vecOpNot, rd, rd, vecArrangement16B)\n+\t\tm.insert(not)\n+\tcase ge:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmge, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase gt:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmgt, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase le:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmge, rd, rm, rn, arr) // rm, rn are swapped\n+\t\tm.insert(cmp)\n+\tcase lt:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmgt, rd, rm, rn, arr) // rm, rn are swapped\n+\t\tm.insert(cmp)\n+\tcase hs:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmhs, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase hi:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmhi, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase ls:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmhs, rd, rm, rn, arr) // rm, rn are swapped\n+\t\tm.insert(cmp)\n+\tcase lo:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpCmhi, rd, rm, rn, arr) // rm, rn are swapped\n+\t\tm.insert(cmp)\n+\t}\n+}\n+\n+func (m *machine) lowerVFcmp(si *ssa.Instruction) {\n+\tx, y, c, lane := si.VFcmpData()\n+\tflag := condFlagFromSSAFloatCmpCond(c)\n+\tarr := ssaLaneToArrangement(lane)\n+\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(si.Return()))\n+\n+\tswitch flag {\n+\tcase eq:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpFcmeq, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase ne:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpFcmeq, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\t\tnot := m.allocateInstr()\n+\t\tnot.asVecMisc(vecOpNot, rd, rd, vecArrangement16B)\n+\t\tm.insert(not)\n+\tcase ge:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpFcmge, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase gt:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpFcmgt, rd, rn, rm, arr)\n+\t\tm.insert(cmp)\n+\tcase mi:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpFcmgt, rd, rm, rn, arr) // rm, rn are swapped\n+\t\tm.insert(cmp)\n+\tcase ls:\n+\t\tcmp := m.allocateInstr()\n+\t\tcmp.asVecRRR(vecOpFcmge, rd, rm, rn, arr) // rm, rn are swapped\n+\t\tm.insert(cmp)\n+\t}\n+}\n+\n+func (m *machine) lowerVfpuToInt(rd, rn operand, arr vecArrangement, signed bool) {\n+\tcvt := m.allocateInstr()\n+\tif signed {\n+\t\tcvt.asVecMisc(vecOpFcvtzs, rd, rn, arr)\n+\t} else {\n+\t\tcvt.asVecMisc(vecOpFcvtzu, rd, rn, arr)\n+\t}\n+\tm.insert(cvt)\n+\n+\tif arr == vecArrangement2D {\n+\t\tnarrow := m.allocateInstr()\n+\t\tif signed {\n+\t\t\tnarrow.asVecMisc(vecOpSqxtn, rd, rd, vecArrangement2S)\n+\t\t} else {\n+\t\t\tnarrow.asVecMisc(vecOpUqxtn, rd, rd, vecArrangement2S)\n+\t\t}\n+\t\tm.insert(narrow)\n+\t}\n+}\n+\n+func (m *machine) lowerVfpuFromInt(rd, rn operand, arr vecArrangement, signed bool) {\n+\tcvt := m.allocateInstr()\n+\tif signed {\n+\t\tcvt.asVecMisc(vecOpScvtf, rd, rn, arr)\n+\t} else {\n+\t\tcvt.asVecMisc(vecOpUcvtf, rd, rn, arr)\n+\t}\n+\tm.insert(cvt)\n+}\n+\n+func (m *machine) lowerShifts(si *ssa.Instruction, ext extMode, aluOp aluOp) {\n+\tx, amount := si.Arg2()\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), ext)\n+\trm := m.getOperand_ShiftImm_NR(m.compiler.ValueDefinition(amount), ext, x.Type().Bits())\n+\trd := operandNR(m.compiler.VRegOf(si.Return()))\n+\n+\talu := m.allocateInstr()\n+\talu.asALUShift(aluOp, rd, rn, rm, x.Type().Bits() == 64)\n+\tm.insert(alu)\n+}\n+\n+func (m *machine) lowerBitwiseAluOp(si *ssa.Instruction, op aluOp, ignoreResult bool) {\n+\tx, y := si.Arg2()\n+\n+\txDef, yDef := m.compiler.ValueDefinition(x), m.compiler.ValueDefinition(y)\n+\trn := m.getOperand_NR(xDef, extModeNone)\n+\n+\tvar rd operand\n+\tif ignoreResult {\n+\t\trd = operandNR(xzrVReg)\n+\t} else {\n+\t\trd = operandNR(m.compiler.VRegOf(si.Return()))\n+\t}\n+\n+\t_64 := x.Type().Bits() == 64\n+\talu := m.allocateInstr()\n+\tif instr := yDef.Instr; instr != nil && instr.Constant() {\n+\t\tc := instr.ConstantVal()\n+\t\tif isBitMaskImmediate(c, _64) {\n+\t\t\t// Constant bit wise operations can be lowered to a single instruction.\n+\t\t\talu.asALUBitmaskImm(op, rd.nr(), rn.nr(), c, _64)\n+\t\t\tm.insert(alu)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\trm := m.getOperand_SR_NR(yDef, extModeNone)\n+\talu.asALU(op, rd, rn, rm, _64)\n+\tm.insert(alu)\n+}\n+\n+func (m *machine) lowerRotl(si *ssa.Instruction) {\n+\tx, y := si.Arg2()\n+\tr := si.Return()\n+\t_64 := r.Type().Bits() == 64\n+\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\tvar tmp operand\n+\tif _64 {\n+\t\ttmp = operandNR(m.compiler.AllocateVReg(ssa.TypeI64))\n+\t} else {\n+\t\ttmp = operandNR(m.compiler.AllocateVReg(ssa.TypeI32))\n+\t}\n+\trd := operandNR(m.compiler.VRegOf(r))\n+\n+\t// Encode rotl as neg + rotr: neg is a sub against the zero-reg.\n+\tm.lowerRotlImpl(rd, rn, rm, tmp, _64)\n+}\n+\n+func (m *machine) lowerRotlImpl(rd, rn, rm, tmp operand, is64bit bool) {\n+\t// Encode rotl as neg + rotr: neg is a sub against the zero-reg.\n+\tneg := m.allocateInstr()\n+\tneg.asALU(aluOpSub, tmp, operandNR(xzrVReg), rm, is64bit)\n+\tm.insert(neg)\n+\talu := m.allocateInstr()\n+\talu.asALU(aluOpRotR, rd, rn, tmp, is64bit)\n+\tm.insert(alu)\n+}\n+\n+func (m *machine) lowerRotr(si *ssa.Instruction) {\n+\tx, y := si.Arg2()\n+\n+\txDef, yDef := m.compiler.ValueDefinition(x), m.compiler.ValueDefinition(y)\n+\trn := m.getOperand_NR(xDef, extModeNone)\n+\trm := m.getOperand_NR(yDef, extModeNone)\n+\trd := operandNR(m.compiler.VRegOf(si.Return()))\n+\n+\talu := m.allocateInstr()\n+\talu.asALU(aluOpRotR, rd, rn, rm, si.Return().Type().Bits() == 64)\n+\tm.insert(alu)\n+}\n+\n+func (m *machine) lowerExtend(arg, ret ssa.Value, from, to byte, signed bool) {\n+\trd := m.compiler.VRegOf(ret)\n+\tdef := m.compiler.ValueDefinition(arg)\n+\n+\tif instr := def.Instr; !signed && from == 32 && instr != nil {\n+\t\t// We can optimize out the unsigned extend because:\n+\t\t// \tWrites to the W register set bits [63:32] of the X register to zero\n+\t\t//  https://developer.arm.com/documentation/den0024/a/An-Introduction-to-the-ARMv8-Instruction-Sets/The-ARMv8-instruction-sets/Distinguishing-between-32-bit-and-64-bit-A64-instructions\n+\t\tswitch instr.Opcode() {\n+\t\tcase\n+\t\t\tssa.OpcodeIadd, ssa.OpcodeIsub, ssa.OpcodeLoad,\n+\t\t\tssa.OpcodeBand, ssa.OpcodeBor, ssa.OpcodeBnot,\n+\t\t\tssa.OpcodeIshl, ssa.OpcodeUshr, ssa.OpcodeSshr,\n+\t\t\tssa.OpcodeRotl, ssa.OpcodeRotr,\n+\t\t\tssa.OpcodeUload8, ssa.OpcodeUload16, ssa.OpcodeUload32:\n+\t\t\t// So, if the argument is the result of a 32-bit operation, we can just copy the register.\n+\t\t\t// It is highly likely that this copy will be optimized out after register allocation.\n+\t\t\trn := m.compiler.VRegOf(arg)\n+\t\t\tmov := m.allocateInstr()\n+\t\t\t// Note: do not use move32 as it will be lowered to a 32-bit move, which is not copy (that is actually the impl of UExtend).\n+\t\t\tmov.asMove64(rd, rn)\n+\t\t\tm.insert(mov)\n+\t\t\treturn\n+\t\tdefault:\n+\t\t}\n+\t}\n+\trn := m.getOperand_NR(def, extModeNone)\n+\n+\text := m.allocateInstr()\n+\text.asExtend(rd, rn.nr(), from, to, signed)\n+\tm.insert(ext)\n+}\n+\n+func (m *machine) lowerFcmp(x, y, result ssa.Value, c ssa.FloatCmpCond) {\n+\trn, rm := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone), m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\n+\tfc := m.allocateInstr()\n+\tfc.asFpuCmp(rn, rm, x.Type().Bits() == 64)\n+\tm.insert(fc)\n+\n+\tcset := m.allocateInstr()\n+\tcset.asCSet(m.compiler.VRegOf(result), false, condFlagFromSSAFloatCmpCond(c))\n+\tm.insert(cset)\n+}\n+\n+func (m *machine) lowerImul(x, y, result ssa.Value) {\n+\trd := m.compiler.VRegOf(result)\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\n+\t// TODO: if this comes before Add/Sub, we could merge it by putting it into the place of xzrVReg.\n+\n+\tmul := m.allocateInstr()\n+\tmul.asALURRRR(aluOpMAdd, operandNR(rd), rn, rm, operandNR(xzrVReg), x.Type().Bits() == 64)\n+\tm.insert(mul)\n+}\n+\n+func (m *machine) lowerClz(x, result ssa.Value) {\n+\trd := m.compiler.VRegOf(result)\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\tclz := m.allocateInstr()\n+\tclz.asBitRR(bitOpClz, rd, rn.nr(), x.Type().Bits() == 64)\n+\tm.insert(clz)\n+}\n+\n+func (m *machine) lowerCtz(x, result ssa.Value) {\n+\trd := m.compiler.VRegOf(result)\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trbit := m.allocateInstr()\n+\t_64 := x.Type().Bits() == 64\n+\tvar tmpReg regalloc.VReg\n+\tif _64 {\n+\t\ttmpReg = m.compiler.AllocateVReg(ssa.TypeI64)\n+\t} else {\n+\t\ttmpReg = m.compiler.AllocateVReg(ssa.TypeI32)\n+\t}\n+\trbit.asBitRR(bitOpRbit, tmpReg, rn.nr(), _64)\n+\tm.insert(rbit)\n+\n+\tclz := m.allocateInstr()\n+\tclz.asBitRR(bitOpClz, rd, tmpReg, _64)\n+\tm.insert(clz)\n+}\n+\n+func (m *machine) lowerPopcnt(x, result ssa.Value) {\n+\t// arm64 doesn't have an instruction for population count on scalar register,\n+\t// so we use the vector instruction `cnt`.\n+\t// This is exactly what the official Go implements bits.OneCount.\n+\t// For example, \"func () int { return bits.OneCount(10) }\" is compiled as\n+\t//\n+\t//    MOVD    $10, R0 ;; Load 10.\n+\t//    FMOVD   R0, F0\n+\t//    VCNT    V0.B8, V0.B8\n+\t//    UADDLV  V0.B8, V0\n+\t//\n+\t// In aarch64 asm, FMOVD is encoded as `ins`, VCNT is `cnt`,\n+\t// and the registers may use different names. In our encoding we use the following\n+\t// instructions:\n+\t//\n+\t//    ins v0.d[0], x0     ;; mov from GPR to vec (FMOV above) is encoded as INS\n+\t//    cnt v0.16b, v0.16b  ;; we use vec arrangement 16b\n+\t//    uaddlv h0, v0.8b    ;; h0 is still v0 with the dest width specifier 'H', implied when src arrangement is 8b\n+\t//    mov x5, v0.d[0]     ;; finally we mov the result back to a GPR\n+\t//\n+\n+\trd := operandNR(m.compiler.VRegOf(result))\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\n+\trf1 := operandNR(m.compiler.AllocateVReg(ssa.TypeF64))\n+\tins := m.allocateInstr()\n+\tins.asMovToVec(rf1, rn, vecArrangementD, vecIndex(0))\n+\tm.insert(ins)\n+\n+\trf2 := operandNR(m.compiler.AllocateVReg(ssa.TypeF64))\n+\tcnt := m.allocateInstr()\n+\tcnt.asVecMisc(vecOpCnt, rf2, rf1, vecArrangement16B)\n+\tm.insert(cnt)\n+\n+\trf3 := operandNR(m.compiler.AllocateVReg(ssa.TypeF64))\n+\tuaddlv := m.allocateInstr()\n+\tuaddlv.asVecLanes(vecOpUaddlv, rf3, rf2, vecArrangement8B)\n+\tm.insert(uaddlv)\n+\n+\tmov := m.allocateInstr()\n+\tmov.asMovFromVec(rd, rf3, vecArrangementD, vecIndex(0), false)\n+\tm.insert(mov)\n+}\n+\n+// lowerExitWithCode lowers the lowerExitWithCode takes a context pointer as argument.\n+func (m *machine) lowerExitWithCode(execCtxVReg regalloc.VReg, code wazevoapi.ExitCode) {\n+\ttmpReg1 := m.compiler.AllocateVReg(ssa.TypeI32)\n+\tloadExitCodeConst := m.allocateInstr()\n+\tloadExitCodeConst.asMOVZ(tmpReg1, uint64(code), 0, true)\n+\n+\tsetExitCode := m.allocateInstr()\n+\tsetExitCode.asStore(operandNR(tmpReg1),\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   execCtxVReg, imm: wazevoapi.ExecutionContextOffsetExitCodeOffset.I64(),\n+\t\t}, 32)\n+\n+\t// In order to unwind the stack, we also need to push the current stack pointer:\n+\ttmp2 := m.compiler.AllocateVReg(ssa.TypeI64)\n+\tmovSpToTmp := m.allocateInstr()\n+\tmovSpToTmp.asMove64(tmp2, spVReg)\n+\tstrSpToExecCtx := m.allocateInstr()\n+\tstrSpToExecCtx.asStore(operandNR(tmp2),\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   execCtxVReg, imm: wazevoapi.ExecutionContextOffsetStackPointerBeforeGoCall.I64(),\n+\t\t}, 64)\n+\t// Also the address of this exit.\n+\ttmp3 := m.compiler.AllocateVReg(ssa.TypeI64)\n+\tcurrentAddrToTmp := m.allocateInstr()\n+\tcurrentAddrToTmp.asAdr(tmp3, 0)\n+\tstoreCurrentAddrToExecCtx := m.allocateInstr()\n+\tstoreCurrentAddrToExecCtx.asStore(operandNR(tmp3),\n+\t\taddressMode{\n+\t\t\tkind: addressModeKindRegUnsignedImm12,\n+\t\t\trn:   execCtxVReg, imm: wazevoapi.ExecutionContextOffsetGoCallReturnAddress.I64(),\n+\t\t}, 64)\n+\n+\texitSeq := m.allocateInstr()\n+\texitSeq.asExitSequence(execCtxVReg)\n+\n+\tm.insert(loadExitCodeConst)\n+\tm.insert(setExitCode)\n+\tm.insert(movSpToTmp)\n+\tm.insert(strSpToExecCtx)\n+\tm.insert(currentAddrToTmp)\n+\tm.insert(storeCurrentAddrToExecCtx)\n+\tm.insert(exitSeq)\n+}\n+\n+func (m *machine) lowerIcmpToFlag(x, y ssa.Value, signed bool) {\n+\tif x.Type() != y.Type() {\n+\t\tpanic(\n+\t\t\tfmt.Sprintf(\"TODO(maybe): support icmp with different types: v%d=%s != v%d=%s\",\n+\t\t\t\tx.ID(), x.Type(), y.ID(), y.Type()))\n+\t}\n+\n+\textMod := extModeOf(x.Type(), signed)\n+\n+\t// First operand must be in pure register form.\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extMod)\n+\t// Second operand can be in any of Imm12, ER, SR, or NR form supported by the SUBS instructions.\n+\trm := m.getOperand_Imm12_ER_SR_NR(m.compiler.ValueDefinition(y), extMod)\n+\n+\talu := m.allocateInstr()\n+\t// subs zr, rn, rm\n+\talu.asALU(\n+\t\taluOpSubS,\n+\t\t// We don't need the result, just need to set flags.\n+\t\toperandNR(xzrVReg),\n+\t\trn,\n+\t\trm,\n+\t\tx.Type().Bits() == 64,\n+\t)\n+\tm.insert(alu)\n+}\n+\n+func (m *machine) lowerFcmpToFlag(x, y ssa.Value) {\n+\tif x.Type() != y.Type() {\n+\t\tpanic(\"TODO(maybe): support icmp with different types\")\n+\t}\n+\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\tcmp := m.allocateInstr()\n+\tcmp.asFpuCmp(rn, rm, x.Type().Bits() == 64)\n+\tm.insert(cmp)\n+}\n+\n+func (m *machine) lowerExitIfTrueWithCode(execCtxVReg regalloc.VReg, cond ssa.Value, code wazevoapi.ExitCode) {\n+\tcondDef := m.compiler.ValueDefinition(cond)\n+\tif !m.compiler.MatchInstr(condDef, ssa.OpcodeIcmp) {\n+\t\tpanic(\"TODO: OpcodeExitIfTrueWithCode must come after Icmp at the moment: \" + condDef.Instr.Opcode().String())\n+\t}\n+\tcondDef.Instr.MarkLowered()\n+\n+\tcvalInstr := condDef.Instr\n+\tx, y, c := cvalInstr.IcmpData()\n+\tsigned := c.Signed()\n+\n+\tif !m.tryLowerBandToFlag(x, y) {\n+\t\tm.lowerIcmpToFlag(x, y, signed)\n+\t}\n+\n+\t// We need to copy the execution context to a temp register, because if it's spilled,\n+\t// it might end up being reloaded inside the exiting branch.\n+\texecCtxTmp := m.copyToTmp(execCtxVReg)\n+\n+\t// We have to skip the entire exit sequence if the condition is false.\n+\tcbr := m.allocateInstr()\n+\tm.insert(cbr)\n+\tm.lowerExitWithCode(execCtxTmp, code)\n+\t// conditional branch target is after exit.\n+\tl := m.insertBrTargetLabel()\n+\tcbr.asCondBr(condFlagFromSSAIntegerCmpCond(c).invert().asCond(), l, false /* ignored */)\n+}\n+\n+func (m *machine) lowerSelect(c, x, y, result ssa.Value) {\n+\tcvalDef := m.compiler.ValueDefinition(c)\n+\n+\tvar cc condFlag\n+\tswitch {\n+\tcase m.compiler.MatchInstr(cvalDef, ssa.OpcodeIcmp): // This case, we can use the ALU flag set by SUBS instruction.\n+\t\tcvalInstr := cvalDef.Instr\n+\t\tx, y, c := cvalInstr.IcmpData()\n+\t\tcc = condFlagFromSSAIntegerCmpCond(c)\n+\t\tm.lowerIcmpToFlag(x, y, c.Signed())\n+\t\tcvalDef.Instr.MarkLowered()\n+\tcase m.compiler.MatchInstr(cvalDef, ssa.OpcodeFcmp): // This case we can use the Fpu flag directly.\n+\t\tcvalInstr := cvalDef.Instr\n+\t\tx, y, c := cvalInstr.FcmpData()\n+\t\tcc = condFlagFromSSAFloatCmpCond(c)\n+\t\tm.lowerFcmpToFlag(x, y)\n+\t\tcvalDef.Instr.MarkLowered()\n+\tdefault:\n+\t\trn := m.getOperand_NR(cvalDef, extModeNone)\n+\t\tif c.Type() != ssa.TypeI32 && c.Type() != ssa.TypeI64 {\n+\t\t\tpanic(\"TODO?BUG?: support select with non-integer condition\")\n+\t\t}\n+\t\talu := m.allocateInstr()\n+\t\t// subs zr, rn, zr\n+\t\talu.asALU(\n+\t\t\taluOpSubS,\n+\t\t\t// We don't need the result, just need to set flags.\n+\t\t\toperandNR(xzrVReg),\n+\t\t\trn,\n+\t\t\toperandNR(xzrVReg),\n+\t\t\tc.Type().Bits() == 64,\n+\t\t)\n+\t\tm.insert(alu)\n+\t\tcc = ne\n+\t}\n+\n+\trn := m.getOperand_NR(m.compiler.ValueDefinition(x), extModeNone)\n+\trm := m.getOperand_NR(m.compiler.ValueDefinition(y), extModeNone)\n+\n+\trd := operandNR(m.compiler.VRegOf(result))\n+\tswitch x.Type() {\n+\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\t// csel rd, rn, rm, cc\n+\t\tcsel := m.allocateInstr()\n+\t\tcsel.asCSel(rd, rn, rm, cc, x.Type().Bits() == 64)\n+\t\tm.insert(csel)\n+\tcase ssa.TypeF32, ssa.TypeF64:\n+\t\t// fcsel rd, rn, rm, cc\n+\t\tfcsel := m.allocateInstr()\n+\t\tfcsel.asFpuCSel(rd, rn, rm, cc, x.Type().Bits() == 64)\n+\t\tm.insert(fcsel)\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+}\n+\n+func (m *machine) lowerSelectVec(rc, rn, rm, rd operand) {\n+\t// First check if `rc` is zero or not.\n+\tcheckZero := m.allocateInstr()\n+\tcheckZero.asALU(aluOpSubS, operandNR(xzrVReg), rc, operandNR(xzrVReg), false)\n+\tm.insert(checkZero)\n+\n+\t// Then use CSETM to set all bits to one if `rc` is zero.\n+\tallOnesOrZero := m.compiler.AllocateVReg(ssa.TypeI64)\n+\tcset := m.allocateInstr()\n+\tcset.asCSet(allOnesOrZero, true, ne)\n+\tm.insert(cset)\n+\n+\t// Then move the bits to the result vector register.\n+\ttmp2 := operandNR(m.compiler.AllocateVReg(ssa.TypeV128))\n+\tdup := m.allocateInstr()\n+\tdup.asVecDup(tmp2, operandNR(allOnesOrZero), vecArrangement2D)\n+\tm.insert(dup)\n+\n+\t// Now that `tmp2` has either all bits one or zero depending on `rc`,\n+\t// we can use bsl to select between `rn` and `rm`.\n+\tins := m.allocateInstr()\n+\tins.asVecRRRRewrite(vecOpBsl, tmp2, rn, rm, vecArrangement16B)\n+\tm.insert(ins)\n+\n+\t// Finally, move the result to the destination register.\n+\tmov2 := m.allocateInstr()\n+\tmov2.asFpuMov128(rd.nr(), tmp2.nr())\n+\tm.insert(mov2)\n+}\n+\n+func (m *machine) lowerAtomicRmw(si *ssa.Instruction) {\n+\tssaOp, size := si.AtomicRmwData()\n+\n+\tvar op atomicRmwOp\n+\tvar negateArg bool\n+\tvar flipArg bool\n+\tswitch ssaOp {\n+\tcase ssa.AtomicRmwOpAdd:\n+\t\top = atomicRmwOpAdd\n+\tcase ssa.AtomicRmwOpSub:\n+\t\top = atomicRmwOpAdd\n+\t\tnegateArg = true\n+\tcase ssa.AtomicRmwOpAnd:\n+\t\top = atomicRmwOpClr\n+\t\tflipArg = true\n+\tcase ssa.AtomicRmwOpOr:\n+\t\top = atomicRmwOpSet\n+\tcase ssa.AtomicRmwOpXor:\n+\t\top = atomicRmwOpEor\n+\tcase ssa.AtomicRmwOpXchg:\n+\t\top = atomicRmwOpSwp\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"unknown ssa atomic rmw op: %s\", ssaOp))\n+\t}\n+\n+\taddr, val := si.Arg2()\n+\taddrDef, valDef := m.compiler.ValueDefinition(addr), m.compiler.ValueDefinition(val)\n+\trn := m.getOperand_NR(addrDef, extModeNone)\n+\trt := operandNR(m.compiler.VRegOf(si.Return()))\n+\trs := m.getOperand_NR(valDef, extModeNone)\n+\n+\t_64 := si.Return().Type().Bits() == 64\n+\tvar tmp operand\n+\tif _64 {\n+\t\ttmp = operandNR(m.compiler.AllocateVReg(ssa.TypeI64))\n+\t} else {\n+\t\ttmp = operandNR(m.compiler.AllocateVReg(ssa.TypeI32))\n+\t}\n+\tm.lowerAtomicRmwImpl(op, rn, rs, rt, tmp, size, negateArg, flipArg, _64)\n+}\n+\n+func (m *machine) lowerAtomicRmwImpl(op atomicRmwOp, rn, rs, rt, tmp operand, size uint64, negateArg, flipArg, dst64bit bool) {\n+\tswitch {\n+\tcase negateArg:\n+\t\tneg := m.allocateInstr()\n+\t\tneg.asALU(aluOpSub, tmp, operandNR(xzrVReg), rs, dst64bit)\n+\t\tm.insert(neg)\n+\tcase flipArg:\n+\t\tflip := m.allocateInstr()\n+\t\tflip.asALU(aluOpOrn, tmp, operandNR(xzrVReg), rs, dst64bit)\n+\t\tm.insert(flip)\n+\tdefault:\n+\t\ttmp = rs\n+\t}\n+\n+\trmw := m.allocateInstr()\n+\trmw.asAtomicRmw(op, rn, tmp, rt, size)\n+\tm.insert(rmw)\n+}\n+\n+func (m *machine) lowerAtomicCas(si *ssa.Instruction) {\n+\taddr, exp, repl := si.Arg3()\n+\tsize := si.AtomicTargetSize()\n+\n+\taddrDef, expDef, replDef := m.compiler.ValueDefinition(addr), m.compiler.ValueDefinition(exp), m.compiler.ValueDefinition(repl)\n+\trn := m.getOperand_NR(addrDef, extModeNone)\n+\trt := m.getOperand_NR(replDef, extModeNone)\n+\trs := m.getOperand_NR(expDef, extModeNone)\n+\ttmp := operandNR(m.compiler.AllocateVReg(si.Return().Type()))\n+\n+\t_64 := si.Return().Type().Bits() == 64\n+\t// rs is overwritten by CAS, so we need to move it to the result register before the instruction\n+\t// in case when it is used somewhere else.\n+\tmov := m.allocateInstr()\n+\tif _64 {\n+\t\tmov.asMove64(tmp.nr(), rs.nr())\n+\t} else {\n+\t\tmov.asMove32(tmp.nr(), rs.nr())\n+\t}\n+\tm.insert(mov)\n+\n+\tm.lowerAtomicCasImpl(rn, tmp, rt, size)\n+\n+\tmov2 := m.allocateInstr()\n+\trd := m.compiler.VRegOf(si.Return())\n+\tif _64 {\n+\t\tmov2.asMove64(rd, tmp.nr())\n+\t} else {\n+\t\tmov2.asMove32(rd, tmp.nr())\n+\t}\n+\tm.insert(mov2)\n+}\n+\n+func (m *machine) lowerAtomicCasImpl(rn, rs, rt operand, size uint64) {\n+\tcas := m.allocateInstr()\n+\tcas.asAtomicCas(rn, rs, rt, size)\n+\tm.insert(cas)\n+}\n+\n+func (m *machine) lowerAtomicLoad(si *ssa.Instruction) {\n+\taddr := si.Arg()\n+\tsize := si.AtomicTargetSize()\n+\n+\taddrDef := m.compiler.ValueDefinition(addr)\n+\trn := m.getOperand_NR(addrDef, extModeNone)\n+\trt := operandNR(m.compiler.VRegOf(si.Return()))\n+\n+\tm.lowerAtomicLoadImpl(rn, rt, size)\n+}\n+\n+func (m *machine) lowerAtomicLoadImpl(rn, rt operand, size uint64) {\n+\tld := m.allocateInstr()\n+\tld.asAtomicLoad(rn, rt, size)\n+\tm.insert(ld)\n+}\n+\n+func (m *machine) lowerAtomicStore(si *ssa.Instruction) {\n+\taddr, val := si.Arg2()\n+\tsize := si.AtomicTargetSize()\n+\n+\taddrDef := m.compiler.ValueDefinition(addr)\n+\tvalDef := m.compiler.ValueDefinition(val)\n+\trn := m.getOperand_NR(addrDef, extModeNone)\n+\trt := m.getOperand_NR(valDef, extModeNone)\n+\n+\tm.lowerAtomicStoreImpl(rn, rt, size)\n+}\n+\n+func (m *machine) lowerAtomicStoreImpl(rn, rt operand, size uint64) {\n+\tld := m.allocateInstr()\n+\tld.asAtomicStore(rn, rt, size)\n+\tm.insert(ld)\n+}\n+\n+// copyToTmp copies the given regalloc.VReg to a temporary register. This is called before cbr to avoid the regalloc issue\n+// e.g. reload happening in the middle of the exit sequence which is not the path the normal path executes\n+func (m *machine) copyToTmp(v regalloc.VReg) regalloc.VReg {\n+\ttyp := m.compiler.TypeOf(v)\n+\tmov := m.allocateInstr()\n+\ttmp := m.compiler.AllocateVReg(typ)\n+\tif typ.IsInt() {\n+\t\tmov.asMove64(tmp, v)\n+\t} else {\n+\t\tmov.asFpuMov128(tmp, v)\n+\t}\n+\tm.insert(mov)\n+\treturn tmp\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/lower_instr_operands.go",
          "status": "added",
          "additions": 350,
          "deletions": 0,
          "patch": "@@ -0,0 +1,350 @@\n+package arm64\n+\n+// This file contains the logic to \"find and determine operands\" for instructions.\n+// In order to finalize the form of an operand, we might end up merging/eliminating\n+// the source instructions into an operand whenever possible.\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+)\n+\n+type (\n+\t// operand represents an operand of an instruction whose type is determined by the kind.\n+\toperand struct {\n+\t\tkind        operandKind\n+\t\tdata, data2 uint64\n+\t}\n+\toperandKind byte\n+)\n+\n+// Here's the list of operand kinds. We use the abbreviation of the kind name not only for these consts,\n+// but also names of functions which return the operand of the kind.\n+const (\n+\t// operandKindNR represents \"NormalRegister\" (NR). This is literally the register without any special operation unlike others.\n+\toperandKindNR operandKind = iota\n+\t// operandKindSR represents \"Shifted Register\" (SR). This is a register which is shifted by a constant.\n+\t// Some of the arm64 instructions can take this kind of operand.\n+\toperandKindSR\n+\t// operandKindER represents \"Extended Register (ER). This is a register which is sign/zero-extended to a larger size.\n+\t// Some of the arm64 instructions can take this kind of operand.\n+\toperandKindER\n+\t// operandKindImm12 represents \"Immediate 12\" (Imm12). This is a 12-bit immediate value which can be either shifted by 12 or not.\n+\t// See asImm12 function for detail.\n+\toperandKindImm12\n+\t// operandKindShiftImm represents \"Shifted Immediate\" (ShiftImm) used by shift operations.\n+\toperandKindShiftImm\n+)\n+\n+// String implements fmt.Stringer for debugging.\n+func (o operand) format(size byte) string {\n+\tswitch o.kind {\n+\tcase operandKindNR:\n+\t\treturn formatVRegSized(o.nr(), size)\n+\tcase operandKindSR:\n+\t\tr, amt, sop := o.sr()\n+\t\treturn fmt.Sprintf(\"%s, %s #%d\", formatVRegSized(r, size), sop, amt)\n+\tcase operandKindER:\n+\t\tr, eop, _ := o.er()\n+\t\treturn fmt.Sprintf(\"%s %s\", formatVRegSized(r, size), eop)\n+\tcase operandKindImm12:\n+\t\timm12, shiftBit := o.imm12()\n+\t\tif shiftBit == 1 {\n+\t\t\treturn fmt.Sprintf(\"#%#x\", uint64(imm12)<<12)\n+\t\t} else {\n+\t\t\treturn fmt.Sprintf(\"#%#x\", imm12)\n+\t\t}\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"unknown operand kind: %d\", o.kind))\n+\t}\n+}\n+\n+// operandNR encodes the given VReg as an operand of operandKindNR.\n+func operandNR(r regalloc.VReg) operand {\n+\treturn operand{kind: operandKindNR, data: uint64(r)}\n+}\n+\n+// nr decodes the underlying VReg assuming the operand is of operandKindNR.\n+func (o operand) nr() regalloc.VReg {\n+\treturn regalloc.VReg(o.data)\n+}\n+\n+// operandER encodes the given VReg as an operand of operandKindER.\n+func operandER(r regalloc.VReg, eop extendOp, to byte) operand {\n+\tif to < 32 {\n+\t\tpanic(\"TODO?BUG?: when we need to extend to less than 32 bits?\")\n+\t}\n+\treturn operand{kind: operandKindER, data: uint64(r), data2: uint64(eop)<<32 | uint64(to)}\n+}\n+\n+// er decodes the underlying VReg, extend operation, and the target size assuming the operand is of operandKindER.\n+func (o operand) er() (r regalloc.VReg, eop extendOp, to byte) {\n+\treturn regalloc.VReg(o.data), extendOp(o.data2>>32) & 0xff, byte(o.data2 & 0xff)\n+}\n+\n+// operandSR encodes the given VReg as an operand of operandKindSR.\n+func operandSR(r regalloc.VReg, amt byte, sop shiftOp) operand {\n+\treturn operand{kind: operandKindSR, data: uint64(r), data2: uint64(amt)<<32 | uint64(sop)}\n+}\n+\n+// sr decodes the underlying VReg, shift amount, and shift operation assuming the operand is of operandKindSR.\n+func (o operand) sr() (r regalloc.VReg, amt byte, sop shiftOp) {\n+\treturn regalloc.VReg(o.data), byte(o.data2>>32) & 0xff, shiftOp(o.data2) & 0xff\n+}\n+\n+// operandImm12 encodes the given imm12 as an operand of operandKindImm12.\n+func operandImm12(imm12 uint16, shiftBit byte) operand {\n+\treturn operand{kind: operandKindImm12, data: uint64(imm12) | uint64(shiftBit)<<32}\n+}\n+\n+// imm12 decodes the underlying imm12 data assuming the operand is of operandKindImm12.\n+func (o operand) imm12() (v uint16, shiftBit byte) {\n+\treturn uint16(o.data), byte(o.data >> 32)\n+}\n+\n+// operandShiftImm encodes the given amount as an operand of operandKindShiftImm.\n+func operandShiftImm(amount byte) operand {\n+\treturn operand{kind: operandKindShiftImm, data: uint64(amount)}\n+}\n+\n+// shiftImm decodes the underlying shift amount data assuming the operand is of operandKindShiftImm.\n+func (o operand) shiftImm() byte {\n+\treturn byte(o.data)\n+}\n+\n+// reg returns the register of the operand if applicable.\n+func (o operand) reg() regalloc.VReg {\n+\tswitch o.kind {\n+\tcase operandKindNR:\n+\t\treturn o.nr()\n+\tcase operandKindSR:\n+\t\tr, _, _ := o.sr()\n+\t\treturn r\n+\tcase operandKindER:\n+\t\tr, _, _ := o.er()\n+\t\treturn r\n+\tcase operandKindImm12:\n+\t\t// Does not have a register.\n+\tcase operandKindShiftImm:\n+\t\t// Does not have a register.\n+\tdefault:\n+\t\tpanic(o.kind)\n+\t}\n+\treturn regalloc.VRegInvalid\n+}\n+\n+func (o operand) realReg() regalloc.RealReg {\n+\treturn o.nr().RealReg()\n+}\n+\n+func (o operand) assignReg(v regalloc.VReg) operand {\n+\tswitch o.kind {\n+\tcase operandKindNR:\n+\t\treturn operandNR(v)\n+\tcase operandKindSR:\n+\t\t_, amt, sop := o.sr()\n+\t\treturn operandSR(v, amt, sop)\n+\tcase operandKindER:\n+\t\t_, eop, to := o.er()\n+\t\treturn operandER(v, eop, to)\n+\tcase operandKindImm12:\n+\t\t// Does not have a register.\n+\tcase operandKindShiftImm:\n+\t\t// Does not have a register.\n+\t}\n+\tpanic(o.kind)\n+}\n+\n+// ensureValueNR returns an operand of either operandKindER, operandKindSR, or operandKindNR from the given value (defined by `def).\n+//\n+// `mode` is used to extend the operand if the bit length is smaller than mode.bits().\n+// If the operand can be expressed as operandKindImm12, `mode` is ignored.\n+func (m *machine) getOperand_Imm12_ER_SR_NR(def *backend.SSAValueDefinition, mode extMode) (op operand) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn operandNR(def.BlkParamVReg)\n+\t}\n+\n+\tinstr := def.Instr\n+\tif instr.Opcode() == ssa.OpcodeIconst {\n+\t\tif imm12Op, ok := asImm12Operand(instr.ConstantVal()); ok {\n+\t\t\tinstr.MarkLowered()\n+\t\t\treturn imm12Op\n+\t\t}\n+\t}\n+\treturn m.getOperand_ER_SR_NR(def, mode)\n+}\n+\n+// getOperand_MaybeNegatedImm12_ER_SR_NR is almost the same as getOperand_Imm12_ER_SR_NR, but this might negate the immediate value.\n+// If the immediate value is negated, the second return value is true, otherwise always false.\n+func (m *machine) getOperand_MaybeNegatedImm12_ER_SR_NR(def *backend.SSAValueDefinition, mode extMode) (op operand, negatedImm12 bool) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn operandNR(def.BlkParamVReg), false\n+\t}\n+\n+\tinstr := def.Instr\n+\tif instr.Opcode() == ssa.OpcodeIconst {\n+\t\tc := instr.ConstantVal()\n+\t\tif imm12Op, ok := asImm12Operand(c); ok {\n+\t\t\tinstr.MarkLowered()\n+\t\t\treturn imm12Op, false\n+\t\t}\n+\n+\t\tsignExtended := int64(c)\n+\t\tif def.SSAValue().Type().Bits() == 32 {\n+\t\t\tsignExtended = (signExtended << 32) >> 32\n+\t\t}\n+\t\tnegatedWithoutSign := -signExtended\n+\t\tif imm12Op, ok := asImm12Operand(uint64(negatedWithoutSign)); ok {\n+\t\t\tinstr.MarkLowered()\n+\t\t\treturn imm12Op, true\n+\t\t}\n+\t}\n+\treturn m.getOperand_ER_SR_NR(def, mode), false\n+}\n+\n+// ensureValueNR returns an operand of either operandKindER, operandKindSR, or operandKindNR from the given value (defined by `def).\n+//\n+// `mode` is used to extend the operand if the bit length is smaller than mode.bits().\n+func (m *machine) getOperand_ER_SR_NR(def *backend.SSAValueDefinition, mode extMode) (op operand) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn operandNR(def.BlkParamVReg)\n+\t}\n+\n+\tif m.compiler.MatchInstr(def, ssa.OpcodeSExtend) || m.compiler.MatchInstr(def, ssa.OpcodeUExtend) {\n+\t\textInstr := def.Instr\n+\n+\t\tsigned := extInstr.Opcode() == ssa.OpcodeSExtend\n+\t\tinnerExtFromBits, innerExtToBits := extInstr.ExtendFromToBits()\n+\t\tmodeBits, modeSigned := mode.bits(), mode.signed()\n+\t\tif mode == extModeNone || innerExtToBits == modeBits {\n+\t\t\teop := extendOpFrom(signed, innerExtFromBits)\n+\t\t\textArg := m.getOperand_NR(m.compiler.ValueDefinition(extInstr.Arg()), extModeNone)\n+\t\t\top = operandER(extArg.nr(), eop, innerExtToBits)\n+\t\t\textInstr.MarkLowered()\n+\t\t\treturn\n+\t\t}\n+\n+\t\tif innerExtToBits > modeBits {\n+\t\t\tpanic(\"BUG?TODO?: need the results of inner extension to be larger than the mode\")\n+\t\t}\n+\n+\t\tswitch {\n+\t\tcase (!signed && !modeSigned) || (signed && modeSigned):\n+\t\t\t// Two sign/zero extensions are equivalent to one sign/zero extension for the larger size.\n+\t\t\teop := extendOpFrom(modeSigned, innerExtFromBits)\n+\t\t\top = operandER(m.compiler.VRegOf(extInstr.Arg()), eop, modeBits)\n+\t\t\textInstr.MarkLowered()\n+\t\tcase (signed && !modeSigned) || (!signed && modeSigned):\n+\t\t\t// We need to {sign, zero}-extend the result of the {zero,sign} extension.\n+\t\t\teop := extendOpFrom(modeSigned, innerExtToBits)\n+\t\t\top = operandER(m.compiler.VRegOf(extInstr.Return()), eop, modeBits)\n+\t\t\t// Note that we failed to merge the inner extension instruction this case.\n+\t\t}\n+\t\treturn\n+\t}\n+\treturn m.getOperand_SR_NR(def, mode)\n+}\n+\n+// ensureValueNR returns an operand of either operandKindSR or operandKindNR from the given value (defined by `def).\n+//\n+// `mode` is used to extend the operand if the bit length is smaller than mode.bits().\n+func (m *machine) getOperand_SR_NR(def *backend.SSAValueDefinition, mode extMode) (op operand) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn operandNR(def.BlkParamVReg)\n+\t}\n+\n+\tif m.compiler.MatchInstr(def, ssa.OpcodeIshl) {\n+\t\t// Check if the shift amount is constant instruction.\n+\t\ttargetVal, amountVal := def.Instr.Arg2()\n+\t\ttargetVReg := m.getOperand_NR(m.compiler.ValueDefinition(targetVal), extModeNone).nr()\n+\t\tamountDef := m.compiler.ValueDefinition(amountVal)\n+\t\tif amountDef.IsFromInstr() && amountDef.Instr.Constant() {\n+\t\t\t// If that is the case, we can use the shifted register operand (SR).\n+\t\t\tc := byte(amountDef.Instr.ConstantVal()) & (targetVal.Type().Bits() - 1) // Clears the unnecessary bits.\n+\t\t\tdef.Instr.MarkLowered()\n+\t\t\tamountDef.Instr.MarkLowered()\n+\t\t\treturn operandSR(targetVReg, c, shiftOpLSL)\n+\t\t}\n+\t}\n+\treturn m.getOperand_NR(def, mode)\n+}\n+\n+// getOperand_ShiftImm_NR returns an operand of either operandKindShiftImm or operandKindNR from the given value (defined by `def).\n+func (m *machine) getOperand_ShiftImm_NR(def *backend.SSAValueDefinition, mode extMode, shiftBitWidth byte) (op operand) {\n+\tif def.IsFromBlockParam() {\n+\t\treturn operandNR(def.BlkParamVReg)\n+\t}\n+\n+\tinstr := def.Instr\n+\tif instr.Constant() {\n+\t\tamount := byte(instr.ConstantVal()) & (shiftBitWidth - 1) // Clears the unnecessary bits.\n+\t\treturn operandShiftImm(amount)\n+\t}\n+\treturn m.getOperand_NR(def, mode)\n+}\n+\n+// ensureValueNR returns an operand of operandKindNR from the given value (defined by `def).\n+//\n+// `mode` is used to extend the operand if the bit length is smaller than mode.bits().\n+func (m *machine) getOperand_NR(def *backend.SSAValueDefinition, mode extMode) (op operand) {\n+\tvar v regalloc.VReg\n+\tif def.IsFromBlockParam() {\n+\t\tv = def.BlkParamVReg\n+\t} else {\n+\t\tinstr := def.Instr\n+\t\tif instr.Constant() {\n+\t\t\t// We inline all the constant instructions so that we could reduce the register usage.\n+\t\t\tv = m.lowerConstant(instr)\n+\t\t\tinstr.MarkLowered()\n+\t\t} else {\n+\t\t\tif n := def.N; n == 0 {\n+\t\t\t\tv = m.compiler.VRegOf(instr.Return())\n+\t\t\t} else {\n+\t\t\t\t_, rs := instr.Returns()\n+\t\t\t\tv = m.compiler.VRegOf(rs[n-1])\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tr := v\n+\tswitch inBits := def.SSAValue().Type().Bits(); {\n+\tcase mode == extModeNone:\n+\tcase inBits == 32 && (mode == extModeZeroExtend32 || mode == extModeSignExtend32):\n+\tcase inBits == 32 && mode == extModeZeroExtend64:\n+\t\textended := m.compiler.AllocateVReg(ssa.TypeI64)\n+\t\text := m.allocateInstr()\n+\t\text.asExtend(extended, v, 32, 64, false)\n+\t\tm.insert(ext)\n+\t\tr = extended\n+\tcase inBits == 32 && mode == extModeSignExtend64:\n+\t\textended := m.compiler.AllocateVReg(ssa.TypeI64)\n+\t\text := m.allocateInstr()\n+\t\text.asExtend(extended, v, 32, 64, true)\n+\t\tm.insert(ext)\n+\t\tr = extended\n+\tcase inBits == 64 && (mode == extModeZeroExtend64 || mode == extModeSignExtend64):\n+\t}\n+\treturn operandNR(r)\n+}\n+\n+func asImm12Operand(val uint64) (op operand, ok bool) {\n+\tv, shiftBit, ok := asImm12(val)\n+\tif !ok {\n+\t\treturn operand{}, false\n+\t}\n+\treturn operandImm12(v, shiftBit), true\n+}\n+\n+func asImm12(val uint64) (v uint16, shiftBit byte, ok bool) {\n+\tconst mask1, mask2 uint64 = 0xfff, 0xfff_000\n+\tif val&^mask1 == 0 {\n+\t\treturn uint16(val), 0, true\n+\t} else if val&^mask2 == 0 {\n+\t\treturn uint16(val >> 12), 1, true\n+\t} else {\n+\t\treturn 0, 0, false\n+\t}\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/lower_mem.go",
          "status": "added",
          "additions": 440,
          "deletions": 0,
          "patch": "@@ -0,0 +1,440 @@\n+package arm64\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+type (\n+\t// addressMode represents an ARM64 addressing mode.\n+\t//\n+\t// https://developer.arm.com/documentation/102374/0101/Loads-and-stores---addressing\n+\t// TODO: use the bit-packed layout like operand struct.\n+\taddressMode struct {\n+\t\tkind   addressModeKind\n+\t\trn, rm regalloc.VReg\n+\t\textOp  extendOp\n+\t\timm    int64\n+\t}\n+\n+\t// addressModeKind represents the kind of ARM64 addressing mode.\n+\taddressModeKind byte\n+)\n+\n+const (\n+\t// addressModeKindRegExtended takes a base register and an index register. The index register is sign/zero-extended,\n+\t// and then scaled by bits(type)/8.\n+\t//\n+\t// e.g.\n+\t// \t- ldrh w1, [x2, w3, SXTW #1] ;; sign-extended and scaled by 2 (== LSL #1)\n+\t// \t- strh w1, [x2, w3, UXTW #1] ;; zero-extended and scaled by 2 (== LSL #1)\n+\t// \t- ldr w1, [x2, w3, SXTW #2] ;; sign-extended and scaled by 4 (== LSL #2)\n+\t// \t- str x1, [x2, w3, UXTW #3] ;; zero-extended and scaled by 8 (== LSL #3)\n+\t//\n+\t// See the following pages:\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDRH--register---Load-Register-Halfword--register--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDR--register---Load-Register--register--\n+\taddressModeKindRegScaledExtended addressModeKind = iota\n+\n+\t// addressModeKindRegScaled is the same as addressModeKindRegScaledExtended, but without extension factor.\n+\taddressModeKindRegScaled\n+\n+\t// addressModeKindRegScaled is the same as addressModeKindRegScaledExtended, but without scale factor.\n+\taddressModeKindRegExtended\n+\n+\t// addressModeKindRegReg takes a base register and an index register. The index register is not either scaled or extended.\n+\taddressModeKindRegReg\n+\n+\t// addressModeKindRegSignedImm9 takes a base register and a 9-bit \"signed\" immediate offset (-256 to 255).\n+\t// The immediate will be sign-extended, and be added to the base register.\n+\t// This is a.k.a. \"unscaled\" since the immediate is not scaled.\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDUR--Load-Register--unscaled--\n+\taddressModeKindRegSignedImm9\n+\n+\t// addressModeKindRegUnsignedImm12 takes a base register and a 12-bit \"unsigned\" immediate offset.  scaled by\n+\t// the size of the type. In other words, the actual offset will be imm12 * bits(type)/8.\n+\t// See \"Unsigned offset\" in the following pages:\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDRB--immediate---Load-Register-Byte--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDRH--immediate---Load-Register-Halfword--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDR--immediate---Load-Register--immediate--\n+\taddressModeKindRegUnsignedImm12\n+\n+\t// addressModePostIndex takes a base register and a 9-bit \"signed\" immediate offset.\n+\t// After the load/store, the base register will be updated by the offset.\n+\t//\n+\t// Note that when this is used for pair load/store, the offset will be 7-bit \"signed\" immediate offset.\n+\t//\n+\t// See \"Post-index\" in the following pages for examples:\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDRB--immediate---Load-Register-Byte--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDRH--immediate---Load-Register-Halfword--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDR--immediate---Load-Register--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDP--Load-Pair-of-Registers-\n+\taddressModeKindPostIndex\n+\n+\t// addressModePostIndex takes a base register and a 9-bit \"signed\" immediate offset.\n+\t// Before the load/store, the base register will be updated by the offset.\n+\t//\n+\t// Note that when this is used for pair load/store, the offset will be 7-bit \"signed\" immediate offset.\n+\t//\n+\t// See \"Pre-index\" in the following pages for examples:\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDRB--immediate---Load-Register-Byte--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDRH--immediate---Load-Register-Halfword--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDR--immediate---Load-Register--immediate--\n+\t// https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDP--Load-Pair-of-Registers-\n+\taddressModeKindPreIndex\n+\n+\t// addressModeKindArgStackSpace is used to resolve the address of the argument stack space\n+\t// exiting right above the stack pointer. Since we don't know the exact stack space needed for a function\n+\t// at a compilation phase, this is used as a placeholder and further lowered to a real addressing mode like above.\n+\taddressModeKindArgStackSpace\n+\n+\t// addressModeKindResultStackSpace is used to resolve the address of the result stack space\n+\t// exiting right above the stack pointer. Since we don't know the exact stack space needed for a function\n+\t// at a compilation phase, this is used as a placeholder and further lowered to a real addressing mode like above.\n+\taddressModeKindResultStackSpace\n+)\n+\n+func (a addressMode) format(dstSizeBits byte) (ret string) {\n+\tbase := formatVRegSized(a.rn, 64)\n+\tif rn := a.rn; rn.RegType() != regalloc.RegTypeInt {\n+\t\tpanic(\"invalid base register type: \" + a.rn.RegType().String())\n+\t} else if rn.IsRealReg() && v0 <= a.rn.RealReg() && a.rn.RealReg() <= v30 {\n+\t\tpanic(\"BUG: likely a bug in reg alloc or reset behavior\")\n+\t}\n+\n+\tswitch a.kind {\n+\tcase addressModeKindRegScaledExtended:\n+\t\tamount := a.sizeInBitsToShiftAmount(dstSizeBits)\n+\t\tret = fmt.Sprintf(\"[%s, %s, %s #%#x]\", base, formatVRegSized(a.rm, a.indexRegBits()), a.extOp, amount)\n+\tcase addressModeKindRegScaled:\n+\t\tamount := a.sizeInBitsToShiftAmount(dstSizeBits)\n+\t\tret = fmt.Sprintf(\"[%s, %s, lsl #%#x]\", base, formatVRegSized(a.rm, a.indexRegBits()), amount)\n+\tcase addressModeKindRegExtended:\n+\t\tret = fmt.Sprintf(\"[%s, %s, %s]\", base, formatVRegSized(a.rm, a.indexRegBits()), a.extOp)\n+\tcase addressModeKindRegReg:\n+\t\tret = fmt.Sprintf(\"[%s, %s]\", base, formatVRegSized(a.rm, a.indexRegBits()))\n+\tcase addressModeKindRegSignedImm9:\n+\t\tif a.imm != 0 {\n+\t\t\tret = fmt.Sprintf(\"[%s, #%#x]\", base, a.imm)\n+\t\t} else {\n+\t\t\tret = fmt.Sprintf(\"[%s]\", base)\n+\t\t}\n+\tcase addressModeKindRegUnsignedImm12:\n+\t\tif a.imm != 0 {\n+\t\t\tret = fmt.Sprintf(\"[%s, #%#x]\", base, a.imm)\n+\t\t} else {\n+\t\t\tret = fmt.Sprintf(\"[%s]\", base)\n+\t\t}\n+\tcase addressModeKindPostIndex:\n+\t\tret = fmt.Sprintf(\"[%s], #%#x\", base, a.imm)\n+\tcase addressModeKindPreIndex:\n+\t\tret = fmt.Sprintf(\"[%s, #%#x]!\", base, a.imm)\n+\tcase addressModeKindArgStackSpace:\n+\t\tret = fmt.Sprintf(\"[#arg_space, #%#x]\", a.imm)\n+\tcase addressModeKindResultStackSpace:\n+\t\tret = fmt.Sprintf(\"[#ret_space, #%#x]\", a.imm)\n+\t}\n+\treturn\n+}\n+\n+func addressModePreOrPostIndex(rn regalloc.VReg, imm int64, preIndex bool) addressMode {\n+\tif !offsetFitsInAddressModeKindRegSignedImm9(imm) {\n+\t\tpanic(fmt.Sprintf(\"BUG: offset %#x does not fit in addressModeKindRegSignedImm9\", imm))\n+\t}\n+\tif preIndex {\n+\t\treturn addressMode{kind: addressModeKindPreIndex, rn: rn, imm: imm}\n+\t} else {\n+\t\treturn addressMode{kind: addressModeKindPostIndex, rn: rn, imm: imm}\n+\t}\n+}\n+\n+func offsetFitsInAddressModeKindRegUnsignedImm12(dstSizeInBits byte, offset int64) bool {\n+\tdivisor := int64(dstSizeInBits) / 8\n+\treturn 0 < offset && offset%divisor == 0 && offset/divisor < 4096\n+}\n+\n+func offsetFitsInAddressModeKindRegSignedImm9(offset int64) bool {\n+\treturn -256 <= offset && offset <= 255\n+}\n+\n+func (a addressMode) indexRegBits() byte {\n+\tbits := a.extOp.srcBits()\n+\tif bits != 32 && bits != 64 {\n+\t\tpanic(\"invalid index register for address mode. it must be either 32 or 64 bits\")\n+\t}\n+\treturn bits\n+}\n+\n+func (a addressMode) sizeInBitsToShiftAmount(sizeInBits byte) (lsl byte) {\n+\tswitch sizeInBits {\n+\tcase 8:\n+\t\tlsl = 0\n+\tcase 16:\n+\t\tlsl = 1\n+\tcase 32:\n+\t\tlsl = 2\n+\tcase 64:\n+\t\tlsl = 3\n+\t}\n+\treturn\n+}\n+\n+func extLoadSignSize(op ssa.Opcode) (size byte, signed bool) {\n+\tswitch op {\n+\tcase ssa.OpcodeUload8:\n+\t\tsize, signed = 8, false\n+\tcase ssa.OpcodeUload16:\n+\t\tsize, signed = 16, false\n+\tcase ssa.OpcodeUload32:\n+\t\tsize, signed = 32, false\n+\tcase ssa.OpcodeSload8:\n+\t\tsize, signed = 8, true\n+\tcase ssa.OpcodeSload16:\n+\t\tsize, signed = 16, true\n+\tcase ssa.OpcodeSload32:\n+\t\tsize, signed = 32, true\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\treturn\n+}\n+\n+func (m *machine) lowerExtLoad(op ssa.Opcode, ptr ssa.Value, offset uint32, ret regalloc.VReg) {\n+\tsize, signed := extLoadSignSize(op)\n+\tamode := m.lowerToAddressMode(ptr, offset, size)\n+\tload := m.allocateInstr()\n+\tif signed {\n+\t\tload.asSLoad(operandNR(ret), amode, size)\n+\t} else {\n+\t\tload.asULoad(operandNR(ret), amode, size)\n+\t}\n+\tm.insert(load)\n+}\n+\n+func (m *machine) lowerLoad(ptr ssa.Value, offset uint32, typ ssa.Type, ret ssa.Value) {\n+\tamode := m.lowerToAddressMode(ptr, offset, typ.Bits())\n+\n+\tdst := m.compiler.VRegOf(ret)\n+\tload := m.allocateInstr()\n+\tswitch typ {\n+\tcase ssa.TypeI32, ssa.TypeI64:\n+\t\tload.asULoad(operandNR(dst), amode, typ.Bits())\n+\tcase ssa.TypeF32, ssa.TypeF64:\n+\t\tload.asFpuLoad(operandNR(dst), amode, typ.Bits())\n+\tcase ssa.TypeV128:\n+\t\tload.asFpuLoad(operandNR(dst), amode, 128)\n+\tdefault:\n+\t\tpanic(\"TODO\")\n+\t}\n+\tm.insert(load)\n+}\n+\n+func (m *machine) lowerLoadSplat(ptr ssa.Value, offset uint32, lane ssa.VecLane, ret ssa.Value) {\n+\t// vecLoad1R has offset address mode (base+imm) only for post index, so we simply add the offset to the base.\n+\tbase := m.getOperand_NR(m.compiler.ValueDefinition(ptr), extModeNone).nr()\n+\toffsetReg := m.compiler.AllocateVReg(ssa.TypeI64)\n+\tm.lowerConstantI64(offsetReg, int64(offset))\n+\taddedBase := m.addReg64ToReg64(base, offsetReg)\n+\n+\trd := operandNR(m.compiler.VRegOf(ret))\n+\n+\tld1r := m.allocateInstr()\n+\tld1r.asVecLoad1R(rd, operandNR(addedBase), ssaLaneToArrangement(lane))\n+\tm.insert(ld1r)\n+}\n+\n+func (m *machine) lowerStore(si *ssa.Instruction) {\n+\t// TODO: merge consecutive stores into a single pair store instruction.\n+\tvalue, ptr, offset, storeSizeInBits := si.StoreData()\n+\tamode := m.lowerToAddressMode(ptr, offset, storeSizeInBits)\n+\n+\tvalueOp := m.getOperand_NR(m.compiler.ValueDefinition(value), extModeNone)\n+\tstore := m.allocateInstr()\n+\tstore.asStore(valueOp, amode, storeSizeInBits)\n+\tm.insert(store)\n+}\n+\n+// lowerToAddressMode converts a pointer to an addressMode that can be used as an operand for load/store instructions.\n+func (m *machine) lowerToAddressMode(ptr ssa.Value, offsetBase uint32, size byte) (amode addressMode) {\n+\t// TODO: currently the instruction selection logic doesn't support addressModeKindRegScaledExtended and\n+\t// addressModeKindRegScaled since collectAddends doesn't take ssa.OpcodeIshl into account. This should be fixed\n+\t// to support more efficient address resolution.\n+\n+\ta32s, a64s, offset := m.collectAddends(ptr)\n+\toffset += int64(offsetBase)\n+\treturn m.lowerToAddressModeFromAddends(a32s, a64s, size, offset)\n+}\n+\n+// lowerToAddressModeFromAddends creates an addressMode from a list of addends collected by collectAddends.\n+// During the construction, this might emit additional instructions.\n+//\n+// Extracted as a separate function for easy testing.\n+func (m *machine) lowerToAddressModeFromAddends(a32s *wazevoapi.Queue[addend32], a64s *wazevoapi.Queue[regalloc.VReg], size byte, offset int64) (amode addressMode) {\n+\tswitch a64sExist, a32sExist := !a64s.Empty(), !a32s.Empty(); {\n+\tcase a64sExist && a32sExist:\n+\t\tvar base regalloc.VReg\n+\t\tbase = a64s.Dequeue()\n+\t\tvar a32 addend32\n+\t\ta32 = a32s.Dequeue()\n+\t\tamode = addressMode{kind: addressModeKindRegExtended, rn: base, rm: a32.r, extOp: a32.ext}\n+\tcase a64sExist && offsetFitsInAddressModeKindRegUnsignedImm12(size, offset):\n+\t\tvar base regalloc.VReg\n+\t\tbase = a64s.Dequeue()\n+\t\tamode = addressMode{kind: addressModeKindRegUnsignedImm12, rn: base, imm: offset}\n+\t\toffset = 0\n+\tcase a64sExist && offsetFitsInAddressModeKindRegSignedImm9(offset):\n+\t\tvar base regalloc.VReg\n+\t\tbase = a64s.Dequeue()\n+\t\tamode = addressMode{kind: addressModeKindRegSignedImm9, rn: base, imm: offset}\n+\t\toffset = 0\n+\tcase a64sExist:\n+\t\tvar base regalloc.VReg\n+\t\tbase = a64s.Dequeue()\n+\t\tif !a64s.Empty() {\n+\t\t\tindex := a64s.Dequeue()\n+\t\t\tamode = addressMode{kind: addressModeKindRegReg, rn: base, rm: index, extOp: extendOpUXTX /* indicates index reg is 64-bit */}\n+\t\t} else {\n+\t\t\tamode = addressMode{kind: addressModeKindRegUnsignedImm12, rn: base, imm: 0}\n+\t\t}\n+\tcase a32sExist:\n+\t\tbase32 := a32s.Dequeue()\n+\n+\t\t// First we need 64-bit base.\n+\t\tbase := m.compiler.AllocateVReg(ssa.TypeI64)\n+\t\tbaseExt := m.allocateInstr()\n+\t\tvar signed bool\n+\t\tif base32.ext == extendOpSXTW {\n+\t\t\tsigned = true\n+\t\t}\n+\t\tbaseExt.asExtend(base, base32.r, 32, 64, signed)\n+\t\tm.insert(baseExt)\n+\n+\t\tif !a32s.Empty() {\n+\t\t\tindex := a32s.Dequeue()\n+\t\t\tamode = addressMode{kind: addressModeKindRegExtended, rn: base, rm: index.r, extOp: index.ext}\n+\t\t} else {\n+\t\t\tamode = addressMode{kind: addressModeKindRegUnsignedImm12, rn: base, imm: 0}\n+\t\t}\n+\tdefault: // Only static offsets.\n+\t\ttmpReg := m.compiler.AllocateVReg(ssa.TypeI64)\n+\t\tm.lowerConstantI64(tmpReg, offset)\n+\t\tamode = addressMode{kind: addressModeKindRegUnsignedImm12, rn: tmpReg, imm: 0}\n+\t\toffset = 0\n+\t}\n+\n+\tbaseReg := amode.rn\n+\tif offset > 0 {\n+\t\tbaseReg = m.addConstToReg64(baseReg, offset) // baseReg += offset\n+\t}\n+\n+\tfor !a64s.Empty() {\n+\t\ta64 := a64s.Dequeue()\n+\t\tbaseReg = m.addReg64ToReg64(baseReg, a64) // baseReg += a64\n+\t}\n+\n+\tfor !a32s.Empty() {\n+\t\ta32 := a32s.Dequeue()\n+\t\tbaseReg = m.addRegToReg64Ext(baseReg, a32.r, a32.ext) // baseReg += (a32 extended to 64-bit)\n+\t}\n+\tamode.rn = baseReg\n+\treturn\n+}\n+\n+var addendsMatchOpcodes = [4]ssa.Opcode{ssa.OpcodeUExtend, ssa.OpcodeSExtend, ssa.OpcodeIadd, ssa.OpcodeIconst}\n+\n+func (m *machine) collectAddends(ptr ssa.Value) (addends32 *wazevoapi.Queue[addend32], addends64 *wazevoapi.Queue[regalloc.VReg], offset int64) {\n+\tm.addendsWorkQueue.Reset()\n+\tm.addends32.Reset()\n+\tm.addends64.Reset()\n+\tm.addendsWorkQueue.Enqueue(ptr)\n+\n+\tfor !m.addendsWorkQueue.Empty() {\n+\t\tv := m.addendsWorkQueue.Dequeue()\n+\n+\t\tdef := m.compiler.ValueDefinition(v)\n+\t\tswitch op := m.compiler.MatchInstrOneOf(def, addendsMatchOpcodes[:]); op {\n+\t\tcase ssa.OpcodeIadd:\n+\t\t\t// If the addend is an add, we recursively collect its operands.\n+\t\t\tx, y := def.Instr.Arg2()\n+\t\t\tm.addendsWorkQueue.Enqueue(x)\n+\t\t\tm.addendsWorkQueue.Enqueue(y)\n+\t\t\tdef.Instr.MarkLowered()\n+\t\tcase ssa.OpcodeIconst:\n+\t\t\t// If the addend is constant, we just statically merge it into the offset.\n+\t\t\tic := def.Instr\n+\t\t\tu64 := ic.ConstantVal()\n+\t\t\tif ic.Return().Type().Bits() == 32 {\n+\t\t\t\toffset += int64(int32(u64)) // sign-extend.\n+\t\t\t} else {\n+\t\t\t\toffset += int64(u64)\n+\t\t\t}\n+\t\t\tdef.Instr.MarkLowered()\n+\t\tcase ssa.OpcodeUExtend, ssa.OpcodeSExtend:\n+\t\t\tinput := def.Instr.Arg()\n+\t\t\tif input.Type().Bits() != 32 {\n+\t\t\t\tpanic(\"illegal size: \" + input.Type().String())\n+\t\t\t}\n+\n+\t\t\tvar ext extendOp\n+\t\t\tif op == ssa.OpcodeUExtend {\n+\t\t\t\text = extendOpUXTW\n+\t\t\t} else {\n+\t\t\t\text = extendOpSXTW\n+\t\t\t}\n+\n+\t\t\tinputDef := m.compiler.ValueDefinition(input)\n+\t\t\tconstInst := inputDef.IsFromInstr() && inputDef.Instr.Constant()\n+\t\t\tswitch {\n+\t\t\tcase constInst && ext == extendOpUXTW:\n+\t\t\t\t// Zero-extension of a 32-bit constant can be merged into the offset.\n+\t\t\t\toffset += int64(uint32(inputDef.Instr.ConstantVal()))\n+\t\t\tcase constInst && ext == extendOpSXTW:\n+\t\t\t\t// Sign-extension of a 32-bit constant can be merged into the offset.\n+\t\t\t\toffset += int64(int32(inputDef.Instr.ConstantVal())) // sign-extend!\n+\t\t\tdefault:\n+\t\t\t\tm.addends32.Enqueue(addend32{r: m.getOperand_NR(inputDef, extModeNone).nr(), ext: ext})\n+\t\t\t}\n+\t\t\tdef.Instr.MarkLowered()\n+\t\t\tcontinue\n+\t\tdefault:\n+\t\t\t// If the addend is not one of them, we simply use it as-is (without merging!), optionally zero-extending it.\n+\t\t\tm.addends64.Enqueue(m.getOperand_NR(def, extModeZeroExtend64 /* optional zero ext */).nr())\n+\t\t}\n+\t}\n+\treturn &m.addends32, &m.addends64, offset\n+}\n+\n+func (m *machine) addConstToReg64(r regalloc.VReg, c int64) (rd regalloc.VReg) {\n+\trd = m.compiler.AllocateVReg(ssa.TypeI64)\n+\talu := m.allocateInstr()\n+\tif imm12Op, ok := asImm12Operand(uint64(c)); ok {\n+\t\talu.asALU(aluOpAdd, operandNR(rd), operandNR(r), imm12Op, true)\n+\t} else if imm12Op, ok = asImm12Operand(uint64(-c)); ok {\n+\t\talu.asALU(aluOpSub, operandNR(rd), operandNR(r), imm12Op, true)\n+\t} else {\n+\t\ttmp := m.compiler.AllocateVReg(ssa.TypeI64)\n+\t\tm.load64bitConst(c, tmp)\n+\t\talu.asALU(aluOpAdd, operandNR(rd), operandNR(r), operandNR(tmp), true)\n+\t}\n+\tm.insert(alu)\n+\treturn\n+}\n+\n+func (m *machine) addReg64ToReg64(rn, rm regalloc.VReg) (rd regalloc.VReg) {\n+\trd = m.compiler.AllocateVReg(ssa.TypeI64)\n+\talu := m.allocateInstr()\n+\talu.asALU(aluOpAdd, operandNR(rd), operandNR(rn), operandNR(rm), true)\n+\tm.insert(alu)\n+\treturn\n+}\n+\n+func (m *machine) addRegToReg64Ext(rn, rm regalloc.VReg, ext extendOp) (rd regalloc.VReg) {\n+\trd = m.compiler.AllocateVReg(ssa.TypeI64)\n+\talu := m.allocateInstr()\n+\talu.asALU(aluOpAdd, operandNR(rd), operandNR(rn), operandER(rm, ext, 64), true)\n+\tm.insert(alu)\n+\treturn\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/machine.go",
          "status": "added",
          "additions": 515,
          "deletions": 0,
          "patch": "@@ -0,0 +1,515 @@\n+package arm64\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"strings\"\n+\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/ssa\"\n+\t\"github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi\"\n+)\n+\n+type (\n+\t// machine implements backend.Machine.\n+\tmachine struct {\n+\t\tcompiler          backend.Compiler\n+\t\texecutableContext *backend.ExecutableContextT[instruction]\n+\t\tcurrentABI        *backend.FunctionABI\n+\n+\t\tregAlloc   regalloc.Allocator\n+\t\tregAllocFn *backend.RegAllocFunction[*instruction, *machine]\n+\n+\t\t// addendsWorkQueue is used during address lowering, defined here for reuse.\n+\t\taddendsWorkQueue wazevoapi.Queue[ssa.Value]\n+\t\taddends32        wazevoapi.Queue[addend32]\n+\t\t// addends64 is used during address lowering, defined here for reuse.\n+\t\taddends64              wazevoapi.Queue[regalloc.VReg]\n+\t\tunresolvedAddressModes []*instruction\n+\n+\t\t// condBrRelocs holds the conditional branches which need offset relocation.\n+\t\tcondBrRelocs []condBrReloc\n+\n+\t\t// jmpTableTargets holds the labels of the jump table targets.\n+\t\tjmpTableTargets [][]uint32\n+\n+\t\t// spillSlotSize is the size of the stack slot in bytes used for spilling registers.\n+\t\t// During the execution of the function, the stack looks like:\n+\t\t//\n+\t\t//\n+\t\t//            (high address)\n+\t\t//          +-----------------+\n+\t\t//          |     .......     |\n+\t\t//          |      ret Y      |\n+\t\t//          |     .......     |\n+\t\t//          |      ret 0      |\n+\t\t//          |      arg X      |\n+\t\t//          |     .......     |\n+\t\t//          |      arg 1      |\n+\t\t//          |      arg 0      |\n+\t\t//          |      xxxxx      |\n+\t\t//          |   ReturnAddress |\n+\t\t//          +-----------------+   <<-|\n+\t\t//          |   ...........   |      |\n+\t\t//          |   spill slot M  |      | <--- spillSlotSize\n+\t\t//          |   ............  |      |\n+\t\t//          |   spill slot 2  |      |\n+\t\t//          |   spill slot 1  |   <<-+\n+\t\t//          |   clobbered N   |\n+\t\t//          |   ...........   |\n+\t\t//          |   clobbered 1   |\n+\t\t//          |   clobbered 0   |\n+\t\t//   SP---> +-----------------+\n+\t\t//             (low address)\n+\t\t//\n+\t\t// and it represents the size of the space between FP and the first spilled slot. This must be a multiple of 16.\n+\t\t// Also note that this is only known after register allocation.\n+\t\tspillSlotSize int64\n+\t\tspillSlots    map[regalloc.VRegID]int64 // regalloc.VRegID to offset.\n+\t\t// clobberedRegs holds real-register backed VRegs saved at the function prologue, and restored at the epilogue.\n+\t\tclobberedRegs []regalloc.VReg\n+\n+\t\tmaxRequiredStackSizeForCalls int64\n+\t\tstackBoundsCheckDisabled     bool\n+\n+\t\tregAllocStarted bool\n+\t}\n+\n+\taddend32 struct {\n+\t\tr   regalloc.VReg\n+\t\text extendOp\n+\t}\n+\n+\tcondBrReloc struct {\n+\t\tcbr *instruction\n+\t\t// currentLabelPos is the labelPosition within which condBr is defined.\n+\t\tcurrentLabelPos *labelPosition\n+\t\t// Next block's labelPosition.\n+\t\tnextLabel label\n+\t\toffset    int64\n+\t}\n+\n+\tlabelPosition = backend.LabelPosition[instruction]\n+\tlabel         = backend.Label\n+)\n+\n+const (\n+\tlabelReturn  = backend.LabelReturn\n+\tlabelInvalid = backend.LabelInvalid\n+)\n+\n+// NewBackend returns a new backend for arm64.\n+func NewBackend() backend.Machine {\n+\tm := &machine{\n+\t\tspillSlots:        make(map[regalloc.VRegID]int64),\n+\t\texecutableContext: newExecutableContext(),\n+\t\tregAlloc:          regalloc.NewAllocator(regInfo),\n+\t}\n+\treturn m\n+}\n+\n+func newExecutableContext() *backend.ExecutableContextT[instruction] {\n+\treturn backend.NewExecutableContextT[instruction](resetInstruction, setNext, setPrev, asNop0)\n+}\n+\n+// ExecutableContext implements backend.Machine.\n+func (m *machine) ExecutableContext() backend.ExecutableContext {\n+\treturn m.executableContext\n+}\n+\n+// RegAlloc implements backend.Machine Function.\n+func (m *machine) RegAlloc() {\n+\trf := m.regAllocFn\n+\tfor _, pos := range m.executableContext.OrderedBlockLabels {\n+\t\trf.AddBlock(pos.SB, pos.L, pos.Begin, pos.End)\n+\t}\n+\n+\tm.regAllocStarted = true\n+\tm.regAlloc.DoAllocation(rf)\n+\t// Now that we know the final spill slot size, we must align spillSlotSize to 16 bytes.\n+\tm.spillSlotSize = (m.spillSlotSize + 15) &^ 15\n+}\n+\n+// Reset implements backend.Machine.\n+func (m *machine) Reset() {\n+\tm.clobberedRegs = m.clobberedRegs[:0]\n+\tfor key := range m.spillSlots {\n+\t\tm.clobberedRegs = append(m.clobberedRegs, regalloc.VReg(key))\n+\t}\n+\tfor _, key := range m.clobberedRegs {\n+\t\tdelete(m.spillSlots, regalloc.VRegID(key))\n+\t}\n+\tm.clobberedRegs = m.clobberedRegs[:0]\n+\tm.regAllocStarted = false\n+\tm.regAlloc.Reset()\n+\tm.regAllocFn.Reset()\n+\tm.spillSlotSize = 0\n+\tm.unresolvedAddressModes = m.unresolvedAddressModes[:0]\n+\tm.maxRequiredStackSizeForCalls = 0\n+\tm.executableContext.Reset()\n+\tm.jmpTableTargets = m.jmpTableTargets[:0]\n+}\n+\n+// SetCurrentABI implements backend.Machine SetCurrentABI.\n+func (m *machine) SetCurrentABI(abi *backend.FunctionABI) {\n+\tm.currentABI = abi\n+}\n+\n+// DisableStackCheck implements backend.Machine DisableStackCheck.\n+func (m *machine) DisableStackCheck() {\n+\tm.stackBoundsCheckDisabled = true\n+}\n+\n+// SetCompiler implements backend.Machine.\n+func (m *machine) SetCompiler(ctx backend.Compiler) {\n+\tm.compiler = ctx\n+\tm.regAllocFn = backend.NewRegAllocFunction[*instruction, *machine](m, ctx.SSABuilder(), ctx)\n+}\n+\n+func (m *machine) insert(i *instruction) {\n+\tectx := m.executableContext\n+\tectx.PendingInstructions = append(ectx.PendingInstructions, i)\n+}\n+\n+func (m *machine) insertBrTargetLabel() label {\n+\tnop, l := m.allocateBrTarget()\n+\tm.insert(nop)\n+\treturn l\n+}\n+\n+func (m *machine) allocateBrTarget() (nop *instruction, l label) {\n+\tectx := m.executableContext\n+\tl = ectx.AllocateLabel()\n+\tnop = m.allocateInstr()\n+\tnop.asNop0WithLabel(l)\n+\tpos := ectx.AllocateLabelPosition(l)\n+\tpos.Begin, pos.End = nop, nop\n+\tectx.LabelPositions[l] = pos\n+\treturn\n+}\n+\n+// allocateInstr allocates an instruction.\n+func (m *machine) allocateInstr() *instruction {\n+\tinstr := m.executableContext.InstructionPool.Allocate()\n+\tif !m.regAllocStarted {\n+\t\tinstr.addedBeforeRegAlloc = true\n+\t}\n+\treturn instr\n+}\n+\n+func resetInstruction(i *instruction) {\n+\t*i = instruction{}\n+}\n+\n+func (m *machine) allocateNop() *instruction {\n+\tinstr := m.allocateInstr()\n+\tinstr.asNop0()\n+\treturn instr\n+}\n+\n+func (m *machine) resolveAddressingMode(arg0offset, ret0offset int64, i *instruction) {\n+\tamode := &i.amode\n+\tswitch amode.kind {\n+\tcase addressModeKindResultStackSpace:\n+\t\tamode.imm += ret0offset\n+\tcase addressModeKindArgStackSpace:\n+\t\tamode.imm += arg0offset\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\n+\tvar sizeInBits byte\n+\tswitch i.kind {\n+\tcase store8, uLoad8:\n+\t\tsizeInBits = 8\n+\tcase store16, uLoad16:\n+\t\tsizeInBits = 16\n+\tcase store32, fpuStore32, uLoad32, fpuLoad32:\n+\t\tsizeInBits = 32\n+\tcase store64, fpuStore64, uLoad64, fpuLoad64:\n+\t\tsizeInBits = 64\n+\tcase fpuStore128, fpuLoad128:\n+\t\tsizeInBits = 128\n+\tdefault:\n+\t\tpanic(\"BUG\")\n+\t}\n+\n+\tif offsetFitsInAddressModeKindRegUnsignedImm12(sizeInBits, amode.imm) {\n+\t\tamode.kind = addressModeKindRegUnsignedImm12\n+\t} else {\n+\t\t// This case, we load the offset into the temporary register,\n+\t\t// and then use it as the index register.\n+\t\tnewPrev := m.lowerConstantI64AndInsert(i.prev, tmpRegVReg, amode.imm)\n+\t\tlinkInstr(newPrev, i)\n+\t\t*amode = addressMode{kind: addressModeKindRegReg, rn: amode.rn, rm: tmpRegVReg, extOp: extendOpUXTX /* indicates rm reg is 64-bit */}\n+\t}\n+}\n+\n+// resolveRelativeAddresses resolves the relative addresses before encoding.\n+func (m *machine) resolveRelativeAddresses(ctx context.Context) {\n+\tectx := m.executableContext\n+\tfor {\n+\t\tif len(m.unresolvedAddressModes) > 0 {\n+\t\t\targ0offset, ret0offset := m.arg0OffsetFromSP(), m.ret0OffsetFromSP()\n+\t\t\tfor _, i := range m.unresolvedAddressModes {\n+\t\t\t\tm.resolveAddressingMode(arg0offset, ret0offset, i)\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Reuse the slice to gather the unresolved conditional branches.\n+\t\tm.condBrRelocs = m.condBrRelocs[:0]\n+\n+\t\tvar fn string\n+\t\tvar fnIndex int\n+\t\tvar labelToSSABlockID map[label]ssa.BasicBlockID\n+\t\tif wazevoapi.PerfMapEnabled {\n+\t\t\tfn = wazevoapi.GetCurrentFunctionName(ctx)\n+\t\t\tlabelToSSABlockID = make(map[label]ssa.BasicBlockID)\n+\t\t\tfor i, l := range ectx.SsaBlockIDToLabels {\n+\t\t\t\tlabelToSSABlockID[l] = ssa.BasicBlockID(i)\n+\t\t\t}\n+\t\t\tfnIndex = wazevoapi.GetCurrentFunctionIndex(ctx)\n+\t\t}\n+\n+\t\t// Next, in order to determine the offsets of relative jumps, we have to calculate the size of each label.\n+\t\tvar offset int64\n+\t\tfor i, pos := range ectx.OrderedBlockLabels {\n+\t\t\tpos.BinaryOffset = offset\n+\t\t\tvar size int64\n+\t\t\tfor cur := pos.Begin; ; cur = cur.next {\n+\t\t\t\tswitch cur.kind {\n+\t\t\t\tcase nop0:\n+\t\t\t\t\tl := cur.nop0Label()\n+\t\t\t\t\tif pos, ok := ectx.LabelPositions[l]; ok {\n+\t\t\t\t\t\tpos.BinaryOffset = offset + size\n+\t\t\t\t\t}\n+\t\t\t\tcase condBr:\n+\t\t\t\t\tif !cur.condBrOffsetResolved() {\n+\t\t\t\t\t\tvar nextLabel label\n+\t\t\t\t\t\tif i < len(ectx.OrderedBlockLabels)-1 {\n+\t\t\t\t\t\t\t// Note: this is only used when the block ends with fallthrough,\n+\t\t\t\t\t\t\t// therefore can be safely assumed that the next block exists when it's needed.\n+\t\t\t\t\t\t\tnextLabel = ectx.OrderedBlockLabels[i+1].L\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tm.condBrRelocs = append(m.condBrRelocs, condBrReloc{\n+\t\t\t\t\t\t\tcbr: cur, currentLabelPos: pos, offset: offset + size,\n+\t\t\t\t\t\t\tnextLabel: nextLabel,\n+\t\t\t\t\t\t})\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tsize += cur.size()\n+\t\t\t\tif cur == pos.End {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif wazevoapi.PerfMapEnabled {\n+\t\t\t\tif size > 0 {\n+\t\t\t\t\tl := pos.L\n+\t\t\t\t\tvar labelStr string\n+\t\t\t\t\tif blkID, ok := labelToSSABlockID[l]; ok {\n+\t\t\t\t\t\tlabelStr = fmt.Sprintf(\"%s::SSA_Block[%s]\", l, blkID)\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tlabelStr = l.String()\n+\t\t\t\t\t}\n+\t\t\t\t\twazevoapi.PerfMap.AddModuleEntry(fnIndex, offset, uint64(size), fmt.Sprintf(\"%s:::::%s\", fn, labelStr))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\toffset += size\n+\t\t}\n+\n+\t\t// Before resolving any offsets, we need to check if all the conditional branches can be resolved.\n+\t\tvar needRerun bool\n+\t\tfor i := range m.condBrRelocs {\n+\t\t\treloc := &m.condBrRelocs[i]\n+\t\t\tcbr := reloc.cbr\n+\t\t\toffset := reloc.offset\n+\n+\t\t\ttarget := cbr.condBrLabel()\n+\t\t\toffsetOfTarget := ectx.LabelPositions[target].BinaryOffset\n+\t\t\tdiff := offsetOfTarget - offset\n+\t\t\tif divided := diff >> 2; divided < minSignedInt19 || divided > maxSignedInt19 {\n+\t\t\t\t// This case the conditional branch is too huge. We place the trampoline instructions at the end of the current block,\n+\t\t\t\t// and jump to it.\n+\t\t\t\tm.insertConditionalJumpTrampoline(cbr, reloc.currentLabelPos, reloc.nextLabel)\n+\t\t\t\t// Then, we need to recall this function to fix up the label offsets\n+\t\t\t\t// as they have changed after the trampoline is inserted.\n+\t\t\t\tneedRerun = true\n+\t\t\t}\n+\t\t}\n+\t\tif needRerun {\n+\t\t\tif wazevoapi.PerfMapEnabled {\n+\t\t\t\twazevoapi.PerfMap.Clear()\n+\t\t\t}\n+\t\t} else {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\n+\tvar currentOffset int64\n+\tfor cur := ectx.RootInstr; cur != nil; cur = cur.next {\n+\t\tswitch cur.kind {\n+\t\tcase br:\n+\t\t\ttarget := cur.brLabel()\n+\t\t\toffsetOfTarget := ectx.LabelPositions[target].BinaryOffset\n+\t\t\tdiff := offsetOfTarget - currentOffset\n+\t\t\tdivided := diff >> 2\n+\t\t\tif divided < minSignedInt26 || divided > maxSignedInt26 {\n+\t\t\t\t// This means the currently compiled single function is extremely large.\n+\t\t\t\tpanic(\"too large function that requires branch relocation of large unconditional branch larger than 26-bit range\")\n+\t\t\t}\n+\t\t\tcur.brOffsetResolve(diff)\n+\t\tcase condBr:\n+\t\t\tif !cur.condBrOffsetResolved() {\n+\t\t\t\ttarget := cur.condBrLabel()\n+\t\t\t\toffsetOfTarget := ectx.LabelPositions[target].BinaryOffset\n+\t\t\t\tdiff := offsetOfTarget - currentOffset\n+\t\t\t\tif divided := diff >> 2; divided < minSignedInt19 || divided > maxSignedInt19 {\n+\t\t\t\t\tpanic(\"BUG: branch relocation for large conditional branch larger than 19-bit range must be handled properly\")\n+\t\t\t\t}\n+\t\t\t\tcur.condBrOffsetResolve(diff)\n+\t\t\t}\n+\t\tcase brTableSequence:\n+\t\t\ttableIndex := cur.u1\n+\t\t\ttargets := m.jmpTableTargets[tableIndex]\n+\t\t\tfor i := range targets {\n+\t\t\t\tl := label(targets[i])\n+\t\t\t\toffsetOfTarget := ectx.LabelPositions[l].BinaryOffset\n+\t\t\t\tdiff := offsetOfTarget - (currentOffset + brTableSequenceOffsetTableBegin)\n+\t\t\t\ttargets[i] = uint32(diff)\n+\t\t\t}\n+\t\t\tcur.brTableSequenceOffsetsResolved()\n+\t\tcase emitSourceOffsetInfo:\n+\t\t\tm.compiler.AddSourceOffsetInfo(currentOffset, cur.sourceOffsetInfo())\n+\t\t}\n+\t\tcurrentOffset += cur.size()\n+\t}\n+}\n+\n+const (\n+\tmaxSignedInt26 = 1<<25 - 1\n+\tminSignedInt26 = -(1 << 25)\n+\n+\tmaxSignedInt19 = 1<<18 - 1\n+\tminSignedInt19 = -(1 << 18)\n+)\n+\n+func (m *machine) insertConditionalJumpTrampoline(cbr *instruction, currentBlk *labelPosition, nextLabel label) {\n+\tcur := currentBlk.End\n+\toriginalTarget := cbr.condBrLabel()\n+\tendNext := cur.next\n+\n+\tif cur.kind != br {\n+\t\t// If the current block ends with a conditional branch, we can just insert the trampoline after it.\n+\t\t// Otherwise, we need to insert \"skip\" instruction to skip the trampoline instructions.\n+\t\tskip := m.allocateInstr()\n+\t\tskip.asBr(nextLabel)\n+\t\tcur = linkInstr(cur, skip)\n+\t}\n+\n+\tcbrNewTargetInstr, cbrNewTargetLabel := m.allocateBrTarget()\n+\tcbr.setCondBrTargets(cbrNewTargetLabel)\n+\tcur = linkInstr(cur, cbrNewTargetInstr)\n+\n+\t// Then insert the unconditional branch to the original, which should be possible to get encoded\n+\t// as 26-bit offset should be enough for any practical application.\n+\tbr := m.allocateInstr()\n+\tbr.asBr(originalTarget)\n+\tcur = linkInstr(cur, br)\n+\n+\t// Update the end of the current block.\n+\tcurrentBlk.End = cur\n+\n+\tlinkInstr(cur, endNext)\n+}\n+\n+// Format implements backend.Machine.\n+func (m *machine) Format() string {\n+\tectx := m.executableContext\n+\tbegins := map[*instruction]label{}\n+\tfor l, pos := range ectx.LabelPositions {\n+\t\tbegins[pos.Begin] = l\n+\t}\n+\n+\tirBlocks := map[label]ssa.BasicBlockID{}\n+\tfor i, l := range ectx.SsaBlockIDToLabels {\n+\t\tirBlocks[l] = ssa.BasicBlockID(i)\n+\t}\n+\n+\tvar lines []string\n+\tfor cur := ectx.RootInstr; cur != nil; cur = cur.next {\n+\t\tif l, ok := begins[cur]; ok {\n+\t\t\tvar labelStr string\n+\t\t\tif blkID, ok := irBlocks[l]; ok {\n+\t\t\t\tlabelStr = fmt.Sprintf(\"%s (SSA Block: %s):\", l, blkID)\n+\t\t\t} else {\n+\t\t\t\tlabelStr = fmt.Sprintf(\"%s:\", l)\n+\t\t\t}\n+\t\t\tlines = append(lines, labelStr)\n+\t\t}\n+\t\tif cur.kind == nop0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tlines = append(lines, \"\\t\"+cur.String())\n+\t}\n+\treturn \"\\n\" + strings.Join(lines, \"\\n\") + \"\\n\"\n+}\n+\n+// InsertReturn implements backend.Machine.\n+func (m *machine) InsertReturn() {\n+\ti := m.allocateInstr()\n+\ti.asRet()\n+\tm.insert(i)\n+}\n+\n+func (m *machine) getVRegSpillSlotOffsetFromSP(id regalloc.VRegID, size byte) int64 {\n+\toffset, ok := m.spillSlots[id]\n+\tif !ok {\n+\t\toffset = m.spillSlotSize\n+\t\t// TODO: this should be aligned depending on the `size` to use Imm12 offset load/store as much as possible.\n+\t\tm.spillSlots[id] = offset\n+\t\tm.spillSlotSize += int64(size)\n+\t}\n+\treturn offset + 16 // spill slot starts above the clobbered registers and the frame size.\n+}\n+\n+func (m *machine) clobberedRegSlotSize() int64 {\n+\treturn int64(len(m.clobberedRegs) * 16)\n+}\n+\n+func (m *machine) arg0OffsetFromSP() int64 {\n+\treturn m.frameSize() +\n+\t\t16 + // 16-byte aligned return address\n+\t\t16 // frame size saved below the clobbered registers.\n+}\n+\n+func (m *machine) ret0OffsetFromSP() int64 {\n+\treturn m.arg0OffsetFromSP() + m.currentABI.ArgStackSize\n+}\n+\n+func (m *machine) requiredStackSize() int64 {\n+\treturn m.maxRequiredStackSizeForCalls +\n+\t\tm.frameSize() +\n+\t\t16 + // 16-byte aligned return address.\n+\t\t16 // frame size saved below the clobbered registers.\n+}\n+\n+func (m *machine) frameSize() int64 {\n+\ts := m.clobberedRegSlotSize() + m.spillSlotSize\n+\tif s&0xf != 0 {\n+\t\tpanic(fmt.Errorf(\"BUG: frame size %d is not 16-byte aligned\", s))\n+\t}\n+\treturn s\n+}\n+\n+func (m *machine) addJmpTableTarget(targets []ssa.BasicBlock) (index int) {\n+\t// TODO: reuse the slice!\n+\tlabels := make([]uint32, len(targets))\n+\tfor j, target := range targets {\n+\t\tlabels[j] = uint32(m.executableContext.GetOrAllocateSSABlockLabel(target))\n+\t}\n+\tindex = len(m.jmpTableTargets)\n+\tm.jmpTableTargets = append(m.jmpTableTargets, labels)\n+\treturn\n+}"
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/machine_pro_epi_logue.go",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/machine_regalloc.go",
          "status": "added",
          "additions": 152,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/machine_relocation.go",
          "status": "added",
          "additions": 118,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/reg.go",
          "status": "added",
          "additions": 397,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/isa/arm64/unwind_stack.go",
          "status": "added",
          "additions": 90,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/machine.go",
          "status": "added",
          "additions": 100,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc.go",
          "status": "added",
          "additions": 319,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc/api.go",
          "status": "added",
          "additions": 136,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc/reg.go",
          "status": "added",
          "additions": 123,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc/regalloc.go",
          "status": "added",
          "additions": 1212,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/regalloc/regset.go",
          "status": "added",
          "additions": 108,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/backend/vdef.go",
          "status": "added",
          "additions": 43,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/call_engine.go",
          "status": "added",
          "additions": 722,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/engine.go",
          "status": "added",
          "additions": 845,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/engine_cache.go",
          "status": "added",
          "additions": 296,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/entrypoint_amd64.go",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/entrypoint_arm64.go",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/entrypoint_others.go",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/frontend/frontend.go",
          "status": "added",
          "additions": 594,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/frontend/lower.go",
          "status": "added",
          "additions": 4268,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/frontend/misc.go",
          "status": "added",
          "additions": 10,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/frontend/sort_id.go",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/frontend/sort_id_old.go",
          "status": "added",
          "additions": 17,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/hostmodule.go",
          "status": "added",
          "additions": 82,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/isa_amd64.go",
          "status": "added",
          "additions": 30,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/isa_arm64.go",
          "status": "added",
          "additions": 32,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/isa_other.go",
          "status": "added",
          "additions": 29,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/memmove.go",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/module_engine.go",
          "status": "added",
          "additions": 344,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/reflect.go",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/reflect_tinygo.go",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/basic_block.go",
          "status": "added",
          "additions": 407,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/basic_block_sort.go",
          "status": "added",
          "additions": 34,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/basic_block_sort_old.go",
          "status": "added",
          "additions": 24,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/builder.go",
          "status": "added",
          "additions": 731,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/cmp.go",
          "status": "added",
          "additions": 107,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/funcref.go",
          "status": "added",
          "additions": 12,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/instructions.go",
          "status": "added",
          "additions": 2959,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/pass.go",
          "status": "added",
          "additions": 417,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/pass_blk_layouts.go",
          "status": "added",
          "additions": 335,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/pass_cfg.go",
          "status": "added",
          "additions": 312,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/signature.go",
          "status": "added",
          "additions": 49,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/ssa.go",
          "status": "added",
          "additions": 14,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/type.go",
          "status": "added",
          "additions": 112,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/ssa/vs.go",
          "status": "added",
          "additions": 87,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/debug_options.go",
          "status": "added",
          "additions": 196,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/exitcode.go",
          "status": "added",
          "additions": 109,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/offsetdata.go",
          "status": "added",
          "additions": 216,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/perfmap.go",
          "status": "added",
          "additions": 96,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/perfmap_disabled.go",
          "status": "added",
          "additions": 5,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/perfmap_enabled.go",
          "status": "added",
          "additions": 5,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/pool.go",
          "status": "added",
          "additions": 215,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/ptr.go",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/queue.go",
          "status": "added",
          "additions": 26,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/engine/wazevo/wazevoapi/resetmap.go",
          "status": "added",
          "additions": 13,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/expctxkeys/checkpoint.go",
          "status": "added",
          "additions": 10,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/expctxkeys/close.go",
          "status": "added",
          "additions": 5,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/expctxkeys/expctxkeys.go",
          "status": "added",
          "additions": 2,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/expctxkeys/listener.go",
          "status": "added",
          "additions": 7,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/expctxkeys/memory.go",
          "status": "added",
          "additions": 4,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/fsapi/poll.go",
          "status": "added",
          "additions": 20,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/platform/cpuid.go",
          "status": "added",
          "additions": 25,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/platform/cpuid_unsupported.go",
          "status": "added",
          "additions": 14,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sock/sock_supported.go",
          "status": "added",
          "additions": 11,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sock/sock_unsupported.go",
          "status": "added",
          "additions": 10,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/datasync_tinygo.go",
          "status": "added",
          "additions": 13,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/dirfs_supported.go",
          "status": "added",
          "additions": 42,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/dirfs_unsupported.go",
          "status": "added",
          "additions": 34,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/ino.go",
          "status": "added",
          "additions": 22,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/ino_plan9.go",
          "status": "added",
          "additions": 15,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/ino_tinygo.go",
          "status": "added",
          "additions": 14,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/ino_windows.go",
          "status": "added",
          "additions": 28,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/nonblock_unsupported.go",
          "status": "added",
          "additions": 13,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/open_file_tinygo.go",
          "status": "added",
          "additions": 25,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/poll.go",
          "status": "added",
          "additions": 18,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/poll_darwin.go",
          "status": "added",
          "additions": 55,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/poll_darwin.s",
          "status": "added",
          "additions": 8,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/poll_linux.go",
          "status": "added",
          "additions": 59,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/poll_unsupported.go",
          "status": "added",
          "additions": 13,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/poll_windows.go",
          "status": "added",
          "additions": 224,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/rename_plan9.go",
          "status": "added",
          "additions": 14,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/sock_supported.go",
          "status": "added",
          "additions": 77,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/sysfs/unlink_plan9.go",
          "status": "added",
          "additions": 12,
          "deletions": 0,
          "patch": null
        },
        {
          "filename": "vendor/github.com/tetratelabs/wazero/internal/wasm/module_instance_lookup.go",
          "status": "added",
          "additions": 73,
          "deletions": 0,
          "patch": null
        }
      ],
      "file_patterns": {
        "security_files": 5,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 16,
        "max_directory_depth": 10
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "53c61cb0241f30492bf719979b10647c9d48c7b4",
            "date": "2025-01-07T15:25:50Z",
            "author_login": "TimBF"
          },
          {
            "sha": "3d70247f8b74adc8035dad0e70bf5b4d510e261d",
            "date": "2025-01-07T14:33:29Z",
            "author_login": "TimBF"
          },
          {
            "sha": "52b1c7b93b5675f62ba7d896e5cde0eff32b4a00",
            "date": "2025-01-07T13:57:24Z",
            "author_login": "TimBF"
          },
          {
            "sha": "daeb463f77950e72f3cdcffcd1db6dbc9b5de31e",
            "date": "2025-01-07T13:46:32Z",
            "author_login": "TimBF"
          },
          {
            "sha": "0c75f70a1de3cb6efff753d3d8a798b9a0fdcd07",
            "date": "2025-01-06T19:41:51Z",
            "author_login": "moloch--"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.2,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:H",
    "cwe_id": "CWE-74",
    "description": "Sliver is an open source cross-platform adversary emulation/red team framework, it can be used by organizations of all sizes to perform security testing. Sliver version 1.6.0 (prerelease) is vulnerable to RCE on the teamserver by a low-privileged \"operator\" user. The RCE is as the system root user. The exploit is pretty fun as we make the Sliver server pwn itself. As described in a past issue (#65), \"there is a clear security boundary between the operator and server, an operator should not inherently be able to run commands or code on the server.\" An operator who exploited this vulnerability would be able to view all console logs, kick all other operators, view and modify files stored on the server, and ultimately delete the server. This issue has not yet be addressed but is expected to be resolved before the full release of version 1.6.0. Users of the 1.6.0 prerelease should avoid using Silver in production.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-07-18T23:15:02.280",
    "last_modified": "2024-11-21T09:32:15.313",
    "fix_date": "2024-04-30T21:34:59Z"
  },
  "references": [
    {
      "url": "https://github.com/BishopFox/sliver/commit/5016fb8d7cdff38c79e22e8293e58300f8d3bd57",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/BishopFox/sliver/issues/65",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/BishopFox/sliver/pull/1281",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/BishopFox/sliver/security/advisories/GHSA-hc5w-gxxr-w8x8",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://sliver.sh/docs?name=Multi-player+Mode",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/BishopFox/sliver/commit/5016fb8d7cdff38c79e22e8293e58300f8d3bd57",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://github.com/BishopFox/sliver/issues/65",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://github.com/BishopFox/sliver/pull/1281",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://github.com/BishopFox/sliver/security/advisories/GHSA-hc5w-gxxr-w8x8",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://sliver.sh/docs?name=Multi-player+Mode",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:31.494972",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "sliver",
    "owner": "BishopFox",
    "created_at": "2019-01-17T22:07:38Z",
    "updated_at": "2025-01-14T14:04:44Z",
    "pushed_at": "2025-01-13T03:30:32Z",
    "size": 172583,
    "stars": 8768,
    "forks": 1173,
    "open_issues": 217,
    "watchers": 8768,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "Go": 3371789,
      "Shell": 14048,
      "Makefile": 10918,
      "Rust": 4283,
      "Python": 3422,
      "Dockerfile": 3387,
      "C": 1809
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "gpl-3.0"
    },
    "collected_at": "2025-01-14T15:05:20.449005"
  }
}