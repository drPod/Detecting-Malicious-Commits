{
  "cve_id": "CVE-2024-42478",
  "github_data": {
    "repository": "ggerganov/llama.cpp",
    "fix_commit": "b72942fac998672a79a1ae3c03b340f7e629980b",
    "related_commits": [
      "b72942fac998672a79a1ae3c03b340f7e629980b"
    ],
    "patch_url": "https://github.com/ggerganov/llama.cpp/commit/b72942fac998672a79a1ae3c03b340f7e629980b.patch",
    "fix_commit_details": {
      "sha": "b72942fac998672a79a1ae3c03b340f7e629980b",
      "commit_date": "2024-08-09T20:03:21Z",
      "author": {
        "login": "ggerganov",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "Merge commit from fork",
        "length": 22,
        "has_description": false,
        "references_issue": false
      },
      "stats": {
        "total": 56,
        "additions": 53,
        "deletions": 3
      },
      "files": [
        {
          "filename": "examples/rpc/README.md",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -1,5 +1,9 @@\n ## Overview\n \n+> [!IMPORTANT]\n+> This example and the RPC backend are currently in a proof-of-concept development stage. As such, the functionality is fragile and\n+> insecure. **Never run the RPC server on an open network or in a sensitive environment!**\n+\n The `rpc-server` allows  running `ggml` backend on a remote host.\n The RPC backend communicates with one or several instances of `rpc-server` and offloads computations to them.\n This can be used for distributed LLM inference with `llama.cpp` in the following way:"
        },
        {
          "filename": "examples/rpc/rpc-server.cpp",
          "status": "modified",
          "additions": 12,
          "deletions": 1,
          "patch": "@@ -16,7 +16,7 @@\n #include <stdio.h>\n \n struct rpc_server_params {\n-    std::string host        = \"0.0.0.0\";\n+    std::string host        = \"127.0.0.1\";\n     int         port        = 50052;\n     size_t      backend_mem = 0;\n };\n@@ -114,6 +114,17 @@ int main(int argc, char * argv[]) {\n         fprintf(stderr, \"Invalid parameters\\n\");\n         return 1;\n     }\n+\n+    if (params.host != \"127.0.0.1\") {\n+        fprintf(stderr, \"\\n\");\n+        fprintf(stderr, \"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n+        fprintf(stderr, \"WARNING: Host ('%s') is != '127.0.0.1'\\n\", params.host.c_str());\n+        fprintf(stderr, \"         Never expose the RPC server to an open network!\\n\");\n+        fprintf(stderr, \"         This is an experimental feature and is not secure!\\n\");\n+        fprintf(stderr, \"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n+        fprintf(stderr, \"\\n\");\n+    }\n+\n     ggml_backend_t backend = create_backend();\n     if (!backend) {\n         fprintf(stderr, \"Failed to create backend\\n\");"
        },
        {
          "filename": "ggml/src/ggml-rpc.cpp",
          "status": "modified",
          "additions": 35,
          "deletions": 1,
          "patch": "@@ -197,6 +197,10 @@ static std::shared_ptr<socket_t> create_server_socket(const char * host, int por\n         fprintf(stderr, \"Failed to set SO_REUSEADDR\\n\");\n         return nullptr;\n     }\n+    if (inet_addr(host) == INADDR_NONE) {\n+        fprintf(stderr, \"Invalid host address: %s\\n\", host);\n+        return nullptr;\n+    }\n     struct sockaddr_in serv_addr;\n     serv_addr.sin_family = AF_INET;\n     serv_addr.sin_addr.s_addr = inet_addr(host);\n@@ -879,6 +883,14 @@ ggml_tensor * rpc_server::deserialize_tensor(struct ggml_context * ctx, const rp\n     if (result->buffer && buffers.find(result->buffer) == buffers.end()) {\n         return nullptr;\n     }\n+\n+    // require that the tensor data does not go beyond the buffer end\n+    uint64_t tensor_size = (uint64_t) ggml_nbytes(result);\n+    uint64_t buffer_start = (uint64_t) ggml_backend_buffer_get_base(result->buffer);\n+    uint64_t buffer_size = (uint64_t) ggml_backend_buffer_get_size(result->buffer);\n+    GGML_ASSERT(tensor->data + tensor_size >= tensor->data); // check for overflow\n+    GGML_ASSERT(tensor->data >= buffer_start && tensor->data + tensor_size <= buffer_start + buffer_size);\n+\n     result->op = (ggml_op) tensor->op;\n     for (uint32_t i = 0; i < GGML_MAX_OP_PARAMS / sizeof(int32_t); i++) {\n         result->op_params[i] = tensor->op_params[i];\n@@ -898,7 +910,7 @@ bool rpc_server::set_tensor(const std::vector<uint8_t> & input) {\n     const rpc_tensor * in_tensor = (const rpc_tensor *)input.data();\n     uint64_t offset;\n     memcpy(&offset, input.data() + sizeof(rpc_tensor), sizeof(offset));\n-    size_t size = input.size() - sizeof(rpc_tensor) - sizeof(offset);\n+    const size_t size = input.size() - sizeof(rpc_tensor) - sizeof(offset);\n \n     struct ggml_init_params params {\n         /*.mem_size   =*/ ggml_tensor_overhead(),\n@@ -913,6 +925,17 @@ bool rpc_server::set_tensor(const std::vector<uint8_t> & input) {\n         return false;\n     }\n     GGML_PRINT_DEBUG(\"[%s] buffer: %p, data: %p, offset: %\" PRIu64 \", size: %zu\\n\", __func__, (void*)tensor->buffer, tensor->data, offset, size);\n+\n+    // sanitize tensor->data\n+    {\n+        const size_t p0 = (size_t) ggml_backend_buffer_get_base(tensor->buffer);\n+        const size_t p1 = p0 + ggml_backend_buffer_get_size(tensor->buffer);\n+\n+        if (in_tensor->data + offset < p0 || in_tensor->data + offset >= p1 || size > (p1 - in_tensor->data - offset)) {\n+            GGML_ABORT(\"[%s] tensor->data out of bounds\\n\", __func__);\n+        }\n+    }\n+\n     const void * data = input.data() + sizeof(rpc_tensor) + sizeof(offset);\n     ggml_backend_tensor_set(tensor, data, offset, size);\n     ggml_free(ctx);\n@@ -943,6 +966,17 @@ bool rpc_server::get_tensor(const std::vector<uint8_t> & input, std::vector<uint\n         return false;\n     }\n     GGML_PRINT_DEBUG(\"[%s] buffer: %p, data: %p, offset: %\" PRIu64 \", size: %\" PRIu64 \"\\n\", __func__, (void*)tensor->buffer, tensor->data, offset, size);\n+\n+    // sanitize tensor->data\n+    {\n+        const size_t p0 = (size_t) ggml_backend_buffer_get_base(tensor->buffer);\n+        const size_t p1 = p0 + ggml_backend_buffer_get_size(tensor->buffer);\n+\n+        if (in_tensor->data + offset < p0 || in_tensor->data + offset >= p1 || size > (p1 - in_tensor->data - offset)) {\n+            GGML_ABORT(\"[%s] tensor->data out of bounds\\n\", __func__);\n+        }\n+    }\n+\n     // output serialization format: | data (size bytes) |\n     output.resize(size, 0);\n     ggml_backend_tensor_get(tensor, output.data(), offset, size);"
        },
        {
          "filename": "ggml/src/ggml.c",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -3724,7 +3724,8 @@ static struct ggml_tensor * ggml_new_tensor_impl(\n         struct ggml_tensor  * view_src,\n         size_t                view_offs) {\n \n-    assert(n_dims >= 1 && n_dims <= GGML_MAX_DIMS);\n+    GGML_ASSERT(type >= 0 && type < GGML_TYPE_COUNT);\n+    GGML_ASSERT(n_dims >= 1 && n_dims <= GGML_MAX_DIMS);\n \n     // find the base tensor and absolute offset\n     if (view_src != NULL && view_src->view_src != NULL) {"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 2,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "bbf3e55e352d309573bdafee01a014b0a2492155",
            "date": "2025-01-14T10:54:58Z",
            "author_login": "ggerganov"
          },
          {
            "sha": "c5bf0d1bd7eb8762edca47e0a95f64e6fbfaf5b1",
            "date": "2025-01-14T10:39:33Z",
            "author_login": "ebraminio"
          },
          {
            "sha": "091592d758cb55af7bfadd6c397f61db387aa8f3",
            "date": "2025-01-14T10:16:41Z",
            "author_login": "ochafik"
          },
          {
            "sha": "44d1e796d08641e7083fcbf37b33c79842a2f01e",
            "date": "2025-01-14T08:39:42Z",
            "author_login": "ggerganov"
          },
          {
            "sha": "a4f3f5d8e64b10fcf59913d487c1782dd0bc23e0",
            "date": "2025-01-14T07:40:15Z",
            "author_login": "ggerganov"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 5.3,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:N/A:N",
    "cwe_id": "CWE-125",
    "description": "llama.cpp provides LLM inference in C/C++. The unsafe `data` pointer member in the `rpc_tensor` structure can cause arbitrary address reading. This vulnerability is fixed in b3561.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-08-12T15:15:21.277",
    "last_modified": "2024-08-15T14:03:32.867",
    "fix_date": "2024-08-09T20:03:21Z"
  },
  "references": [
    {
      "url": "https://github.com/ggerganov/llama.cpp/commit/b72942fac998672a79a1ae3c03b340f7e629980b",
      "source": "security-advisories@github.com",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-5vm9-p64x-gqw9",
      "source": "security-advisories@github.com",
      "tags": [
        "Exploit",
        "Vendor Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:34.557042",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "llama.cpp",
    "owner": "ggerganov",
    "created_at": "2023-03-10T18:58:00Z",
    "updated_at": "2025-01-14T12:46:17Z",
    "pushed_at": "2025-01-14T11:22:37Z",
    "size": 75454,
    "stars": 70673,
    "forks": 10209,
    "open_issues": 606,
    "watchers": 70673,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "C++": 6234774,
      "C": 1800946,
      "Python": 696344,
      "Cuda": 685936,
      "Metal": 263028,
      "Objective-C": 262890,
      "CMake": 120280,
      "Shell": 77636,
      "Makefile": 51590,
      "Nix": 24423,
      "Dockerfile": 16865,
      "Batchfile": 802,
      "JavaScript": 402,
      "Swift": 364
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "mit"
    },
    "collected_at": "2025-01-14T14:01:26.608192"
  }
}