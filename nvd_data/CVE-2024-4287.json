{
  "cve_id": "CVE-2024-4287",
  "github_data": {
    "repository": "mintplex-labs/anything-llm",
    "fix_commit": "94b58249a37a21b1c08deaa2d1edfdecbb6deb18",
    "related_commits": [
      "94b58249a37a21b1c08deaa2d1edfdecbb6deb18",
      "94b58249a37a21b1c08deaa2d1edfdecbb6deb18"
    ],
    "patch_url": "https://github.com/mintplex-labs/anything-llm/commit/94b58249a37a21b1c08deaa2d1edfdecbb6deb18.patch",
    "fix_commit_details": {
      "sha": "94b58249a37a21b1c08deaa2d1edfdecbb6deb18",
      "commit_date": "2024-04-05T17:58:36Z",
      "author": {
        "login": "timothycarambat",
        "type": "User",
        "stats": {
          "total_commits": 911,
          "average_weekly_commits": 10.593023255813954,
          "total_additions": 194013,
          "total_deletions": 85618,
          "weeks_active": 81
        }
      },
      "commit_message": {
        "title": "Enable per-workspace provider/model combination (#1042)",
        "length": 315,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 1297,
        "additions": 836,
        "deletions": 461
      },
      "files": [
        {
          "filename": "frontend/src/components/LLMSelection/AnthropicAiOptions/index.jsx",
          "status": "modified",
          "additions": 29,
          "deletions": 47,
          "patch": "@@ -1,26 +1,6 @@\n-import { Info } from \"@phosphor-icons/react\";\n-import paths from \"@/utils/paths\";\n-\n-export default function AnthropicAiOptions({ settings, showAlert = false }) {\n+export default function AnthropicAiOptions({ settings }) {\n   return (\n     <div className=\"w-full flex flex-col\">\n-      {showAlert && (\n-        <div className=\"flex flex-col md:flex-row md:items-center gap-x-2 text-white mb-6 bg-blue-800/30 w-fit rounded-lg px-4 py-2\">\n-          <div className=\"gap-x-2 flex items-center\">\n-            <Info size={12} className=\"hidden md:visible\" />\n-            <p className=\"text-sm md:text-base\">\n-              Anthropic as your LLM requires you to set an embedding service to\n-              use.\n-            </p>\n-          </div>\n-          <a\n-            href={paths.settings.embeddingPreference()}\n-            className=\"text-sm md:text-base my-2 underline\"\n-          >\n-            Manage embedding &rarr;\n-          </a>\n-        </div>\n-      )}\n       <div className=\"w-full flex items-center gap-4\">\n         <div className=\"flex flex-col w-60\">\n           <label className=\"text-white text-sm font-semibold block mb-4\">\n@@ -38,32 +18,34 @@ export default function AnthropicAiOptions({ settings, showAlert = false }) {\n           />\n         </div>\n \n-        <div className=\"flex flex-col w-60\">\n-          <label className=\"text-white text-sm font-semibold block mb-4\">\n-            Chat Model Selection\n-          </label>\n-          <select\n-            name=\"AnthropicModelPref\"\n-            defaultValue={settings?.AnthropicModelPref || \"claude-2\"}\n-            required={true}\n-            className=\"bg-zinc-900 border-gray-500 text-white text-sm rounded-lg block w-full p-2.5\"\n-          >\n-            {[\n-              \"claude-instant-1.2\",\n-              \"claude-2.0\",\n-              \"claude-2.1\",\n-              \"claude-3-haiku-20240307\",\n-              \"claude-3-opus-20240229\",\n-              \"claude-3-sonnet-20240229\",\n-            ].map((model) => {\n-              return (\n-                <option key={model} value={model}>\n-                  {model}\n-                </option>\n-              );\n-            })}\n-          </select>\n-        </div>\n+        {!settings?.credentialsOnly && (\n+          <div className=\"flex flex-col w-60\">\n+            <label className=\"text-white text-sm font-semibold block mb-4\">\n+              Chat Model Selection\n+            </label>\n+            <select\n+              name=\"AnthropicModelPref\"\n+              defaultValue={settings?.AnthropicModelPref || \"claude-2\"}\n+              required={true}\n+              className=\"bg-zinc-900 border-gray-500 text-white text-sm rounded-lg block w-full p-2.5\"\n+            >\n+              {[\n+                \"claude-instant-1.2\",\n+                \"claude-2.0\",\n+                \"claude-2.1\",\n+                \"claude-3-haiku-20240307\",\n+                \"claude-3-opus-20240229\",\n+                \"claude-3-sonnet-20240229\",\n+              ].map((model) => {\n+                return (\n+                  <option key={model} value={model}>\n+                    {model}\n+                  </option>\n+                );\n+              })}\n+            </select>\n+          </div>\n+        )}\n       </div>\n     </div>\n   );"
        },
        {
          "filename": "frontend/src/components/LLMSelection/GeminiLLMOptions/index.jsx",
          "status": "modified",
          "additions": 21,
          "deletions": 19,
          "patch": "@@ -18,25 +18,27 @@ export default function GeminiLLMOptions({ settings }) {\n           />\n         </div>\n \n-        <div className=\"flex flex-col w-60\">\n-          <label className=\"text-white text-sm font-semibold block mb-4\">\n-            Chat Model Selection\n-          </label>\n-          <select\n-            name=\"GeminiLLMModelPref\"\n-            defaultValue={settings?.GeminiLLMModelPref || \"gemini-pro\"}\n-            required={true}\n-            className=\"bg-zinc-900 border-gray-500 text-white text-sm rounded-lg block w-full p-2.5\"\n-          >\n-            {[\"gemini-pro\"].map((model) => {\n-              return (\n-                <option key={model} value={model}>\n-                  {model}\n-                </option>\n-              );\n-            })}\n-          </select>\n-        </div>\n+        {!settings?.credentialsOnly && (\n+          <div className=\"flex flex-col w-60\">\n+            <label className=\"text-white text-sm font-semibold block mb-4\">\n+              Chat Model Selection\n+            </label>\n+            <select\n+              name=\"GeminiLLMModelPref\"\n+              defaultValue={settings?.GeminiLLMModelPref || \"gemini-pro\"}\n+              required={true}\n+              className=\"bg-zinc-900 border-gray-500 text-white text-sm rounded-lg block w-full p-2.5\"\n+            >\n+              {[\"gemini-pro\"].map((model) => {\n+                return (\n+                  <option key={model} value={model}>\n+                    {model}\n+                  </option>\n+                );\n+              })}\n+            </select>\n+          </div>\n+        )}\n       </div>\n     </div>\n   );"
        },
        {
          "filename": "frontend/src/components/LLMSelection/GroqAiOptions/index.jsx",
          "status": "modified",
          "additions": 21,
          "deletions": 19,
          "patch": "@@ -17,25 +17,27 @@ export default function GroqAiOptions({ settings }) {\n         />\n       </div>\n \n-      <div className=\"flex flex-col w-60\">\n-        <label className=\"text-white text-sm font-semibold block mb-4\">\n-          Chat Model Selection\n-        </label>\n-        <select\n-          name=\"GroqModelPref\"\n-          defaultValue={settings?.GroqModelPref || \"llama2-70b-4096\"}\n-          required={true}\n-          className=\"bg-zinc-900 border-gray-500 text-white text-sm rounded-lg block w-full p-2.5\"\n-        >\n-          {[\"llama2-70b-4096\", \"mixtral-8x7b-32768\"].map((model) => {\n-            return (\n-              <option key={model} value={model}>\n-                {model}\n-              </option>\n-            );\n-          })}\n-        </select>\n-      </div>\n+      {!settings?.credentialsOnly && (\n+        <div className=\"flex flex-col w-60\">\n+          <label className=\"text-white text-sm font-semibold block mb-4\">\n+            Chat Model Selection\n+          </label>\n+          <select\n+            name=\"GroqModelPref\"\n+            defaultValue={settings?.GroqModelPref || \"llama2-70b-4096\"}\n+            required={true}\n+            className=\"bg-zinc-900 border-gray-500 text-white text-sm rounded-lg block w-full p-2.5\"\n+          >\n+            {[\"llama2-70b-4096\", \"mixtral-8x7b-32768\"].map((model) => {\n+              return (\n+                <option key={model} value={model}>\n+                  {model}\n+                </option>\n+              );\n+            })}\n+          </select>\n+        </div>\n+      )}\n     </div>\n   );\n }"
        },
        {
          "filename": "frontend/src/components/LLMSelection/LMStudioOptions/index.jsx",
          "status": "modified",
          "additions": 21,
          "deletions": 17,
          "patch": "@@ -46,23 +46,27 @@ export default function LMStudioOptions({ settings, showAlert = false }) {\n             onBlur={() => setBasePath(basePathValue)}\n           />\n         </div>\n-        <LMStudioModelSelection settings={settings} basePath={basePath} />\n-        <div className=\"flex flex-col w-60\">\n-          <label className=\"text-white text-sm font-semibold block mb-4\">\n-            Token context window\n-          </label>\n-          <input\n-            type=\"number\"\n-            name=\"LMStudioTokenLimit\"\n-            className=\"bg-zinc-900 text-white placeholder:text-white/20 text-sm rounded-lg focus:border-white block w-full p-2.5\"\n-            placeholder=\"4096\"\n-            min={1}\n-            onScroll={(e) => e.target.blur()}\n-            defaultValue={settings?.LMStudioTokenLimit}\n-            required={true}\n-            autoComplete=\"off\"\n-          />\n-        </div>\n+        {!settings?.credentialsOnly && (\n+          <>\n+            <LMStudioModelSelection settings={settings} basePath={basePath} />\n+            <div className=\"flex flex-col w-60\">\n+              <label className=\"text-white text-sm font-semibold block mb-4\">\n+                Token context window\n+              </label>\n+              <input\n+                type=\"number\"\n+                name=\"LMStudioTokenLimit\"\n+                className=\"bg-zinc-900 text-white placeholder:text-white/20 text-sm rounded-lg focus:border-white block w-full p-2.5\"\n+                placeholder=\"4096\"\n+                min={1}\n+                onScroll={(e) => e.target.blur()}\n+                defaultValue={settings?.LMStudioTokenLimit}\n+                required={true}\n+                autoComplete=\"off\"\n+              />\n+            </div>\n+          </>\n+        )}\n       </div>\n     </div>\n   );"
        },
        {
          "filename": "frontend/src/components/LLMSelection/LocalAiOptions/index.jsx",
          "status": "modified",
          "additions": 25,
          "deletions": 21,
          "patch": "@@ -46,27 +46,31 @@ export default function LocalAiOptions({ settings, showAlert = false }) {\n             onBlur={() => setBasePath(basePathValue)}\n           />\n         </div>\n-        <LocalAIModelSelection\n-          settings={settings}\n-          basePath={basePath}\n-          apiKey={apiKey}\n-        />\n-        <div className=\"flex flex-col w-60\">\n-          <label className=\"text-white text-sm font-semibold block mb-4\">\n-            Token context window\n-          </label>\n-          <input\n-            type=\"number\"\n-            name=\"LocalAiTokenLimit\"\n-            className=\"bg-zinc-900 text-white placeholder:text-white/20 text-sm rounded-lg focus:border-white block w-full p-2.5\"\n-            placeholder=\"4096\"\n-            min={1}\n-            onScroll={(e) => e.target.blur()}\n-            defaultValue={settings?.LocalAiTokenLimit}\n-            required={true}\n-            autoComplete=\"off\"\n-          />\n-        </div>\n+        {!settings?.credentialsOnly && (\n+          <>\n+            <LocalAIModelSelection\n+              settings={settings}\n+              basePath={basePath}\n+              apiKey={apiKey}\n+            />\n+            <div className=\"flex flex-col w-60\">\n+              <label className=\"text-white text-sm font-semibold block mb-4\">\n+                Token context window\n+              </label>\n+              <input\n+                type=\"number\"\n+                name=\"LocalAiTokenLimit\"\n+                className=\"bg-zinc-900 text-white placeholder:text-white/20 text-sm rounded-lg focus:border-white block w-full p-2.5\"\n+                placeholder=\"4096\"\n+                min={1}\n+                onScroll={(e) => e.target.blur()}\n+                defaultValue={settings?.LocalAiTokenLimit}\n+                required={true}\n+                autoComplete=\"off\"\n+              />\n+            </div>\n+          </>\n+        )}\n       </div>\n       <div className=\"w-full flex items-center gap-4\">\n         <div className=\"flex flex-col w-60\">"
        },
        {
          "filename": "frontend/src/components/LLMSelection/MistralOptions/index.jsx",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -24,7 +24,9 @@ export default function MistralOptions({ settings }) {\n           onBlur={() => setMistralKey(inputValue)}\n         />\n       </div>\n-      <MistralModelSelection settings={settings} apiKey={mistralKey} />\n+      {!settings?.credentialsOnly && (\n+        <MistralModelSelection settings={settings} apiKey={mistralKey} />\n+      )}\n     </div>\n   );\n }"
        },
        {
          "filename": "frontend/src/components/LLMSelection/OllamaLLMOptions/index.jsx",
          "status": "modified",
          "additions": 21,
          "deletions": 17,
          "patch": "@@ -27,23 +27,27 @@ export default function OllamaLLMOptions({ settings }) {\n             onBlur={() => setBasePath(basePathValue)}\n           />\n         </div>\n-        <OllamaLLMModelSelection settings={settings} basePath={basePath} />\n-        <div className=\"flex flex-col w-60\">\n-          <label className=\"text-white text-sm font-semibold block mb-4\">\n-            Token context window\n-          </label>\n-          <input\n-            type=\"number\"\n-            name=\"OllamaLLMTokenLimit\"\n-            className=\"bg-zinc-900 text-white placeholder:text-white/20 text-sm rounded-lg focus:border-white block w-full p-2.5\"\n-            placeholder=\"4096\"\n-            min={1}\n-            onScroll={(e) => e.target.blur()}\n-            defaultValue={settings?.OllamaLLMTokenLimit}\n-            required={true}\n-            autoComplete=\"off\"\n-          />\n-        </div>\n+        {!settings?.credentialsOnly && (\n+          <>\n+            <OllamaLLMModelSelection settings={settings} basePath={basePath} />\n+            <div className=\"flex flex-col w-60\">\n+              <label className=\"text-white text-sm font-semibold block mb-4\">\n+                Token context window\n+              </label>\n+              <input\n+                type=\"number\"\n+                name=\"OllamaLLMTokenLimit\"\n+                className=\"bg-zinc-900 text-white placeholder:text-white/20 text-sm rounded-lg focus:border-white block w-full p-2.5\"\n+                placeholder=\"4096\"\n+                min={1}\n+                onScroll={(e) => e.target.blur()}\n+                defaultValue={settings?.OllamaLLMTokenLimit}\n+                required={true}\n+                autoComplete=\"off\"\n+              />\n+            </div>\n+          </>\n+        )}\n       </div>\n     </div>\n   );"
        },
        {
          "filename": "frontend/src/components/LLMSelection/OpenAiOptions/index.jsx",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -24,7 +24,9 @@ export default function OpenAiOptions({ settings }) {\n           onBlur={() => setOpenAIKey(inputValue)}\n         />\n       </div>\n-      <OpenAIModelSelection settings={settings} apiKey={openAIKey} />\n+      {!settings?.credentialsOnly && (\n+        <OpenAIModelSelection settings={settings} apiKey={openAIKey} />\n+      )}\n     </div>\n   );\n }"
        },
        {
          "filename": "frontend/src/components/LLMSelection/OpenRouterOptions/index.jsx",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -19,7 +19,9 @@ export default function OpenRouterOptions({ settings }) {\n           spellCheck={false}\n         />\n       </div>\n-      <OpenRouterModelSelection settings={settings} />\n+      {!settings?.credentialsOnly && (\n+        <OpenRouterModelSelection settings={settings} />\n+      )}\n     </div>\n   );\n }\n@@ -84,7 +86,7 @@ function OpenRouterModelSelection({ settings }) {\n                 <option\n                   key={model.id}\n                   value={model.id}\n-                  selected={settings.OpenRouterModelPref === model.id}\n+                  selected={settings?.OpenRouterModelPref === model.id}\n                 >\n                   {model.name}\n                 </option>"
        },
        {
          "filename": "frontend/src/components/LLMSelection/PerplexityOptions/index.jsx",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -19,7 +19,9 @@ export default function PerplexityOptions({ settings }) {\n           spellCheck={false}\n         />\n       </div>\n-      <PerplexityModelSelection settings={settings} />\n+      {!settings?.credentialsOnly && (\n+        <PerplexityModelSelection settings={settings} />\n+      )}\n     </div>\n   );\n }"
        },
        {
          "filename": "frontend/src/components/LLMSelection/TogetherAiOptions/index.jsx",
          "status": "modified",
          "additions": 4,
          "deletions": 2,
          "patch": "@@ -19,7 +19,9 @@ export default function TogetherAiOptions({ settings }) {\n           spellCheck={false}\n         />\n       </div>\n-      <TogetherAiModelSelection settings={settings} />\n+      {!settings?.credentialsOnly && (\n+        <TogetherAiModelSelection settings={settings} />\n+      )}\n     </div>\n   );\n }\n@@ -84,7 +86,7 @@ function TogetherAiModelSelection({ settings }) {\n                 <option\n                   key={model.id}\n                   value={model.id}\n-                  selected={settings.OpenRouterModelPref === model.id}\n+                  selected={settings?.OpenRouterModelPref === model.id}\n                 >\n                   {model.name}\n                 </option>"
        },
        {
          "filename": "frontend/src/hooks/useGetProvidersModels.js",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -2,7 +2,7 @@ import System from \"@/models/system\";\n import { useEffect, useState } from \"react\";\n \n // Providers which cannot use this feature for workspace<>model selection\n-export const DISABLED_PROVIDERS = [\"azure\", \"lmstudio\"];\n+export const DISABLED_PROVIDERS = [\"azure\", \"lmstudio\", \"native\"];\n const PROVIDER_DEFAULT_MODELS = {\n   openai: [\n     \"gpt-3.5-turbo\","
        },
        {
          "filename": "frontend/src/pages/GeneralSettings/LLMPreference/index.jsx",
          "status": "modified",
          "additions": 131,
          "deletions": 110,
          "patch": "@@ -36,6 +36,130 @@ import GroqAiOptions from \"@/components/LLMSelection/GroqAiOptions\";\n import LLMItem from \"@/components/LLMSelection/LLMItem\";\n import { CaretUpDown, MagnifyingGlass, X } from \"@phosphor-icons/react\";\n \n+export const AVAILABLE_LLM_PROVIDERS = [\n+  {\n+    name: \"OpenAI\",\n+    value: \"openai\",\n+    logo: OpenAiLogo,\n+    options: (settings) => <OpenAiOptions settings={settings} />,\n+    description: \"The standard option for most non-commercial use.\",\n+    requiredConfig: [\"OpenAiKey\"],\n+  },\n+  {\n+    name: \"Azure OpenAI\",\n+    value: \"azure\",\n+    logo: AzureOpenAiLogo,\n+    options: (settings) => <AzureAiOptions settings={settings} />,\n+    description: \"The enterprise option of OpenAI hosted on Azure services.\",\n+    requiredConfig: [\"AzureOpenAiEndpoint\"],\n+  },\n+  {\n+    name: \"Anthropic\",\n+    value: \"anthropic\",\n+    logo: AnthropicLogo,\n+    options: (settings) => <AnthropicAiOptions settings={settings} />,\n+    description: \"A friendly AI Assistant hosted by Anthropic.\",\n+    requiredConfig: [\"AnthropicApiKey\"],\n+  },\n+  {\n+    name: \"Gemini\",\n+    value: \"gemini\",\n+    logo: GeminiLogo,\n+    options: (settings) => <GeminiLLMOptions settings={settings} />,\n+    description: \"Google's largest and most capable AI model\",\n+    requiredConfig: [\"GeminiLLMApiKey\"],\n+  },\n+  {\n+    name: \"HuggingFace\",\n+    value: \"huggingface\",\n+    logo: HuggingFaceLogo,\n+    options: (settings) => <HuggingFaceOptions settings={settings} />,\n+    description:\n+      \"Access 150,000+ open-source LLMs and the world's AI community\",\n+    requiredConfig: [\n+      \"HuggingFaceLLMEndpoint\",\n+      \"HuggingFaceLLMAccessToken\",\n+      \"HuggingFaceLLMTokenLimit\",\n+    ],\n+  },\n+  {\n+    name: \"Ollama\",\n+    value: \"ollama\",\n+    logo: OllamaLogo,\n+    options: (settings) => <OllamaLLMOptions settings={settings} />,\n+    description: \"Run LLMs locally on your own machine.\",\n+    requiredConfig: [\"OllamaLLMBasePath\"],\n+  },\n+  {\n+    name: \"LM Studio\",\n+    value: \"lmstudio\",\n+    logo: LMStudioLogo,\n+    options: (settings) => <LMStudioOptions settings={settings} />,\n+    description:\n+      \"Discover, download, and run thousands of cutting edge LLMs in a few clicks.\",\n+    requiredConfig: [\"LMStudioBasePath\"],\n+  },\n+  {\n+    name: \"Local AI\",\n+    value: \"localai\",\n+    logo: LocalAiLogo,\n+    options: (settings) => <LocalAiOptions settings={settings} />,\n+    description: \"Run LLMs locally on your own machine.\",\n+    requiredConfig: [\"LocalAiApiKey\", \"LocalAiBasePath\", \"LocalAiTokenLimit\"],\n+  },\n+  {\n+    name: \"Together AI\",\n+    value: \"togetherai\",\n+    logo: TogetherAILogo,\n+    options: (settings) => <TogetherAiOptions settings={settings} />,\n+    description: \"Run open source models from Together AI.\",\n+    requiredConfig: [\"TogetherAiApiKey\"],\n+  },\n+  {\n+    name: \"Mistral\",\n+    value: \"mistral\",\n+    logo: MistralLogo,\n+    options: (settings) => <MistralOptions settings={settings} />,\n+    description: \"Run open source models from Mistral AI.\",\n+    requiredConfig: [\"MistralApiKey\"],\n+  },\n+  {\n+    name: \"Perplexity AI\",\n+    value: \"perplexity\",\n+    logo: PerplexityLogo,\n+    options: (settings) => <PerplexityOptions settings={settings} />,\n+    description:\n+      \"Run powerful and internet-connected models hosted by Perplexity AI.\",\n+    requiredConfig: [\"PerplexityApiKey\"],\n+  },\n+  {\n+    name: \"OpenRouter\",\n+    value: \"openrouter\",\n+    logo: OpenRouterLogo,\n+    options: (settings) => <OpenRouterOptions settings={settings} />,\n+    description: \"A unified interface for LLMs.\",\n+    requiredConfig: [\"OpenRouterApiKey\"],\n+  },\n+  {\n+    name: \"Groq\",\n+    value: \"groq\",\n+    logo: GroqLogo,\n+    options: (settings) => <GroqAiOptions settings={settings} />,\n+    description:\n+      \"The fastest LLM inferencing available for real-time AI applications.\",\n+    requiredConfig: [\"GroqApiKey\"],\n+  },\n+  {\n+    name: \"Native\",\n+    value: \"native\",\n+    logo: AnythingLLMIcon,\n+    options: (settings) => <NativeLLMOptions settings={settings} />,\n+    description:\n+      \"Use a downloaded custom Llama model for chatting on this AnythingLLM instance.\",\n+    requiredConfig: [],\n+  },\n+];\n+\n export default function GeneralLLMPreference() {\n   const [saving, setSaving] = useState(false);\n   const [hasChanges, setHasChanges] = useState(false);\n@@ -94,120 +218,15 @@ export default function GeneralLLMPreference() {\n   }, []);\n \n   useEffect(() => {\n-    const filtered = LLMS.filter((llm) =>\n+    const filtered = AVAILABLE_LLM_PROVIDERS.filter((llm) =>\n       llm.name.toLowerCase().includes(searchQuery.toLowerCase())\n     );\n     setFilteredLLMs(filtered);\n   }, [searchQuery, selectedLLM]);\n \n-  const LLMS = [\n-    {\n-      name: \"OpenAI\",\n-      value: \"openai\",\n-      logo: OpenAiLogo,\n-      options: <OpenAiOptions settings={settings} />,\n-      description: \"The standard option for most non-commercial use.\",\n-    },\n-    {\n-      name: \"Azure OpenAI\",\n-      value: \"azure\",\n-      logo: AzureOpenAiLogo,\n-      options: <AzureAiOptions settings={settings} />,\n-      description: \"The enterprise option of OpenAI hosted on Azure services.\",\n-    },\n-    {\n-      name: \"Anthropic\",\n-      value: \"anthropic\",\n-      logo: AnthropicLogo,\n-      options: <AnthropicAiOptions settings={settings} />,\n-      description: \"A friendly AI Assistant hosted by Anthropic.\",\n-    },\n-    {\n-      name: \"Gemini\",\n-      value: \"gemini\",\n-      logo: GeminiLogo,\n-      options: <GeminiLLMOptions settings={settings} />,\n-      description: \"Google's largest and most capable AI model\",\n-    },\n-    {\n-      name: \"HuggingFace\",\n-      value: \"huggingface\",\n-      logo: HuggingFaceLogo,\n-      options: <HuggingFaceOptions settings={settings} />,\n-      description:\n-        \"Access 150,000+ open-source LLMs and the world's AI community\",\n-    },\n-    {\n-      name: \"Ollama\",\n-      value: \"ollama\",\n-      logo: OllamaLogo,\n-      options: <OllamaLLMOptions settings={settings} />,\n-      description: \"Run LLMs locally on your own machine.\",\n-    },\n-    {\n-      name: \"LM Studio\",\n-      value: \"lmstudio\",\n-      logo: LMStudioLogo,\n-      options: <LMStudioOptions settings={settings} />,\n-      description:\n-        \"Discover, download, and run thousands of cutting edge LLMs in a few clicks.\",\n-    },\n-    {\n-      name: \"Local AI\",\n-      value: \"localai\",\n-      logo: LocalAiLogo,\n-      options: <LocalAiOptions settings={settings} />,\n-      description: \"Run LLMs locally on your own machine.\",\n-    },\n-    {\n-      name: \"Together AI\",\n-      value: \"togetherai\",\n-      logo: TogetherAILogo,\n-      options: <TogetherAiOptions settings={settings} />,\n-      description: \"Run open source models from Together AI.\",\n-    },\n-    {\n-      name: \"Mistral\",\n-      value: \"mistral\",\n-      logo: MistralLogo,\n-      options: <MistralOptions settings={settings} />,\n-      description: \"Run open source models from Mistral AI.\",\n-    },\n-    {\n-      name: \"Perplexity AI\",\n-      value: \"perplexity\",\n-      logo: PerplexityLogo,\n-      options: <PerplexityOptions settings={settings} />,\n-      description:\n-        \"Run powerful and internet-connected models hosted by Perplexity AI.\",\n-    },\n-    {\n-      name: \"OpenRouter\",\n-      value: \"openrouter\",\n-      logo: OpenRouterLogo,\n-      options: <OpenRouterOptions settings={settings} />,\n-      description: \"A unified interface for LLMs.\",\n-    },\n-    {\n-      name: \"Groq\",\n-      value: \"groq\",\n-      logo: GroqLogo,\n-      options: <GroqAiOptions settings={settings} />,\n-      description:\n-        \"The fastest LLM inferencing available for real-time AI applications.\",\n-    },\n-    {\n-      name: \"Native\",\n-      value: \"native\",\n-      logo: AnythingLLMIcon,\n-      options: <NativeLLMOptions settings={settings} />,\n-      description:\n-        \"Use a downloaded custom Llama model for chatting on this AnythingLLM instance.\",\n-    },\n-  ];\n-\n-  const selectedLLMObject = LLMS.find((llm) => llm.value === selectedLLM);\n-\n+  const selectedLLMObject = AVAILABLE_LLM_PROVIDERS.find(\n+    (llm) => llm.value === selectedLLM\n+  );\n   return (\n     <div className=\"w-screen h-screen overflow-hidden bg-sidebar flex\">\n       <Sidebar />\n@@ -339,7 +358,9 @@ export default function GeneralLLMPreference() {\n                 className=\"mt-4 flex flex-col gap-y-1\"\n               >\n                 {selectedLLM &&\n-                  LLMS.find((llm) => llm.value === selectedLLM)?.options}\n+                  AVAILABLE_LLM_PROVIDERS.find(\n+                    (llm) => llm.value === selectedLLM\n+                  )?.options?.(settings)}\n               </div>\n             </div>\n           </form>"
        },
        {
          "filename": "frontend/src/pages/WorkspaceSettings/ChatSettings/ChatModelSelection/index.jsx",
          "status": "modified",
          "additions": 6,
          "deletions": 11,
          "patch": "@@ -3,21 +3,20 @@ import useGetProviderModels, {\n } from \"@/hooks/useGetProvidersModels\";\n \n export default function ChatModelSelection({\n-  settings,\n+  provider,\n   workspace,\n   setHasChanges,\n }) {\n-  const { defaultModels, customModels, loading } = useGetProviderModels(\n-    settings?.LLMProvider\n-  );\n-  if (DISABLED_PROVIDERS.includes(settings?.LLMProvider)) return null;\n+  const { defaultModels, customModels, loading } =\n+    useGetProviderModels(provider);\n+  if (DISABLED_PROVIDERS.includes(provider)) return null;\n \n   if (loading) {\n     return (\n       <div>\n         <div className=\"flex flex-col\">\n           <label htmlFor=\"name\" className=\"block input-label\">\n-            Chat model\n+            Workspace Chat model\n           </label>\n           <p className=\"text-white text-opacity-60 text-xs font-medium py-1.5\">\n             The specific chat model that will be used for this workspace. If\n@@ -42,8 +41,7 @@ export default function ChatModelSelection({\n     <div>\n       <div className=\"flex flex-col\">\n         <label htmlFor=\"name\" className=\"block input-label\">\n-          Chat model{\" \"}\n-          <span className=\"font-normal\">({settings?.LLMProvider})</span>\n+          Workspace Chat model\n         </label>\n         <p className=\"text-white text-opacity-60 text-xs font-medium py-1.5\">\n           The specific chat model that will be used for this workspace. If\n@@ -59,9 +57,6 @@ export default function ChatModelSelection({\n         }}\n         className=\"bg-zinc-900 text-white text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full p-2.5\"\n       >\n-        <option disabled={true} selected={workspace?.chatModel === null}>\n-          System default\n-        </option>\n         {defaultModels.length > 0 && (\n           <optgroup label=\"General models\">\n             {defaultModels.map((model) => {"
        },
        {
          "filename": "frontend/src/pages/WorkspaceSettings/ChatSettings/WorkspaceLLMSelection/WorkspaceLLMItem/index.jsx",
          "status": "added",
          "additions": 151,
          "deletions": 0,
          "patch": "@@ -0,0 +1,151 @@\n+// This component differs from the main LLMItem in that it shows if a provider is\n+// \"ready for use\" and if not - will then highjack the click handler to show a modal\n+// of the provider options that must be saved to continue.\n+import { createPortal } from \"react-dom\";\n+import ModalWrapper from \"@/components/ModalWrapper\";\n+import { useModal } from \"@/hooks/useModal\";\n+import { X } from \"@phosphor-icons/react\";\n+import System from \"@/models/system\";\n+import showToast from \"@/utils/toast\";\n+\n+export default function WorkspaceLLM({\n+  llm,\n+  availableLLMs,\n+  settings,\n+  checked,\n+  onClick,\n+}) {\n+  const { isOpen, openModal, closeModal } = useModal();\n+  const { name, value, logo, description } = llm;\n+\n+  function handleProviderSelection() {\n+    // Determine if provider needs additional setup because its minimum required keys are\n+    // not yet set in settings.\n+    const requiresAdditionalSetup = (llm.requiredConfig || []).some(\n+      (key) => !settings[key]\n+    );\n+    if (requiresAdditionalSetup) {\n+      openModal();\n+      return;\n+    }\n+    onClick(value);\n+  }\n+\n+  return (\n+    <>\n+      <div\n+        onClick={handleProviderSelection}\n+        className={`w-full p-2 rounded-md hover:cursor-pointer hover:bg-white/10 ${\n+          checked ? \"bg-white/10\" : \"\"\n+        }`}\n+      >\n+        <input\n+          type=\"checkbox\"\n+          value={value}\n+          className=\"peer hidden\"\n+          checked={checked}\n+          readOnly={true}\n+          formNoValidate={true}\n+        />\n+        <div className=\"flex gap-x-4 items-center\">\n+          <img\n+            src={logo}\n+            alt={`${name} logo`}\n+            className=\"w-10 h-10 rounded-md\"\n+          />\n+          <div className=\"flex flex-col\">\n+            <div className=\"text-sm font-semibold text-white\">{name}</div>\n+            <div className=\"mt-1 text-xs text-[#D2D5DB]\">{description}</div>\n+          </div>\n+        </div>\n+      </div>\n+      <SetupProvider\n+        availableLLMs={availableLLMs}\n+        isOpen={isOpen}\n+        provider={value}\n+        closeModal={closeModal}\n+        postSubmit={onClick}\n+      />\n+    </>\n+  );\n+}\n+\n+function SetupProvider({\n+  availableLLMs,\n+  isOpen,\n+  provider,\n+  closeModal,\n+  postSubmit,\n+}) {\n+  if (!isOpen) return null;\n+  const LLMOption = availableLLMs.find((llm) => llm.value === provider);\n+  if (!LLMOption) return null;\n+\n+  async function handleUpdate(e) {\n+    e.preventDefault();\n+    e.stopPropagation();\n+    const data = {};\n+    const form = new FormData(e.target);\n+    for (var [key, value] of form.entries()) data[key] = value;\n+    const { error } = await System.updateSystem(data);\n+    if (error) {\n+      showToast(`Failed to save ${LLMOption.name} settings: ${error}`, \"error\");\n+      return;\n+    }\n+\n+    closeModal();\n+    postSubmit();\n+    return false;\n+  }\n+\n+  // Cannot do nested forms, it will cause all sorts of issues, so we portal this out\n+  // to the parent container form so we don't have nested forms.\n+  return createPortal(\n+    <ModalWrapper isOpen={isOpen}>\n+      <div className=\"relative w-fit max-w-1/2 max-h-full\">\n+        <div className=\"relative bg-main-gradient rounded-xl shadow-[0_4px_14px_rgba(0,0,0,0.25)]\">\n+          <div className=\"flex items-start justify-between p-4 border-b rounded-t border-gray-500/50\">\n+            <h3 className=\"text-xl font-semibold text-white\">\n+              Setup {LLMOption.name}\n+            </h3>\n+            <button\n+              onClick={closeModal}\n+              type=\"button\"\n+              className=\"transition-all duration-300 text-gray-400 bg-transparent hover:border-white/60 rounded-lg text-sm p-1.5 ml-auto inline-flex items-center bg-sidebar-button hover:bg-menu-item-selected-gradient hover:border-slate-100 hover:border-opacity-50 border-transparent border\"\n+              data-modal-hide=\"staticModal\"\n+            >\n+              <X className=\"text-gray-300 text-lg\" />\n+            </button>\n+          </div>\n+\n+          <form id=\"provider-form\" onSubmit={handleUpdate}>\n+            <div className=\"py-[17px] px-[20px] flex flex-col gap-y-6\">\n+              <p className=\"text-sm text-white\">\n+                To use {LLMOption.name} as this workspace's LLM you need to set\n+                it up first.\n+              </p>\n+              <div>{LLMOption.options({ credentialsOnly: true })}</div>\n+            </div>\n+            <div className=\"flex w-full justify-between items-center p-3 space-x-2 border-t rounded-b border-gray-500/50\">\n+              <button\n+                type=\"button\"\n+                onClick={closeModal}\n+                className=\"text-xs px-2 py-1 font-semibold rounded-lg bg-white hover:bg-transparent border-2 border-transparent hover:border-white hover:text-white h-[32px] w-fit -mr-8 whitespace-nowrap shadow-[0_4px_14px_rgba(0,0,0,0.25)]\"\n+              >\n+                Cancel\n+              </button>\n+              <button\n+                type=\"submit\"\n+                form=\"provider-form\"\n+                className=\"text-xs px-2 py-1 font-semibold rounded-lg bg-[#46C8FF] hover:bg-[#2C2F36] border-2 border-transparent hover:border-[#46C8FF] hover:text-white h-[32px] w-fit -mr-8 whitespace-nowrap shadow-[0_4px_14px_rgba(0,0,0,0.25)]\"\n+              >\n+                Save {LLMOption.name} settings\n+              </button>\n+            </div>\n+          </form>\n+        </div>\n+      </div>\n+    </ModalWrapper>,\n+    document.getElementById(\"workspace-chat-settings-container\")\n+  );\n+}"
        },
        {
          "filename": "frontend/src/pages/WorkspaceSettings/ChatSettings/WorkspaceLLMSelection/index.jsx",
          "status": "added",
          "additions": 159,
          "deletions": 0,
          "patch": "@@ -0,0 +1,159 @@\n+import React, { useEffect, useRef, useState } from \"react\";\n+import AnythingLLMIcon from \"@/media/logo/anything-llm-icon.png\";\n+import WorkspaceLLMItem from \"./WorkspaceLLMItem\";\n+import { AVAILABLE_LLM_PROVIDERS } from \"@/pages/GeneralSettings/LLMPreference\";\n+import { CaretUpDown, MagnifyingGlass, X } from \"@phosphor-icons/react\";\n+import ChatModelSelection from \"../ChatModelSelection\";\n+\n+const DISABLED_PROVIDERS = [\"azure\", \"lmstudio\", \"native\"];\n+const LLM_DEFAULT = {\n+  name: \"System default\",\n+  value: \"default\",\n+  logo: AnythingLLMIcon,\n+  options: () => <React.Fragment />,\n+  description: \"Use the system LLM preference for this workspace.\",\n+  requiredConfig: [],\n+};\n+\n+export default function WorkspaceLLMSelection({\n+  settings,\n+  workspace,\n+  setHasChanges,\n+}) {\n+  const [filteredLLMs, setFilteredLLMs] = useState([]);\n+  const [selectedLLM, setSelectedLLM] = useState(\n+    workspace?.chatProvider ?? \"default\"\n+  );\n+  const [searchQuery, setSearchQuery] = useState(\"\");\n+  const [searchMenuOpen, setSearchMenuOpen] = useState(false);\n+  const searchInputRef = useRef(null);\n+  const LLMS = [LLM_DEFAULT, ...AVAILABLE_LLM_PROVIDERS].filter(\n+    (llm) => !DISABLED_PROVIDERS.includes(llm.value)\n+  );\n+\n+  function updateLLMChoice(selection) {\n+    console.log({ selection });\n+    setSearchQuery(\"\");\n+    setSelectedLLM(selection);\n+    setSearchMenuOpen(false);\n+    setHasChanges(true);\n+  }\n+\n+  function handleXButton() {\n+    if (searchQuery.length > 0) {\n+      setSearchQuery(\"\");\n+      if (searchInputRef.current) searchInputRef.current.value = \"\";\n+    } else {\n+      setSearchMenuOpen(!searchMenuOpen);\n+    }\n+  }\n+\n+  useEffect(() => {\n+    const filtered = LLMS.filter((llm) =>\n+      llm.name.toLowerCase().includes(searchQuery.toLowerCase())\n+    );\n+    setFilteredLLMs(filtered);\n+  }, [LLMS, searchQuery, selectedLLM]);\n+\n+  const selectedLLMObject = LLMS.find((llm) => llm.value === selectedLLM);\n+  return (\n+    <div className=\"border-b border-white/40 pb-8\">\n+      <div className=\"flex flex-col\">\n+        <label htmlFor=\"name\" className=\"block input-label\">\n+          Workspace LLM Provider\n+        </label>\n+        <p className=\"text-white text-opacity-60 text-xs font-medium py-1.5\">\n+          The specific LLM provider & model that will be used for this\n+          workspace. By default, it uses the system LLM provider and settings.\n+        </p>\n+      </div>\n+\n+      <div className=\"relative\">\n+        <input type=\"hidden\" name=\"chatProvider\" value={selectedLLM} />\n+        {searchMenuOpen && (\n+          <div\n+            className=\"fixed top-0 left-0 w-full h-full bg-black bg-opacity-70 backdrop-blur-sm z-10\"\n+            onClick={() => setSearchMenuOpen(false)}\n+          />\n+        )}\n+        {searchMenuOpen ? (\n+          <div className=\"absolute top-0 left-0 w-full max-w-[640px] max-h-[310px] overflow-auto white-scrollbar min-h-[64px] bg-[#18181B] rounded-lg flex flex-col justify-between cursor-pointer border-2 border-[#46C8FF] z-20\">\n+            <div className=\"w-full flex flex-col gap-y-1\">\n+              <div className=\"flex items-center sticky top-0 border-b border-[#9CA3AF] mx-4 bg-[#18181B]\">\n+                <MagnifyingGlass\n+                  size={20}\n+                  weight=\"bold\"\n+                  className=\"absolute left-4 z-30 text-white -ml-4 my-2\"\n+                />\n+                <input\n+                  type=\"text\"\n+                  name=\"llm-search\"\n+                  autoComplete=\"off\"\n+                  placeholder=\"Search all LLM providers\"\n+                  className=\"-ml-4 my-2 bg-transparent z-20 pl-12 h-[38px] w-full px-4 py-1 text-sm outline-none focus:border-white text-white placeholder:text-white placeholder:font-medium\"\n+                  onChange={(e) => setSearchQuery(e.target.value)}\n+                  ref={searchInputRef}\n+                  onKeyDown={(e) => {\n+                    if (e.key === \"Enter\") e.preventDefault();\n+                  }}\n+                />\n+                <X\n+                  size={20}\n+                  weight=\"bold\"\n+                  className=\"cursor-pointer text-white hover:text-[#9CA3AF]\"\n+                  onClick={handleXButton}\n+                />\n+              </div>\n+              <div className=\"flex-1 pl-4 pr-2 flex flex-col gap-y-1 overflow-y-auto white-scrollbar pb-4\">\n+                {filteredLLMs.map((llm) => {\n+                  return (\n+                    <WorkspaceLLMItem\n+                      llm={llm}\n+                      key={llm.name}\n+                      availableLLMs={LLMS}\n+                      settings={settings}\n+                      checked={selectedLLM === llm.value}\n+                      onClick={() => updateLLMChoice(llm.value)}\n+                    />\n+                  );\n+                })}\n+              </div>\n+            </div>\n+          </div>\n+        ) : (\n+          <button\n+            className=\"w-full max-w-[640px] h-[64px] bg-[#18181B] rounded-lg flex items-center p-[14px] justify-between cursor-pointer border-2 border-transparent hover:border-[#46C8FF] transition-all duration-300\"\n+            type=\"button\"\n+            onClick={() => setSearchMenuOpen(true)}\n+          >\n+            <div className=\"flex gap-x-4 items-center\">\n+              <img\n+                src={selectedLLMObject.logo}\n+                alt={`${selectedLLMObject.name} logo`}\n+                className=\"w-10 h-10 rounded-md\"\n+              />\n+              <div className=\"flex flex-col text-left\">\n+                <div className=\"text-sm font-semibold text-white\">\n+                  {selectedLLMObject.name}\n+                </div>\n+                <div className=\"mt-1 text-xs text-[#D2D5DB]\">\n+                  {selectedLLMObject.description}\n+                </div>\n+              </div>\n+            </div>\n+            <CaretUpDown size={24} weight=\"bold\" className=\"text-white\" />\n+          </button>\n+        )}\n+      </div>\n+      {selectedLLM !== \"default\" && (\n+        <div className=\"mt-4 flex flex-col gap-y-1\">\n+          <ChatModelSelection\n+            provider={selectedLLM}\n+            workspace={workspace}\n+            setHasChanges={setHasChanges}\n+          />\n+        </div>\n+      )}\n+    </div>\n+  );\n+}"
        },
        {
          "filename": "frontend/src/pages/WorkspaceSettings/ChatSettings/index.jsx",
          "status": "modified",
          "additions": 41,
          "deletions": 31,
          "patch": "@@ -3,11 +3,11 @@ import Workspace from \"@/models/workspace\";\n import showToast from \"@/utils/toast\";\n import { castToType } from \"@/utils/types\";\n import { useEffect, useRef, useState } from \"react\";\n-import ChatModelSelection from \"./ChatModelSelection\";\n import ChatHistorySettings from \"./ChatHistorySettings\";\n import ChatPromptSettings from \"./ChatPromptSettings\";\n import ChatTemperatureSettings from \"./ChatTemperatureSettings\";\n import ChatModeSelection from \"./ChatModeSelection\";\n+import WorkspaceLLMSelection from \"./WorkspaceLLMSelection\";\n \n export default function ChatSettings({ workspace }) {\n   const [settings, setSettings] = useState({});\n@@ -44,35 +44,45 @@ export default function ChatSettings({ workspace }) {\n \n   if (!workspace) return null;\n   return (\n-    <form\n-      ref={formEl}\n-      onSubmit={handleUpdate}\n-      className=\"w-1/2 flex flex-col gap-y-6\"\n-    >\n-      <ChatModeSelection workspace={workspace} setHasChanges={setHasChanges} />\n-      <ChatModelSelection\n-        settings={settings}\n-        workspace={workspace}\n-        setHasChanges={setHasChanges}\n-      />\n-      <ChatHistorySettings\n-        workspace={workspace}\n-        setHasChanges={setHasChanges}\n-      />\n-      <ChatPromptSettings workspace={workspace} setHasChanges={setHasChanges} />\n-      <ChatTemperatureSettings\n-        settings={settings}\n-        workspace={workspace}\n-        setHasChanges={setHasChanges}\n-      />\n-      {hasChanges && (\n-        <button\n-          type=\"submit\"\n-          className=\"w-fit transition-all duration-300 border border-slate-200 px-5 py-2.5 rounded-lg text-white text-sm items-center flex gap-x-2 hover:bg-slate-200 hover:text-slate-800 focus:ring-gray-800\"\n-        >\n-          {saving ? \"Updating...\" : \"Update workspace\"}\n-        </button>\n-      )}\n-    </form>\n+    <div id=\"workspace-chat-settings-container\">\n+      <form\n+        ref={formEl}\n+        onSubmit={handleUpdate}\n+        id=\"chat-settings-form\"\n+        className=\"w-1/2 flex flex-col gap-y-6\"\n+      >\n+        <WorkspaceLLMSelection\n+          settings={settings}\n+          workspace={workspace}\n+          setHasChanges={setHasChanges}\n+        />\n+        <ChatModeSelection\n+          workspace={workspace}\n+          setHasChanges={setHasChanges}\n+        />\n+        <ChatHistorySettings\n+          workspace={workspace}\n+          setHasChanges={setHasChanges}\n+        />\n+        <ChatPromptSettings\n+          workspace={workspace}\n+          setHasChanges={setHasChanges}\n+        />\n+        <ChatTemperatureSettings\n+          settings={settings}\n+          workspace={workspace}\n+          setHasChanges={setHasChanges}\n+        />\n+        {hasChanges && (\n+          <button\n+            type=\"submit\"\n+            form=\"chat-settings-form\"\n+            className=\"w-fit transition-all duration-300 border border-slate-200 px-5 py-2.5 rounded-lg text-white text-sm items-center flex gap-x-2 hover:bg-slate-200 hover:text-slate-800 focus:ring-gray-800\"\n+          >\n+            {saving ? \"Updating...\" : \"Update workspace\"}\n+          </button>\n+        )}\n+      </form>\n+    </div>\n   );\n }"
        },
        {
          "filename": "server/endpoints/workspaces.js",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -508,7 +508,7 @@ function workspaceEndpoints(app) {\n           if (fs.existsSync(oldPfpPath)) fs.unlinkSync(oldPfpPath);\n         }\n \n-        const { workspace, message } = await Workspace.update(\n+        const { workspace, message } = await Workspace._update(\n           workspaceRecord.id,\n           {\n             pfpFilename: uploadedFileName,\n@@ -547,7 +547,7 @@ function workspaceEndpoints(app) {\n           if (fs.existsSync(oldPfpPath)) fs.unlinkSync(oldPfpPath);\n         }\n \n-        const { workspace, message } = await Workspace.update(\n+        const { workspace, message } = await Workspace._update(\n           workspaceRecord.id,\n           {\n             pfpFilename: null,"
        },
        {
          "filename": "server/models/systemSettings.js",
          "status": "modified",
          "additions": 104,
          "deletions": 92,
          "patch": "@@ -57,103 +57,13 @@ const SystemSettings = {\n       // VectorDB Provider Selection Settings & Configs\n       // --------------------------------------------------------\n       VectorDB: vectorDB,\n-      // Pinecone DB Keys\n-      PineConeKey: !!process.env.PINECONE_API_KEY,\n-      PineConeIndex: process.env.PINECONE_INDEX,\n-\n-      // Chroma DB Keys\n-      ChromaEndpoint: process.env.CHROMA_ENDPOINT,\n-      ChromaApiHeader: process.env.CHROMA_API_HEADER,\n-      ChromaApiKey: !!process.env.CHROMA_API_KEY,\n-\n-      // Weaviate DB Keys\n-      WeaviateEndpoint: process.env.WEAVIATE_ENDPOINT,\n-      WeaviateApiKey: process.env.WEAVIATE_API_KEY,\n-\n-      // QDrant DB Keys\n-      QdrantEndpoint: process.env.QDRANT_ENDPOINT,\n-      QdrantApiKey: process.env.QDRANT_API_KEY,\n-\n-      // Milvus DB Keys\n-      MilvusAddress: process.env.MILVUS_ADDRESS,\n-      MilvusUsername: process.env.MILVUS_USERNAME,\n-      MilvusPassword: !!process.env.MILVUS_PASSWORD,\n-\n-      // Zilliz DB Keys\n-      ZillizEndpoint: process.env.ZILLIZ_ENDPOINT,\n-      ZillizApiToken: process.env.ZILLIZ_API_TOKEN,\n-\n-      // AstraDB Keys\n-      AstraDBApplicationToken: process?.env?.ASTRA_DB_APPLICATION_TOKEN,\n-      AstraDBEndpoint: process?.env?.ASTRA_DB_ENDPOINT,\n+      ...this.vectorDBPreferenceKeys(),\n \n       // --------------------------------------------------------\n       // LLM Provider Selection Settings & Configs\n       // --------------------------------------------------------\n       LLMProvider: llmProvider,\n-      // OpenAI Keys\n-      OpenAiKey: !!process.env.OPEN_AI_KEY,\n-      OpenAiModelPref: process.env.OPEN_MODEL_PREF || \"gpt-3.5-turbo\",\n-\n-      // Azure + OpenAI Keys\n-      AzureOpenAiEndpoint: process.env.AZURE_OPENAI_ENDPOINT,\n-      AzureOpenAiKey: !!process.env.AZURE_OPENAI_KEY,\n-      AzureOpenAiModelPref: process.env.OPEN_MODEL_PREF,\n-      AzureOpenAiEmbeddingModelPref: process.env.EMBEDDING_MODEL_PREF,\n-      AzureOpenAiTokenLimit: process.env.AZURE_OPENAI_TOKEN_LIMIT || 4096,\n-\n-      // Anthropic Keys\n-      AnthropicApiKey: !!process.env.ANTHROPIC_API_KEY,\n-      AnthropicModelPref: process.env.ANTHROPIC_MODEL_PREF || \"claude-2\",\n-\n-      // Gemini Keys\n-      GeminiLLMApiKey: !!process.env.GEMINI_API_KEY,\n-      GeminiLLMModelPref: process.env.GEMINI_LLM_MODEL_PREF || \"gemini-pro\",\n-\n-      // LMStudio Keys\n-      LMStudioBasePath: process.env.LMSTUDIO_BASE_PATH,\n-      LMStudioTokenLimit: process.env.LMSTUDIO_MODEL_TOKEN_LIMIT,\n-      LMStudioModelPref: process.env.LMSTUDIO_MODEL_PREF,\n-\n-      // LocalAI Keys\n-      LocalAiApiKey: !!process.env.LOCAL_AI_API_KEY,\n-      LocalAiBasePath: process.env.LOCAL_AI_BASE_PATH,\n-      LocalAiModelPref: process.env.LOCAL_AI_MODEL_PREF,\n-      LocalAiTokenLimit: process.env.LOCAL_AI_MODEL_TOKEN_LIMIT,\n-\n-      // Ollama LLM Keys\n-      OllamaLLMBasePath: process.env.OLLAMA_BASE_PATH,\n-      OllamaLLMModelPref: process.env.OLLAMA_MODEL_PREF,\n-      OllamaLLMTokenLimit: process.env.OLLAMA_MODEL_TOKEN_LIMIT,\n-\n-      // TogetherAI Keys\n-      TogetherAiApiKey: !!process.env.TOGETHER_AI_API_KEY,\n-      TogetherAiModelPref: process.env.TOGETHER_AI_MODEL_PREF,\n-\n-      // Perplexity AI Keys\n-      PerplexityApiKey: !!process.env.PERPLEXITY_API_KEY,\n-      PerplexityModelPref: process.env.PERPLEXITY_MODEL_PREF,\n-\n-      // OpenRouter Keys\n-      OpenRouterApiKey: !!process.env.OPENROUTER_API_KEY,\n-      OpenRouterModelPref: process.env.OPENROUTER_MODEL_PREF,\n-\n-      // Mistral AI (API) Keys\n-      MistralApiKey: !!process.env.MISTRAL_API_KEY,\n-      MistralModelPref: process.env.MISTRAL_MODEL_PREF,\n-\n-      // Groq AI API Keys\n-      GroqApiKey: !!process.env.GROQ_API_KEY,\n-      GroqModelPref: process.env.GROQ_MODEL_PREF,\n-\n-      // Native LLM Keys\n-      NativeLLMModelPref: process.env.NATIVE_LLM_MODEL_PREF,\n-      NativeLLMTokenLimit: process.env.NATIVE_LLM_MODEL_TOKEN_LIMIT,\n-\n-      // HuggingFace Dedicated Inference\n-      HuggingFaceLLMEndpoint: process.env.HUGGING_FACE_LLM_ENDPOINT,\n-      HuggingFaceLLMAccessToken: !!process.env.HUGGING_FACE_LLM_API_KEY,\n-      HuggingFaceLLMTokenLimit: process.env.HUGGING_FACE_LLM_TOKEN_LIMIT,\n+      ...this.llmPreferenceKeys(),\n \n       // --------------------------------------------------------\n       // Whisper (Audio transcription) Selection Settings & Configs\n@@ -273,6 +183,108 @@ const SystemSettings = {\n       return false;\n     }\n   },\n+\n+  vectorDBPreferenceKeys: function () {\n+    return {\n+      // Pinecone DB Keys\n+      PineConeKey: !!process.env.PINECONE_API_KEY,\n+      PineConeIndex: process.env.PINECONE_INDEX,\n+\n+      // Chroma DB Keys\n+      ChromaEndpoint: process.env.CHROMA_ENDPOINT,\n+      ChromaApiHeader: process.env.CHROMA_API_HEADER,\n+      ChromaApiKey: !!process.env.CHROMA_API_KEY,\n+\n+      // Weaviate DB Keys\n+      WeaviateEndpoint: process.env.WEAVIATE_ENDPOINT,\n+      WeaviateApiKey: process.env.WEAVIATE_API_KEY,\n+\n+      // QDrant DB Keys\n+      QdrantEndpoint: process.env.QDRANT_ENDPOINT,\n+      QdrantApiKey: process.env.QDRANT_API_KEY,\n+\n+      // Milvus DB Keys\n+      MilvusAddress: process.env.MILVUS_ADDRESS,\n+      MilvusUsername: process.env.MILVUS_USERNAME,\n+      MilvusPassword: !!process.env.MILVUS_PASSWORD,\n+\n+      // Zilliz DB Keys\n+      ZillizEndpoint: process.env.ZILLIZ_ENDPOINT,\n+      ZillizApiToken: process.env.ZILLIZ_API_TOKEN,\n+\n+      // AstraDB Keys\n+      AstraDBApplicationToken: process?.env?.ASTRA_DB_APPLICATION_TOKEN,\n+      AstraDBEndpoint: process?.env?.ASTRA_DB_ENDPOINT,\n+    };\n+  },\n+\n+  llmPreferenceKeys: function () {\n+    return {\n+      // OpenAI Keys\n+      OpenAiKey: !!process.env.OPEN_AI_KEY,\n+      OpenAiModelPref: process.env.OPEN_MODEL_PREF || \"gpt-3.5-turbo\",\n+\n+      // Azure + OpenAI Keys\n+      AzureOpenAiEndpoint: process.env.AZURE_OPENAI_ENDPOINT,\n+      AzureOpenAiKey: !!process.env.AZURE_OPENAI_KEY,\n+      AzureOpenAiModelPref: process.env.OPEN_MODEL_PREF,\n+      AzureOpenAiEmbeddingModelPref: process.env.EMBEDDING_MODEL_PREF,\n+      AzureOpenAiTokenLimit: process.env.AZURE_OPENAI_TOKEN_LIMIT || 4096,\n+\n+      // Anthropic Keys\n+      AnthropicApiKey: !!process.env.ANTHROPIC_API_KEY,\n+      AnthropicModelPref: process.env.ANTHROPIC_MODEL_PREF || \"claude-2\",\n+\n+      // Gemini Keys\n+      GeminiLLMApiKey: !!process.env.GEMINI_API_KEY,\n+      GeminiLLMModelPref: process.env.GEMINI_LLM_MODEL_PREF || \"gemini-pro\",\n+\n+      // LMStudio Keys\n+      LMStudioBasePath: process.env.LMSTUDIO_BASE_PATH,\n+      LMStudioTokenLimit: process.env.LMSTUDIO_MODEL_TOKEN_LIMIT,\n+      LMStudioModelPref: process.env.LMSTUDIO_MODEL_PREF,\n+\n+      // LocalAI Keys\n+      LocalAiApiKey: !!process.env.LOCAL_AI_API_KEY,\n+      LocalAiBasePath: process.env.LOCAL_AI_BASE_PATH,\n+      LocalAiModelPref: process.env.LOCAL_AI_MODEL_PREF,\n+      LocalAiTokenLimit: process.env.LOCAL_AI_MODEL_TOKEN_LIMIT,\n+\n+      // Ollama LLM Keys\n+      OllamaLLMBasePath: process.env.OLLAMA_BASE_PATH,\n+      OllamaLLMModelPref: process.env.OLLAMA_MODEL_PREF,\n+      OllamaLLMTokenLimit: process.env.OLLAMA_MODEL_TOKEN_LIMIT,\n+\n+      // TogetherAI Keys\n+      TogetherAiApiKey: !!process.env.TOGETHER_AI_API_KEY,\n+      TogetherAiModelPref: process.env.TOGETHER_AI_MODEL_PREF,\n+\n+      // Perplexity AI Keys\n+      PerplexityApiKey: !!process.env.PERPLEXITY_API_KEY,\n+      PerplexityModelPref: process.env.PERPLEXITY_MODEL_PREF,\n+\n+      // OpenRouter Keys\n+      OpenRouterApiKey: !!process.env.OPENROUTER_API_KEY,\n+      OpenRouterModelPref: process.env.OPENROUTER_MODEL_PREF,\n+\n+      // Mistral AI (API) Keys\n+      MistralApiKey: !!process.env.MISTRAL_API_KEY,\n+      MistralModelPref: process.env.MISTRAL_MODEL_PREF,\n+\n+      // Groq AI API Keys\n+      GroqApiKey: !!process.env.GROQ_API_KEY,\n+      GroqModelPref: process.env.GROQ_MODEL_PREF,\n+\n+      // Native LLM Keys\n+      NativeLLMModelPref: process.env.NATIVE_LLM_MODEL_PREF,\n+      NativeLLMTokenLimit: process.env.NATIVE_LLM_MODEL_TOKEN_LIMIT,\n+\n+      // HuggingFace Dedicated Inference\n+      HuggingFaceLLMEndpoint: process.env.HUGGING_FACE_LLM_ENDPOINT,\n+      HuggingFaceLLMAccessToken: !!process.env.HUGGING_FACE_LLM_API_KEY,\n+      HuggingFaceLLMTokenLimit: process.env.HUGGING_FACE_LLM_TOKEN_LIMIT,\n+    };\n+  },\n };\n \n module.exports.SystemSettings = SystemSettings;"
        },
        {
          "filename": "server/models/workspace.js",
          "status": "modified",
          "additions": 54,
          "deletions": 37,
          "patch": "@@ -19,6 +19,7 @@ const Workspace = {\n     \"lastUpdatedAt\",\n     \"openAiPrompt\",\n     \"similarityThreshold\",\n+    \"chatProvider\",\n     \"chatModel\",\n     \"topN\",\n     \"chatMode\",\n@@ -52,19 +53,42 @@ const Workspace = {\n     }\n   },\n \n-  update: async function (id = null, data = {}) {\n+  update: async function (id = null, updates = {}) {\n     if (!id) throw new Error(\"No workspace id provided for update\");\n \n-    const validKeys = Object.keys(data).filter((key) =>\n+    const validFields = Object.keys(updates).filter((key) =>\n       this.writable.includes(key)\n     );\n-    if (validKeys.length === 0)\n+\n+    Object.entries(updates).forEach(([key]) => {\n+      if (validFields.includes(key)) return;\n+      delete updates[key];\n+    });\n+\n+    if (Object.keys(updates).length === 0)\n       return { workspace: { id }, message: \"No valid fields to update!\" };\n \n+    // If the user unset the chatProvider we will need\n+    // to then clear the chatModel as well to prevent confusion during\n+    // LLM loading.\n+    if (updates?.chatProvider === \"default\") {\n+      updates.chatProvider = null;\n+      updates.chatModel = null;\n+    }\n+\n+    return this._update(id, updates);\n+  },\n+\n+  // Explicit update of settings + key validations.\n+  // Only use this method when directly setting a key value\n+  // that takes no user input for the keys being modified.\n+  _update: async function (id = null, data = {}) {\n+    if (!id) throw new Error(\"No workspace id provided for update\");\n+\n     try {\n       const workspace = await prisma.workspaces.update({\n         where: { id },\n-        data, // TODO: strict validation on writables here.\n+        data,\n       });\n       return { workspace, message: null };\n     } catch (error) {\n@@ -229,47 +253,40 @@ const Workspace = {\n     }\n   },\n \n-  resetWorkspaceChatModels: async () => {\n-    try {\n-      await prisma.workspaces.updateMany({\n-        data: {\n-          chatModel: null,\n-        },\n-      });\n-      return { success: true, error: null };\n-    } catch (error) {\n-      console.error(\"Error resetting workspace chat models:\", error.message);\n-      return { success: false, error: error.message };\n-    }\n-  },\n-\n   trackChange: async function (prevData, newData, user) {\n     try {\n-      const { Telemetry } = require(\"./telemetry\");\n-      const { EventLogs } = require(\"./eventLogs\");\n-      if (\n-        !newData?.openAiPrompt ||\n-        newData?.openAiPrompt === this.defaultPrompt ||\n-        newData?.openAiPrompt === prevData?.openAiPrompt\n-      )\n-        return;\n-\n-      await Telemetry.sendTelemetry(\"workspace_prompt_changed\");\n-      await EventLogs.logEvent(\n-        \"workspace_prompt_changed\",\n-        {\n-          workspaceName: prevData?.name,\n-          prevSystemPrompt: prevData?.openAiPrompt || this.defaultPrompt,\n-          newSystemPrompt: newData?.openAiPrompt,\n-        },\n-        user?.id\n-      );\n+      await this._trackWorkspacePromptChange(prevData, newData, user);\n       return;\n     } catch (error) {\n       console.error(\"Error tracking workspace change:\", error.message);\n       return;\n     }\n   },\n+\n+  // We are only tracking this change to determine the need to a prompt library or\n+  // prompt assistant feature. If this is something you would like to see - tell us on GitHub!\n+  _trackWorkspacePromptChange: async function (prevData, newData, user) {\n+    const { Telemetry } = require(\"./telemetry\");\n+    const { EventLogs } = require(\"./eventLogs\");\n+    if (\n+      !newData?.openAiPrompt ||\n+      newData?.openAiPrompt === this.defaultPrompt ||\n+      newData?.openAiPrompt === prevData?.openAiPrompt\n+    )\n+      return;\n+\n+    await Telemetry.sendTelemetry(\"workspace_prompt_changed\");\n+    await EventLogs.logEvent(\n+      \"workspace_prompt_changed\",\n+      {\n+        workspaceName: prevData?.name,\n+        prevSystemPrompt: prevData?.openAiPrompt || this.defaultPrompt,\n+        newSystemPrompt: newData?.openAiPrompt,\n+      },\n+      user?.id\n+    );\n+    return;\n+  },\n };\n \n module.exports = { Workspace };"
        },
        {
          "filename": "server/prisma/migrations/20240405015034_init/migration.sql",
          "status": "added",
          "additions": 2,
          "deletions": 0,
          "patch": "@@ -0,0 +1,2 @@\n+-- AlterTable\n+ALTER TABLE \"workspaces\" ADD COLUMN \"chatProvider\" TEXT;"
        },
        {
          "filename": "server/prisma/schema.prisma",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -98,6 +98,7 @@ model workspaces {\n   lastUpdatedAt                DateTime                       @default(now())\n   openAiPrompt                 String?\n   similarityThreshold          Float?                         @default(0.25)\n+  chatProvider                 String?\n   chatModel                    String?\n   topN                         Int?                           @default(4)\n   chatMode                     String?                        @default(\"chat\")"
        },
        {
          "filename": "server/utils/chats/embed.js",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -28,7 +28,9 @@ async function streamChatWithForEmbed(\n     embed.workspace.openAiTemp = parseFloat(temperatureOverride);\n \n   const uuid = uuidv4();\n-  const LLMConnector = getLLMProvider(chatModel ?? embed.workspace?.chatModel);\n+  const LLMConnector = getLLMProvider({\n+    model: chatModel ?? embed.workspace?.chatModel,\n+  });\n   const VectorDb = getVectorDbClass();\n   const { safe, reasons = [] } = await LLMConnector.isSafe(message);\n   if (!safe) {"
        },
        {
          "filename": "server/utils/chats/index.js",
          "status": "modified",
          "additions": 4,
          "deletions": 1,
          "patch": "@@ -37,7 +37,10 @@ async function chatWithWorkspace(\n     return await VALID_COMMANDS[command](workspace, message, uuid, user);\n   }\n \n-  const LLMConnector = getLLMProvider(workspace?.chatModel);\n+  const LLMConnector = getLLMProvider({\n+    provider: workspace?.chatProvider,\n+    model: workspace?.chatModel,\n+  });\n   const VectorDb = getVectorDbClass();\n   const { safe, reasons = [] } = await LLMConnector.isSafe(message);\n   if (!safe) {"
        },
        {
          "filename": "server/utils/chats/stream.js",
          "status": "modified",
          "additions": 4,
          "deletions": 1,
          "patch": "@@ -35,7 +35,10 @@ async function streamChatWithWorkspace(\n     return;\n   }\n \n-  const LLMConnector = getLLMProvider(workspace?.chatModel);\n+  const LLMConnector = getLLMProvider({\n+    provider: workspace?.chatProvider,\n+    model: workspace?.chatModel,\n+  });\n   const VectorDb = getVectorDbClass();\n   const { safe, reasons = [] } = await LLMConnector.isSafe(message);\n   if (!safe) {"
        },
        {
          "filename": "server/utils/helpers/index.js",
          "status": "modified",
          "additions": 18,
          "deletions": 17,
          "patch": "@@ -30,52 +30,53 @@ function getVectorDbClass() {\n   }\n }\n \n-function getLLMProvider(modelPreference = null) {\n-  const vectorSelection = process.env.LLM_PROVIDER || \"openai\";\n+function getLLMProvider({ provider = null, model = null } = {}) {\n+  const LLMSelection = provider ?? process.env.LLM_PROVIDER ?? \"openai\";\n   const embedder = getEmbeddingEngineSelection();\n-  switch (vectorSelection) {\n+\n+  switch (LLMSelection) {\n     case \"openai\":\n       const { OpenAiLLM } = require(\"../AiProviders/openAi\");\n-      return new OpenAiLLM(embedder, modelPreference);\n+      return new OpenAiLLM(embedder, model);\n     case \"azure\":\n       const { AzureOpenAiLLM } = require(\"../AiProviders/azureOpenAi\");\n-      return new AzureOpenAiLLM(embedder, modelPreference);\n+      return new AzureOpenAiLLM(embedder, model);\n     case \"anthropic\":\n       const { AnthropicLLM } = require(\"../AiProviders/anthropic\");\n-      return new AnthropicLLM(embedder, modelPreference);\n+      return new AnthropicLLM(embedder, model);\n     case \"gemini\":\n       const { GeminiLLM } = require(\"../AiProviders/gemini\");\n-      return new GeminiLLM(embedder, modelPreference);\n+      return new GeminiLLM(embedder, model);\n     case \"lmstudio\":\n       const { LMStudioLLM } = require(\"../AiProviders/lmStudio\");\n-      return new LMStudioLLM(embedder, modelPreference);\n+      return new LMStudioLLM(embedder, model);\n     case \"localai\":\n       const { LocalAiLLM } = require(\"../AiProviders/localAi\");\n-      return new LocalAiLLM(embedder, modelPreference);\n+      return new LocalAiLLM(embedder, model);\n     case \"ollama\":\n       const { OllamaAILLM } = require(\"../AiProviders/ollama\");\n-      return new OllamaAILLM(embedder, modelPreference);\n+      return new OllamaAILLM(embedder, model);\n     case \"togetherai\":\n       const { TogetherAiLLM } = require(\"../AiProviders/togetherAi\");\n-      return new TogetherAiLLM(embedder, modelPreference);\n+      return new TogetherAiLLM(embedder, model);\n     case \"perplexity\":\n       const { PerplexityLLM } = require(\"../AiProviders/perplexity\");\n-      return new PerplexityLLM(embedder, modelPreference);\n+      return new PerplexityLLM(embedder, model);\n     case \"openrouter\":\n       const { OpenRouterLLM } = require(\"../AiProviders/openRouter\");\n-      return new OpenRouterLLM(embedder, modelPreference);\n+      return new OpenRouterLLM(embedder, model);\n     case \"mistral\":\n       const { MistralLLM } = require(\"../AiProviders/mistral\");\n-      return new MistralLLM(embedder, modelPreference);\n+      return new MistralLLM(embedder, model);\n     case \"native\":\n       const { NativeLLM } = require(\"../AiProviders/native\");\n-      return new NativeLLM(embedder, modelPreference);\n+      return new NativeLLM(embedder, model);\n     case \"huggingface\":\n       const { HuggingFaceLLM } = require(\"../AiProviders/huggingface\");\n-      return new HuggingFaceLLM(embedder, modelPreference);\n+      return new HuggingFaceLLM(embedder, model);\n     case \"groq\":\n       const { GroqLLM } = require(\"../AiProviders/groq\");\n-      return new GroqLLM(embedder, modelPreference);\n+      return new GroqLLM(embedder, model);\n     default:\n       throw new Error(\"ENV: No LLM_PROVIDER value found in environment!\");\n   }"
        },
        {
          "filename": "server/utils/helpers/updateENV.js",
          "status": "modified",
          "additions": 0,
          "deletions": 10,
          "patch": "@@ -2,7 +2,6 @@ const KEY_MAPPING = {\n   LLMProvider: {\n     envKey: \"LLM_PROVIDER\",\n     checks: [isNotEmpty, supportedLLM],\n-    postUpdate: [wipeWorkspaceModelPreference],\n   },\n   // OpenAI Settings\n   OpenAiKey: {\n@@ -493,15 +492,6 @@ function validHuggingFaceEndpoint(input = \"\") {\n     : null;\n }\n \n-// If the LLMProvider has changed we need to reset all workspace model preferences to\n-// null since the provider<>model name combination will be invalid for whatever the new\n-// provider is.\n-async function wipeWorkspaceModelPreference(key, prev, next) {\n-  if (prev === next) return;\n-  const { Workspace } = require(\"../../models/workspace\");\n-  await Workspace.resetWorkspaceChatModels();\n-}\n-\n // This will force update .env variables which for any which reason were not able to be parsed or\n // read from an ENV file as this seems to be a complicating step for many so allowing people to write\n // to the process will at least alleviate that issue. It does not perform comprehensive validity checks or sanity checks"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 6,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 23,
        "max_directory_depth": 7
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "21af81085aeb049750942ac5f3b84775cb461693",
            "date": "2025-01-13T21:12:03Z",
            "author_login": "timothycarambat"
          },
          {
            "sha": "665e8e5bfe431ad93bed6736d0b450592617d042",
            "date": "2025-01-09T23:39:56Z",
            "author_login": "shatfield4"
          },
          {
            "sha": "865f7eea296e544b2eb1ab8c1f322208eaf5eb05",
            "date": "2025-01-09T21:32:54Z",
            "author_login": "timothycarambat"
          },
          {
            "sha": "be886f7d61296a30d5b8a095ca8329f58a0c5a0a",
            "date": "2025-01-09T01:21:30Z",
            "author_login": "root-reindeer-flotilla"
          },
          {
            "sha": "487db896c1ce1442e02f7098ad6974ee60b76073",
            "date": "2025-01-07T23:53:34Z",
            "author_login": "timothycarambat"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": null,
    "cvss_vector": null,
    "cwe_id": "CWE-20",
    "description": "In mintplex-labs/anything-llm, a vulnerability exists due to improper input validation in the workspace update process. Specifically, the application fails to validate or format JSON data sent in an HTTP POST request to `/api/workspace/:workspace-slug/update`, allowing it to be executed as part of a database query without restrictions. This flaw enables users with a manager role to craft a request that includes nested write operations, effectively allowing them to create new Administrator accounts.",
    "attack_vector": null,
    "attack_complexity": null
  },
  "temporal_data": {
    "published_date": "2024-05-20T13:15:23.980",
    "last_modified": "2024-11-21T09:42:32.727",
    "fix_date": "2024-04-05T17:58:36Z"
  },
  "references": [
    {
      "url": "https://github.com/mintplex-labs/anything-llm/commit/94b58249a37a21b1c08deaa2d1edfdecbb6deb18",
      "source": "security@huntr.dev",
      "tags": []
    },
    {
      "url": "https://huntr.com/bounties/34491fb7-5133-4e80-8782-74124350bbdb",
      "source": "security@huntr.dev",
      "tags": []
    },
    {
      "url": "https://github.com/mintplex-labs/anything-llm/commit/94b58249a37a21b1c08deaa2d1edfdecbb6deb18",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    },
    {
      "url": "https://huntr.com/bounties/34491fb7-5133-4e80-8782-74124350bbdb",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:24.265294",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "anything-llm",
    "owner": "mintplex-labs",
    "created_at": "2023-06-04T02:29:14Z",
    "updated_at": "2025-01-14T13:49:57Z",
    "pushed_at": "2025-01-13T21:12:06Z",
    "size": 42916,
    "stars": 30237,
    "forks": 3030,
    "open_issues": 206,
    "watchers": 30237,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [],
    "languages": {
      "JavaScript": 3056909,
      "CSS": 73785,
      "Dockerfile": 9030,
      "HTML": 3904,
      "Shell": 1382,
      "HCL": 1211
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "mit"
    },
    "collected_at": "2025-01-14T14:04:33.088245"
  }
}