{
  "cve_id": "CVE-2024-51737",
  "github_data": {
    "repository": "RediSearch/RediSearch",
    "fix_commit": "13a2936d921dbe5a2e3c72653e0bd7b26af3a6cb",
    "related_commits": [
      "13a2936d921dbe5a2e3c72653e0bd7b26af3a6cb"
    ],
    "patch_url": "https://github.com/RediSearch/RediSearch/commit/13a2936d921dbe5a2e3c72653e0bd7b26af3a6cb.patch",
    "fix_commit_details": {
      "sha": "13a2936d921dbe5a2e3c72653e0bd7b26af3a6cb",
      "commit_date": "2025-01-06T12:08:38Z",
      "author": {
        "login": "kei-nan",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "MOD-8486: Sanitize User Input (#5454)",
        "length": 55,
        "has_description": true,
        "references_issue": true
      },
      "stats": {
        "total": 121,
        "additions": 79,
        "deletions": 42
      },
      "files": [
        {
          "filename": "src/aggregate/aggregate_exec.c",
          "status": "modified",
          "additions": 5,
          "deletions": 5,
          "patch": "@@ -378,11 +378,11 @@ static int populateReplyWithResults(RedisModule_Reply *reply,\n long calc_results_len(AREQ *req, size_t limit) {\n   long resultsLen;\n   PLN_ArrangeStep *arng = AGPLN_GetArrangeStep(&req->ap);\n-  size_t reqLimit = arng && arng->isLimited? arng->limit : DEFAULT_LIMIT;\n-  size_t reqOffset = arng && arng->isLimited? arng->offset : 0;\n+  size_t reqLimit = arng && arng->isLimited ? arng->limit : DEFAULT_LIMIT;\n+  size_t reqOffset = arng && arng->isLimited ? arng->offset : 0;\n   size_t resultFactor = getResultsFactor(req);\n \n-  size_t expected_res = reqLimit + reqOffset <= req->maxSearchResults ? req->qiter.totalResults : MIN(req->maxSearchResults, req->qiter.totalResults);\n+  size_t expected_res = ((reqLimit + reqOffset) <= req->maxSearchResults) ? req->qiter.totalResults : MIN(req->maxSearchResults, req->qiter.totalResults);\n   size_t reqResults = expected_res > reqOffset ? expected_res - reqOffset : 0;\n \n   return 1 + MIN(limit, MIN(reqLimit, reqResults)) * resultFactor;\n@@ -673,7 +673,7 @@ void sendChunk(AREQ *req, RedisModule_Reply *reply, size_t limit) {\n   };\n \n   // Set the chunk size limit for the query\n-    req->qiter.resultLimit = limit;\n+  req->qiter.resultLimit = limit;\n \n   if (reply->resp3) {\n     sendChunk_Resp3(req, reply, limit, cv);\n@@ -688,7 +688,7 @@ void sendChunk(AREQ *req, RedisModule_Reply *reply, size_t limit) {\n \n void AREQ_Execute(AREQ *req, RedisModuleCtx *ctx) {\n   RedisModule_Reply _reply = RedisModule_NewReply(ctx), *reply = &_reply;\n-  sendChunk(req, reply, -1);\n+  sendChunk(req, reply, UINT64_MAX);\n   RedisModule_EndReply(reply);\n   AREQ_Free(req);\n }"
        },
        {
          "filename": "src/aggregate/aggregate_request.c",
          "status": "modified",
          "additions": 8,
          "deletions": 14,
          "patch": "@@ -1211,22 +1211,16 @@ static ResultProcessor *getArrangeRP(AREQ *req, AGGPlan *pln, const PLN_BaseStep\n     astp = &astp_s;\n   }\n \n-  size_t limit = astp->offset + astp->limit;\n-  if (!limit) {\n-    limit = DEFAULT_LIMIT;\n+  size_t maxResults = astp->offset + astp->limit;\n+  if (!maxResults) {\n+    maxResults = DEFAULT_LIMIT;\n   }\n \n   // TODO: unify if when req holds only maxResults according to the query type.\n   //(SEARCH / AGGREGATE)\n-  if (IsSearch(req) && req->maxSearchResults != UINT64_MAX) {\n-    limit = MIN(limit, req->maxSearchResults);\n-  }\n-\n-  if (!IsSearch(req) && req->maxAggregateResults != UINT64_MAX) {\n-    limit = MIN(limit, req->maxAggregateResults);\n-  }\n+  maxResults = MIN(maxResults, IsSearch(req) ? req->maxSearchResults : req->maxAggregateResults);\n \n-  if (IsCount(req) || !limit) {\n+  if (IsCount(req) || !maxResults) {\n     rp = RPCounter_New();\n     up = pushRP(req, rp, up);\n     return up;\n@@ -1264,12 +1258,12 @@ static ResultProcessor *getArrangeRP(AREQ *req, AGGPlan *pln, const PLN_BaseStep\n         ResultProcessor *rpLoader = RPLoader_New(req, lk, loadKeys, array_len(loadKeys), forceLoad);\n         up = pushRP(req, rpLoader, up);\n       }\n-      rp = RPSorter_NewByFields(limit, sortkeys, nkeys, astp->sortAscMap);\n+      rp = RPSorter_NewByFields(maxResults, sortkeys, nkeys, astp->sortAscMap);\n       up = pushRP(req, rp, up);\n     } else if (IsSearch(req) && (!IsOptimized(req) || HasScorer(req))) {\n       // No sort? then it must be sort by score, which is the default.\n       // In optimize mode, add sorter for queries with a scorer.\n-      rp = RPSorter_NewByScore(limit);\n+      rp = RPSorter_NewByScore(maxResults);\n       up = pushRP(req, rp, up);\n     }\n   }\n@@ -1278,7 +1272,7 @@ static ResultProcessor *getArrangeRP(AREQ *req, AGGPlan *pln, const PLN_BaseStep\n     rp = RPPager_New(astp->offset, astp->limit);\n     up = pushRP(req, rp, up);\n   } else if (IsSearch(req) && IsOptimized(req) && !rp) {\n-    rp = RPPager_New(0, limit);\n+    rp = RPPager_New(0, maxResults);\n     up = pushRP(req, rp, up);\n   }\n "
        },
        {
          "filename": "src/buffer.c",
          "status": "modified",
          "additions": 3,
          "deletions": 0,
          "patch": "@@ -6,6 +6,7 @@\n \n #include \"buffer.h\"\n #include \"rmalloc.h\"\n+#include \"rmutil/rm_assert.h\"\n #include <sys/param.h>\n \n size_t Buffer_Grow(Buffer *buf, size_t extraLen) {\n@@ -14,6 +15,8 @@ size_t Buffer_Grow(Buffer *buf, size_t extraLen) {\n     buf->cap += MIN(1 + buf->cap / 5, 1024 * 1024);\n   } while (buf->offset + extraLen > buf->cap);\n \n+  RS_LOG_ASSERT_FMT(extraLen <= UINT32_MAX && buf->cap > originalCap, \"Buffer_Grow: cap is not growing, extraLen: %zu, originalCap: %zu, buf->cap: %zu\", extraLen, originalCap, buf->cap);\n+\n   buf->data = rm_realloc(buf->data, buf->cap);\n   return (buf->cap - originalCap);\n }"
        },
        {
          "filename": "src/config.c",
          "status": "modified",
          "additions": 20,
          "deletions": 16,
          "patch": "@@ -99,7 +99,7 @@ CONFIG_GETTER(getForkGCSleep) {\n CONFIG_SETTER(setMaxDocTableSize) {\n   size_t newsize = 0;\n   int acrc = AC_GetSize(ac, &newsize, AC_F_GE1);\n-  CHECK_RETURN_PARSE_ERROR(acrc);\n+  CHECK_RETURN_PARSE_ERROR(acrc)\n   if (newsize > MAX_DOC_TABLE_SIZE) {\n     QueryError_SetError(status, QUERY_ELIMIT, \"Value exceeds maximum possible document table size\");\n     return REDISMODULE_ERR;\n@@ -115,39 +115,43 @@ CONFIG_GETTER(getMaxDocTableSize) {\n \n // MAXSEARCHRESULTS\n CONFIG_SETTER(setMaxSearchResults) {\n-  long long newsize = 0;\n-  int acrc = AC_GetLongLong(ac, &newsize, 0);\n-  CHECK_RETURN_PARSE_ERROR(acrc);\n-  if (newsize == -1) {\n-    newsize = UINT64_MAX;\n+  long long newSize = 0;\n+  int acrc = AC_GetLongLong(ac, &newSize, 0);\n+  CHECK_RETURN_PARSE_ERROR(acrc)\n+  if (newSize < 0) {\n+    newSize = MAX_SEARCH_REQUEST_RESULTS;\n+  } else {\n+    newSize = MIN(newSize, MAX_SEARCH_REQUEST_RESULTS);\n   }\n-  config->maxSearchResults = newsize;\n+  config->maxSearchResults = newSize;\n   return REDISMODULE_OK;\n }\n \n CONFIG_GETTER(getMaxSearchResults) {\n   sds ss = sdsempty();\n-  if (config->maxSearchResults == UINT64_MAX) {\n+  if (config->maxSearchResults == MAX_SEARCH_REQUEST_RESULTS) {\n     return sdscatprintf(ss, \"unlimited\");\n   }\n   return sdscatprintf(ss, \"%lu\", config->maxSearchResults);\n }\n \n // MAXAGGREGATERESULTS\n CONFIG_SETTER(setMaxAggregateResults) {\n-  long long newsize = 0;\n-  int acrc = AC_GetLongLong(ac, &newsize, 0);\n-  CHECK_RETURN_PARSE_ERROR(acrc);\n-  if (newsize == -1) {\n-    newsize = UINT64_MAX;\n+  long long newSize = 0;\n+  int acrc = AC_GetLongLong(ac, &newSize, 0);\n+  CHECK_RETURN_PARSE_ERROR(acrc)\n+  if (newSize < 0) {\n+    newSize = MAX_AGGREGATE_REQUEST_RESULTS;\n+  } else {\n+    newSize = MIN(newSize, MAX_AGGREGATE_REQUEST_RESULTS);\n   }\n-  config->maxAggregateResults = newsize;\n+  config->maxAggregateResults = newSize;\n   return REDISMODULE_OK;\n }\n \n CONFIG_GETTER(getMaxAggregateResults) {\n   sds ss = sdsempty();\n-  if (config->maxAggregateResults == UINT64_MAX) {\n+  if (config->maxAggregateResults == MAX_AGGREGATE_REQUEST_RESULTS) {\n     return sdscatprintf(ss, \"unlimited\");\n   }\n   return sdscatprintf(ss, \"%lu\", config->maxAggregateResults);\n@@ -1008,7 +1012,7 @@ sds RSConfig_GetInfoString(const RSConfig *config) {\n   ss = sdscatprintf(ss, \"cursor max idle (ms): %lld, \", config->cursorMaxIdle);\n   ss = sdscatprintf(ss, \"max doctable size: %lu, \", config->maxDocTableSize);\n   ss = sdscatprintf(ss, \"max number of search results: \");\n-  ss = (config->maxSearchResults == UINT64_MAX)\n+  ss = (config->maxSearchResults == MAX_SEARCH_REQUEST_RESULTS)\n            ?  // value for MaxSearchResults\n            sdscatprintf(ss, \"unlimited, \")\n            : sdscatprintf(ss, \" %lu, \", config->maxSearchResults);"
        },
        {
          "filename": "src/config.h",
          "status": "modified",
          "additions": 7,
          "deletions": 3,
          "patch": "@@ -220,7 +220,11 @@ void UpgradeDeprecatedMTConfigs();\n #define DEFAULT_MIN_PHONETIC_TERM_LEN 3\n #define DEFAULT_FORK_GC_RUN_INTERVAL 30\n #define DEFAULT_INDEX_CURSOR_LIMIT 128\n-#define SEARCH_REQUEST_RESULTS_MAX 1000000\n+#define MAX_AGGREGATE_REQUEST_RESULTS (1ULL << 31)\n+#define DEFAULT_MAX_AGGREGATE_REQUEST_RESULTS MAX_AGGREGATE_REQUEST_RESULTS\n+#define DEFAULT_MAX_SEARCH_REQUEST_RESULTS 1000000\n+#define MAX_SEARCH_REQUEST_RESULTS (1ULL << 31)\n+#define MAX_KNN_K (1ULL << 58)\n #define NR_MAX_DEPTH_BALANCE 2\n #define VECSIM_DEFAULT_BLOCK_SIZE   1024\n #define DEFAULT_MIN_STEM_LENGTH 4\n@@ -252,8 +256,8 @@ void UpgradeDeprecatedMTConfigs();\n     .gcConfigParams.forkGc.forkGcCleanThreshold = 100,                         \\\n     .noMemPool = 0,                                                            \\\n     .filterCommands = 0,                                                       \\\n-    .maxSearchResults = SEARCH_REQUEST_RESULTS_MAX,                            \\\n-    .maxAggregateResults = -1,                                                 \\\n+    .maxSearchResults = DEFAULT_MAX_SEARCH_REQUEST_RESULTS,                            \\\n+    .maxAggregateResults = DEFAULT_MAX_AGGREGATE_REQUEST_RESULTS,                                                 \\\n     .iteratorsConfigParams.minUnionIterHeap = 20,                              \\\n     .numericCompress = false,                                                  \\\n     .numericTreeMaxDepthRange = 0,                                             \\"
        },
        {
          "filename": "src/coord/dist_aggregate.c",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -758,7 +758,7 @@ void RSExecDistAggregate(RedisModuleCtx *ctx, RedisModuleString **argv, int argc\n       goto err;\n     }\n   } else {\n-    sendChunk(r, reply, -1);\n+    sendChunk(r, reply, UINT64_MAX);\n     AREQ_Free(r);\n   }\n   SpecialCaseCtx_Free(knnCtx);"
        },
        {
          "filename": "src/module.c",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "patch": "@@ -1672,11 +1672,16 @@ specialCaseCtx *prepareOptionalTopKCase(const char *query_string, RedisModuleStr\n         goto cleanup;\n       }\n       Param_DictFree(params);\n+      params = NULL;\n   }\n \n   if (queryNode->type == QN_VECTOR) {\n     QueryVectorNode queryVectorNode = queryNode->vn;\n     size_t k = queryVectorNode.vq->knn.k;\n+    if (k > MAX_KNN_K) {\n+      QueryError_SetErrorFmt(status, QUERY_ELIMIT, VECSIM_KNN_K_TOO_LARGE_ERR_MSG \", max supported K value is %zu\", MAX_KNN_K);\n+      goto cleanup;\n+    }\n     specialCaseCtx *ctx = SpecialCaseCtx_New();\n     ctx->knn.k = k;\n     ctx->knn.fieldName = queryNode->opts.distField ? queryNode->opts.distField : queryVectorNode.vq->scoreField;"
        },
        {
          "filename": "src/varint.c",
          "status": "modified",
          "additions": 3,
          "deletions": 2,
          "patch": "@@ -60,9 +60,10 @@ size_t WriteVarint(uint32_t value, BufferWriter *w) {\n   varintBuf varint;\n   size_t pos = varintEncode(value, varint);\n   size_t nw = VARINT_LEN(pos);\n-  size_t mem_growth = 0;\n \n-  if(!!(mem_growth = Buffer_Reserve(w->buf, nw))) {\n+  size_t mem_growth = 0;\n+  // we assume buffer reserve will not fail\n+  if (!!(mem_growth = Buffer_Reserve(w->buf, nw))) {\n     w->pos = w->buf->data + w->buf->offset;\n   }\n "
        },
        {
          "filename": "src/vector_index.c",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "patch": "@@ -96,6 +96,11 @@ IndexIterator *NewVectorIterator(QueryEvalCtx *q, VectorQuery *vq, IndexIterator\n                                     &qParams, queryType, q->status) != VecSim_OK)  {\n         return NULL;\n       }\n+      if (vq->knn.k > MAX_KNN_K) {\n+        QueryError_SetErrorFmt(q->status, QUERY_EINVAL,\n+                               \"Error parsing vector similarity query: query \" VECSIM_KNN_K_TOO_LARGE_ERR_MSG \", must not exceed %zu\", MAX_KNN_K);\n+        return NULL;\n+      }\n       HybridIteratorParams hParams = {.index = vecsim,\n                                       .dim = dim,\n                                       .elementType = type,"
        },
        {
          "filename": "src/vector_index.h",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "patch": "@@ -41,6 +41,8 @@\n #define VECSIM_ERR_MANDATORY(status,algorithm,arg) \\\n   QERR_MKBADARGS_FMT(status, \"Missing mandatory parameter: cannot create %s index without specifying %s argument\", algorithm, arg)\n \n+#define VECSIM_KNN_K_TOO_LARGE_ERR_MSG \"KNN K parameter is too large\"\n+\n typedef enum {\n   VECSIM_QT_KNN,\n   VECSIM_QT_RANGE"
        },
        {
          "filename": "tests/pytests/test_issues.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -190,8 +190,10 @@ def test_issue1880(env):\n def test_issue1932(env):\n     conn = getConnectionByEnv(env)\n     env.cmd('FT.CREATE', 'idx', 'SCHEMA', 't', 'TEXT')\n-    env.expect('FT.AGGREGATE', 'idx', '*', 'LIMIT', '100000000000000000', '100000000000', 'SORTBY', '1', '@t').error() \\\n+    env.expect('FT.AGGREGATE', 'idx', '*', 'LIMIT', '100000000000000000', '1000000', 'SORTBY', '1', '@t').error() \\\n       .contains('OFFSET exceeds maximum of 1000000')\n+    env.expect('FT.AGGREGATE', 'idx', '*', 'LIMIT', '1000000', '100000000000000000', 'SORTBY', '1', '@t').error() \\\n+      .contains('LIMIT exceeds maximum of 2147483648')\n \n def test_issue1988(env):\n     conn = getConnectionByEnv(env)"
        },
        {
          "filename": "tests/pytests/test_vecsim.py",
          "status": "modified",
          "additions": 17,
          "deletions": 0,
          "patch": "@@ -2394,3 +2394,20 @@ def test_switch_write_mode_multiple_indexes(env):\n     if bg_indexing == 0:\n         prefix = \"::warning title=Bad scenario in test_vecsim:test_switch_write_mode_multiple_indexes::\" if GHA else ''\n         print(f\"{prefix}All vectors were done reindex before switching back to in-place mode\")\n+\n+\n+def test_max_knn_k():\n+    env = Env(moduleArgs='DEFAULT_DIALECT 3')\n+    conn = getConnectionByEnv(env)\n+    dim = 2\n+    k = pow(2, 59)\n+    score_name = 'SCORE'\n+    vec_fieldname = 'VEC'\n+    conn.execute_command('FT.CREATE', 'idx', 'SCHEMA',\n+                         vec_fieldname, 'VECTOR', 'FLAT', '6', 'TYPE', 'FLOAT32', 'DIM', dim, 'DISTANCE_METRIC', 'L2')\n+    for i in range(10):\n+        conn.execute_command(\"HSET\", f'doc{i}', vec_fieldname, create_np_array_typed([i] * dim).tobytes())\n+    env.expect('FT.SEARCH', 'idx', f'*=>[KNN {k} @{vec_fieldname} $BLOB AS {score_name.lower()}]',\n+               'PARAMS', 2, 'BLOB', create_np_array_typed([0] * dim).tobytes(),\n+               'RETURN', '1', score_name).error().contains('KNN K parameter is too large')\n+"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 2,
        "dependency_files": 0,
        "test_files": 2,
        "unique_directories": 4,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "64cde63aeb63b6d6c87a8329b310c98073138b9c",
            "date": "2025-01-14T13:34:22Z",
            "author_login": "nafraf"
          },
          {
            "sha": "41c1e8e9ffdd7820417f01c1a05bb16bfa1723a0",
            "date": "2025-01-14T10:28:19Z",
            "author_login": "GuyAv46"
          },
          {
            "sha": "802d72e1803b7cbba1be0533ad0bf2cd793d8ec5",
            "date": "2025-01-13T19:17:09Z",
            "author_login": "raz-mon"
          },
          {
            "sha": "f1eb3aeff00a411849930808d0398e6fdfdaabbb",
            "date": "2025-01-13T16:39:29Z",
            "author_login": "Itzikvaknin"
          },
          {
            "sha": "decd4feeec4e52f3f1f5885afa56b2224793065b",
            "date": "2025-01-13T12:59:39Z",
            "author_login": "Itzikvaknin"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 7.0,
    "cvss_vector": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H",
    "cwe_id": "CWE-122",
    "description": "RediSearch is a Redis module that provides querying, secondary indexing, and full-text search for Redis. An authenticated redis user executing FT.SEARCH or FT.AGGREGATE with a specially crafted LIMIT command argument, or FT.SEARCH with a specially crafted KNN command argument, can trigger an integer overflow, leading to heap overflow and potential remote code execution. This vulnerability is fixed in 2.6.24, 2.8.21, and 2.10.10. Avoid setting value of -1 or large values for configuration parameters MAXSEARCHRESULTS and MAXAGGREGATERESULTS, to avoid exploiting large LIMIT arguments.",
    "attack_vector": "LOCAL",
    "attack_complexity": "HIGH"
  },
  "temporal_data": {
    "published_date": "2025-01-08T16:15:35.170",
    "last_modified": "2025-01-08T16:15:35.170",
    "fix_date": "2025-01-06T12:08:38Z"
  },
  "references": [
    {
      "url": "https://github.com/RediSearch/RediSearch/commit/13a2936d921dbe5a2e3c72653e0bd7b26af3a6cb",
      "source": "security-advisories@github.com",
      "tags": []
    },
    {
      "url": "https://github.com/RediSearch/RediSearch/security/advisories/GHSA-p2pg-67m3-4c76",
      "source": "security-advisories@github.com",
      "tags": []
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:09:58.226543",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "RediSearch",
    "owner": "RediSearch",
    "created_at": "2016-05-05T13:25:43Z",
    "updated_at": "2025-01-14T14:55:22Z",
    "pushed_at": "2025-01-14T16:19:45Z",
    "size": 44581,
    "stars": 5611,
    "forks": 528,
    "open_issues": 575,
    "watchers": 5611,
    "has_security_policy": false,
    "default_branch": "master",
    "protected_branches": [
      "1.0",
      "1.1",
      "1.2",
      "1.4",
      "1.6",
      "1.8",
      "2.0",
      "2.2",
      "2.4",
      "2.6",
      "2.8",
      "2.10",
      "8.0"
    ],
    "languages": {
      "C": 2983494,
      "Python": 1710651,
      "C++": 425486,
      "Shell": 65639,
      "Yacc": 53845,
      "Ragel": 24069,
      "Makefile": 22435,
      "CMake": 13491
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": false,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "other"
    },
    "collected_at": "2025-01-14T16:25:01.565696"
  }
}