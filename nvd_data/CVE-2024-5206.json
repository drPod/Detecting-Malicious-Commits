{
  "cve_id": "CVE-2024-5206",
  "github_data": {
    "repository": "scikit-learn/scikit-learn",
    "fix_commit": "70ca21f106b603b611da73012c9ade7cd8e438b8",
    "related_commits": [
      "70ca21f106b603b611da73012c9ade7cd8e438b8",
      "70ca21f106b603b611da73012c9ade7cd8e438b8"
    ],
    "patch_url": null,
    "fix_commit_details": {
      "sha": "70ca21f106b603b611da73012c9ade7cd8e438b8",
      "commit_date": "2024-04-22T13:10:46Z",
      "author": {
        "login": "ogrisel",
        "type": "User",
        "stats": {
          "total_commits": 3004,
          "average_weekly_commits": 3.8170266836086406,
          "total_additions": 0,
          "total_deletions": 0,
          "weeks_active": 496
        }
      },
      "commit_message": {
        "title": "FIX remove the computed stop_words_ attribute of text vectorizer (#28823)",
        "length": 73,
        "has_description": false,
        "references_issue": true
      },
      "stats": {
        "total": 96,
        "additions": 20,
        "deletions": 76
      },
      "files": [
        {
          "filename": "doc/whats_new/v1.5.rst",
          "status": "modified",
          "additions": 18,
          "deletions": 0,
          "patch": "@@ -22,6 +22,24 @@ Version 1.5.0\n \n **In Development**\n \n+Security\n+--------\n+\n+- |Fix| :class:`feature_extraction.text.CountVectorizer` and\n+  :class:`feature_extraction.text.TfidfVectorizer` no longer store discarded\n+  tokens from the training set in their `stop_words_` attribute. This attribute\n+  would hold too frequent (above `max_df`) but also too rare tokens (below\n+  `min_df`). This fixes a potential security issue (data leak) if the discarded\n+  rare tokens hold sensitive information from the training set without the\n+  model developer's knowledge.\n+\n+  Note: users of those classes are encouraged to either retrain their pipelines\n+  with the new scikit-learn version or to manually clear the `stop_words_`\n+  attribute from previously trained instances of those transformers. This\n+  attribute was designed only for model inspection purposes and has no impact\n+  on the behavior of the transformers.\n+  :pr:`28823` by :user:`Olivier Grisel <ogrisel>`.\n+\n Changed models\n --------------\n "
        },
        {
          "filename": "sklearn/feature_extraction/tests/test_text.py",
          "status": "modified",
          "additions": 0,
          "deletions": 42,
          "patch": "@@ -756,21 +756,11 @@ def test_feature_names():\n @pytest.mark.parametrize(\"Vectorizer\", (CountVectorizer, TfidfVectorizer))\n def test_vectorizer_max_features(Vectorizer):\n     expected_vocabulary = {\"burger\", \"beer\", \"salad\", \"pizza\"}\n-    expected_stop_words = {\n-        \"celeri\",\n-        \"tomato\",\n-        \"copyright\",\n-        \"coke\",\n-        \"sparkling\",\n-        \"water\",\n-        \"the\",\n-    }\n \n     # test bounded number of extracted features\n     vectorizer = Vectorizer(max_df=0.6, max_features=4)\n     vectorizer.fit(ALL_FOOD_DOCS)\n     assert set(vectorizer.vocabulary_) == expected_vocabulary\n-    assert vectorizer.stop_words_ == expected_stop_words\n \n \n def test_count_vectorizer_max_features():\n@@ -805,21 +795,16 @@ def test_vectorizer_max_df():\n     vect.fit(test_data)\n     assert \"a\" in vect.vocabulary_.keys()\n     assert len(vect.vocabulary_.keys()) == 6\n-    assert len(vect.stop_words_) == 0\n \n     vect.max_df = 0.5  # 0.5 * 3 documents -> max_doc_count == 1.5\n     vect.fit(test_data)\n     assert \"a\" not in vect.vocabulary_.keys()  # {ae} ignored\n     assert len(vect.vocabulary_.keys()) == 4  # {bcdt} remain\n-    assert \"a\" in vect.stop_words_\n-    assert len(vect.stop_words_) == 2\n \n     vect.max_df = 1\n     vect.fit(test_data)\n     assert \"a\" not in vect.vocabulary_.keys()  # {ae} ignored\n     assert len(vect.vocabulary_.keys()) == 4  # {bcdt} remain\n-    assert \"a\" in vect.stop_words_\n-    assert len(vect.stop_words_) == 2\n \n \n def test_vectorizer_min_df():\n@@ -828,21 +813,16 @@ def test_vectorizer_min_df():\n     vect.fit(test_data)\n     assert \"a\" in vect.vocabulary_.keys()\n     assert len(vect.vocabulary_.keys()) == 6\n-    assert len(vect.stop_words_) == 0\n \n     vect.min_df = 2\n     vect.fit(test_data)\n     assert \"c\" not in vect.vocabulary_.keys()  # {bcdt} ignored\n     assert len(vect.vocabulary_.keys()) == 2  # {ae} remain\n-    assert \"c\" in vect.stop_words_\n-    assert len(vect.stop_words_) == 4\n \n     vect.min_df = 0.8  # 0.8 * 3 documents -> min_doc_count == 2.4\n     vect.fit(test_data)\n     assert \"c\" not in vect.vocabulary_.keys()  # {bcdet} ignored\n     assert len(vect.vocabulary_.keys()) == 1  # {a} remains\n-    assert \"c\" in vect.stop_words_\n-    assert len(vect.stop_words_) == 5\n \n \n def test_count_binary_occurrences():\n@@ -1155,28 +1135,6 @@ def test_countvectorizer_vocab_dicts_when_pickling():\n         )\n \n \n-def test_stop_words_removal():\n-    # Ensure that deleting the stop_words_ attribute doesn't affect transform\n-\n-    fitted_vectorizers = (\n-        TfidfVectorizer().fit(JUNK_FOOD_DOCS),\n-        CountVectorizer(preprocessor=strip_tags).fit(JUNK_FOOD_DOCS),\n-        CountVectorizer(strip_accents=strip_eacute).fit(JUNK_FOOD_DOCS),\n-    )\n-\n-    for vect in fitted_vectorizers:\n-        vect_transform = vect.transform(JUNK_FOOD_DOCS).toarray()\n-\n-        vect.stop_words_ = None\n-        stop_None_transform = vect.transform(JUNK_FOOD_DOCS).toarray()\n-\n-        delattr(vect, \"stop_words_\")\n-        stop_del_transform = vect.transform(JUNK_FOOD_DOCS).toarray()\n-\n-        assert_array_equal(stop_None_transform, vect_transform)\n-        assert_array_equal(stop_del_transform, vect_transform)\n-\n-\n def test_pickling_transformer():\n     X = CountVectorizer().fit_transform(JUNK_FOOD_DOCS)\n     orig = TfidfTransformer().fit(X)"
        },
        {
          "filename": "sklearn/feature_extraction/text.py",
          "status": "modified",
          "additions": 2,
          "deletions": 34,
          "patch": "@@ -1079,15 +1079,6 @@ class CountVectorizer(_VectorizerMixin, BaseEstimator):\n         True if a fixed vocabulary of term to indices mapping\n         is provided by the user.\n \n-    stop_words_ : set\n-        Terms that were ignored because they either:\n-\n-          - occurred in too many documents (`max_df`)\n-          - occurred in too few documents (`min_df`)\n-          - were cut off by feature selection (`max_features`).\n-\n-        This is only available if no vocabulary was given.\n-\n     See Also\n     --------\n     HashingVectorizer : Convert a collection of text documents to a\n@@ -1096,12 +1087,6 @@ class CountVectorizer(_VectorizerMixin, BaseEstimator):\n     TfidfVectorizer : Convert a collection of raw documents to a matrix\n         of TF-IDF features.\n \n-    Notes\n-    -----\n-    The ``stop_words_`` attribute can get large and increase the model size\n-    when pickling. This attribute is provided only for introspection and can\n-    be safely removed using delattr or set to None before pickling.\n-\n     Examples\n     --------\n     >>> from sklearn.feature_extraction.text import CountVectorizer\n@@ -1240,19 +1225,17 @@ def _limit_features(self, X, vocabulary, high=None, low=None, limit=None):\n             mask = new_mask\n \n         new_indices = np.cumsum(mask) - 1  # maps old indices to new\n-        removed_terms = set()\n         for term, old_index in list(vocabulary.items()):\n             if mask[old_index]:\n                 vocabulary[term] = new_indices[old_index]\n             else:\n                 del vocabulary[term]\n-                removed_terms.add(term)\n         kept_indices = np.where(mask)[0]\n         if len(kept_indices) == 0:\n             raise ValueError(\n                 \"After pruning, no terms remain. Try a lower min_df or a higher max_df.\"\n             )\n-        return X[:, kept_indices], removed_terms\n+        return X[:, kept_indices]\n \n     def _count_vocab(self, raw_documents, fixed_vocab):\n         \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\"\"\"\n@@ -1397,7 +1380,7 @@ def fit_transform(self, raw_documents, y=None):\n                 raise ValueError(\"max_df corresponds to < documents than min_df\")\n             if max_features is not None:\n                 X = self._sort_features(X, vocabulary)\n-            X, self.stop_words_ = self._limit_features(\n+            X = self._limit_features(\n                 X, vocabulary, max_doc_count, min_doc_count, max_features\n             )\n             if max_features is None:\n@@ -1911,28 +1894,13 @@ class TfidfVectorizer(CountVectorizer):\n         The inverse document frequency (IDF) vector; only defined\n         if ``use_idf`` is True.\n \n-    stop_words_ : set\n-        Terms that were ignored because they either:\n-\n-          - occurred in too many documents (`max_df`)\n-          - occurred in too few documents (`min_df`)\n-          - were cut off by feature selection (`max_features`).\n-\n-        This is only available if no vocabulary was given.\n-\n     See Also\n     --------\n     CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n \n     TfidfTransformer : Performs the TF-IDF transformation from a provided\n         matrix of counts.\n \n-    Notes\n-    -----\n-    The ``stop_words_`` attribute can get large and increase the model size\n-    when pickling. This attribute is provided only for introspection and can\n-    be safely removed using delattr or set to None before pickling.\n-\n     Examples\n     --------\n     >>> from sklearn.feature_extraction.text import TfidfVectorizer"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 1,
        "unique_directories": 3,
        "max_directory_depth": 3
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "e44742ea6c06ee891e92facb886f268f7cfc033b",
            "date": "2025-01-24T17:38:22Z",
            "author_login": "lesteve"
          },
          {
            "sha": "a575b54e78d2602cb7839ce1bda67ff4fe56597a",
            "date": "2025-01-23T15:50:24Z",
            "author_login": "scikit-learn-bot"
          },
          {
            "sha": "1be6ca9e9bce6be7d322549c7f7d7b07e6b9092b",
            "date": "2025-01-23T12:01:22Z",
            "author_login": "virchan"
          },
          {
            "sha": "73db8f116661854cb1b2e43a45401624c1fba2b7",
            "date": "2025-01-23T10:31:46Z",
            "author_login": "jeremiedbb"
          },
          {
            "sha": "34200f0167d73ed9e4fef8ccfe65f828e2e923c7",
            "date": "2025-01-23T10:18:29Z",
            "author_login": "StefanieSenger"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 4.7,
    "cvss_vector": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N",
    "cwe_id": "CWE-921",
    "description": "A sensitive data leakage vulnerability was identified in scikit-learn's TfidfVectorizer, specifically in versions up to and including 1.4.1.post1, which was fixed in version 1.5.0. The vulnerability arises from the unexpected storage of all tokens present in the training data within the `stop_words_` attribute, rather than only storing the subset of tokens required for the TF-IDF technique to function. This behavior leads to the potential leakage of sensitive information, as the `stop_words_` attribute could contain tokens that were meant to be discarded and not stored, such as passwords or keys. The impact of this vulnerability varies based on the nature of the data being processed by the vectorizer.",
    "attack_vector": "LOCAL",
    "attack_complexity": "HIGH"
  },
  "temporal_data": {
    "published_date": "2024-06-06T19:16:06.363",
    "last_modified": "2024-11-21T09:47:11.143",
    "fix_date": "2024-04-22T13:10:46Z"
  },
  "references": [
    {
      "url": "https://github.com/scikit-learn/scikit-learn/commit/70ca21f106b603b611da73012c9ade7cd8e438b8",
      "source": "security@huntr.dev",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://huntr.com/bounties/14bc0917-a85b-4106-a170-d09d5191517c",
      "source": "security@huntr.dev",
      "tags": [
        "Third Party Advisory"
      ]
    },
    {
      "url": "https://github.com/scikit-learn/scikit-learn/commit/70ca21f106b603b611da73012c9ade7cd8e438b8",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://huntr.com/bounties/14bc0917-a85b-4106-a170-d09d5191517c",
      "source": "af854a3a-2127-422b-91ae-364da2661108",
      "tags": [
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:08:26.351209",
    "processing_status": "enhanced"
  },
  "repository_context": {
    "name": "scikit-learn",
    "owner": "scikit-learn",
    "created_at": "2010-08-17T09:43:38Z",
    "updated_at": "2025-01-26T01:47:17Z",
    "pushed_at": "2025-01-24T17:38:22Z",
    "size": 166883,
    "stars": 60869,
    "forks": 25536,
    "open_issues": 2105,
    "watchers": 60869,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [
      "0.6.X",
      "0.7.X",
      "0.8.X",
      "0.9.X",
      "0.10.X",
      "0.11.X",
      "0.12.X",
      "0.13.X",
      "0.14.X",
      "0.15.X",
      "0.16.X",
      "0.17.X",
      "0.18.X",
      "0.19.X",
      "0.20.X",
      "0.21.X",
      "0.22.X",
      "0.23.X",
      "0.24.X",
      "1.0.X",
      "1.1.X",
      "1.2.X",
      "1.3.X",
      "1.4.X",
      "1.5.X",
      "1.6.X"
    ],
    "languages": {
      "Python": 12321475,
      "Cython": 729063,
      "C++": 147429,
      "Shell": 48871,
      "C": 41896,
      "Meson": 32489,
      "CSS": 11277,
      "JavaScript": 1661,
      "Starlark": 1240,
      "Makefile": 1034
    },
    "commit_activity": {
      "total_commits_last_year": 1873,
      "avg_commits_per_week": 36.01923076923077,
      "days_active_last_year": 271
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": true,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "bsd-3-clause"
    },
    "collected_at": "2025-01-26T07:53:22.224717"
  }
}