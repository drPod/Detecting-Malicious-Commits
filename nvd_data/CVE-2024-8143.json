{
  "cve_id": "CVE-2024-8143",
  "github_data": {
    "repository": "gaizhenbiao/chuanhuchatgpt",
    "fix_commit": "ccc7479ace5c9e1a1d9f4daf2e794ffd3865fc2b",
    "related_commits": [
      "ccc7479ace5c9e1a1d9f4daf2e794ffd3865fc2b"
    ],
    "patch_url": "https://github.com/gaizhenbiao/chuanhuchatgpt/commit/ccc7479ace5c9e1a1d9f4daf2e794ffd3865fc2b.patch",
    "fix_commit_details": {
      "sha": "ccc7479ace5c9e1a1d9f4daf2e794ffd3865fc2b",
      "commit_date": "2024-09-19T09:55:42Z",
      "author": {
        "login": "GaiZhenbiao",
        "type": "User",
        "stats": null
      },
      "commit_message": {
        "title": "bugfix: private history download",
        "length": 32,
        "has_description": false,
        "references_issue": true
      },
      "stats": {
        "total": 74,
        "additions": 43,
        "deletions": 31
      },
      "files": [
        {
          "filename": "ChuanhuChatbot.py",
          "status": "modified",
          "additions": 10,
          "deletions": 17,
          "patch": "@@ -87,12 +87,6 @@ def create_new_model():\n                             with gr.Column(min_width=42, scale=1):\n                                 historyDeleteBtn = gr.Button(\n                                     \"\ud83d\uddd1\ufe0f\", elem_id=\"gr-history-delete-btn\")\n-                            with gr.Column(min_width=42, scale=1):\n-                                historyDownloadBtn = gr.Button(\n-                                    \"\u23ec\", elem_id=\"gr-history-download-btn\")\n-                            with gr.Column(min_width=42, scale=1):\n-                                historyMarkdownDownloadBtn = gr.Button(\n-                                    \"\u2935\ufe0f\", elem_id=\"gr-history-mardown-download-btn\")\n                     with gr.Row(visible=False):\n                         with gr.Column(scale=6):\n                             saveFileName = gr.Textbox(\n@@ -126,6 +120,9 @@ def create_new_model():\n                         label=i18n(\"\u9009\u62e9\u6a21\u578b\"), choices=[], multiselect=False, interactive=True, visible=False,\n                         container=False,\n                     )\n+                    # with gr.Column(min_width=150, scale=1, elem_id=\"chatbot-header-btn-bar\"):\n+                    downloadHistoryJSONBtn = gr.DownloadButton(i18n(\"\u5386\u53f2\u8bb0\u5f55\uff08JSON\uff09\"))\n+                    downloadHistoryMarkdownBtn = gr.DownloadButton(i18n(\"\u5bfc\u51fa\u4e3a Markdown\"))\n                     gr.HTML(get_html(\"chatbot_header_btn.html\").format(\n                         json_label=i18n(\"\u5386\u53f2\u8bb0\u5f55\uff08JSON\uff09\"),\n                         md_label=i18n(\"\u5bfc\u51fa\u4e3a Markdown\")\n@@ -519,10 +516,10 @@ def create_greeting(request: gr.Request):\n             loaded_stuff = current_model.auto_load()\n         else:\n             current_model.new_auto_history_filename()\n-            loaded_stuff = [gr.update(), gr.update(), gr.Chatbot(label=MODELS[DEFAULT_MODEL]), current_model.single_turn, current_model.temperature, current_model.top_p, current_model.n_choices, current_model.stop_sequence, current_model.token_upper_limit, current_model.max_generation_token, current_model.presence_penalty, current_model.frequency_penalty, current_model.logit_bias, current_model.user_identifier]\n+            loaded_stuff = [gr.update(), gr.update(), gr.Chatbot(label=MODELS[DEFAULT_MODEL]), current_model.single_turn, current_model.temperature, current_model.top_p, current_model.n_choices, current_model.stop_sequence, current_model.token_upper_limit, current_model.max_generation_token, current_model.presence_penalty, current_model.frequency_penalty, current_model.logit_bias, current_model.user_identifier, gr.DownloadButton(), gr.DownloadButton()]\n         return user_info, user_name, current_model, toggle_like_btn_visibility(DEFAULT_MODEL), *loaded_stuff, init_history_list(user_name, prepend=current_model.history_file_path.rstrip(\".json\"))\n     demo.load(create_greeting, inputs=None, outputs=[\n-              user_info, user_name, current_model, like_dislike_area, saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, historySelectList], api_name=\"load\")\n+              user_info, user_name, current_model, like_dislike_area, saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn, historySelectList], api_name=\"load\")\n     chatgpt_predict_args = dict(\n         fn=predict,\n         inputs=[\n@@ -566,7 +563,7 @@ def create_greeting(request: gr.Request):\n     load_history_from_file_args = dict(\n         fn=load_chat_history,\n         inputs=[current_model, historySelectList],\n-        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox],\n+        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn],\n     )\n \n     refresh_history_args = dict(\n@@ -709,11 +706,7 @@ def create_greeting(request: gr.Request):\n     )\n     historySelectList.select(**load_history_from_file_args)\n     uploadHistoryBtn.upload(upload_chat_history, [current_model, uploadHistoryBtn], [\n-                        saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, historySelectList]).then(**refresh_history_args)\n-    historyDownloadBtn.click(None, [\n-                             user_name, historySelectList], None, js='(a,b)=>{return downloadHistory(a,b,\".json\");}')\n-    historyMarkdownDownloadBtn.click(None, [\n-                                     user_name, historySelectList], None, js='(a,b)=>{return downloadHistory(a,b,\".md\");}')\n+                        saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn, historySelectList]).then(**refresh_history_args)\n     historySearchTextbox.input(\n         filter_history,\n         [user_name, historySearchTextbox],\n@@ -807,7 +800,7 @@ def create_greeting(request: gr.Request):\n     historySelectBtn.click(  # This is an experimental feature... Not actually used.\n         fn=load_chat_history,\n         inputs=[current_model, historySelectList],\n-        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox],\n+        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn],\n         js='(a,b)=>{return bgSelectHistory(a,b);}'\n     )\n # \u9ed8\u8ba4\u5f00\u542f\u672c\u5730\u670d\u52a1\u5668\uff0c\u9ed8\u8ba4\u53ef\u4ee5\u76f4\u63a5\u4eceIP\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u4e0d\u521b\u5efa\u516c\u5f00\u5206\u4eab\u94fe\u63a5\n@@ -817,8 +810,8 @@ def create_greeting(request: gr.Request):\n     reload_javascript()\n     setup_wizard()\n     demo.queue().launch(\n-        allowed_paths=[\"history\", \"web_assets\"],\n-        blocked_paths=[\"config.json\", \"files\", \"models\", \"lora\", \"modules\"],\n+        allowed_paths=[\"web_assets\"],\n+        blocked_paths=[\"config.json\", \"files\", \"models\", \"lora\", \"modules\", \"history\"],\n         server_name=server_name,\n         server_port=server_port,\n         share=share,"
        },
        {
          "filename": "modules/models/base_model.py",
          "status": "modified",
          "additions": 18,
          "deletions": 5,
          "patch": "@@ -17,6 +17,8 @@\n from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n from uuid import UUID\n from langchain_core.outputs import ChatGenerationChunk, GenerationChunk\n+from gradio.utils import get_upload_folder\n+from gradio.processing_utils import save_file_to_cache\n \n import colorama\n import PIL\n@@ -34,6 +36,7 @@\n from ..presets import *\n from ..utils import *\n \n+GRADIO_CACHE = get_upload_folder()\n \n class CallbackToIterator:\n     def __init__(self):\n@@ -1005,13 +1008,14 @@ def upload_chat_history(self, new_history_file_content=None):\n                     with open(new_history_file_path, 'w', encoding='utf-8') as f:\n                         json.dump(json_content, f, ensure_ascii=False, indent=2)\n \n-                    self.history_file_path = new_history_filename\n-                    logging.info(f\"History file uploaded and saved as {new_history_filename}\")\n+                    self.history_file_path = new_history_filename[:-5]\n+                    save_md_file(os.path.join(HISTORY_DIR, self.user_name, new_history_filename))\n+                    logging.info(f\"History file uploaded and saved as {self.history_file_path}\")\n                 except json.JSONDecodeError:\n                     logging.error(\"Uploaded content is not valid JSON. Using default history.\")\n             else:\n                 logging.warning(\"Unexpected type for new_history_file_content. Using default history.\")\n-        return *self.load_chat_history(new_history_file_path), init_history_list(self.user_name)\n+        return *self.load_chat_history(), init_history_list(self.user_name)\n \n     def load_chat_history(self, new_history_file_path=None):\n         logging.debug(f\"{self.user_name} \u52a0\u8f7d\u5bf9\u8bdd\u5386\u53f2\u4e2d\u2026\u2026\")\n@@ -1074,6 +1078,11 @@ def load_chat_history(self, new_history_file_path=None):\n             self.metadata = saved_json.get(\"metadata\", self.metadata)\n             self.stream = saved_json.get(\"stream\", self.stream)\n             self.chatbot = saved_json[\"chatbot\"]\n+\n+            history_json_path = os.path.realpath(os.path.join(HISTORY_DIR, self.user_name, self.history_file_path + \".json\"))\n+            history_md_path = os.path.realpath(os.path.join(HISTORY_DIR, self.user_name, self.history_file_path + \".md\"))\n+            tmp_json_for_download = save_file_to_cache(history_json_path, GRADIO_CACHE)\n+            tmp_md_for_download = save_file_to_cache(history_md_path, GRADIO_CACHE)\n             return (\n                 os.path.basename(self.history_file_path)[:-5],\n                 saved_json[\"system\"],\n@@ -1089,7 +1098,9 @@ def load_chat_history(self, new_history_file_path=None):\n                 self.frequency_penalty,\n                 self.logit_bias,\n                 self.user_identifier,\n-                self.stream\n+                self.stream,\n+                gr.DownloadButton(value=tmp_json_for_download, interactive=True),\n+                gr.DownloadButton(value=tmp_md_for_download, interactive=True),\n             )\n         except:\n             # \u6ca1\u6709\u5bf9\u8bdd\u5386\u53f2\u6216\u8005\u5bf9\u8bdd\u5386\u53f2\u89e3\u6790\u5931\u8d25\n@@ -1110,7 +1121,9 @@ def load_chat_history(self, new_history_file_path=None):\n                 self.frequency_penalty,\n                 self.logit_bias,\n                 self.user_identifier,\n-                self.stream\n+                self.stream,\n+                gr.DownloadButton(value=None, interactive=False),\n+                gr.DownloadButton(value=None, interactive=False),\n             )\n \n     def delete_chat_history(self, filename):"
        },
        {
          "filename": "modules/utils.py",
          "status": "modified",
          "additions": 15,
          "deletions": 9,
          "patch": "@@ -454,17 +454,20 @@ def save_file(filename, model):\n     with open(history_file_path, \"w\", encoding=\"utf-8\") as f:\n         json.dump(json_s, f, ensure_ascii=False, indent=4)\n \n-    filename = os.path.basename(filename)\n-    filename_md = filename[:-5] + \".md\"\n-    md_s = f\"system: \\n- {system} \\n\"\n-    for data in history:\n+    save_md_file(history_file_path)\n+    return history_file_path\n+\n+def save_md_file(json_file_path):\n+    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n+        json_data = json.load(f)\n+\n+    md_file_path = json_file_path[:-5] + \".md\"\n+    md_s = f\"system: \\n- {json_data['system']} \\n\"\n+    for data in json_data['history']:\n         md_s += f\"\\n{data['role']}: \\n- {data['content']} \\n\"\n-    with open(\n-        os.path.join(HISTORY_DIR, user_name, filename_md), \"w\", encoding=\"utf8\"\n-    ) as f:\n-        f.write(md_s)\n-    return os.path.join(HISTORY_DIR, user_name, filename)\n \n+    with open(md_file_path, \"w\", encoding=\"utf8\") as f:\n+        f.write(md_s)\n \n def sorted_by_pinyin(list):\n     return sorted(list, key=lambda char: lazy_pinyin(char)[0][0])\n@@ -1543,3 +1546,6 @@ def setPlaceholder(model_name: str | None = \"\", model: BaseLLMModel | None = Non\n             chatbot_ph_slogan_class = slogan_class,\n             chatbot_ph_question_class = question_class\n         )\n+\n+def download_file(path):\n+    print(path)"
        }
      ],
      "file_patterns": {
        "security_files": 0,
        "config_files": 0,
        "dependency_files": 0,
        "test_files": 0,
        "unique_directories": 3,
        "max_directory_depth": 2
      },
      "context": {
        "surrounding_commits": [
          {
            "sha": "3747cdbb95f9395146f29da7dcdc31c82cc23717",
            "date": "2024-12-12T14:54:00Z",
            "author_login": "GaiZhenbiao"
          },
          {
            "sha": "33869648f2a97759ec48cac475f7bc890b10e981",
            "date": "2024-11-16T06:20:15Z",
            "author_login": "Keldos-Li"
          },
          {
            "sha": "fb97fd65aee852248cfcf0d88f44e81304f58109",
            "date": "2024-11-13T16:17:40Z",
            "author_login": "Keldos-Li"
          },
          {
            "sha": "d1ca9f33c531ceaccd62dd73b819c9246c44813c",
            "date": "2024-11-13T16:08:51Z",
            "author_login": "Keldos-Li"
          },
          {
            "sha": "0752e6ec84434838de4cf00e6a38919a01cf0c33",
            "date": "2024-10-22T07:02:06Z",
            "author_login": "Keldos-Li"
          }
        ]
      }
    }
  },
  "vulnerability_details": {
    "cvss_score": 4.3,
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:L/I:N/A:N",
    "cwe_id": "CWE-1057",
    "description": "In the latest version (20240628) of gaizhenbiao/chuanhuchatgpt, an issue exists in the /file endpoint that allows authenticated users to access the chat history of other users. When a user logs in, a directory is created in the history folder with the user's name. By manipulating the /file endpoint, an authenticated user can enumerate and access files in other users' directories, leading to unauthorized access to private chat histories. This vulnerability can be exploited to read any user's private chat history.",
    "attack_vector": "NETWORK",
    "attack_complexity": "LOW"
  },
  "temporal_data": {
    "published_date": "2024-10-29T13:15:10.750",
    "last_modified": "2024-10-31T16:23:35.827",
    "fix_date": "2024-09-19T09:55:42Z"
  },
  "references": [
    {
      "url": "https://github.com/gaizhenbiao/chuanhuchatgpt/commit/ccc7479ace5c9e1a1d9f4daf2e794ffd3865fc2b",
      "source": "security@huntr.dev",
      "tags": [
        "Patch"
      ]
    },
    {
      "url": "https://huntr.com/bounties/71c5ea4b-524a-4173-8fd4-2fbabd69502e",
      "source": "security@huntr.dev",
      "tags": [
        "Exploit",
        "Third Party Advisory"
      ]
    }
  ],
  "collection_metadata": {
    "collected_at": "2025-01-11T23:09:27.105646",
    "processing_status": "raw"
  },
  "repository_context": {
    "name": "ChuanhuChatGPT",
    "owner": "gaizhenbiao",
    "created_at": "2023-03-02T13:37:13Z",
    "updated_at": "2025-01-14T08:24:11Z",
    "pushed_at": "2024-12-12T15:01:12Z",
    "size": 3223,
    "stars": 15343,
    "forks": 2288,
    "open_issues": 127,
    "watchers": 15343,
    "has_security_policy": false,
    "default_branch": "main",
    "protected_branches": [],
    "languages": {
      "Python": 433757,
      "JavaScript": 92092,
      "CSS": 63899,
      "HTML": 52103,
      "Shell": 1338,
      "Dockerfile": 1275,
      "Batchfile": 464
    },
    "commit_activity": {
      "total_commits_last_year": 0,
      "avg_commits_per_week": 0,
      "days_active_last_year": 0
    },
    "security_features": {
      "has_security_policy": false,
      "has_protected_branches": false,
      "has_wiki": true,
      "has_issues": true,
      "allow_forking": true,
      "is_template": false,
      "license": "gpl-3.0"
    },
    "collected_at": "2025-01-14T13:28:37.886262"
  }
}